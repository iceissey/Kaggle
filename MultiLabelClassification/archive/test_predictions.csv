text,true_labels,pred_labels
"Auto-Encoding Total Correlation Explanation   Advances in unsupervised learning enable reconstruction and generation of
samples from complex distributions, but this success is marred by the
inscrutability of the representations learned. We propose an
information-theoretic approach to characterizing disentanglement and dependence
in representation learning using multivariate mutual information, also called
total correlation. The principle of total Cor-relation Ex-planation (CorEx) has
motivated successful unsupervised learning applications across a variety of
domains, but under some restrictive assumptions. Here we relax those
restrictions by introducing a flexible variational lower bound to CorEx.
Surprisingly, we find that this lower bound is equivalent to the one in
variational autoencoders (VAE) under certain conditions. This
information-theoretic view of VAE deepens our understanding of hierarchical VAE
and motivates a new algorithm, AnchorVAE, that makes latent codes more
interpretable through information maximization and enables generation of richer
and more realistic samples.
",Statistics,Computer Science; Statistics
"Zhu reduction for Jacobi $n$-point functions and applications   We establish precise Zhu reduction formulas for Jacobi $n$-point functions
which show the absence of any possible poles arising in these formulas. We then
exploit this to produce results concerning the structure of strongly regular
vertex operator algebras, and also to motivate new differential operators
acting on Jacobi forms. Finally, we apply the reduction formulas to the Fermion
model in order to create polynomials of quasi-Jacobi forms which are Jacobi
forms.
",Mathematics,Mathematics
"A Large-Scale CNN Ensemble for Medication Safety Analysis   Revealing Adverse Drug Reactions (ADR) is an essential part of post-marketing
drug surveillance, and data from health-related forums and medical communities
can be of a great significance for estimating such effects. In this paper, we
propose an end-to-end CNN-based method for predicting drug safety on user
comments from healthcare discussion forums. We present an architecture that is
based on a vast ensemble of CNNs with varied structural parameters, where the
prediction is determined by the majority vote. To evaluate the performance of
the proposed solution, we present a large-scale dataset collected from a
medical website that consists of over 50 thousand reviews for more than 4000
drugs. The results demonstrate that our model significantly outperforms
conventional approaches and predicts medicine safety with an accuracy of 87.17%
for binary and 62.88% for multi-classification tasks.
",Computer Science,Computer Science
"Concentration of Multilinear Functions of the Ising Model with Applications to Network Data   We prove near-tight concentration of measure for polynomial functions of the
Ising model under high temperature. For any degree $d$, we show that a
degree-$d$ polynomial of a $n$-spin Ising model exhibits exponential tails that
scale as $\exp(-r^{2/d})$ at radius $r=\tilde{\Omega}_d(n^{d/2})$. Our
concentration radius is optimal up to logarithmic factors for constant $d$,
improving known results by polynomial factors in the number of spins. We
demonstrate the efficacy of polynomial functions as statistics for testing the
strength of interactions in social networks in both synthetic and real world
data.
",Computer Science; Mathematics; Statistics,Computer Science; Physics
"Evidence Logics with Relational Evidence   Dynamic evidence logics are logics for reasoning about the evidence and
evidence-based beliefs of agents in a dynamic environment. In this paper, we
introduce a family of logics for reasoning about relational evidence: evidence
that involves an orderings of states in terms of their relative plausibility.
We provide sound and complete axiomatizations for the logics. We also present
several evidential actions and prove soundness and completeness for the
associated dynamic logics.
",Computer Science,Computer Science
"Right Amenability And Growth Of Finitely Right Generated Left Group Sets   We introduce right generating sets, Cayley graphs, growth functions, types
and rates, and isoperimetric constants for left homogeneous spaces equipped
with coordinate systems; characterise right amenable finitely right generated
left homogeneous spaces with finite stabilisers as those whose isoperimetric
constant is $0$; and prove that finitely right generated left homogeneous
spaces with finite stabilisers of sub-exponential growth are right amenable, in
particular, quotient sets of groups of sub-exponential growth by finite
subgroups are right amenable.
",Mathematics,Mathematics
"Performance of Range Separated Hybrids: Study within BECKE88 family and Semilocal Exchange Hole based Range Separated Hybrid   A long range corrected range separated hybrid functional is developed based
on the density matrix expansion (DME) based semilocal exchange hole with
Lee-Yang-Parr (LYP) correlation. An extensive study involving the proposed
range separated hybrid for thermodynamic as well as properties related to the
fractional occupation number is compared with different BECKE88 family
semilocal, hybrid and range separated hybrids. It has been observed that using
Kohn-Sham kinetic energy dependent exchange hole several properties related to
the fractional occupation number can be improved without hindering the
thermochemical accuracy. The newly constructed range separated hybrid
accurately describe the hydrogen and non-hydrogen reaction barrier heights. The
present range separated functional has been constructed using full semilocal
meta-GGA type exchange hole having exact properties related to exchange hole
therefore, it has a strong physical basis.
",Physics,Physics
"Graphons: A Nonparametric Method to Model, Estimate, and Design Algorithms for Massive Networks   Many social and economic systems are naturally represented as networks, from
off-line and on-line social networks, to bipartite networks, like Netflix and
Amazon, between consumers and products. Graphons, developed as limits of
graphs, form a natural, nonparametric method to describe and estimate large
networks like Facebook and LinkedIn. Here we describe the development of the
theory of graphons, for both dense and sparse networks, over the last decade.
We also review theorems showing that we can consistently estimate graphons from
massive networks in a wide variety of models. Finally, we show how to use
graphons to estimate missing links in a sparse network, which has applications
from estimating social and information networks in development economics, to
rigorously and efficiently doing collaborative filtering with applications to
movie recommendations in Netflix and product suggestions in Amazon.
",Computer Science; Physics,Computer Science
"Prospects of detecting HI using redshifted 21 cm radiation at z ~ 3   Distribution of cold gas in the post-reionization era provides an important
link between distribution of galaxies and the process of star formation.
Redshifted 21 cm radiation from the Hyperfine transition of neutral Hydrogen
allows us to probe the neutral component of cold gas, most of which is to be
found in the interstellar medium of galaxies. Existing and upcoming radio
telescopes can probe the large scale distribution of neutral Hydrogen via HI
intensity mapping. In this paper we use an estimate of the HI power spectrum
derived using an ansatz to compute the expected signal from the large scale HI
distribution at z ~ 3. We find that the scale dependence of bias at small
scales makes a significant difference to the expected signal even at large
angular scales. We compare the predicted signal strength with the sensitivity
of radio telescopes that can observe such radiation and calculate the
observation time required for detecting neutral Hydrogen at these redshifts. We
find that OWFA (Ooty Wide Field Array) offers the best possibility to detect
neutral Hydrogen at z ~ 3 before the SKA (Square Kilometer Array) becomes
operational. We find that the OWFA should be able to make a 3 sigma or a more
significant detection in 2000 hours of observations at several angular scales.
Calculations done using the Fisher matrix approach indicate that a 5 sigma
detection of the binned HI power spectrum via measurement of the amplitude of
the HI power spectrum is possible in 1000 hours (Sarkar, Bharadwaj and Ali,
2017).
",Physics,Physics
"A Local-Search Algorithm for Steiner Forest   In the Steiner Forest problem, we are given a graph and a collection of
source-sink pairs, and the goal is to find a subgraph of minimum total length
such that all pairs are connected. The problem is APX-Hard and can be
2-approximated by, e.g., the elegant primal-dual algorithm of Agrawal, Klein,
and Ravi from 1995.
We give a local-search-based constant-factor approximation for the problem.
Local search brings in new techniques to an area that has for long not seen any
improvements and might be a step towards a combinatorial algorithm for the more
general survivable network design problem. Moreover, local search was an
essential tool to tackle the dynamic MST/Steiner Tree problem, whereas dynamic
Steiner Forest is still wide open.
It is easy to see that any constant factor local search algorithm requires
steps that add/drop many edges together. We propose natural local moves which,
at each step, either (a) add a shortest path in the current graph and then drop
a bunch of inessential edges, or (b) add a set of edges to the current
solution. This second type of moves is motivated by the potential function we
use to measure progress, combining the cost of the solution with a penalty for
each connected component. Our carefully-chosen local moves and potential
function work in tandem to eliminate bad local minima that arise when using
more traditional local moves.
",Computer Science,Computer Science
"A unified view of entropy-regularized Markov decision processes   We propose a general framework for entropy-regularized average-reward
reinforcement learning in Markov decision processes (MDPs). Our approach is
based on extending the linear-programming formulation of policy optimization in
MDPs to accommodate convex regularization functions. Our key result is showing
that using the conditional entropy of the joint state-action distributions as
regularization yields a dual optimization problem closely resembling the
Bellman optimality equations. This result enables us to formalize a number of
state-of-the-art entropy-regularized reinforcement learning algorithms as
approximate variants of Mirror Descent or Dual Averaging, and thus to argue
about the convergence properties of these methods. In particular, we show that
the exact version of the TRPO algorithm of Schulman et al. (2015) actually
converges to the optimal policy, while the entropy-regularized policy gradient
methods of Mnih et al. (2016) may fail to converge to a fixed point. Finally,
we illustrate empirically the effects of using various regularization
techniques on learning performance in a simple reinforcement learning setup.
",Computer Science; Statistics,Computer Science; Statistics
"Interpretable Feature Recommendation for Signal Analytics   This paper presents an automated approach for interpretable feature
recommendation for solving signal data analytics problems. The method has been
tested by performing experiments on datasets in the domain of prognostics where
interpretation of features is considered very important. The proposed approach
is based on Wide Learning architecture and provides means for interpretation of
the recommended features. It is to be noted that such an interpretation is not
available with feature learning approaches like Deep Learning (such as
Convolutional Neural Network) or feature transformation approaches like
Principal Component Analysis. Results show that the feature recommendation and
interpretation techniques are quite effective for the problems at hand in terms
of performance and drastic reduction in time to develop a solution. It is
further shown by an example, how this human-in-loop interpretation system can
be used as a prescriptive system.
",Computer Science; Statistics,Computer Science
"Edge states in non-Fermi liquids   We devise an approach to the calculation of scaling dimensions of generic
operators describing scattering within multi-channel Luttinger liquid. The
local impurity scattering in an arbitrary configuration of conducting and
insulating channels is investigated and the problem is reduced to a single
algebraic matrix equation. In particular, the solution to this equation is
found for a finite array of chains described by Luttinger liquid models. It is
found that for a weak inter-chain hybridisation and intra-channel
electron-electron attraction the edge wires are robust against disorder whereas
bulk wires, on contrary, become insulating. Thus, the edge state may exist in a
finite sliding Luttinger liquid without time-reversal symmetry breaking
(quantum Hall systems) or spin-orbit interaction (topological insulators).
",Physics,Physics
"Stable absorbing boundary conditions for molecular dynamics in general domains   A new type of absorbing boundary conditions for molecular dynamics
simulations are presented. The exact boundary conditions for crystalline solids
with harmonic approximation are expressed as a dynamic Dirichlet- to-Neumann
(DtN) map. It connects the displacement of the atoms at the boundary to the
traction on these atoms. The DtN map is valid for a domain with general
geometry. To avoid evaluating the time convo- lution of the dynamic DtN map, we
approximate the associated kernel function by rational functions in the Laplace
domain. The parameters in the approximations are determined by interpolations.
The explicit forms of the zeroth, first, and second order approximations will
be presented. The stability of the molecular dynamics model, supplemented with
these absorbing boundary conditions is established. Two numerical simulations
are performed to demonstrate the effectiveness of the methods.
",Physics,Physics
"A criterion related to the Riemann Hypothesis   A crucial role in the Nyman-Beurling-Báez-Duarte approach to the Riemann
Hypothesis is played by the distance \[
d_N^2:=\inf_{A_N}\frac{1}{2\pi}\int_{-\infty}^\infty\left|1-\zeta
A_N\left(\frac{1}{2}+it\right)\right|^2\frac{dt}{\frac{1}{4}+t^2}\:, \] where
the infimum is over all Dirichlet polynomials
$$A_N(s)=\sum_{n=1}^{N}\frac{a_n}{n^s}$$ of length $N$. In this paper we
investigate $d_N^2$ under the assumption that the Riemann zeta function has
four non-trivial zeros off the critical line. Thus we obtain a criterion for
the non validity of the Riemann Hypothesis.
",Mathematics,Mathematics
"Designing Coalition-Proof Reverse Auctions over Continuous Goods   This paper investigates reverse auctions that involve continuous values of
different types of goods, general nonconvex constraints, and second stage
costs. We seek to design the payment rules and conditions under which
coalitions of participants cannot influence the auction outcome in order to
obtain higher collective utility. Under the incentive-compatible
Vickrey-Clarke-Groves mechanism, we show that coalition-proof outcomes are
achieved if the submitted bids are convex and the constraint sets are of a
polymatroid-type. These conditions, however, do not capture the complexity of
the general class of reverse auctions under consideration. By relaxing the
property of incentive-compatibility, we investigate further payment rules that
are coalition-proof without any extra conditions on the submitted bids and the
constraint sets. Since calculating the payments directly for these mechanisms
is computationally difficult for auctions involving many participants, we
present two computationally efficient methods. Our results are verified with
several case studies based on electricity market data.
",Computer Science,Computer Science
"Uncertainty quantification of coal seam gas production prediction using Polynomial Chaos   A surrogate model approximates a computationally expensive solver. Polynomial
Chaos is a method to construct surrogate models by summing combinations of
carefully chosen polynomials. The polynomials are chosen to respect the
probability distributions of the uncertain input variables (parameters); this
allows for both uncertainty quantification and global sensitivity analysis.
In this paper we apply these techniques to a commercial solver for the
estimation of peak gas rate and cumulative gas extraction from a coal seam gas
well. The polynomial expansion is shown to honour the underlying geophysics
with low error when compared to a much more complex and computationally slower
commercial solver. We make use of advanced numerical integration techniques to
achieve this accuracy using relatively small amounts of training data.
",Mathematics,Statistics
"On noncommutative geometry of the Standard Model: fermion multiplet as internal forms   We unveil the geometric nature of the multiplet of fundamental fermions in
the Standard Model of fundamental particles as a noncommutative analogue of de
Rham forms on the internal finite quantum space.
",Mathematics,Physics
"Verifying Patterns of Dynamic Architectures using Model Checking   Architecture patterns capture architectural design experience and provide
abstract solutions to recurring architectural design problems. They consist of
a description of component types and restrict component connection and
activation. Therefore, they guarantee some desired properties for architectures
employing the pattern. Unfortunately, most documented patterns do not provide a
formal guarantee of whether their specification indeed leads to the desired
guarantee. Failure in doing so, however, might lead to wrong architectures,
i.e., architectures wrongly supposed to show certain desired properties. Since
architectures, in general, have a high impact on the quality of the resulting
system and architectural flaws are only difficult, if not to say impossible, to
repair, this may lead to badly reparable quality issues in the resulting
system. To address this problem, we propose an approach based on model checking
to verify pattern specifications w.r.t. their guarantees. In the following we
apply the approach to three well-known patterns for dynamic architectures: the
Singleton, the Model-View-Controller, and the Broker pattern. Thereby, we
discovered ambiguities and missing constraints for all three specifications.
Thus, we conclude that verifying patterns of dynamic architectures using model
checking is feasible and useful to discover ambiguities and flaws in pattern
specifications.
",Computer Science,Computer Science
"Transition to turbulence when the Tollmien-Schlichting and bypass routes coexist   Plane Poiseuille flow, the pressure driven flow between parallel plates,
shows a route to turbulence connected with a linear instability to
Tollmien-Schlichting (TS) waves, and another one, the bypass transition, that
is triggered with finite amplitude perturbation. We use direct numerical
simulations to explore the arrangement of the different routes to turbulence
among the set of initial conditions. For plates that are a distance $2H$ apart
and in a domain of width $2\pi H$ and length $2\pi H$ the subcritical
instability to TS waves sets in at $Re_{c}=5815$ that extends down to
$Re_{TS}\approx4884$. The bypass route becomes available above $Re_E=459$ with
the appearance of three-dimensional finite-amplitude traveling waves. The
bypass transition covers a large set of finite amplitude perturbations. Below
$Re_c$, TS appear for a tiny set of initial conditions that grows with
increasing Reynolds number. Above $Re_c$ the previously stable region becomes
unstable via TS waves, but a sharp transition to the bypass route can still be
identified. Both routes lead to the same turbulent in the final stage of the
transition, but on different time scales. Similar phenomena can be expected in
other flows where two or more routes to turbulence compete.
",Physics,Physics
"Solving Non-parametric Inverse Problem in Continuous Markov Random Field using Loopy Belief Propagation   In this paper, we address the inverse problem, or the statistical machine
learning problem, in Markov random fields with a non-parametric pair-wise
energy function with continuous variables. The inverse problem is formulated by
maximum likelihood estimation. The exact treatment of maximum likelihood
estimation is intractable because of two problems: (1) it includes the
evaluation of the partition function and (2) it is formulated in the form of
functional optimization. We avoid Problem (1) by using Bethe approximation.
Bethe approximation is an approximation technique equivalent to the loopy
belief propagation. Problem (2) can be solved by using orthonormal function
expansion. Orthonormal function expansion can reduce a functional optimization
problem to a function optimization problem. Our method can provide an analytic
form of the solution of the inverse problem within the framework of Bethe
approximation.
",Computer Science; Physics; Statistics,Computer Science; Statistics
"Extension complexities of Cartesian products involving a pyramid   It is an open question whether the linear extension complexity of the
Cartesian product of two polytopes P, Q is the sum of the extension
complexities of P and Q. We give an affirmative answer to this question for the
case that one of the two polytopes is a pyramid.
",Computer Science; Mathematics,Mathematics
"Towards Neural Co-Processors for the Brain: Combining Decoding and Encoding in Brain-Computer Interfaces   The field of brain-computer interfaces is poised to advance from the
traditional goal of controlling prosthetic devices using brain signals to
combining neural decoding and encoding within a single neuroprosthetic device.
Such a device acts as a ""co-processor"" for the brain, with applications ranging
from inducing Hebbian plasticity for rehabilitation after brain injury to
reanimating paralyzed limbs and enhancing memory. We review recent progress in
simultaneous decoding and encoding for closed-loop control and plasticity
induction. To address the challenge of multi-channel decoding and encoding, we
introduce a unifying framework for developing brain co-processors based on
artificial neural networks and deep learning. These ""neural co-processors"" can
be used to jointly optimize cost functions with the nervous system to achieve
desired behaviors ranging from targeted neuro-rehabilitation to augmentation of
brain function.
",Quantitative Biology,Computer Science
"Learning Graphical Models Using Multiplicative Weights   We give a simple, multiplicative-weight update algorithm for learning
undirected graphical models or Markov random fields (MRFs). The approach is
new, and for the well-studied case of Ising models or Boltzmann machines, we
obtain an algorithm that uses a nearly optimal number of samples and has
quadratic running time (up to logarithmic factors), subsuming and improving on
all prior work. Additionally, we give the first efficient algorithm for
learning Ising models over general alphabets.
Our main application is an algorithm for learning the structure of t-wise
MRFs with nearly-optimal sample complexity (up to polynomial losses in
necessary terms that depend on the weights) and running time that is
$n^{O(t)}$. In addition, given $n^{O(t)}$ samples, we can also learn the
parameters of the model and generate a hypothesis that is close in statistical
distance to the true MRF. All prior work runs in time $n^{\Omega(d)}$ for
graphs of bounded degree d and does not generate a hypothesis close in
statistical distance even for t=3. We observe that our runtime has the correct
dependence on n and t assuming the hardness of learning sparse parities with
noise.
Our algorithm--the Sparsitron-- is easy to implement (has only one parameter)
and holds in the on-line setting. Its analysis applies a regret bound from
Freund and Schapire's classic Hedge algorithm. It also gives the first solution
to the problem of learning sparse Generalized Linear Models (GLMs).
",Computer Science; Mathematics; Statistics,Computer Science; Statistics
"Angpow: a software for the fast computation of accurate tomographic power spectra   The statistical distribution of galaxies is a powerful probe to constrain
cosmological models and gravity. In particular the matter power spectrum $P(k)$
brings information about the cosmological distance evolution and the galaxy
clustering together. However the building of $P(k)$ from galaxy catalogues
needs a cosmological model to convert angles on the sky and redshifts into
distances, which leads to difficulties when comparing data with predicted
$P(k)$ from other cosmological models, and for photometric surveys like LSST.
The angular power spectrum $C_\ell(z_1,z_2)$ between two bins located at
redshift $z_1$ and $z_2$ contains the same information than the matter power
spectrum, is free from any cosmological assumption, but the prediction of
$C_\ell(z_1,z_2)$ from $P(k)$ is a costly computation when performed exactly.
The Angpow software aims at computing quickly and accurately the auto
($z_1=z_2$) and cross ($z_1 \neq z_2$) angular power spectra between redshift
bins. We describe the developed algorithm, based on developments on the
Chebyshev polynomial basis and on the Clenshaw-Curtis quadrature method. We
validate the results with other codes, and benchmark the performance. Angpow is
flexible and can handle any user defined power spectra, transfer functions, and
redshift selection windows. The code is fast enough to be embedded inside
programs exploring large cosmological parameter spaces through the
$C_\ell(z_1,z_2)$ comparison with data. We emphasize that the Limber's
approximation, often used to fasten the computation, gives wrong $C_\ell$
values for cross-correlations.
",Physics,Physics
"Efficient SMC$^2$ schemes for stochastic kinetic models   Fitting stochastic kinetic models represented by Markov jump processes within
the Bayesian paradigm is complicated by the intractability of the observed data
likelihood. There has therefore been considerable attention given to the design
of pseudo-marginal Markov chain Monte Carlo algorithms for such models.
However, these methods are typically computationally intensive, often require
careful tuning and must be restarted from scratch upon receipt of new
observations. Sequential Monte Carlo (SMC) methods on the other hand aim to
efficiently reuse posterior samples at each time point. Despite their appeal,
applying SMC schemes in scenarios with both dynamic states and static
parameters is made difficult by the problem of particle degeneracy. A
principled approach for overcoming this problem is to move each parameter
particle through a Metropolis-Hastings kernel that leaves the target invariant.
This rejuvenation step is key to a recently proposed SMC$^2$ algorithm, which
can be seen as the pseudo-marginal analogue of an idealised scheme known as
iterated batch importance sampling. Computing the parameter weights in SMC$^2$
requires running a particle filter over dynamic states to unbiasedly estimate
the intractable observed data likelihood contributions at each time point. In
this paper, we propose to use an auxiliary particle filter inside the SMC$^2$
scheme. Our method uses two recently proposed constructs for sampling
conditioned jump processes and we find that the resulting inference schemes
typically require fewer state particles than when using a simple bootstrap
filter. Using two applications, we compare the performance of the proposed
approach with various competing methods, including two global MCMC schemes.
",Statistics,Statistics
"Self-injective commutative rings have no nontrivial rigid ideals   We establish a link between trace modules and rigidity in modules over
Noetherian rings. Using the theory of trace ideals we make partial progress on
a question of Dao, and on the Auslander-Reiten conjecture over Artinian
Gorenstein rings.
",Mathematics,Mathematics
"Efficient and principled score estimation with Nyström kernel exponential families   We propose a fast method with statistical guarantees for learning an
exponential family density model where the natural parameter is in a
reproducing kernel Hilbert space, and may be infinite-dimensional. The model is
learned by fitting the derivative of the log density, the score, thus avoiding
the need to compute a normalization constant. Our approach improves the
computational efficiency of an earlier solution by using a low-rank,
Nyström-like solution. The new solution retains the consistency and
convergence rates of the full-rank solution (exactly in Fisher distance, and
nearly in other distances), with guarantees on the degree of cost and storage
reduction. We evaluate the method in experiments on density estimation and in
the construction of an adaptive Hamiltonian Monte Carlo sampler. Compared to an
existing score learning approach using a denoising autoencoder, our estimator
is empirically more data-efficient when estimating the score, runs faster, and
has fewer parameters (which can be tuned in a principled and interpretable
way), in addition to providing statistical guarantees.
",Computer Science; Statistics,Statistics
"QCD-Aware Recursive Neural Networks for Jet Physics   Recent progress in applying machine learning for jet physics has been built
upon an analogy between calorimeters and images. In this work, we present a
novel class of recursive neural networks built instead upon an analogy between
QCD and natural languages. In the analogy, four-momenta are like words and the
clustering history of sequential recombination jet algorithms is like the
parsing of a sentence. Our approach works directly with the four-momenta of a
variable-length set of particles, and the jet-based tree structure varies on an
event-by-event basis. Our experiments highlight the flexibility of our method
for building task-specific jet embeddings and show that recursive architectures
are significantly more accurate and data efficient than previous image-based
networks. We extend the analogy from individual jets (sentences) to full events
(paragraphs), and show for the first time an event-level classifier operating
on all the stable particles produced in an LHC event.
",Physics; Statistics,Computer Science
"A multiplier inclusion theorem on product domains   In this note it is shown that the class of all multipliers from the
$d$-parameter Hardy space $H^1_{\mathrm{prod}} (\mathbb{T}^d)$ to $L^2
(\mathbb{T}^d)$ is properly contained in the class of all multipliers from $L
\log^{d/2} L (\mathbb{T}^d)$ to $L^2(\mathbb{T}^d)$.
",Mathematics,Mathematics
"Resonating Valence Bond Theory of Superconductivity: Beyond Cuprates   Resonating valence bond (RVB) theory of high Tc superconductivity, an
electron correlation based mechanism, began as an insightful response by
Anderson, to Bednorz and Muller's discovery of high Tc superconductivity in
cuprates in late 1986. Shortly a theoretical framework for quantum spin liquids
and superconductivity was developed. This theory adresses a formidable strong
coupling quantum manybody problem, in modern times. It is built on certain key
experimental facts: i) survival of a dynamical Mott localization in a metallic
state, ii) proliferation of bond singlets and iii) absence of fermi liquid
quasi particles. After summarising RVB theory I will provide an aerial view of,
mostly, new superconductors where I believe that, to a large degree RVB
mechanism is at work and indicate prospects for even higher Tc's.
",Physics,Physics
"Accretion of Planetary Material onto Host Stars   Accretion of planetary material onto host stars may occur throughout a star's
life. Especially prone to accretion, extrasolar planets in short-period orbits,
while relatively rare, constitute a significant fraction of the known
population, and these planets are subject to dynamical and atmospheric
influences that can drive significant mass loss. Theoretical models frame
expectations regarding the rates and extent of this planetary accretion. For
instance, tidal interactions between planets and stars may drive complete
orbital decay during the main sequence. Many planets that survive their stars'
main sequence lifetime will still be engulfed when the host stars become red
giant stars. There is some observational evidence supporting these predictions,
such as a dearth of close-in planets around fast stellar rotators, which is
consistent with tidal spin-up and planet accretion. There remains no clear
chemical evidence for pollution of the atmospheres of main sequence or red
giant stars by planetary materials, but a wealth of evidence points to active
accretion by white dwarfs. In this article, we review the current understanding
of accretion of planetary material, from the pre- to the post-main sequence and
beyond. The review begins with the astrophysical framework for that process and
then considers accretion during various phases of a host star's life, during
which the details of accretion vary, and the observational evidence for
accretion during these phases.
",Physics,Physics
"Hausdorff dimension, projections, intersections, and Besicovitch sets   This is a survey on recent developments on the Hausdorff dimension of
projections and intersections for general subsets of Euclidean spaces, with an
emphasis on estimates of the Hausdorff dimension of exceptional sets and on
restricted projection families. We shall also discuss relations between
projections and Hausdorff dimension of Besicovitch sets.
",Mathematics,Mathematics
"Pattern-forming fronts in a Swift-Hohenberg equation with directional quenching - parallel and oblique stripes   We study the effect of domain growth on the orientation of striped phases in
a Swift-Hohenberg equation. Domain growth is encoded in a step-like parameter
dependence that allows stripe formation in a half plane, and suppresses
patterns in the complement, while the boundary of the pattern-forming region is
propagating with fixed normal velocity. We construct front solutions that leave
behind stripes in the pattern-forming region that are parallel to or at a small
oblique angle to the boundary.
Technically, the construction of stripe formation parallel to the boundary
relies on ill-posed, infinite-dimensional spatial dynamics. Stripes forming at
a small oblique angle are constructed using a functional-analytic, perturbative
approach. Here, the main difficulties are the presence of continuous spectrum
and the fact that small oblique angles appear as a singular perturbation in a
traveling-wave problem. We resolve the former difficulty using a farfield-core
decomposition and Fredholm theory in weighted spaces. The singular perturbation
problem is resolved using preconditioners and boot-strapping.
",Physics,Physics
"Morgan type uncertainty principle and unique continuation properties for abstract Schrödinger equations   In this paper, Morgan type uncertainty principle and unique continuation
properties of abstract Schrödinger equations with time dependent potentials
in vector-valued classes are obtained. The equation involves a possible linear
operators considered in the Hilbert spaces. So, by choosing the corresponding
spaces H and operators we derived unique continuation properties for numerous
classes of Schrödinger type equations and its systems which occur in a wide
variety of physical systems
",Mathematics,Mathematics
"Faster Bounding Box Annotation for Object Detection in Indoor Scenes   This paper proposes an approach for rapid bounding box annotation for object
detection datasets. The procedure consists of two stages: The first step is to
annotate a part of the dataset manually, and the second step proposes
annotations for the remaining samples using a model trained with the first
stage annotations. We experimentally study which first/second stage split
minimizes to total workload. In addition, we introduce a new fully labeled
object detection dataset collected from indoor scenes. Compared to other indoor
datasets, our collection has more class categories, different backgrounds,
lighting conditions, occlusion and high intra-class differences. We train deep
learning based object detectors with a number of state-of-the-art models and
compare them in terms of speed and accuracy. The fully annotated dataset is
released freely available for the research community.
",Statistics,Computer Science
"A/D Converter Architectures for Energy-Efficient Vision Processor   AI applications have emerged in current world. Among AI applications,
computer-vision (CV) related applications have attracted high interest.
Hardware implementation of CV processors necessitates a high performance but
low-power image detector. The key to energy-efficiency work lies in
analog-digital converting, where output of imaging detectors is transferred to
digital domain and CV algorithms can be performed on data. In this paper,
analog-digital converter architectures are compared, and an example ADC design
is proposed which achieves both good performance and low power consumption.
",Computer Science,Computer Science
"A novel distribution-free hybrid regression model for manufacturing process efficiency improvement   This work is motivated by a particular problem of a modern paper
manufacturing industry, in which maximum efficiency of the fiber-filler
recovery process is desired. A lot of unwanted materials along with valuable
fibers and fillers come out as a by-product of the paper manufacturing process
and mostly goes as waste. The job of an efficient Krofta supracell is to
separate the unwanted materials from the valuable ones so that fibers and
fillers can be collected from the waste materials and reused in the
manufacturing process. The efficiency of Krofta depends on several crucial
process parameters and monitoring them is a difficult proposition. To solve
this problem, we propose a novel hybridization of regression trees (RT) and
artificial neural networks (ANN), hybrid RT-ANN model, to solve the problem of
low recovery percentage of the supracell. This model is used to achieve the
goal of improving supracell efficiency, viz., gain in percentage recovery. In
addition, theoretical results for the universal consistency of the proposed
model are given with the optimal value of a vital model parameter. Experimental
findings show that the proposed hybrid RT-ANN model achieves higher accuracy in
predicting Krofta recovery percentage than other conventional regression models
for solving the Krofta efficiency problem. This work will help the paper
manufacturing company to become environmentally friendly with minimal
ecological damage and improved waste recovery.
",Statistics,Computer Science; Statistics
"Assessing Excited State Energy Gaps with Time-Dependent Density Functional Theory on Ru(II) Complexes   A set of density functionals coming from different rungs on Jacob's ladder
are employed to evaluate the electronic excited states of three Ru(II)
complexes. While most studies on the performance of density functionals compare
the vertical excitation energies, in this work we focus on the energy gaps
between the electronic excited states, of the same and different multiplicity.
Excited state energy gaps are important for example to determine radiationless
transition probabilities. Besides energies, a functional should deliver the
correct state character and state ordering. Therefore, wavefunction overlaps
are introduced to systematically evaluate the effect of different functionals
on the character of the excited states. As a reference, the energies and state
characters from multi-state second-order perturbation theory complete active
space (MS-CASPT2) are used. In comparison to MS-CASPT2, it is found that while
hybrid functionals provide better vertical excitation energies, pure
functionals typically give more accurate excited state energy gaps. Pure
functionals are also found to reproduce the state character and ordering in
closer agreement to MS-CASPT2 than the hybrid functionals.
",Physics,Physics
"Approximation of Bandwidth for the Interactive Operation in Video on Demand System   An interactive session of video-on-demand (VOD) streaming procedure deserves
smooth data transportation for the viewer, irrespective of their geographic
location. To access the required video, bandwidth management during the video
objects transportation at any interactive session is a mandatory prerequisite.
It has been observed in the domain likes movie on demand, electronic
encyclopedia, interactive games, and educational resources. The required data
is imported from the distributed storage servers through the high speed
backbone network. This paper presents the viewer driven session based
multi-user model with respect to the overlay mesh network. In virtue of
reality, the direct implication of this work elaborately shows the required
bandwidth is a causal part in the video on demand system. The analytic model of
session based single viewer bandwidth requirement model presents the bandwidth
requirement for any interactive session like, pause, move slow, rewind, skip
some number of frames, or move fast with some constant number of frames. This
work presents the bandwidth requirement model for any interactive session that
brings the trade-off in data-transportation and storage costs for different
system resources and also for the various system configurations.
",Computer Science,Computer Science
"GOLDRUSH. II. Clustering of Galaxies at $z\sim 4-6$ Revealed with the Half-Million Dropouts Over the 100 deg$^2$ Area Corresponding to 1 Gpc$^3$   We present clustering properties from 579,492 Lyman break galaxies (LBGs) at
z~4-6 over the 100 deg^2 sky (corresponding to a 1.4 Gpc^3 volume) identified
in early data of the Hyper Suprime-Cam (HSC) Subaru strategic program survey.
We derive angular correlation functions (ACFs) of the HSC LBGs with
unprecedentedly high statistical accuracies at z~4-6, and compare them with the
halo occupation distribution (HOD) models. We clearly identify significant ACF
excesses in 10""<$\theta$<90"", the transition scale between 1- and 2-halo terms,
suggestive of the existence of the non-linear halo bias effect. Combining the
HOD models and previous clustering measurements of faint LBGs at z~4-7, we
investigate dark-matter halo mass (Mh) of the z~4-7 LBGs and its correlation
with various physical properties including the star-formation rate (SFR), the
stellar-to-halo mass ratio (SHMR), and the dark matter accretion rate (dotMh)
over a wide-mass range of Mh/M$_\odot$=4x10^10-4x10^12. We find that the SHMR
increases from z~4 to 7 by a factor of ~4 at Mh~1x10^11 M$_\odot$, while the
SHMR shows no strong evolution in the similar redshift range at Mh~1x10^12
M$_\odot$. Interestingly, we identify a tight relation of SFR/dotMh-Mh showing
no significant evolution beyond 0.15 dex in this wide-mass range over z~4-7.
This weak evolution suggests that the SFR/dotMh-Mh relation is a fundamental
relation in high-redshift galaxy formation whose star formation activities are
regulated by the dark matter mass assembly. Assuming this fundamental relation,
we calculate the cosmic SFR densities (SFRDs) over z=0-10 (a.k.a. Madau-Lilly
plot). The cosmic SFRD evolution based on the fundamental relation agrees with
the one obtained by observations, suggesting that the cosmic SFRD increase from
z~10 to 4-2 (decrease from z~4-2 to 0) is mainly driven by the increase of the
halo abundance (the decrease of the accretion rate).
",Physics,Physics
"Lipschitz continuity of quasiconformal mappings and of the solutions to second order elliptic PDE with respect to the distance ratio metric   The main aim of this paper is to study the Lipschitz continuity of certain
$(K, K')$-quasiconformal mappings with respect to the distance ratio metric,
and the Lipschitz continuity of the solution of a quasilinear differential
equation with respect to the distance ratio metric.
",Mathematics,Mathematics
"Distributed Holistic Clustering on Linked Data   Link discovery is an active field of research to support data integration in
the Web of Data. Due to the huge size and number of available data sources,
efficient and effective link discovery is a very challenging task. Common
pairwise link discovery approaches do not scale to many sources with very large
entity sets. We here propose a distributed holistic approach to link many data
sources based on a clustering of entities that represent the same real-world
object. Our clustering approach provides a compact and fused representation of
entities, and can identify errors in existing links as well as many new links.
We support a distributed execution of the clustering approach to achieve faster
execution times and scalability for large real-world data sets. We provide a
novel gold standard for multi-source clustering, and evaluate our methods with
respect to effectiveness and efficiency for large data sets from the geographic
and music domains.
",Computer Science,Computer Science
"Fine-grained Event Learning of Human-Object Interaction with LSTM-CRF   Event learning is one of the most important problems in AI. However,
notwithstanding significant research efforts, it is still a very complex task,
especially when the events involve the interaction of humans or agents with
other objects, as it requires modeling human kinematics and object movements.
This study proposes a methodology for learning complex human-object interaction
(HOI) events, involving the recording, annotation and classification of event
interactions. For annotation, we allow multiple interpretations of a motion
capture by slicing over its temporal span, for classification, we use
Long-Short Term Memory (LSTM) sequential models with Conditional Randon Field
(CRF) for constraints of outputs. Using a setup involving captures of
human-object interaction as three dimensional inputs, we argue that this
approach could be used for event types involving complex spatio-temporal
dynamics.
",Computer Science,Computer Science
"Multi-Scale Continuous CRFs as Sequential Deep Networks for Monocular Depth Estimation   This paper addresses the problem of depth estimation from a single still
image. Inspired by recent works on multi- scale convolutional neural networks
(CNN), we propose a deep model which fuses complementary information derived
from multiple CNN side outputs. Different from previous methods, the
integration is obtained by means of continuous Conditional Random Fields
(CRFs). In particular, we propose two different variations, one based on a
cascade of multiple CRFs, the other on a unified graphical model. By designing
a novel CNN implementation of mean-field updates for continuous CRFs, we show
that both proposed models can be regarded as sequential deep networks and that
training can be performed end-to-end. Through extensive experimental evaluation
we demonstrate the effective- ness of the proposed approach and establish new
state of the art results on publicly available datasets.
",Computer Science,Computer Science; Statistics
"Topological Sieving of Rings According to their Rigidity   We present a novel mechanism for resolving the mechanical rigidity of
nanoscopic circular polymers that flow in a complex environment. The emergence
of a regime of negative differential mobility induced by topological
interactions between the rings and the substrate is the key mechanism for
selective sieving of circular polymers with distinct flexibilities. A simple
model accurately describes the sieving process observed in molecular dynamics
simulations and yields experimentally verifiable analytical predictions, which
can be used as a reference guide for improving filtration procedures of
circular filaments. The topological sieving mechanism we propose ought to be
relevant also in probing the microscopic details of complex substrates.
",Quantitative Biology,Physics
"Locally Repairable Codes with Multiple $(r_{i}, δ_{i})$-Localities   In distributed storage systems, locally repairable codes (LRCs) are
introduced to realize low disk I/O and repair cost. In order to tolerate
multiple node failures, the LRCs with \emph{$(r, \delta)$-locality} are further
proposed. Since hot data is not uncommon in a distributed storage system, both
Zeh \emph{et al.} and Kadhe \emph{et al.} focus on the LRCs with \emph{multiple
localities or unequal localities} (ML-LRCs) recently, which said that the
localities among the code symbols can be different. ML-LRCs are attractive and
useful in reducing repair cost for hot data. In this paper, we generalize the
ML-LRCs to the $(r,\delta)$-locality case of multiple node failures, and define
an LRC with multiple $(r_{i}, \delta_{i})_{i\in [s]}$ localities ($s\ge 2$),
where $r_{1}\leq r_{2}\leq\dots\leq r_{s}$ and
$\delta_{1}\geq\delta_{2}\geq\dots\geq\delta_{s}\geq2$. Such codes ensure that
some hot data could be repaired more quickly and have better failure-tolerance
in certain cases because of relatively smaller $r_{i}$ and larger $\delta_{i}$.
Then, we derive a Singleton-like upper bound on the minimum distance for the
proposed LRCs by employing the regenerating-set technique. Finally, we obtain a
class of explicit and structured constructions of optimal ML-LRCs, and further
extend them to the cases of multiple $(r_{i}, \delta)_{i\in [s]}$ localities.
",Computer Science,Computer Science
"Self-bound quantum droplets in atomic mixtures   Self-bound quantum droplets are a newly discovered phase in the context of
ultracold atoms. In this work we report their experimental realization
following the original proposal by Petrov [Phys. Rev. Lett. 115, 155302
(2015)], using an attractive bosonic mixture. In this system spherical droplets
form due to the balance of competing attractive and repulsive forces, provided
by the mean-field energy close to the collapse threshold and the first-order
correction due to quantum fluctuations. Thanks to an optical levitating
potential with negligible residual confinement we observe self-bound droplets
in free space and we characterize the conditions for their formation as well as
their equilibrium properties. This work sets the stage for future studies on
quantum droplets, from the measurement of their peculiar excitation spectrum,
to the exploration of their superfluid nature.
",Physics,Physics
"Spreading of correlations in the Falicov-Kimball model   We study dynamical properties of the one- and two-dimensional Falicov-Kimball
model using lattice Monte Carlo simulations. In particular, we calculate the
spreading of charge correlations in the equilibrium model and after an
interaction quench. The results show a reduction of the light-cone velocity
with interaction strength at low temperature, while the phase velocity
increases. At higher temperature, the initial spreading is determined by the
Fermi velocity of the noninteracting system and the maximum range of the
correlations decreases with increasing interaction strength. Charge order
correlations in the disorder potential enhance the range of the correlations.
We also use the numerically exact lattice Monte Carlo results to benchmark the
accuracy of equilibrium and nonequilibrium dynamical cluster approximation
calculations. It is shown that the bias introduced by the mapping to a
periodized cluster is substantial, and that from a numerical point of view, it
is more efficient to simulate the lattice model directly.
",Physics,Physics
"An open-source platform to study uniaxial stress effects on nanoscale devices   We present an automatic measurement platform that enables the
characterization of nanodevices by electrical transport and optical
spectroscopy as a function of uniaxial stress. We provide insights into and
detailed descriptions of the mechanical device, the substrate design and
fabrication, and the instrument control software, which is provided under
open-source license. The capability of the platform is demonstrated by
characterizing the piezo-resistance of an InAs nanowire device using a
combination of electrical transport and Raman spectroscopy. The advantages of
this measurement platform are highlighted by comparison with state-of-the-art
piezo-resistance measurements in InAs nanowires. We envision that the
systematic application of this methodology will provide new insights into the
physics of nanoscale devices and novel materials for electronics, and thus
contribute to the assessment of the potential of strain as a technology booster
for nanoscale electronics.
",Physics,Physics
"Distributive Aronszajn trees   Ben-David and Shelah proved that if $\lambda$ is a singular strong-limit
cardinal and $2^\lambda=\lambda^+$, then $\square^*_\lambda$ entails the
existence of a normal $\lambda$-distributive $\lambda^+$-Aronszajn tree. Here,
it is proved that the same conclusion remains valid after replacing the
hypothesis $\square^*_\lambda$ by $\square(\lambda^+,{<}\lambda)$.
As $\square(\lambda^+,{<}\lambda)$ does not impose a bound on the order-type
of the witnessing clubs, our construction is necessarily different from that of
Ben-David and Shelah, and instead uses walks on ordinals augmented with club
guessing.
A major component of this work is the study of postprocessing functions and
their effect on square sequences. A byproduct of this study is the finding that
for $\kappa$ regular uncountable, $\square(\kappa)$ entails the existence of a
partition of $\kappa$ into $\kappa$ many fat sets. When contrasted with a
classic model of Magidor, this shows that it is equiconsistent with the
existence of a weakly compact cardinal that $\omega_2$ cannot be split into two
fat sets.
",Mathematics,Mathematics
"Continuous Learning in Single-Incremental-Task Scenarios   It was recently shown that architectural, regularization and rehearsal
strategies can be used to train deep models sequentially on a number of
disjoint tasks without forgetting previously acquired knowledge. However, these
strategies are still unsatisfactory if the tasks are not disjoint but
constitute a single incremental task (e.g., class-incremental learning). In
this paper we point out the differences between multi-task and
single-incremental-task scenarios and show that well-known approaches such as
LWF, EWC and SI are not ideal for incremental task scenarios. A new approach,
denoted as AR1, combining architectural and regularization strategies is then
specifically proposed. AR1 overhead (in term of memory and computation) is very
small thus making it suitable for online learning. When tested on CORe50 and
iCIFAR-100, AR1 outperformed existing regularization strategies by a good
margin.
",Statistics,Computer Science; Statistics
"On certain geometric properties in Banach spaces of vector-valued functions   We consider a certain type of geometric properties of Banach spaces, which
includes for instance octahedrality, almost squareness, lushness and the
Daugavet property. For this type of properties, we obtain a general reduction
theorem, which, roughly speaking, states the following: if the property in
question is stable under certain finite absolute sums (for example finite
$\ell^p$-sums), then it is also stable under the formation of corresponding
Köthe-Bochner spaces (for example $L^p$-Bochner spaces). From this general
theorem, we obtain as corollaries a number of new results as well as some
alternative proofs of already known results concerning octahedral and almost
square spaces and their relatives, diameter-two-properties, lush spaces and
other classes.
",Mathematics,Mathematics
"Maximum likelihood estimation of determinantal point processes   Determinantal point processes (DPPs) have wide-ranging applications in
machine learning, where they are used to enforce the notion of diversity in
subset selection problems. Many estimators have been proposed, but surprisingly
the basic properties of the maximum likelihood estimator (MLE) have received
little attention. The difficulty is that it is a non-concave maximization
problem, and such functions are notoriously difficult to understand in high
dimensions, despite their importance in modern machine learning. Here we study
both the local and global geometry of the expected log-likelihood function. We
prove several rates of convergence for the MLE and give a complete
characterization of the case where these are parametric. We also exhibit a
potential curse of dimensionality where the asymptotic variance of the MLE
scales exponentially with the dimension of the problem. Moreover, we exhibit an
exponential number of saddle points, and give evidence that these may be the
only critical points.
",Mathematics; Statistics,Mathematics; Statistics
"Towards a Bootstrap approach to higher orders of epsilon expansion   We employ a hybrid approach in determining the anomalous dimension and OPE
coefficient of higher spin operators in the Wilson-Fisher theory. First we do a
large spin analysis for CFT data where we use results obtained from the usual
and the Mellin Bootstrap and also from Feynman diagram literature. This gives
new predictions at $O(\epsilon^4)$ and $O(\epsilon^5)$ for anomalous dimensions
and OPE coefficients, and also provides a cross-check for the results from
Mellin Bootstrap. These higher orders get contributions from all higher spin
operators in the crossed channel. We also use the Bootstrap in Mellin space
method for $\phi^3$ in $d=6-\epsilon$ CFT where we calculate general higher
spin OPE data. We demonstrate a higher loop order calculation in this approach
by summing over contributions from higher spin operators of the crossed channel
in the same spirit as before.
",Physics,Mathematics
"Magnetic domains in thin ferromagnetic films with strong perpendicular anisotropy   We investigate the scaling of the ground state energy and optimal domain
patterns in thin ferromagnetic films with strong uniaxial anisotropy and the
easy axis perpendicular to the film plane. Starting from the full
three-dimensional micromagnetic model, we identify the critical scaling where
the transition from single domain to multidomain ground states such as bubble
or maze patterns occurs. Furthermore, we analyze the asymptotic behavior of the
energy in two regimes separated by a transition. In the single domain regime,
the energy $\Gamma$-converges towards a much simpler two-dimensional and local
model. In the second regime, we derive the scaling of the minimal energy and
deduce a scaling law for the typical domain size.
",Physics; Mathematics,Physics
"New ADS Functionality for the Curator   In this paper we provide an update concerning the operations of the NASA
Astrophysics Data System (ADS), its services and user interface, and the
content currently indexed in its database. As the primary information system
used by researchers in Astronomy, the ADS aims to provide a comprehensive index
of all scholarly resources appearing in the literature. With the current effort
in our community to support data and software citations, we discuss what steps
the ADS is taking to provide the needed infrastructure in collaboration with
publishers and data providers. A new API provides access to the ADS search
interface, metrics, and libraries allowing users to programmatically automate
discovery and curation tasks. The new ADS interface supports a greater
integration of content and services with a variety of partners, including ORCID
claiming, indexing of SIMBAD objects, and article graphics from a variety of
publishers. Finally, we highlight how librarians can facilitate the ingest of
gray literature that they curate into our system.
",Computer Science; Physics,Computer Science
"Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples   Self-paced learning and hard example mining re-weight training instances to
improve learning accuracy. This paper presents two improved alternatives based
on lightweight estimates of sample uncertainty in stochastic gradient descent
(SGD): the variance in predicted probability of the correct class across
iterations of mini-batch SGD, and the proximity of the correct class
probability to the decision threshold. Extensive experimental results on six
datasets show that our methods reliably improve accuracy in various network
architectures, including additional gains on top of other popular training
techniques, such as residual learning, momentum, ADAM, batch normalization,
dropout, and distillation.
",Statistics,Statistics
"Partial Knowledge In Embeddings   Representing domain knowledge is crucial for any task. There has been a wide
range of techniques developed to represent this knowledge, from older logic
based approaches to the more recent deep learning based techniques (i.e.
embeddings). In this paper, we discuss some of these methods, focusing on the
representational expressiveness tradeoffs that are often made. In particular,
we focus on the the ability of various techniques to encode `partial knowledge'
- a key component of successful knowledge systems. We introduce and describe
the concepts of `ensembles of embeddings' and `aggregate embeddings' and
demonstrate how they allow for partial knowledge.
",Computer Science,Computer Science
"Off-diagonal asymptotic properties of Bergman kernels associated to analytic Kähler potentials   We prove a new off-diagonal asymptotic of the Bergman kernels associated to
tensor powers of a positive line bundle on a compact Kähler manifold. We show
that if the Kähler potential is real analytic, then the Bergman kernel
accepts a complete asymptotic expansion in a neighborhood of the diagonal of
shrinking size $k^{-\frac14}$. These improve the earlier results in the subject
for smooth potentials, where an expansion exists in a $k^{-\frac12}$
neighborhood of the diagonal. We obtain our results by finding upper bounds of
the form $C^m m!^{2}$ for the Bergman coefficients $b_m(x, \bar y)$, which is
an interesting problem on its own. We find such upper bounds using the method
of Berman-Berndtsson-Sjöstrand. We also show that sharpening these upper
bounds would improve the rate of shrinking neighborhoods of the diagonal $x=y$
in our results. In the special case of metrics with local constant holomorphic
sectional curvatures, we obtain off-diagonal asymptotic in a fixed (as $k \to
\infty$) neighborhood of the diagonal, which recovers a result of Berman [Ber]
(see Remark 3.5 of [Ber] for higher dimensions). In this case, we also find an
explicit formula for the Bergman kernel mod $O(e^{-k \delta} )$.
",Mathematics,Mathematics
"An Empirical Analysis of Vulnerabilities in Python Packages for Web Applications   This paper examines software vulnerabilities in common Python packages used
particularly for web development. The empirical dataset is based on the PyPI
package repository and the so-called Safety DB used to track vulnerabilities in
selected packages within the repository. The methodological approach builds on
a release-based time series analysis of the conditional probabilities for the
releases of the packages to be vulnerable. According to the results, many of
the Python vulnerabilities observed seem to be only modestly severe; input
validation and cross-site scripting have been the most typical vulnerabilities.
In terms of the time series analysis based on the release histories, only the
recent past is observed to be relevant for statistical predictions; the
classical Markov property holds.
",Computer Science,Computer Science
"Predictive Independence Testing, Predictive Conditional Independence Testing, and Predictive Graphical Modelling   Testing (conditional) independence of multivariate random variables is a task
central to statistical inference and modelling in general - though
unfortunately one for which to date there does not exist a practicable
workflow. State-of-art workflows suffer from the need for heuristic or
subjective manual choices, high computational complexity, or strong parametric
assumptions.
We address these problems by establishing a theoretical link between
multivariate/conditional independence testing, and model comparison in the
multivariate predictive modelling aka supervised learning task. This link
allows advances in the extensively studied supervised learning workflow to be
directly transferred to independence testing workflows - including automated
tuning of machine learning type which addresses the need for a heuristic
choice, the ability to quantitatively trade-off computational demand with
accuracy, and the modern black-box philosophy for checking and interfacing.
As a practical implementation of this link between the two workflows, we
present a python package 'pcit', which implements our novel multivariate and
conditional independence tests, interfacing the supervised learning API of the
scikit-learn package. Theory and package also allow for straightforward
independence test based learning of graphical model structure.
We empirically show that our proposed predictive independence test outperform
or are on par to current practice, and the derived graphical model structure
learning algorithms asymptotically recover the 'true' graph. This paper, and
the 'pcit' package accompanying it, thus provide powerful, scalable,
generalizable, and easy-to-use methods for multivariate and conditional
independence testing, as well as for graphical model structure learning.
",Computer Science; Mathematics; Statistics,Statistics
"Face centered cubic and hexagonal close packed skyrmion crystals in centro-symmetric magnets   Skyrmions are disk-like objects that typically form triangular crystals in
two dimensional systems. This situation is analogous to the so-called ""pancake
vortices"" of quasi-two dimensional superconductors. The way in which skyrmion
disks or pancake skyrmions pile up in layered centro-symmetric materials is
dictated by the inter-layer exchange. Unbiased Monte Carlo simulations and
simple stabilization arguments reveal face centered cubic and hexagonal close
packed skyrmion crystals for different choices of the inter-layer exchange, in
addition to the conventional triangular crystal of skyrmion lines. Moreover, an
inhomogeneous current induces sliding motion of pancake skyrmions, indicating
that they behave as effective mesoscale particles.
",Physics,Physics
"Distinct evolutions of Weyl fermion quasiparticles and Fermi arcs with bulk band topology in Weyl semimetals   The Weyl semimetal phase is a recently discovered topological quantum state
of matter characterized by the presence of topologically protected degeneracies
near the Fermi level. These degeneracies are the source of exotic phenomena,
including the realization of chiral Weyl fermions as quasiparticles in the bulk
and the formation of Fermi arc states on the surfaces. Here, we demonstrate
that these two key signatures show distinct evolutions with the bulk band
topology by performing angle-resolved photoemission spectroscopy, supported by
first-principle calculations, on transition-metal monophosphides. While Weyl
fermion quasiparticles exist only when the chemical potential is located
between two saddle points of the Weyl cone features, the Fermi arc states
extend in a larger energy scale and are robust across the bulk Lifshitz
transitions associated with the recombination of two non-trivial Fermi surfaces
enclosing one Weyl point into a single trivial Fermi surface enclosing two Weyl
points of opposite chirality. Therefore, in some systems (e.g. NbP),
topological Fermi arc states are preserved even if Weyl fermion quasiparticles
are absent in the bulk. Our findings not only provide insight into the
relationship between the exotic physical phenomena and the intrinsic bulk band
topology in Weyl semimetals, but also resolve the apparent puzzle of the
different magneto-transport properties observed in TaAs, TaP and NbP, where the
Fermi arc states are similar.
",Physics,Physics
"An improved high order finite difference method for non-conforming grid interfaces for the wave equation   This paper presents an extension of a recently developed high order finite
difference method for the wave equation on a grid with non-conforming
interfaces. The stability proof of the existing methods relies on the
interpolation operators being norm-contracting, which is satisfied by the
second and fourth order operators, but not by the sixth order operator. We
construct new penalty terms to impose interface conditions such that the
stability proof does not require the norm-contracting condition. As a
consequence, the sixth order accurate scheme is also provably stable. Numerical
experiments demonstrate the improved stability and accuracy property.
",Mathematics,Mathematics
"Incomplete Gauss sums modulo primes   We obtain a new bound for incomplete Gauss sums modulo primes. Our argument
falls under the framework of Vinogradov's method which we use to reduce the
problem under consideration to bounding the number of solutions to two distinct
systems of congruences. The first is related to Vinogradov's mean value
theorem, although the second does not appear to have been considered before.
Our bound improves on current results in the range $N\ge
q^{2k^{-1/2}+O(k^{-3/2})}$.
",Mathematics,Mathematics
"The Mismeasure of Mergers: Revised Limits on Self-interacting Dark Matter in Merging Galaxy Clusters   In an influential recent paper, Harvey et al (2015) derive an upper limit to
the self-interaction cross section of dark matter ($\sigma_{\rm DM} < 0.47$
cm$^2$/g at 95\% confidence) by averaging the dark matter-galaxy offsets in a
sample of merging galaxy clusters. Using much more comprehensive data on the
same clusters, we identify several substantial errors in their offset
measurements. Correcting these errors relaxes the upper limit on $\sigma_{\rm
DM}$ to $\lesssim 2$ cm$^2$/g, following the Harvey et al prescription for
relating offsets to cross sections in a simple solid body scattering model.
Furthermore, many clusters in the sample violate the assumptions behind this
prescription, so even this revised upper limit should be used with caution.
Although this particular sample does not tightly constrain self-interacting
dark matter models when analyzed this way, we discuss how merger ensembles may
be used more effectively in the future. We conclude that errors inherent in
using single-band imaging to identify mass and light peaks do not necessarily
average out in a sample of this size, particularly when a handful of
substructures constitute a majority of the weight in the ensemble.
",Physics,Physics
"Quantum groups, Yang-Baxter maps and quasi-determinants   For any quasi-triangular Hopf algebra, there exists the universal R-matrix,
which satisfies the Yang-Baxter equation. It is known that the adjoint action
of the universal R-matrix on the elements of the tensor square of the algebra
constitutes a quantum Yang-Baxter map, which satisfies the set-theoretic
Yang-Baxter equation. The map has a zero curvature representation among
L-operators defined as images of the universal R-matrix. We find that the zero
curvature representation can be solved by the Gauss decomposition of a product
of L-operators. Thereby obtained a quasi-determinant expression of the quantum
Yang-Baxter map associated with the quantum algebra $U_{q}(gl(n))$. Moreover,
the map is identified with products of quasi-Plücker coordinates over a
matrix composed of the L-operators. We also consider the quasi-classical limit,
where the underlying quantum algebra reduces to a Poisson algebra. The
quasi-determinant expression of the quantum Yang-Baxter map reduces to ratios
of determinants, which give a new expression of a classical Yang-Baxter map.
",Physics; Mathematics,Mathematics
"Ultra-Fast Reactive Transport Simulations When Chemical Reactions Meet Machine Learning: Chemical Equilibrium   During reactive transport modeling, the computational cost associated with
chemical reaction calculations is often 10-100 times higher than that of
transport calculations. Most of these costs results from chemical equilibrium
calculations that are performed at least once in every mesh cell and at every
time step of the simulation. Calculating chemical equilibrium is an iterative
process, where each iteration is in general so computationally expensive that
even if every calculation converged in a single iteration, the resulting
speedup would not be significant. Thus, rather than proposing a fast-converging
numerical method for solving chemical equilibrium equations, we present a
machine learning method that enables new equilibrium states to be quickly and
accurately estimated, whenever a previous equilibrium calculation with similar
input conditions has been performed. We demonstrate the use of this smart
chemical equilibrium method in a reactive transport modeling example and show
that, even at early simulation times, the majority of all equilibrium
calculations are quickly predicted and, after some time steps, the
machine-learning-accelerated chemical solver has been fully trained to rapidly
perform all subsequent equilibrium calculations, resulting in speedups of
almost two orders of magnitude. We remark that our new on-demand machine
learning method can be applied to any case in which a massive number of
sequential/parallel evaluations of a computationally expensive function $f$
needs to be done, $y=f(x)$. We remark, that, in contrast to traditional machine
learning algorithms, our on-demand training approach does not require a
statistics-based training phase before the actual simulation of interest
commences. The introduced on-demand training scheme requires, however, the
first-order derivatives $\partial f/\partial x$ for later smart predictions.
",Physics; Statistics,Computer Science; Statistics
"Discrete CMC surfaces in R^3 and discrete minimal surfaces in S^3. A discrete Lawson correspondence   The main result of this paper is a discrete Lawson correspondence between
discrete CMC surfaces in R^3 and discrete minimal surfaces in S^3. This is a
correspondence between two discrete isothermic surfaces. We show that this
correspondence is an isometry in the following sense: it preserves the metric
coefficients introduced previously by Bobenko and Suris for isothermic nets.
Exactly as in the smooth case, this is a correspondence between nets with the
same Lax matrices, and the immersion formulas also coincide with the smooth
case.
",Physics; Mathematics,Mathematics
"Merging fragments of classical logic   We investigate the possibility of extending the non-functionally complete
logic of a collection of Boolean connectives by the addition of further Boolean
connectives that make the resulting set of connectives functionally complete.
More precisely, we will be interested in checking whether an axiomatization for
Classical Propositional Logic may be produced by merging Hilbert-style calculi
for two disjoint incomplete fragments of it. We will prove that the answer to
that problem is a negative one, unless one of the components includes only
top-like connectives.
",Computer Science; Mathematics,Mathematics
"Quantum gauge symmetry of reducible gauge theory   We derive the gaugeon formalism of the Kalb-Ramond field theory, a reducible
gauge theory, which discusses the quantum gauge freedom. In gaugeon formalism,
theory admits quantum gauge symmetry which leaves the action form-invariant.
The BRST symmetric gaugeon formalism is also studied which introduces the
gaugeon ghost fields and gaugeon ghosts of ghosts fields. To replace the
Yokoyama subsidiary conditions by a single Kugo-Ojima type condition the virtue
of BRST symmetry is utilized. Under generalized BRST transformations, we show
that the gaugeon fields appear naturally in the reducible gauge theory.
",Physics,Physics
"Gradient descent GAN optimization is locally stable   Despite the growing prominence of generative adversarial networks (GANs),
optimization in GANs is still a poorly understood topic. In this paper, we
analyze the ""gradient descent"" form of GAN optimization i.e., the natural
setting where we simultaneously take small gradient steps in both generator and
discriminator parameters. We show that even though GAN optimization does not
correspond to a convex-concave game (even for simple parameterizations), under
proper conditions, equilibrium points of this optimization procedure are still
\emph{locally asymptotically stable} for the traditional GAN formulation. On
the other hand, we show that the recently proposed Wasserstein GAN can have
non-convergent limit cycles near equilibrium. Motivated by this stability
analysis, we propose an additional regularization term for gradient descent GAN
updates, which \emph{is} able to guarantee local stability for both the WGAN
and the traditional GAN, and also shows practical promise in speeding up
convergence and addressing mode collapse.
",Computer Science; Mathematics; Statistics,Computer Science; Mathematics
"JointGAN: Multi-Domain Joint Distribution Learning with Generative Adversarial Nets   A new generative adversarial network is developed for joint distribution
matching. Distinct from most existing approaches, that only learn conditional
distributions, the proposed model aims to learn a joint distribution of
multiple random variables (domains). This is achieved by learning to sample
from conditional distributions between the domains, while simultaneously
learning to sample from the marginals of each individual domain. The proposed
framework consists of multiple generators and a single softmax-based critic,
all jointly trained via adversarial learning. From a simple noise source, the
proposed framework allows synthesis of draws from the marginals, conditional
draws given observations from a subset of random variables, or complete draws
from the full joint distribution. Most examples considered are for joint
analysis of two domains, with examples for three domains also presented.
",Statistics,Computer Science; Statistics
"An Applied Knowledge Framework to Study Complex Systems   The complexity of knowledge production on complex systems is well-known, but
there still lacks knowledge framework that would both account for a certain
structure of knowledge production at an epistemological level and be directly
applicable to the study and management of complex systems. We set a basis for
such a framework, by first analyzing in detail a case study of the construction
of a geographical theory of complex territorial systems, through mixed methods,
namely qualitative interview analysis and quantitative citation network
analysis. We can therethrough inductively build a framework that considers
knowledge entreprises as perspectives, with co-evolving components within
complementary knowledge domains. We finally discuss potential applications and
developments.
",Computer Science; Physics,Computer Science; Mathematics
"Electronic structure of transferred graphene/h-BN van der Waals heterostructures with nonzero stacking angles by nano-ARPES   In van der Waals heterostructures, the periodic potential from the Moiré
superlattice can be used as a control knob to modulate the electronic structure
of the constituent materials. Here we present a nanoscale angle-resolved
photoemission spectroscopy (Nano-ARPES) study of transferred graphene/h-BN
heterostructures with two different stacking angles of 2.4° and 4.3°
respectively. Our measurements reveal six replicas of graphene Dirac cones at
the superlattice Brillouin zone (SBZ) centers. The size of the SBZ and its
relative rotation angle to the graphene BZ are in good agreement with Moiré
superlattice period extracted from atomic force microscopy (AFM) measurements.
Comparison to epitaxial graphene/h-BN with 0° stacking angles suggests
that the interaction between graphene and h-BN decreases with increasing
stacking angle.
",Physics,Physics
"Axiomatizing Epistemic Logic of Friendship via Tree Sequent Calculus   This paper positively solves an open problem if it is possible to provide a
Hilbert system to Epistemic Logic of Friendship (EFL) by Seligman, Girard and
Liu. To find a Hilbert system, we first introduce a sound, complete and
cut-free tree (or nested) sequent calculus for EFL, which is an integrated
combination of Seligman's sequent calculus for basic hybrid logic and a tree
sequent calculus for modal logic. Then we translate a tree sequent into an
ordinary formula to specify a Hilbert system of EFL and finally show that our
Hilbert system is sound and complete for the intended two-dimensional
semantics.
",Computer Science; Mathematics,Computer Science
"Fan-type spin structure in uni-axial chiral magnets   We investigate the spin structure of a uni-axial chiral magnet near the
transition temperatures in low fields perpendicular to the helical axis. We
find a fan-type modulation structure where the clockwise and counterclockwise
windings appear alternatively along the propagation direction of the modulation
structure. This structure is often realized in a Yoshimori-type (non-chiral)
helimagnet but it is rarely realized in a chiral helimagnet. To discuss
underlying physics of this structure, we reconsider the phase diagram (phase
boundary and crossover lines) through the free energy and asymptotic behaviors
of isolated solitons. The fan structure appears slightly below the phase
boundary of the continuous transition of instability-type. In this region,
there are no solutions containing any types of isolated solitons to the mean
field equations.
",Physics,Physics
"Biocompatible Writing of Data into DNA   A simple DNA-based data storage scheme is demonstrated in which information
is written using ""addressing"" oligonucleotides. In contrast to other methods
that allow arbitrary code to be stored, the resulting DNA is suitable for
downstream enzymatic and biological processing. This capability is crucial for
DNA computers, and may allow for a diverse array of computational operations to
be carried out using this DNA. Although here we use gel-based methods for
information readout, we also propose more advanced methods involving
protein/DNA complexes and atomic force microscopy/nano-pore schemes for data
readout.
",Computer Science; Physics,Computer Science; Physics
"Community structure detection and evaluation during the pre- and post-ictal hippocampal depth recordings   Detecting and evaluating regions of brain under various circumstances is one
of the most interesting topics in computational neuroscience. However, the
majority of the studies on detecting communities of a functional connectivity
network of the brain is done on networks obtained from coherency attributes,
and not from correlation. This lack of studies, in part, is due to the fact
that many common methods for clustering graphs require the nodes of the network
to be `positively' linked together, a property that is guaranteed by a
coherency matrix, by definition. However, correlation matrices reveal more
information regarding how each pair of nodes are linked together. In this
study, for the first time we simultaneously examine four inherently different
network clustering methods (spectral, heuristic, and optimization methods)
applied to the functional connectivity networks of the CA1 region of the
hippocampus of an anaesthetized rat during pre-ictal and post-ictal states. The
networks are obtained from correlation matrices, and its results are compared
with the ones obtained by applying the same methods to coherency matrices. The
correlation matrices show a much finer community structure compared to the
coherency matrices. Furthermore, we examine the potential smoothing effect of
choosing various window sizes for computing the correlation/coherency matrices.
",Computer Science; Quantitative Biology,Computer Science; Physics
"Accurate, Efficient and Scalable Graph Embedding   The Graph Convolutional Network (GCN) model and its variants are powerful
graph embedding tools for facilitating classification and clustering on graphs.
However, a major challenge is to reduce the complexity of layered GCNs and make
them parallelizable and scalable on very large graphs --- state-of the art
techniques are unable to achieve scalability without losing accuracy and
efficiency. In this paper, we propose novel parallelization techniques for
graph sampling-based GCNs that achieve superior scalable performance on very
large graphs without compromising accuracy. Specifically, our GCN guarantees
work-efficient training and produces order of magnitude savings in computation
and communication. To scale GCN training on tightly-coupled shared memory
systems, we develop parallelization strategies for the key steps in training:
For the graph sampling step, we exploit parallelism within and across multiple
sampling instances, and devise an efficient data structure for concurrent
accesses that provides theoretical guarantee of near-linear speedup with number
of processing units. For the feature propagation step within the sampled graph,
we improve cache utilization and reduce DRAM communication by data
partitioning. We prove that our partitioning strategy is a 2-approximation for
minimizing the communication time compared to the optimal strategy. We
demonstrate that our parallel graph embedding outperforms state-of-the-art
methods in scalability (with respect to number of processors, graph size and
GCN model size), efficiency and accuracy on several large datasets. On a
40-core Xeon platform, our parallel training achieves 64$\times$ speedup (with
AVX) in the sampling step and 25$\times$ speedup in the feature propagation
step, compared to the serial implementation, resulting in a net speedup of
21$\times$.
",Computer Science,Computer Science
"New constraints on the millimetre emission of six debris disks   The presence of dusty debris around main sequence stars denotes the existence
of planetary systems. Such debris disks are often identified by the presence of
excess continuum emission at infrared and (sub-)millimetre wavelengths, with
measurements at longer wavelengths tracing larger and cooler dust grains. The
exponent of the slope of the disk emission at sub-millimetre wavelengths, `q',
defines the size distribution of dust grains in the disk. This size
distribution is a function of the rigid strength of the dust producing parent
planetesimals. As part of the survey `PLAnetesimals around TYpical Pre-main
seqUence Stars' (PLATYPUS) we observed six debris disks at 9-mm using the
Australian Telescope Compact Array. We obtain marginal (~3-\sigma) detections
of three targets: HD 105, HD 61005, and HD 131835. Upper limits for the three
remaining disks, HD20807, HD109573, and HD109085, provide further constraint of
the (sub-)millimetre slope of their spectral energy distributions. The values
of q (or their limits) derived from our observations are all smaller than the
oft-assumed steady state collisional cascade model (q = 3.5), but lie well
within the theoretically expected range for debris disks q ~ 3 to 4. The
measured q values for our targets are all < 3.3, consistent with both
collisional modelling results and theoretical predictions for parent
planetesimal bodies being `rubble piles' held together loosely by their
self-gravity.
",Physics,Physics
"Elliptic operators on refined Sobolev scales on vector bundles   We introduce a refined Sobolev scale on a vector bundle over a closed
infinitely smooth manifold. This scale consists of inner product Hörmander
spaces parametrized with a real number and a function varying slowly at
infinity in the sense of Karamata. We prove that these spaces are obtained by
the interpolation with a function parameter between inner product Sobolev
spaces. An arbitrary classical elliptic pseudodifferential operator acting
between vector bundles of the same rank is investigated on this scale. We prove
that this operator is bounded and Fredholm on pairs of appropriate Hörmander
spaces. We also prove that the solutions to the corresponding elliptic equation
satisfy a certain a priori estimate on these spaces. The local regularity of
these solutions is investigated on the refined Sobolev scale. We find new
sufficient conditions for the solutions to have continuous derivatives of a
given order.
",Mathematics,Mathematics
"Selected topics on Toric Varieties   This article is based on a series of lectures on toric varieties given at
RIMS, Kyoto. We start by introducing toric varieties, their basic properties
and later pass to more advanced topics relating mostly to combinatorics.
",Mathematics,Computer Science
"New descriptions of the weighted Reed-Muller codes and the homogeneous Reed-Muller codes   We give a description of the weighted Reed-Muller codes over a prime field in
a modular algebra. A description of the homogeneous Reed-Muller codes in the
same ambient space is presented for the binary case. A decoding procedure using
the Landrock-Manz method is developed.
",Computer Science; Mathematics,Mathematics
"Analysing the Potential of BLE to Support Dynamic Broadcasting Scenarios   In this paper, we present a novel approach for broadcasting information based
on a Bluetooth Low Energy (BLE) ibeacon technology. We propose a dynamic method
that uses a combination of Wi-Fi and BLE technology where every technology
plays a part in a user discovery and broadcasting process. In such system, a
specific ibeacon device broadcasts the information when a user is in proximity.
Using experiments, we conduct a scenario where the system discovers users,
disseminates information, and later we use collected data to examine the system
performance and capability. The results show that our proposed approach has a
promising potential to become a powerful tool in the discovery and broadcasting
concept that can be easily implemented and used in business environments.
",Computer Science,Computer Science
"Journal of Open Source Software (JOSS): design and first-year review   This article describes the motivation, design, and progress of the Journal of
Open Source Software (JOSS). JOSS is a free and open-access journal that
publishes articles describing research software. It has the dual goals of
improving the quality of the software submitted and providing a mechanism for
research software developers to receive credit. While designed to work within
the current merit system of science, JOSS addresses the dearth of rewards for
key contributions to science made in the form of software. JOSS publishes
articles that encapsulate scholarship contained in the software itself, and its
rigorous peer review targets the software components: functionality,
documentation, tests, continuous integration, and the license. A JOSS article
contains an abstract describing the purpose and functionality of the software,
references, and a link to the software archive. The article is the entry point
of a JOSS submission, which encompasses the full set of software artifacts.
Submission and review proceed in the open, on GitHub. Editors, reviewers, and
authors work collaboratively and openly. Unlike other journals, JOSS does not
reject articles requiring major revision; while not yet accepted, articles
remain visible and under review until the authors make adequate changes (or
withdraw, if unable to meet requirements). Once an article is accepted, JOSS
gives it a DOI, deposits its metadata in Crossref, and the article can begin
collecting citations on indexers like Google Scholar and other services.
Authors retain copyright of their JOSS article, releasing it under a Creative
Commons Attribution 4.0 International License. In its first year, starting in
May 2016, JOSS published 111 articles, with more than 40 additional articles
under review. JOSS is a sponsored project of the nonprofit organization
NumFOCUS and is an affiliate of the Open Source Initiative.
",Computer Science,Computer Science
"Visualizing Time-Varying Particle Flows with Diffusion Geometry   The tasks of identifying separation structures and clusters in flow data are
fundamental to flow visualization. Significant work has been devoted to these
tasks in flow represented by vector fields, but there are unique challenges in
addressing these tasks for time-varying particle data. The unstructured nature
of particle data, nonuniform and sparse sampling, and the inability to access
arbitrary particles in space-time make it difficult to define separation and
clustering for particle data. We observe that weaker notions of separation and
clustering through continuous measures of these structures are meaningful when
coupled with user exploration. We achieve this goal by defining a measure of
particle similarity between pairs of particles. More specifically, separation
occurs when spatially-localized particles are dissimilar, while clustering is
characterized by sets of particles that are similar to one another. To be
robust to imperfections in sampling we use diffusion geometry to compute
particle similarity. Diffusion geometry is parameterized by a scale that allows
a user to explore separation and clustering in a continuous manner. We
illustrate the benefits of our technique on a variety of 2D and 3D flow
datasets, from particles integrated in fluid simulations based on time-varying
vector fields, to particle-based simulations in astrophysics.
",Computer Science,Computer Science; Physics
"Are theoretical results 'Results'?   Yes.
",Quantitative Biology,Mathematics
"High-transmissivity Silicon Visible-wavelength Metasurface Designs based on Truncated-cone Nanoantennae   High-transmissivity all-dielectric metasurfaces have recently attracted
attention towards the realization of ultra-compact optical devices and systems.
Silicon based metasurfaces, in particular, are highly promising considering the
possibility of monolithic integration with VLSI circuits. Realization of
silicon based metasurfaces operational in the visible wavelengths remains a
challenge. A numerical study of silicon metasurfaces based on stepped truncated
cone shaped nanoantenna elements is presented. Metasurfaces based on the
stepped conical geometry can be designed for operation in the 700nm to 800nm
wavelength window and achieve full cycle phase response (0 to pi with an
improved transmittance in comparison with previously reported cylindrical
geometry [1]. A systematic parameter study of the influence of various
geometrical parameters on the achievable amplitude and phase coverage is
reported.
",Physics,Physics
"On topological cyclic homology   Topological cyclic homology is a refinement of Connes--Tsygan's cyclic
homology which was introduced by Bökstedt--Hsiang--Madsen in 1993 as an
approximation to algebraic $K$-theory. There is a trace map from algebraic
$K$-theory to topological cyclic homology, and a theorem of
Dundas--Goodwillie--McCarthy asserts that this induces an equivalence of
relative theories for nilpotent immersions, which gives a way for computing
$K$-theory in various situations. The construction of topological cyclic
homology is based on genuine equivariant homotopy theory, the use of explicit
point-set models, and the elaborate notion of a cyclotomic spectrum.
The goal of this paper is to revisit this theory using only
homotopy-invariant notions. In particular, we give a new construction of
topological cyclic homology. This is based on a new definition of the
$\infty$-category of cyclotomic spectra: We define a cyclotomic spectrum to be
a spectrum $X$ with $S^1$-action (in the most naive sense) together with
$S^1$-equivariant maps $\varphi_p: X\to X^{tC_p}$ for all primes $p$. Here
$X^{tC_p}=\mathrm{cofib}(\mathrm{Nm}: X_{hC_p}\to X^{hC_p})$ is the Tate
construction. On bounded below spectra, we prove that this agrees with previous
definitions. As a consequence, we obtain a new and simple formula for
topological cyclic homology.
In order to construct the maps $\varphi_p: X\to X^{tC_p}$ in the example of
topological Hochschild homology we introduce and study Tate diagonals for
spectra and Frobenius homomorphisms of commutative ring spectra. In particular
we prove a version of the Segal conjecture for the Tate diagonals and relate
these Frobenius homomorphisms to power operations.
",Mathematics,Mathematics
"Fast Snapshottable Concurrent Braun Heaps   This paper proposes a new concurrent heap algorithm, based on a stateless
shape property, which efficiently maintains balance during insert and removeMin
operations implemented with hand-over-hand locking. It also provides a O(1)
linearizable snapshot operation based on lazy copy-on-write semantics. Such
snapshots can be used to provide consistent views of the heap during iteration,
as well as to make speculative updates (which can later be dropped).
The simplicity of the algorithm allows it to be easily proven correct, and
the choice of shape property provides priority queue performance which is
competitive with highly optimized skiplist implementations (and has stronger
bounds on worst-case time complexity).
A Scala reference implementation is provided.
",Computer Science,Computer Science
"Trading the Twitter Sentiment with Reinforcement Learning   This paper is to explore the possibility to use alternative data and
artificial intelligence techniques to trade stocks. The efficacy of the daily
Twitter sentiment on predicting the stock return is examined using machine
learning methods. Reinforcement learning(Q-learning) is applied to generate the
optimal trading policy based on the sentiment signal. The predicting power of
the sentiment signal is more significant if the stock price is driven by the
expectation of the company growth and when the company has a major event that
draws the public attention. The optimal trading strategy based on reinforcement
learning outperforms the trading strategy based on the machine learning
prediction.
",Computer Science,Computer Science; Statistics
"A Liouville theorem for the Euler equations in the plane   This paper is concerned with qualitative properties of bounded steady flows
of an ideal incompressible fluid with no stagnation point in the
two-dimensional plane R^2. We show that any such flow is a shear flow, that is,
it is parallel to some constant vector. The proof of this Liouville-type result
is firstly based on the study of the geometric properties of the level curves
of the stream function and secondly on the derivation of some estimates on the
at most logarithmic growth of the argument of the flow. These estimates lead to
the conclusion that the streamlines of the flow are all parallel lines.
",Mathematics,Mathematics
"An Empirical Bayes Approach to Regularization Using Previously Published Models   This manuscript proposes a novel empirical Bayes technique for regularizing
regression coefficients in predictive models. When predictions from a
previously published model are available, this empirical Bayes method provides
a natural mathematical framework for shrinking coefficients toward the
estimates implied by the body of existing research rather than the shrinkage
toward zero provided by traditional L1 and L2 penalization schemes. The method
is applied to two different prediction problems. The first involves the
construction of a model for predicting whether a single nucleotide polymorphism
(SNP) of the KCNQ1 gene will result in dysfunction of the corresponding voltage
gated ion channel. The second involves the prediction of preoperative serum
creatinine change in patients undergoing cardiac surgery.
",Statistics,Statistics
"Differentially Private ANOVA Testing   Modern society generates an incredible amount of data about individuals, and
releasing summary statistics about this data in a manner that provably protects
individual privacy would offer a valuable resource for researchers in many
fields. We present the first algorithm for analysis of variance (ANOVA) that
preserves differential privacy, allowing this important statistical test to be
conducted (and the results released) on databases of sensitive information. In
addition to our private algorithm for the F test statistic, we show a rigorous
way to compute p-values that accounts for the added noise needed to preserve
privacy. Finally, we present experimental results quantifying the statistical
power of this differentially private version of the test, finding that a sample
of several thousand observations is frequently enough to detect variation
between groups. The differentially private ANOVA algorithm is a promising
approach for releasing a common test statistic that is valuable in fields in
the sciences and social sciences.
",Computer Science; Statistics,Computer Science; Statistics
"The application of Monte Carlo methods for learning generalized linear model   Monte Carlo method is a broad class of computational algorithms that rely on
repeated random sampling to obtain numerical results. They are often used in
physical and mathematical problems and are most useful when it is difficult or
impossible to use other mathematical methods. Basically, many statisticians
have been increasingly drawn to Monte Carlo method in three distinct problem
classes: optimization, numerical integration, and generating draws from a
probability distribution. In this paper, we will introduce the Monte Carlo
method for calculating coefficients in Generalized Linear Model(GLM),
especially for Logistic Regression. Our main methods are Metropolis
Hastings(MH) Algorithms and Stochastic Approximation in Monte Carlo
Computation(SAMC). For comparison, we also get results automatically using MLE
method in R software.
",Statistics,Statistics
"Detecting laws in power subgroups   A group law is said to be detectable in power subgroups if, for all coprime
$m$ and $n$, a group $G$ satisfies the law if and only if the power subgroups
$G^m$ and $G^n$ both satisfy the law. We prove that for all positive integers
$c$, nilpotency of class at most $c$ is detectable in power subgroups, as is
the $k$-Engel law for $k$ at most 4. In contrast, detectability in power
subgroups fails for solvability of given derived length: we construct a finite
group $W$ such that $W^2$ and $W^3$ are metabelian but $W$ has derived length
$3$. We analyse the complexity of the detectability of commutativity in power
subgroups, in terms of finite presentations that encode a proof of the result.
",Mathematics,Mathematics
"Mitigating Confirmation Bias on Twitter by Recommending Opposing Views   In this work, we propose a content-based recommendation approach to increase
exposure to opposing beliefs and opinions. Our aim is to help provide users
with more diverse viewpoints on issues, which are discussed in partisan groups
from different perspectives. Since due to the backfire effect, people's
original beliefs tend to strengthen when challenged with counter evidence, we
need to expose them to opposing viewpoints at the right time. The preliminary
work presented here describes our first step into this direction. As
illustrative showcase, we take the political debate on Twitter around the
presidency of Donald Trump.
",Computer Science,Computer Science
"A Distance Between Filtered Spaces Via Tripods   We present a simplified treatment of stability of filtrations on finite
spaces. Interestingly, we can lift the stability result for combinatorial
filtrations from [CSEM06] to the case when two filtrations live on different
spaces without directly invoking the concept of interleaving. We then prove
that this distance is intrinsic by constructing explicit geodesics between any
pair of filtered spaces. Finally we use this construction to obtain a
strengthening of the stability result.
",Computer Science; Mathematics,Physics
"Fitch-Style Modal Lambda Calculi   Fitch-style modal deduction, in which modalities are eliminated by opening a
subordinate proof, and introduced by shutting one, were investigated in the
1990s as a basis for lambda calculi. We show that such calculi have good
computational properties for a variety of intuitionistic modal logics.
Semantics are given in cartesian closed categories equipped with an adjunction
of endofunctors, with the necessity modality interpreted by the right adjoint.
Where this functor is an idempotent comonad, a coherence result on the
semantics allows us to present a calculus for intuitionistic S4 that is simpler
than others in the literature. We show the calculi can be extended à la
tense logic with the left adjoint of necessity, and are then complete for the
categorical semantics.
",Computer Science,Computer Science
"CoAP over ICN   The Constrained Application Protocol (CoAP) is a specialized Web transfer
protocol for resource-oriented applications intended to run on constrained
devices, typically part of the Internet of Things. In this paper we leverage
Information-Centric Networking (ICN), deployed within the domain of a network
provider that interconnects, in addition to other terminals, CoAP endpoints in
order to provide enhanced CoAP services. We present various CoAP-specific
communication scenarios and discuss how ICN can provide benefits to both
network providers and CoAP applications, even though the latter are not aware
of the existence of ICN. In particular, the use of ICN results in smaller state
management complexity at CoAP endpoints, simpler implementation at CoAP
endpoints, and less communication overhead in the network.
",Computer Science,Computer Science
"Gaussian Processes for HRF estimation for BOLD fMRI   We present a non-parametric joint estimation method for fMRI task activation
values and the hemodynamic response function (HRF). The HRF is modeled as a
Gaussian process, making continuous evaluation possible for jittered paradigms
and providing a variance estimate at each point.
",Statistics,Statistics
"Pinned, locked, pushed, and pulled traveling waves in structured environments   Traveling fronts describe the transition between two alternative states in a
great number of physical and biological systems. Examples include the spread of
beneficial mutations, chemical reactions, and the invasions by foreign species.
In homogeneous environments, the alternative states are separated by a smooth
front moving at a constant velocity. This simple picture can break down in
structured environments such as tissues, patchy landscapes, and microfluidic
devices. Habitat fragmentation can pin the front at a particular location or
lock invasion velocities into specific values. Locked velocities are not
sensitive to moderate changes in dispersal or growth and are determined by the
spatial and temporal periodicity of the environment. The synchronization with
the environment results in discontinuous fronts that propagate as periodic
pulses. We characterize the transition from continuous to locked invasions and
show that it is controlled by positive density-dependence in dispersal or
growth. We also demonstrate that velocity locking is robust to demographic and
environmental fluctuations and examine stochastic dynamics and evolution in
locked invasions.
",Quantitative Biology,Physics
"Practical Integer-to-Binary Mapping for Quantum Annealers   Recent advancements in quantum annealing hardware and numerous studies in
this area suggests that quantum annealers have the potential to be effective in
solving unconstrained binary quadratic programming problems. Naturally, one may
desire to expand the application domain of these machines to problems with
general discrete variables. In this paper, we explore the possibility of
employing quantum annealers to solve unconstrained quadratic programming
problems over a bounded integer domain. We present an approach for encoding
integer variables into binary ones, thereby representing unconstrained integer
quadratic programming problems as unconstrained binary quadratic programming
problems. To respect some of the limitations of the currently developed quantum
annealers, we propose an integer encoding, named bounded- coefficient encoding,
in which we limit the size of the coefficients that appear in the encoding.
Furthermore, we propose an algorithm for finding the upper bound on the
coefficients of the encoding using the precision of the machine and the
coefficients of the original integer problem. Finally, we experimentally show
that this approach is far more resilient to the noise of the quantum annealers
compared to traditional approaches for the encoding of integers in base two.
",Computer Science; Mathematics,Computer Science
"Pluricanonical Periods over Compact Riemann Surfaces of Genus at least 2   This article is an attempt to generalize Riemann's bilinear relations on
compact Riemann surface of genus at least 2, which may lead to new structures
in the theory of hyperbolic Riemann surfaces. No significant result is
obtained, the article serves to bring the readers' attention to the observation
made by [Bol-1949], and some easy consequences.
",Mathematics,Mathematics
"The splashback radius of halos from particle dynamics. II. Dependence on mass, accretion rate, redshift, and cosmology   The splashback radius $R_{\rm sp}$, the apocentric radius of particles on
their first orbit after falling into a dark matter halo, has recently been
suggested as a physically motivated halo boundary that separates accreting from
orbiting material. Using the SPARTA code presented in Paper I, we analyze the
orbits of billions of particles in cosmological simulations of structure
formation and measure $R_{\rm sp}$ for a large sample of halos that span a mass
range from dwarf galaxy to massive cluster halos, reach redshift 8, and include
WMAP, Planck, and self-similar cosmologies. We analyze the dependence of
$R_{\rm sp}/R_{\rm 200m}$ and $M_{\rm sp}/M_{\rm 200m}$ on the mass accretion
rate $\Gamma$, halo mass, redshift, and cosmology. The scatter in these
relations varies between 0.02 and 0.1 dex. While we confirm the known trend
that $R_{\rm sp}/R_{\rm 200m}$ decreases with $\Gamma$, the relationships turn
out to be more complex than previously thought, demonstrating that $R_{\rm sp}$
is an independent definition of the halo boundary that cannot trivially be
reconstructed from spherical overdensity definitions. We present fitting
functions for $R_{\rm sp}/R_{\rm 200m}$ and $M_{\rm sp}/M_{\rm 200m}$ as a
function of accretion rate, peak height, and redshift, achieving an accuracy of
5% or better everywhere in the parameter space explored. We discuss the
physical meaning of the distribution of particle apocenters and show that the
previously proposed definition of $R_{\rm sp}$ as the radius of the steepest
logarithmic density slope encloses roughly three-quarters of the apocenters.
Finally, we conclude that no analytical model presented thus far can fully
explain our results.
",Physics,Physics
"Ensemble Classifier for Eye State Classification using EEG Signals   The growing importance and utilization of measuring brain waves (e.g. EEG
signals of eye state) in brain-computer interface (BCI) applications
highlighted the need for suitable classification methods. In this paper, a
comparison between three of well-known classification methods (i.e. support
vector machine (SVM), hidden Markov map (HMM), and radial basis function (RBF))
for EEG based eye state classification was achieved. Furthermore, a suggested
method that is based on ensemble model was tested. The suggested (ensemble
system) method based on a voting algorithm with two kernels: random forest (RF)
and Kstar classification methods. The performance was tested using three
measurement parameters: accuracy, mean absolute error (MAE), and confusion
matrix. Results showed that the proposed method outperforms the other tested
methods. For instance, the suggested method's performance was 97.27% accuracy
and 0.13 MAE.
",Computer Science,Computer Science
"On Conjugates and Adjoint Descent   In this note we present an $\infty$-categorical framework for descent along
adjunctions and a general formula for counting conjugates up to equivalence
which unifies several known formulae from different fields.
",Mathematics,Mathematics
"Linear Convergence of An Iterative Phase Retrieval Algorithm with Data Reuse   Phase retrieval has been an attractive but difficult problem rising from
physical science, and there has been a gap between state-of-the-art theoretical
convergence analyses and the corresponding efficient retrieval methods.
Firstly, these analyses all assume that the sensing vectors and the iterative
updates are independent, which only fits the ideal model with infinite
measurements but not the reality, where data are limited and have to be reused.
Secondly, the empirical results of some efficient methods, such as the
randomized Kaczmarz method, show linear convergence, which is beyond existing
theoretical explanations considering its randomness and reuse of data. In this
work, we study for the first time, without the independence assumption, the
convergence behavior of the randomized Kaczmarz method for phase retrieval.
Specifically, beginning from taking expectation of the squared estimation error
with respect to the index of measurement by fixing the sensing vector and the
error in the previous step, we discard the independence assumption, rigorously
derive the upper and lower bounds of the reduction of the mean squared error,
and prove the linear convergence. This work fills the gap between a fast
converging algorithm and its theoretical understanding. The proposed
methodology may contribute to the study of other iterative algorithms for phase
retrieval and other problems in the broad area of signal processing and machine
learning.
",Computer Science,Computer Science; Statistics
"LD-SDS: Towards an Expressive Spoken Dialogue System based on Linked-Data   In this work we discuss the related challenges and describe an approach
towards the fusion of state-of-the-art technologies from the Spoken Dialogue
Systems (SDS) and the Semantic Web and Information Retrieval domains. We
envision a dialogue system named LD-SDS that will support advanced, expressive,
and engaging user requests, over multiple, complex, rich, and open-domain data
sources that will leverage the wealth of the available Linked Data.
Specifically, we focus on: a) improving the identification, disambiguation and
linking of entities occurring in data sources and user input; b) offering
advanced query services for exploiting the semantics of the data, with
reasoning and exploratory capabilities; and c) expanding the typical
information seeking dialogue model (slot filling) to better reflect real-world
conversational search scenarios.
",Computer Science,Computer Science
"Convolution Aware Initialization   Initialization of parameters in deep neural networks has been shown to have a
big impact on the performance of the networks (Mishkin & Matas, 2015). The
initialization scheme devised by He et al, allowed convolution activations to
carry a constrained mean which allowed deep networks to be trained effectively
(He et al., 2015a). Orthogonal initializations and more generally orthogonal
matrices in standard recurrent networks have been proved to eradicate the
vanishing and exploding gradient problem (Pascanu et al., 2012). Majority of
current initialization schemes do not take fully into account the intrinsic
structure of the convolution operator. Using the duality of the Fourier
transform and the convolution operator, Convolution Aware Initialization builds
orthogonal filters in the Fourier space, and using the inverse Fourier
transform represents them in the standard space. With Convolution Aware
Initialization we noticed not only higher accuracy and lower loss, but faster
convergence. We achieve new state of the art on the CIFAR10 dataset, and
achieve close to state of the art on various other tasks.
",Computer Science; Statistics,Computer Science; Statistics
"Acyclic cluster algebras, reflection groups, and curves on a punctured disc   We establish a bijective correspondence between certain non-self-intersecting
curves in an $n$-punctured disc and positive ${\mathbf c}$-vectors of acyclic
cluster algebras whose quivers have multiple arrows between every pair of
vertices. As a corollary, we obtain a proof of a conjecture by K.-H. Lee and K.
Lee (arXiv:1703.09113) on the combinatorial description of real Schur roots for
acyclic quivers with multiple arrows, and give a combinatorial characterization
of seeds in terms of curves in an $n$-punctured disc.
",Mathematics,Mathematics
"Affiliation networks with an increasing degree sequence   Affiliation network is one kind of two-mode social network with two different
sets of nodes (namely, a set of actors and a set of social events) and edges
representing the affiliation of the actors with the social events. Although a
number of statistical models are proposed to analyze affiliation networks, the
asymptotic behaviors of the estimator are still unknown or have not been
properly explored. In this paper, we study an affiliation model with the degree
sequence as the exclusively natural sufficient statistic in the exponential
family distributions. We establish the uniform consistency and asymptotic
normality of the maximum likelihood estimator when the numbers of actors and
events both go to infinity. Simulation studies and a real data example
demonstrate our theoretical results.
",Mathematics; Statistics,Mathematics; Statistics
"Fraunhofer diffraction at the two-dimensional quadratically distorted (QD) Grating   A two-dimensional (2D) mathematical model of quadratically distorted (QD)
grating is established with the principles of Fraunhofer diffraction and
Fourier optics. Discrete sampling and bisection algorithm are applied for
finding numerical solution of the diffraction pattern of QD grating. This 2D
mathematical model allows the precise design of QD grating and improves the
optical performance of simultaneous multiplane imaging system.
",Physics,Physics
"On the status of the Born-Oppenheimer expansion in molecular systems theory   It is shown that the adiabatic Born-Oppenheimer expansion does not satisfy
the necessary condition for the applicability of perturbation theory. A simple
example of an exact solution of a problem that can not be obtained from the
Born-Oppenheimer expansion is given. A new version of perturbation theory for
molecular systems is proposed.
",Physics,Physics
"Randomized Load Balancing on Networks with Stochastic Inputs   Iterative load balancing algorithms for indivisible tokens have been studied
intensively in the past. Complementing previous worst-case analyses, we study
an average-case scenario where the load inputs are drawn from a fixed
probability distribution. For cycles, tori, hypercubes and expanders, we obtain
almost matching upper and lower bounds on the discrepancy, the difference
between the maximum and the minimum load. Our bounds hold for a variety of
probability distributions including the uniform and binomial distribution but
also distributions with unbounded range such as the Poisson and geometric
distribution. For graphs with slow convergence like cycles and tori, our
results demonstrate a substantial difference between the convergence in the
worst- and average-case. An important ingredient in our analysis is new upper
bound on the t-step transition probability of a general Markov chain, which is
derived by invoking the evolving set process.
",Computer Science,Computer Science; Mathematics
"Emergence of Leadership in Communication   We study a neuro-inspired model that mimics a discussion (or information
dissemination) process in a network of agents. During their interaction, agents
redistribute activity and network weights, resulting in emergence of leader(s).
The model is able to reproduce the basic scenarios of leadership known in
nature and society: laissez-faire (irregular activity, weak leadership, sizable
inter-follower interaction, autonomous sub-leaders); participative or
democratic (strong leadership, but with feedback from followers); and
autocratic (no feedback, one-way influence). Several pertinent aspects of these
scenarios are found as well---e.g., hidden leadership (a hidden clique of
agents driving the official autocratic leader), and successive leadership (two
leaders influence followers by turns). We study how these scenarios emerge from
inter-agent dynamics and how they depend on behavior rules of agents---in
particular, on their inertia against state changes.
",Physics,Computer Science
"The border support rank of two-by-two matrix multiplication is seven   We show that the border support rank of the tensor corresponding to
two-by-two matrix multiplication is seven over the complex numbers. We do this
by constructing two polynomials that vanish on all complex tensors with format
four-by-four-by-four and border rank at most six, but that do not vanish
simultaneously on any tensor with the same support as the two-by-two matrix
multiplication tensor. This extends the work of Hauenstein, Ikenmeyer, and
Landsberg. We also give two proofs that the support rank of the two-by-two
matrix multiplication tensor is seven over any field: one proof using a result
of De Groote saying that the decomposition of this tensor is unique up to
sandwiching, and another proof via the substitution method. These results
answer a question asked by Cohn and Umans. Studying the border support rank of
the matrix multiplication tensor is relevant for the design of matrix
multiplication algorithms, because upper bounds on the border support rank of
the matrix multiplication tensor lead to upper bounds on the computational
complexity of matrix multiplication, via a construction of Cohn and Umans.
Moreover, support rank has applications in quantum communication complexity.
",Computer Science; Mathematics,Computer Science; Mathematics
"Nanostructured complex oxides as a route towards thermal behavior in artificial spin ice systems   We have used soft x-ray photoemission electron microscopy to image the
magnetization of single domain La$_{0.7}$Sr$_{0.3}$MnO$_{3}$ nano-islands
arranged in geometrically frustrated configurations such as square ice and
kagome ice geometries. Upon thermal randomization, ensembles of nano-islands
with strong inter-island magnetic coupling relax towards low-energy
configurations. Statistical analysis shows that the likelihood of ensembles
falling into low-energy configurations depends strongly on the annealing
temperature. Annealing to just below the Curie temperature of the ferromagnetic
film (T$_{C}$ = 338 K) allows for a much greater probability of achieving low
energy configurations as compared to annealing above the Curie temperature. At
this thermally active temperature of 325 K, the ensemble of ferromagnetic
nano-islands explore their energy landscape over time and eventually transition
to lower energy states as compared to the frozen-in configurations obtained
upon cooling from above the Curie temperature. Thus, this materials system
allows for a facile method to systematically study thermal evolution of
artificial spin ice arrays of nano-islands at temperatures modestly above room
temperature.
",Physics,Physics
"Dining Philosophers, Leader Election and Ring Size problems, in the quantum setting   We provide the first quantum (exact) protocol for the Dining Philosophers
problem (DP), a central problem in distributed algorithms. It is well known
that the problem cannot be solved exactly in the classical setting. We then use
our DP protocol to provide a new quantum protocol for the tightly related
problem of exact leader election (LE) on a ring, improving significantly in
both time and memory complexity over the known LE protocol by Tani et. al. To
do this, we show that in some sense the exact DP and exact LE problems are
equivalent; interestingly, in the classical non-exact setting they are not.
Hopefully, the results will lead to exact quantum protocols for other important
distributed algorithmic questions; in particular, we discuss interesting
connections to the ring size problem, as well as to a physically motivated
question of breaking symmetry in 1D translationally invariant systems.
",Computer Science,Computer Science; Mathematics
"Infinite-Dimensionality in Quantum Foundations: W*-algebras as Presheaves over Matrix Algebras   In this paper, W*-algebras are presented as canonical colimits of diagrams of
matrix algebras and completely positive maps. In other words, matrix algebras
are dense in W*-algebras.
",Computer Science; Mathematics,Mathematics
"The Gaia-ESO Survey: low-alpha element stars in the Galactic Bulge   We take advantage of the Gaia-ESO Survey iDR4 bulge data to search for
abundance anomalies that could shed light on the composite nature of the Milky
Way bulge. The alpha-elements (Mg, Si, and whenever available, Ca) abundances,
and their trends with Fe abundances have been analysed for a total of 776 bulge
stars. In addition, the aluminum abundances and their ratio to Fe and Mg have
also been examined. Our analysis reveals the existence of low-alpha element
abundance stars with respect to the standard bulge sequence in the [alpha/Fe]
vs. [Fe/H] plane. 18 objects present deviations in [alpha/Fe] ranging from 2.1
to 5.3 sigma with respect to the median standard value. Those stars do not show
Mg-Al anti-correlation patterns. Incidentally, this sign of the existence of
multiple stellar populations is reported firmly for the first time for the
bulge globular cluster NGC 6522. The identified low-alpha abundance stars have
chemical patterns compatible with those of the thin disc. Their link with
massive dwarf galaxies accretion seems unlikely, as larger deviations in alpha
abundance and Al would be expected. The vision of a bulge composite nature and
a complex formation process is reinforced by our results. The used approach, a
multi-method and model-driven analysis of high resolution data seems crucial to
reveal this complexity.
",Physics,Physics
"A Versatile Approach to Evaluating and Testing Automated Vehicles based on Kernel Methods   Evaluation and validation of complicated control systems are crucial to
guarantee usability and safety. Usually, failure happens in some very rarely
encountered situations, but once triggered, the consequence is disastrous.
Accelerated Evaluation is a methodology that efficiently tests those
rarely-occurring yet critical failures via smartly-sampled test cases. The
distribution used in sampling is pivotal to the performance of the method, but
building a suitable distribution requires case-by-case analysis. This paper
proposes a versatile approach for constructing sampling distribution using
kernel method. The approach uses statistical learning tools to approximate the
critical event sets and constructs distributions based on the unique properties
of Gaussian distributions. We applied the method to evaluate the automated
vehicles. Numerical experiments show proposed approach can robustly identify
the rare failures and significantly reduce the evaluation time.
",Computer Science; Statistics,Computer Science; Statistics
"Multimodal Clustering for Community Detection   Multimodal clustering is an unsupervised technique for mining interesting
patterns in $n$-adic binary relations or $n$-mode networks. Among different
types of such generalized patterns one can find biclusters and formal concepts
(maximal bicliques) for 2-mode case, triclusters and triconcepts for 3-mode
case, closed $n$-sets for $n$-mode case, etc. Object-attribute biclustering
(OA-biclustering) for mining large binary datatables (formal contexts or 2-mode
networks) arose by the end of the last decade due to intractability of
computation problems related to formal concepts; this type of patterns was
proposed as a meaningful and scalable approximation of formal concepts. In this
paper, our aim is to present recent advance in OA-biclustering and its
extensions to mining multi-mode communities in SNA setting. We also discuss
connection between clustering coefficients known in SNA community for 1-mode
and 2-mode networks and OA-bicluster density, the main quality measure of an
OA-bicluster. Our experiments with 2-, 3-, and 4-mode large real-world networks
show that this type of patterns is suitable for community detection in
multi-mode cases within reasonable time even though the number of corresponding
$n$-cliques is still unknown due to computation difficulties. An interpretation
of OA-biclusters for 1-mode networks is provided as well.
",Computer Science; Statistics,Computer Science; Statistics
"Deep Multimodal Subspace Clustering Networks   We present convolutional neural network (CNN) based approaches for
unsupervised multimodal subspace clustering. The proposed framework consists of
three main stages - multimodal encoder, self-expressive layer, and multimodal
decoder. The encoder takes multimodal data as input and fuses them to a latent
space representation. The self-expressive layer is responsible for enforcing
the self-expressiveness property and acquiring an affinity matrix corresponding
to the data points. The decoder reconstructs the original input data. The
network uses the distance between the decoder's reconstruction and the original
input in its training. We investigate early, late and intermediate fusion
techniques and propose three different encoders corresponding to them for
spatial fusion. The self-expressive layers and multimodal decoders are
essentially the same for different spatial fusion-based approaches. In addition
to various spatial fusion-based methods, an affinity fusion-based network is
also proposed in which the self-expressive layer corresponding to different
modalities is enforced to be the same. Extensive experiments on three datasets
show that the proposed methods significantly outperform the state-of-the-art
multimodal subspace clustering methods.
",Statistics,Computer Science; Statistics
"Learning Filter Functions in Regularisers by Minimising Quotients   Learning approaches have recently become very popular in the field of inverse
problems. A large variety of methods has been established in recent years,
ranging from bi-level learning to high-dimensional machine learning techniques.
Most learning approaches, however, only aim at fitting parametrised models to
favourable training data whilst ignoring misfit training data completely. In
this paper, we follow up on the idea of learning parametrised regularisation
functions by quotient minimisation as established in [3]. We extend the model
therein to include higher-dimensional filter functions to be learned and allow
for fit- and misfit-training data consisting of multiple functions. We first
present results resembling behaviour of well-established derivative-based
sparse regularisers like total variation or higher-order total variation in
one-dimension. Our second and main contribution is the introduction of novel
families of non-derivative-based regularisers. This is accomplished by learning
favourable scales and geometric properties while at the same time avoiding
unfavourable ones.
",Mathematics,Computer Science; Statistics
"Robust Task Clustering for Deep Many-Task Learning   We investigate task clustering for deep-learning based multi-task and
few-shot learning in a many-task setting. We propose a new method to measure
task similarities with cross-task transfer performance matrix for the deep
learning scenario. Although this matrix provides us critical information
regarding similarity between tasks, its asymmetric property and unreliable
performance scores can affect conventional clustering methods adversely.
Additionally, the uncertain task-pairs, i.e., the ones with extremely
asymmetric transfer scores, may collectively mislead clustering algorithms to
output an inaccurate task-partition. To overcome these limitations, we propose
a novel task-clustering algorithm by using the matrix completion technique. The
proposed algorithm constructs a partially-observed similarity matrix based on
the certainty of cluster membership of the task-pairs. We then use a matrix
completion algorithm to complete the similarity matrix. Our theoretical
analysis shows that under mild constraints, the proposed algorithm will
perfectly recover the underlying ""true"" similarity matrix with a high
probability. Our results show that the new task clustering method can discover
task clusters for training flexible and superior neural network models in a
multi-task learning setup for sentiment classification and dialog intent
classification tasks. Our task clustering approach also extends metric-based
few-shot learning methods to adapt multiple metrics, which demonstrates
empirical advantages when the tasks are diverse.
",Computer Science; Statistics,Computer Science; Statistics
"Asymptotically preserving particle-in-cell methods for inhomogenous strongly magnetized plasmas   We propose a class of Particle-In-Cell (PIC) methods for the Vlasov-Poisson
system with a strong and inhomogeneous external magnetic field with fixed
direction, where we focus on the motion of particles in the plane orthogonal to
the magnetic field (so-called poloidal directions). In this regime, the time
step can be subject to stability constraints related to the smallness of Larmor
radius and plasma frequency. To avoid this limitation, our approach is based on
first and higher-order semi-implicit numerical schemes already validated on
dissipative systems [3] and for homogeneous magnetic fields [10]. Thus, when
the magnitude of the external magnetic field becomes large, this method
provides a consistent PIC discretization of the guiding-center system taking
into account variations of the magnetic field. We carry out some theoretical
proofs and perform several numerical experiments that establish a solid
validation of the method and its underlying concepts.
",Mathematics,Physics
"Optimal Timing to Trade Along a Randomized Brownian Bridge   This paper studies an optimal trading problem that incorporates the trader's
market view on the terminal asset price distribution and uninformative noise
embedded in the asset price dynamics. We model the underlying asset price
evolution by an exponential randomized Brownian bridge (rBb) and consider
various prior distributions for the random endpoint. We solve for the optimal
strategies to sell a stock, call, or put, and analyze the associated delayed
liquidation premia. We solve for the optimal trading strategies numerically and
compare them across different prior beliefs. Among our results, we find that
disconnected continuation/exercise regions arise when the trader prescribe a
two-point discrete distribution and double exponential distribution.
",Quantitative Finance,Quantitative Finance
"General $(α, β)$ metrics with relatively isotroic mean Landsberg curvature   In this paper, we study a new class of Finsler metrics, F=\alpha\phi(b^2,s),
s:=\beta/\alpha, defined by a Riemannian metric \alpha and 1-form \beta. It is
called general (\alpha, \beta) metric. In this paper, we assume \phi be
coefficient by s and \beta be closed and conformal. We find a nessecary and
sufficient condition for the metric of relatively isotropic mean Landsberg
curvature to be Berwald.
",Mathematics,Mathematics
"Estimating the unseen from multiple populations   Given samples from a distribution, how many new elements should we expect to
find if we continue sampling this distribution? This is an important and
actively studied problem, with many applications ranging from unseen species
estimation to genomics. We generalize this extrapolation and related unseen
estimation problems to the multiple population setting, where population $j$
has an unknown distribution $D_j$ from which we observe $n_j$ samples. We
derive an optimal estimator for the total number of elements we expect to find
among new samples across the populations. Surprisingly, we prove that our
estimator's accuracy is independent of the number of populations. We also
develop an efficient optimization algorithm to solve the more general problem
of estimating multi-population frequency distributions. We validate our methods
and theory through extensive experiments. Finally, on a real dataset of human
genomes across multiple ancestries, we demonstrate how our approach for unseen
estimation can enable cohort designs that can discover interesting mutations
with greater efficiency.
",Computer Science; Statistics,Quantitative Biology
"Incidence systems on Cartesian powers of algebraic curves   We show that a reduct of the Zariski structure of an algebraic curve which is
not locally modular interprets a field, answering a question of Zilber's.
",Mathematics,Mathematics
"Grid-forming Control for Power Converters based on Matching of Synchronous Machines   We consider the problem of grid-forming control of power converters in
low-inertia power systems. Starting from an average-switch three-phase inverter
model, we draw parallels to a synchronous machine (SM) model and propose a
novel grid-forming converter control strategy which dwells upon the main
characteristic of a SM: the presence of an internal rotating magnetic field. In
particular, we augment the converter system with a virtual oscillator whose
frequency is driven by the DC-side voltage measurement and which sets the
converter pulse-width-modulation signal, thereby achieving exact matching
between the converter in closed-loop and the SM dynamics. We then provide a
sufficient condition assuring existence, uniqueness, and global asymptotic
stability of equilibria in a coordinate frame attached to the virtual
oscillator angle. By actuating the DC-side input of the converter we are able
to enforce this sufficient condition. In the same setting, we highlight strict
incremental passivity, droop, and power-sharing properties of the proposed
framework, which are compatible with conventional requirements of power system
operation. We subsequently adopt disturbance decoupling techniques to design
additional control loops that regulate the DC-side voltage, as well as AC-side
frequency and amplitude, while in the end validating them with numerical
experiments.
",Mathematics,Computer Science
"Two forms of minimality in ASPIC+   Many systems of structured argumentation explicitly require that the facts
and rules that make up the argument for a conclusion be the minimal set
required to derive the conclusion. ASPIC+ does not place such a requirement on
arguments, instead requiring that every rule and fact that are part of an
argument be used in its construction. Thus ASPIC+ arguments are minimal in the
sense that removing any element of the argument would lead to a structure that
is not an argument. In this brief note we discuss these two types of minimality
and show how the first kind of minimality can, if desired, be recovered in
ASPIC+.
",Computer Science,Computer Science
"Study of deteriorating semiopaque turquoise lead-potassium glass beads at different stages of corrosion using micro-FTIR spectroscopy   Nowadays, a problem of historical beadworks conservation in museum
collections is actual more than ever because of fatal corrosion of the 19th
century glass beads. Vibrational spectroscopy is a powerful method for
investigation of glass, namely, of correlation of the structure-chemical
composition. Therefore, Fourier-transform infrared spectroscopy was used for
examination of degradation processes in cloudy turquoise glass beads, which in
contrast to other color ones deteriorate especially strongly. Micro-X-ray
fluorescence spectrometry of samples has shown that lead-potassium glass
PbO-K$_2$O-SiO$_2$ with small amount of Cu and Sb was used for manufacture of
cloudy turquoise beads. Fourier-transform infrared spectroscopy study of the
beads at different stages of glass corrosion was carried out in the range from
200 to 4000 cm$^{-1}$ in the attenuated total reflection mode. In all the
spectra, we have observed shifts of two major absorption bands to low-frequency
range (~1000 and ~775 cm$^{-1}$) compared to ones typical for amorphous SiO2
(~1100 and 800 cm$^{-1}$, respectively). Such an effect is connected with
Pb$^{2+}$ and K$^+$ appending to the glass network. The presence of a weak band
at ~1630 cm$^{-1}$ in all the spectra is attributed to the adsorption of
H$_2$O. After annealing of the beads, the band disappeared completely in less
deteriorated samples and became significantly weaker in more destroyed ones.
Based on that we conclude that there is adsorbed molecular water on the beads.
However, products of corrosion (e.g., alkali in the form of white crystals or
droplets of liquid alkali) were not observed on the glass surface. We have also
observed glass depolymerisation in the strongly degraded beads, which is
exhibited in domination of the band peaked at ~1000 cm$^{-1}$.
",Physics,Physics
"Methods for Interpreting and Understanding Deep Neural Networks   This paper provides an entry point to the problem of interpreting a deep
neural network model and explaining its predictions. It is based on a tutorial
given at ICASSP 2017. It introduces some recently proposed techniques of
interpretation, along with theory, tricks and recommendations, to make most
efficient use of these techniques on real data. It also discusses a number of
practical applications.
",Computer Science; Statistics,Computer Science; Statistics
"Solving $\ell^p\!$-norm regularization with tensor kernels   In this paper, we discuss how a suitable family of tensor kernels can be used
to efficiently solve nonparametric extensions of $\ell^p$ regularized learning
methods. Our main contribution is proposing a fast dual algorithm, and showing
that it allows to solve the problem efficiently. Our results contrast recent
findings suggesting kernel methods cannot be extended beyond Hilbert setting.
Numerical experiments confirm the effectiveness of the method.
",Mathematics; Statistics,Computer Science; Statistics
"Conservative Exploration using Interleaving   In many practical problems, a learning agent may want to learn the best
action in hindsight without ever taking a bad action, which is significantly
worse than the default production action. In general, this is impossible
because the agent has to explore unknown actions, some of which can be bad, to
learn better actions. However, when the actions are combinatorial, this may be
possible if the unknown action can be evaluated by interleaving it with the
production action. We formalize this concept as learning in stochastic
combinatorial semi-bandits with exchangeable actions. We design efficient
learning algorithms for this problem, bound their n-step regret, and evaluate
them on both synthetic and real-world problems. Our real-world experiments show
that our algorithms can learn to recommend K most attractive movies without
ever violating a strict production constraint, both overall and subject to a
diversity constraint.
",Statistics,Computer Science; Statistics
"Structured Differential Learning for Automatic Threshold Setting   We introduce a technique that can automatically tune the parameters of a
rule-based computer vision system comprised of thresholds, combinational logic,
and time constants. This lets us retain the flexibility and perspicacity of a
conventionally structured system while allowing us to perform approximate
gradient descent using labeled data. While this is only a heuristic procedure,
as far as we are aware there is no other efficient technique for tuning such
systems. We describe the components of the system and the associated supervised
learning mechanism. We also demonstrate the utility of the algorithm by
comparing its performance versus hand tuning for an automotive headlight
controller. Despite having over 100 parameters, the method is able to
profitably adjust the system values given just the desired output for a number
of videos.
",Statistics,Computer Science
"Abelian Tensor Models on the Lattice   We consider a chain of Abelian Klebanov-Tarnopolsky fermionic tensor models
coupled through quartic nearest-neighbor interactions. We characterize the
gauge-singlet spectrum for small chains ($L=2,3,4,5$) and observe that the
spectral statistics exhibits strong evidences in favor of quasi-many body
localization.
",Physics,Physics
"Change of grading, injective dimension and dualizing complexes   Let $G,H$ be groups, $\phi: G \rightarrow H$ a group morphism, and $A$ a
$G$-graded algebra. The morphism $\phi$ induces an $H$-grading on $A$, and on
any $G$-graded $A$-module, which thus becomes an $H$-graded $A$-module. Given
an injective $G$-graded $A$-module, we give bounds for its injective dimension
when seen as $H$-graded $A$-module. Following ideas by Van den Bergh, we give
an application of our results to the stability of dualizing complexes through
change of grading.
",Mathematics,Mathematics
"Homogeneous Kobayashi-hyperbolic manifolds with high-dimensional group of holomorphic automorphisms   We determine all connected homogeneous Kobayashi-hyperbolic manifolds of
dimension $n\ge 2$ whose holomorphic automorphism group has dimension $n^2-2$.
This result complements an existing classification for automorphism group
dimension $n^2-1$ and greater obtained without the homogeneity assumption.
",Mathematics,Mathematics
"ALMA Observations of the Gravitational Lens SDP.9   We present long-baseline ALMA observations of the strong gravitational lens
H-ATLAS J090740.0-004200 (SDP.9), which consists of an elliptical galaxy at
$z_{\mathrm{L}}=0.6129$ lensing a background submillimeter galaxy into two
extended arcs. The data include Band 6 continuum observations, as well as CO
$J$=6$-$5 molecular line observations, from which we measure an updated source
redshift of $z_{\mathrm{S}}=1.5747$. The image morphology in the ALMA data is
different from that of the HST data, indicating a spatial offset between the
stellar, gas, and dust component of the source galaxy. We model the lens as an
elliptical power law density profile with external shear using a combination of
archival HST data and conjugate points identified in the ALMA data. Our best
model has an Einstein radius of $\theta_{\mathrm{E}}=0.66\pm0.01$ and a
slightly steeper than isothermal mass profile slope. We search for the central
image of the lens, which can be used constrain the inner mass distribution of
the lens galaxy including the central supermassive black hole, but do not
detect it in the integrated CO image at a 3$\sigma$ rms level of 0.0471 Jy km
s$^{-1}$.
",Physics,Physics
"Estimates for $π(x)$ for large values of $x$ and Ramanujan's prime counting inequality   In this paper we use refined approximations for Chebyshev's
$\vartheta$-function to establish new explicit estimates for the prime counting
function $\pi(x)$, which improve the current best estimates for large values of
$x$. As an application we find an upper bound for the number $H_0$ which is
defined to be the smallest positive integer so that Ramanujan's prime counting
inequality holds for every $x \geq H_0$.
",Mathematics,Mathematics
"Estimating the rate of defects under imperfect sampling inspection - a new approach   We consider the problem of estimating the the rate of defects (mean number of
defects per item), given counts of defects detected by two independent
imperfect inspectors on a sample of items. In contrast with the well-known
method of Capture-Recapture, here we {\it{do not}} have information regarding
the number of defects jointly detected by {\it{both}} inspectors. We solve this
problem by constructing two types of estimators - a simple moment-type
estimator, and a more complicated maximum-likelihood estimator. The performance
of these estimators is studied analytically and by means of simulations. It is
shown that the maximum-likelihood estimator is superior to the moment-type
estimator. A systematic comparison with the Capture-Recapture method is also
made.
",Statistics,Mathematics; Statistics
"TED Talk Recommender Using Speech Transcripts   Nowadays, online video platforms mostly recommend related videos by analyzing
user-driven data such as viewing patterns, rather than the content of the
videos. However, content is more important than any other element when videos
aim to deliver knowledge. Therefore, we have developed a web application which
recommends related TED lecture videos to the users, considering the content of
the videos from the transcripts. TED Talk Recommender constructs a network for
recommending videos that are similar content-wise and providing a user
interface.
",Computer Science,Computer Science
"Estimation of the discontinuous leverage effect: Evidence from the NASDAQ order book   An extensive empirical literature documents a generally negative correlation,
named the ""leverage effect,"" between asset returns and changes of volatility.
It is more challenging to establish such a return-volatility relationship for
jumps in high-frequency data. We propose new nonparametric methods to assess
and test for a discontinuous leverage effect --- i.e. a relation between
contemporaneous jumps in prices and volatility. The methods are robust to
market microstructure noise and build on a newly developed price-jump
localization and estimation procedure. Our empirical investigation of six years
of transaction data from 320 NASDAQ firms displays no unconditional negative
correlation between price and volatility cojumps. We show, however, that there
is a strong relation between price-volatility cojumps if one conditions on the
sign of price jumps and whether the price jumps are market-wide or
idiosyncratic. Firms' volatility levels strongly explain the cross-section of
discontinuous leverage while debt-to-equity ratios have no significant
explanatory power.
",Mathematics; Statistics,Quantitative Finance
"Maximum principles for the fractional p-Laplacian and symmetry of solutions   In this paper, we consider nonlinear equations involving the fractional
p-Laplacian $$ (-\lap)_p^s u(x)) \equiv C_{n,s,p} PV \int_{\mathbb{R}^n}
\frac{|u(x)-u(y)|^{p-2}[u(x)-u(y)]}{|x-z|^{n+ps}} dz= f(x,u).$$
We prove a {\em maximum principle for anti-symmetric functions} and obtain
other key ingredients for carrying on the method of moving planes, such as {\em
a key boundary estimate lemma}. Then we establish radial symmetry and
monotonicity for positive solutions to semilinear equations involving the
fractional p-Laplacian in a unit ball and in the whole space. We believe that
the methods developed here can be applied to a variety of problems involving
nonlinear nonlocal operators.
",Mathematics,Mathematics
"Learning causal Bayes networks using interventional path queries in polynomial time and sample complexity   Causal discovery from empirical data is a fundamental problem in many
scientific domains. Observational data allows for identifiability only up to
Markov equivalence class. In this paper we first propose a polynomial time
algorithm for learning the exact correctly-oriented structure of the transitive
reduction of any causal Bayesian networks with high probability, by using
interventional path queries. Each path query takes as input an origin node and
a target node, and answers whether there is a directed path from the origin to
the target. This is done by intervening the origin node and observing samples
from the target node. We theoretically show the logarithmic sample complexity
for the size of interventional data per path query, for continuous and discrete
networks. We further extend our work to learn the transitive edges using
logarithmic sample complexity (albeit in time exponential in the maximum number
of parents for discrete networks). This allows us to learn the full network. We
also provide an analysis of imperfect interventions.
",Computer Science; Statistics,Computer Science; Statistics
"A practical guide to the simultaneous determination of protein structure and dynamics using metainference   Accurate protein structural ensembles can be determined with metainference, a
Bayesian inference method that integrates experimental information with prior
knowledge of the system and deals with all sources of uncertainty and errors as
well as with system heterogeneity. Furthermore, metainference can be
implemented using the metadynamics approach, which enables the computational
study of complex biological systems requiring extensive conformational
sampling. In this chapter, we provide a step-by-step guide to perform and
analyse metadynamic metainference simulations using the ISDB module of the
open-source PLUMED library, as well as a series of practical tips to avoid
common mistakes. Specifically, we will guide the reader in the process of
learning how to model the structural ensemble of a small disordered peptide by
combining state-of-the-art molecular mechanics force fields with nuclear
magnetic resonance data, including chemical shifts, scalar couplings and
residual dipolar couplings.
",Quantitative Biology,Physics
"On indirect noise in multicomponent nozzle flows   A one-dimensional, unsteady nozzle flow is modelled to identify the sources
of indirect noise in multicomponent gases. First, from non-equilibrium
thermodynamics relations, it is shown that a compositional inhomogeneity
advected in an accelerating flow is a source of sound induced by
inhomogeneities in the mixture (i) chemical potentials and (ii) specific heat
capacities. Second, it is shown that the acoustic, entropy and compositional
linear perturbations evolve independently from each other and they become
coupled through mean-flow gradients and/or at the boundaries. Third, the
equations are cast in invariant formulation and a mathematical solution is
found by asymptotic expansion of path-ordered integrals with an infinite radius
of convergence. Finally, the transfer functions are calculated for a supersonic
nozzle with finite spatial extent perturbed by a methane-air compositional
inhomogeneity. The proposed framework will help identify and quantify the
sources of sound in nozzles with relevance, for example, to aeronautical gas
turbines.
",Physics,Physics
"A parity-breaking electronic nematic phase transition in the spin-orbit coupled metal Cd$_2$Re$_2$O$_7$   Strong electron interactions can drive metallic systems toward a variety of
well-known symmetry-broken phases, but the instabilities of correlated metals
with strong spin-orbit coupling have only recently begun to be explored. We
uncovered a multipolar nematic phase of matter in the metallic pyrochlore
Cd$_2$Re$_2$O$_7$ using spatially resolved second-harmonic optical anisotropy
measurements. Like previously discovered electronic nematic phases, this
multipolar phase spontaneously breaks rotational symmetry while preserving
translational invariance. However, it has the distinguishing property of being
odd under spatial inversion, which is allowed only in the presence of
spin-orbit coupling. By examining the critical behavior of the multipolar
nematic order parameter, we show that it drives the thermal phase transition
near 200 kelvin in Cd$_2$Re$_2$O$_7$ and induces a parity-breaking lattice
distortion as a secondary order.
",Physics,Physics
"Complete Classification of Generalized Santha-Vazirani Sources   Let $\mathcal{F}$ be a finite alphabet and $\mathcal{D}$ be a finite set of
distributions over $\mathcal{F}$. A Generalized Santha-Vazirani (GSV) source of
type $(\mathcal{F}, \mathcal{D})$, introduced by Beigi, Etesami and Gohari
(ICALP 2015, SICOMP 2017), is a random sequence $(F_1, \dots, F_n)$ in
$\mathcal{F}^n$, where $F_i$ is a sample from some distribution $d \in
\mathcal{D}$ whose choice may depend on $F_1, \dots, F_{i-1}$.
We show that all GSV source types $(\mathcal{F}, \mathcal{D})$ fall into one
of three categories: (1) non-extractable; (2) extractable with error
$n^{-\Theta(1)}$; (3) extractable with error $2^{-\Omega(n)}$. This rules out
other error rates like $1/\log n$ or $2^{-\sqrt{n}}$.
We provide essentially randomness-optimal extraction algorithms for
extractable sources. Our algorithm for category (2) sources extracts with error
$\varepsilon$ from $n = \mathrm{poly}(1/\varepsilon)$ samples in time linear in
$n$. Our algorithm for category (3) sources extracts $m$ bits with error
$\varepsilon$ from $n = O(m + \log 1/\varepsilon)$ samples in time
$\min\{O(nm2^m),n^{O(\lvert\mathcal{F}\rvert)}\}$.
We also give algorithms for classifying a GSV source type $(\mathcal{F},
\mathcal{D})$: Membership in category (1) can be decided in $\mathrm{NP}$,
while membership in category (3) is polynomial-time decidable.
",Computer Science,Computer Science
"Personal Food Computer: A new device for controlled-environment agriculture   Due to their interdisciplinary nature, devices for controlled-environment
agriculture have the possibility to turn into ideal tools not only to conduct
research on plant phenology but also to create curricula in a wide range of
disciplines. Controlled-environment devices are increasing their
functionalities as well as improving their accessibility. Traditionally,
building one of these devices from scratch implies knowledge in fields such as
mechanical engineering, digital electronics, programming, and energy
management. However, the requirements of an effective controlled environment
device for personal use brings new constraints and challenges. This paper
presents the OpenAg Personal Food Computer (PFC); a low cost desktop size
platform, which not only targets plant phenology researchers but also
hobbyists, makers, and teachers from elementary to high-school levels (K-12).
The PFC is completely open-source and it is intended to become a tool that can
be used for collective data sharing and plant growth analysis. Thanks to its
modular design, the PFC can be used in a large spectrum of activities.
",Computer Science,Computer Science
"An aptamer-biosensor for azole class antifungal drugs   This report describes the development of an aptamer for sensing azole
antifungal drugs for therapeutic drug monitoring. Modified Synthetic Evolution
of Ligands through Exponential Enrichment (SELEX) was used to discover a DNA
aptamer recognizing azole class antifungal drugs. This aptamer undergoes a
secondary structural change upon binding to its target molecule as shown
through fluorescence anisotropy-based binding measurements. Experiments using
circular dichroism spectroscopy, revealed a unique double G-quadruplex
structure that was essential and specific for binding to the azole antifungal
target. Aptamer-functionalized Graphene Field Effect Transistor (GFET) devices
were created and used to measure the binding of strength of azole antifungals
to this surface. In total this aptamer and the supporting sensing platform
could provide a valuable tool for improving the treatment of patients with
invasive fungal infections.
",Physics,Quantitative Biology
"Counting Multiplicities in a Hypersurface over a Number Field   We fix a counting function of multiplicities of algebraic points in a
projective hypersurface over a number field, and take the sum over all
algebraic points of bounded height and fixed degree. An upper bound for the sum
with respect to this counting function will be given in terms of the degree of
the hypersurface, the dimension of the singular locus, the upper bounds of
height, and the degree of the field of definition.
",Mathematics,Mathematics
"CosmoGAN: creating high-fidelity weak lensing convergence maps using Generative Adversarial Networks   Inferring model parameters from experimental data is a grand challenge in
many sciences, including cosmology. This often relies critically on high
fidelity numerical simulations, which are prohibitively computationally
expensive. The application of deep learning techniques to generative modeling
is renewing interest in using high dimensional density estimators as
computationally inexpensive emulators of fully-fledged simulations. These
generative models have the potential to make a dramatic shift in the field of
scientific simulations, but for that shift to happen we need to study the
performance of such generators in the precision regime needed for science
applications. To this end, in this work we apply Generative Adversarial
Networks to the problem of generating weak lensing convergence maps. We show
that our generator network produces maps that are described by, with high
statistical confidence, the same summary statistics as the fully simulated
maps.
",Computer Science; Physics,Physics
"Structural Analysis and Optimal Design of Distributed System Throttlers   In this paper, we investigate the performance analysis and synthesis of
distributed system throttlers (DST). A throttler is a mechanism that limits the
flow rate of incoming metrics, e.g., byte per second, network bandwidth usage,
capacity, traffic, etc. This can be used to protect a service's backend/clients
from getting overloaded, or to reduce the effects of uncertainties in demand
for shared services. We study performance deterioration of DSTs subject to
demand uncertainty. We then consider network synthesis problems that aim to
improve the performance of noisy DSTs via communication link modifications as
well as server update cycle modifications.
",Computer Science; Mathematics,Computer Science
"Dependence and dependence structures: estimation and visualization using distance multivariance   Distance multivariance is a multivariate dependence measure, which can detect
dependencies between an arbitrary number of random vectors each of which can
have a distinct dimension. Here we discuss several new aspects and present a
concise overview. We relax the required moment conditions considerably and show
that distance multivariance unifies (and extends) distance covariance and the
Hilbert-Schmidt independence criterion HSIC, moreover also the classical linear
dependence measures: covariance, Pearson's correlation and the RV coefficient
appear as limiting cases. For measures based on distance multivariance the
corresponding resampling tests are introduced, and several related measures are
defined: a new multicorrelation which satisfies a natural set of multivariate
dependence measure axioms and $m$-multivariance which is a new dependence
measure yielding tests for pairwise independence and independence of higher
order. These tests are computationally feasible and under very mild moment
conditions they are consistent against all alternatives. Moreover, a general
visualization scheme for higher order dependencies is proposed.
Many illustrative examples are included. All functions for the use of
distance multivariance in applications are published in the R-package
'multivariance'.
",Mathematics; Statistics,Mathematics; Statistics
"High-speed X-ray imaging spectroscopy system with Zynq SoC for solar observations   We have developed a system combining a back-illuminated
Complementary-Metal-Oxide-Semiconductor (CMOS) imaging sensor and Xilinx Zynq
System-on-Chip (SoC) device for a soft X-ray (0.5-10 keV) imaging spectroscopy
observation of the Sun to investigate the dynamics of the solar corona. Because
typical timescales of energy release phenomena in the corona span a few minutes
at most, we aim to obtain the corresponding energy spectra and derive the
physical parameters, i.e., temperature and emission measure, every few tens of
seconds or less for future solar X-ray observations. An X-ray photon-counting
technique, with a frame rate of a few hundred frames per second or more, can
achieve such results. We used the Zynq SoC device to achieve the requirements.
Zynq contains an ARM processor core, which is also known as the Processing
System (PS) part, and a Programmable Logic (PL) part in a single chip. We use
the PL and PS to control the sensor and seamless recording of data to a storage
system, respectively. We aim to use the system for the third flight of the
Focusing Optics Solar X-ray Imager (FOXSI-3) sounding rocket experiment for the
first photon-counting X-ray imaging and spectroscopy of the Sun.
",Physics,Physics
"Concave losses for robust dictionary learning   Traditional dictionary learning methods are based on quadratic convex loss
function and thus are sensitive to outliers. In this paper, we propose a
generic framework for robust dictionary learning based on concave losses. We
provide results on composition of concave functions, notably regarding
super-gradient computations, that are key for developing generic dictionary
learning algorithms applicable to smooth and non-smooth losses. In order to
improve identification of outliers, we introduce an initialization heuristic
based on undercomplete dictionary learning. Experimental results using
synthetic and real data demonstrate that our method is able to better detect
outliers, is capable of generating better dictionaries, outperforming
state-of-the-art methods such as K-SVD and LC-KSVD.
",Computer Science; Statistics,Computer Science; Statistics
"Using deterministic approximations to accelerate SMC for posterior sampling   Sequential Monte Carlo has become a standard tool for Bayesian Inference of
complex models. This approach can be computationally demanding, especially when
initialized from the prior distribution. On the other hand, deter-ministic
approximations of the posterior distribution are often available with no
theoretical guaranties. We propose a bridge sampling scheme starting from such
a deterministic approximation of the posterior distribution and targeting the
true one. The resulting Shortened Bridge Sampler (SBS) relies on a sequence of
distributions that is determined in an adaptive way. We illustrate the
robustness and the efficiency of the methodology on a large simulation study.
When applied to network datasets, SBS inference leads to different statistical
conclusions from the one supplied by the standard variational Bayes
approximation.
",Statistics,Statistics
"EPTL - A temporal logic for weakly consistent systems   The high availability and scalability of weakly-consistent systems attracts
system designers. Yet, writing correct application code for this type of
systems is difficult; even how to specify the intended behavior of such systems
is still an open question. There has not been established any standard method
to specify the intended dynamic behavior of a weakly consistent system. There
exist specifications of various consistency models for distributed and
concurrent systems; and the semantics of replicated datatypes like CRDTs have
been specified in axiomatic and operational models based on visibility
relations.
In this paper, we present a temporal logic, EPTL, that is tailored to specify
properties of weakly consistent systems. In contrast to LTL and CTL, EPTL takes
into account that operations of weakly consistent systems are in many cases not
serializable and have to be treated respectively to capture the behavior. We
embed our temporal logic in Isabelle/HOL and can thereby leverage strong
semi-automatic proving capabilities.
",Computer Science,Computer Science
"Is Epicurus the father of Reinforcement Learning?   The Epicurean Philosophy is commonly thought as simplistic and hedonistic.
Here I discuss how this is a misconception and explore its link to
Reinforcement Learning. Based on the letters of Epicurus, I construct an
objective function for hedonism which turns out to be equivalent of the
Reinforcement Learning objective function when omitting the discount factor. I
then discuss how Plato and Aristotle 's views that can be also loosely linked
to Reinforcement Learning, as well as their weaknesses in relationship to it.
Finally, I emphasise the close affinity of the Epicurean views and the Bellman
equation.
",Computer Science; Statistics,Mathematics; Statistics
"Hidden order and symmetry protected topological states in quantum link ladders   We show that whereas spin-1/2 one-dimensional U(1) quantum-link models (QLMs)
are topologically trivial, when implemented in ladder-like lattices these
models may present an intriguing ground-state phase diagram, which includes a
symmetry protected topological (SPT) phase that may be readily revealed by
analyzing long-range string spin correlations along the ladder legs. We propose
a simple scheme for the realization of spin-1/2 U(1) QLMs based on
single-component fermions loaded in an optical lattice with s- and p-bands,
showing that the SPT phase may be experimentally realized by adiabatic
preparation.
",Physics,Physics
"Electronic characteristics of ultrathin SrRuO$_3$ films and their relationship with the metal$-$insulator transition   SrRuO$_3$ (SRO) films are known to exhibit insulating behavior as their
thickness approaches four unit cells. We employ electron energy$-$loss (EEL)
spectroscopy to probe the spatially resolved electronic structures of both
insulating and conducting SRO to correlate them with the metal$-$insulator
transition (MIT). Importantly, the central layer of the ultrathin insulating
film exhibits distinct features from the metallic SRO. Moreover, EEL near edge
spectra adjacent to the SrTiO$_3$ (STO) substrate or to the capping layer are
remarkably similar to those of STO. The site$-$projected density of states
based on density functional theory (DFT) partially reflects the characteristics
of the spectra of these layers. These results may provide important information
on the possible influence of STO on the electronic states of ultrathin SRO.
",Physics,Physics
"Endpoint Sobolev and BV continuity for maximal operators   In this paper we investigate some questions related to the continuity of
maximal operators in $W^{1,1}$ and $BV$ spaces, complementing some well-known
boundedness results. Letting $\widetilde M$ be the one-dimensional uncentered
Hardy-Littlewood maximal operator, we prove that the map $f \mapsto
\big(\widetilde Mf\big)'$ is continuous from $W^{1,1}(\mathbb{R})$ to
$L^1(\mathbb{R})$. In the discrete setting, we prove that $\widetilde M:
BV(\mathbb{Z}) \to BV(\mathbb{Z})$ is also continuous. For the one-dimensional
fractional Hardy-Littlewood maximal operator, we prove by means of
counterexamples that the corresponding continuity statements do not hold, both
in the continuous and discrete settings, and for the centered and uncentered
versions.
",Mathematics,Mathematics
"Calculation of thallium hyperfine anomaly   We suggest a method to calculate hyperfine anomaly for many-electron atoms
and ions. At first, we tested this method by calculating hyperfine anomaly for
hydrogen-like thallium ion and obtained fairly good agreement with analytical
expressions. Then we did calculations for the neutral thallium and tested an
assumption, that the the ratio between the anomalies for $s$ and $p_{1/2}$
states is the same for these two systems. Finally, we come up with
recommendations about preferable atomic states for the precision measurements
of the nuclear $g$ factors.
",Physics,Physics
"Lectures on the mean values of functionals -- An elementary introduction to infinite-dimensional probability   This is an elementary introduction to infinite-dimensional probability. In
the lectures, we compute the exact mean values of some functionals on C[0,1]
and L[0,1] by considering these functionals as infinite-dimensional random
variables. The results show that there exist the complete concentration of
measure phenomenon for these mean values since the variances are all zeroes.
",Physics; Mathematics; Statistics,Mathematics; Statistics
"Bias Correction For Paid Search In Media Mix Modeling   Evaluating the return on ad spend (ROAS), the causal effect of advertising on
sales, is critical to advertisers for understanding the performance of their
existing marketing strategy as well as how to improve and optimize it. Media
Mix Modeling (MMM) has been used as a convenient analytical tool to address the
problem using observational data. However it is well recognized that MMM
suffers from various fundamental challenges: data collection, model
specification and selection bias due to ad targeting, among others
\citep{chan2017,wolfe2016}.
In this paper, we study the challenge associated with measuring the impact of
search ads in MMM, namely the selection bias due to ad targeting. Using causal
diagrams of the search ad environment, we derive a statistically principled
method for bias correction based on the \textit{back-door} criterion
\citep{pearl2013causality}. We use case studies to show that the method
provides promising results by comparison with results from randomized
experiments. We also report a more complex case study where the advertiser had
spent on more than a dozen media channels but results from a randomized
experiment are not available. Both our theory and empirical studies suggest
that in some common, practical scenarios, one may be able to obtain an
approximately unbiased estimate of search ad ROAS.
",Statistics,Computer Science
"Modeling of nonlinear audio effects with end-to-end deep neural networks   In the context of music production, distortion effects are mainly used for
aesthetic reasons and are usually applied to electric musical instruments. Most
existing methods for nonlinear modeling are often either simplified or
optimized to a very specific circuit. In this work, we investigate deep
learning architectures for audio processing and we aim to find a general
purpose end-to-end deep neural network to perform modeling of nonlinear audio
effects. We show the network modeling various nonlinearities and we discuss the
generalization capabilities among different instruments.
",Computer Science,Computer Science; Statistics
"An Application of $h$-principle to Manifold Calculus   Manifold calculus is a form of functor calculus that analyzes contravariant
functors from some categories of manifolds to topological spaces by providing
analytic approximations to them. In this paper we apply the theory of
h-principle to construct several examples of analytic functors in this sense.
We prove that the analytic approximation of the Lagrangian embeddings functor
$\mathrm{emb}_{\mathrm{Lag}}(-,N)$ is the totally real embeddings functor
$\mathrm{emb}_{\mathrm{TR}}(-,N)$. Under certain conditions we provide a
geometric construction for the homotopy fiber of $ \mathrm{emb}(M,N)
\rightarrow \mathrm{imm}(M,N)$. This construction also provides an example of a
functor which is itself empty when evaluated on most manifolds but it's
analytic approximation is almost always non-empty.
",Mathematics,Mathematics
"GraphCombEx: A Software Tool for Exploration of Combinatorial Optimisation Properties of Large Graphs   We present a prototype of a software tool for exploration of multiple
combinatorial optimisation problems in large real-world and synthetic complex
networks. Our tool, called GraphCombEx (an acronym of Graph Combinatorial
Explorer), provides a unified framework for scalable computation and
presentation of high-quality suboptimal solutions and bounds for a number of
widely studied combinatorial optimisation problems. Efficient representation
and applicability to large-scale graphs and complex networks are particularly
considered in its design. The problems currently supported include maximum
clique, graph colouring, maximum independent set, minimum vertex clique
covering, minimum dominating set, as well as the longest simple cycle problem.
Suboptimal solutions and intervals for optimal objective values are estimated
using scalable heuristics. The tool is designed with extensibility in mind,
with the view of further problems and both new fast and high-performance
heuristics to be added in the future. GraphCombEx has already been successfully
used as a support tool in a number of recent research studies using
combinatorial optimisation to analyse complex networks, indicating its promise
as a research software tool.
",Computer Science,Computer Science
"Performance study of SKIROC2 and SKIROC2A with BGA testboard   SKIROC2 is an ASIC to readout the silicon pad detectors for the
electromagnetic calorimeter in the International Linear Collider.
Characteristics of SKIROC2 and the new version of SKIROC2A, packaged with BGA,
are measured with testboards and charge injection. The results on the
signal-to-noise ratio of both trigger and ADC output, threshold tuning
capability and timing resolution are presented.
",Physics,Physics
"Modularity Matters: Learning Invariant Relational Reasoning Tasks   We focus on two supervised visual reasoning tasks whose labels encode a
semantic relational rule between two or more objects in an image: the MNIST
Parity task and the colorized Pentomino task. The objects in the images undergo
random translation, scaling, rotation and coloring transformations. Thus these
tasks involve invariant relational reasoning. We report uneven performance of
various deep CNN models on these two tasks. For the MNIST Parity task, we
report that the VGG19 model soundly outperforms a family of ResNet models.
Moreover, the family of ResNet models exhibits a general sensitivity to random
initialization for the MNIST Parity task. For the colorized Pentomino task, now
both the VGG19 and ResNet models exhibit sluggish optimization and very poor
test generalization, hovering around 30% test error. The CNN we tested all
learn hierarchies of fully distributed features and thus encode the distributed
representation prior. We are motivated by a hypothesis from cognitive
neuroscience which posits that the human visual cortex is modularized, and this
allows the visual cortex to learn higher order invariances. To this end, we
consider a modularized variant of the ResNet model, referred to as a Residual
Mixture Network (ResMixNet) which employs a mixture-of-experts architecture to
interleave distributed representations with more specialized, modular
representations. We show that very shallow ResMixNets are capable of learning
each of the two tasks well, attaining less than 2% and 1% test error on the
MNIST Parity and the colorized Pentomino tasks respectively. Most importantly,
the ResMixNet models are extremely parameter efficient: generalizing better
than various non-modular CNNs that have over 10x the number of parameters.
These experimental results support the hypothesis that modularity is a robust
prior for learning invariant relational reasoning.
",Statistics; Quantitative Biology,Computer Science; Statistics
"Search Intelligence: Deep Learning For Dominant Category Prediction   Deep Neural Networks, and specifically fully-connected convolutional neural
networks are achieving remarkable results across a wide variety of domains.
They have been trained to achieve state-of-the-art performance when applied to
problems such as speech recognition, image classification, natural language
processing and bioinformatics. Most of these deep learning models when applied
to classification employ the softmax activation function for prediction and aim
to minimize cross-entropy loss. In this paper, we have proposed a supervised
model for dominant category prediction to improve search recall across all eBay
classifieds platforms. The dominant category label for each query in the last
90 days is first calculated by summing the total number of collaborative clicks
among all categories. The category having the highest number of collaborative
clicks for the given query will be considered its dominant category. Second,
each query is transformed to a numeric vector by mapping each unique word in
the query document to a unique integer value; all padded to equal length based
on the maximum document length within the pre-defined vocabulary size. A
fully-connected deep convolutional neural network (CNN) is then applied for
classification. The proposed model achieves very high classification accuracy
compared to other state-of-the-art machine learning techniques.
",Computer Science; Statistics,Computer Science
"A Semantic Cross-Species Derived Data Management Application   Managing dynamic information in large multi-site, multi-species, and
multi-discipline consortia is a challenging task for data management
applications. Often in academic research studies the goals for informatics
teams are to build applications that provide extract-transform-load (ETL)
functionality to archive and catalog source data that has been collected by the
research teams. In consortia that cross species and methodological or
scientific domains, building interfaces that supply data in a usable fashion
and make intuitive sense to scientists from dramatically different backgrounds
increases the complexity for developers. Further, reusing source data from
outside one's scientific domain is fraught with ambiguities in understanding
the data types, analysis methodologies, and how to combine the data with those
from other research teams. We report on the design, implementation, and
performance of a semantic data management application to support the NIMH
funded Conte Center at the University of California, Irvine. The Center is
testing a theory of the consequences of ""fragmented"" (unpredictable, high
entropy) early-life experiences on adolescent cognitive and emotional outcomes
in both humans and rodents. It employs cross-species neuroimaging, epigenomic,
molecular, and neuroanatomical approaches in humans and rodents to assess the
potential consequences of fragmented unpredictable experience on brain
structure and circuitry. To address this multi-technology, multi-species
approach, the system uses semantic web techniques based on the Neuroimaging
Data Model (NIDM) to facilitate data ETL functionality. We find this approach
enables a low-cost, easy to maintain, and semantically meaningful information
management system, enabling the diverse research teams to access and use the
data.
",Computer Science,Computer Science
"FAST Adaptive Smoothing and Thresholding for Improved Activation Detection in Low-Signal fMRI   Functional Magnetic Resonance Imaging is a noninvasive tool used to study
brain function. Detecting activation is challenged by many factors, and even
more so in low-signal scenarios that arise in the performance of high-level
cognitive tasks. We provide a fully automated and fast adaptive smoothing and
thresholding (FAST) algorithm that uses smoothing and extreme value theory on
correlated statistical parametric maps for thresholding. Performance on
experiments spanning a range of low-signal settings is very encouraging. The
methodology also performs well in a study to identify the cerebral regions that
perceive only-auditory-reliable and only-visual-reliable speech stimuli as well
as those that perceive one but not the other.
",Mathematics; Statistics,Statistics
"Improving Sharir and Welzl's bound on crossing-free matchings through solving a stronger recurrence   Sharir and Welzl [1] derived a bound on crossing-free matchings primarily
based on solving a recurrence based on the size of the matchings. We show that
the recurrence given in Lemma 2.3 in Sharir and Welzl can be improve to
$(2n-6s)\textbf{Ma}_{m}(P)\leq\frac{68}{3}(s+2)\textbf{Ma}_{m-1}(P)$ and
$(3n-7s)\textbf{Ma}_{m}(P)\leq44.5(s+2)\textbf{Ma}_{m-1}(P)$, thereby improving
the upper bound for crossing-free matchings.
",Mathematics,Mathematics
"Revealing patterns in HIV viral load data and classifying patients via a novel machine learning cluster summarization method   HIV RNA viral load (VL) is an important outcome variable in studies of HIV
infected persons. There exists only a handful of methods which classify
patients by viral load patterns. Most methods place limits on the use of viral
load measurements, are often specific to a particular study design, and do not
account for complex, temporal variation. To address this issue, we propose a
set of four unambiguous computable characteristics (features) of time-varying
HIV viral load patterns, along with a novel centroid-based classification
algorithm, which we use to classify a population of 1,576 HIV positive clinic
patients into one of five different viral load patterns (clusters) often found
in the literature: durably suppressed viral load (DSVL), sustained low viral
load (SLVL), sustained high viral load (SHVL), high viral load suppression
(HVLS), and rebounding viral load (RVL). The centroid algorithm summarizes
these clusters in terms of their centroids and radii. We show that this allows
new viral load patterns to be assigned pattern membership based on the distance
from the centroid relative to its radius, which we term radial normalization
classification. This method has the benefit of providing an objective and
quantitative method to assign viral load pattern membership with a concise and
interpretable model that aids clinical decision making. This method also
facilitates meta-analyses by providing computably distinct HIV categories.
Finally we propose that this novel centroid algorithm could also be useful in
the areas of cluster comparison for outcomes research and data reduction in
machine learning.
",Statistics; Quantitative Biology,Statistics
"Optimal Regulation Response of Batteries Under Cycle Aging Mechanisms   When providing frequency regulation in a pay-for-performance market,
batteries need to carefully balance the trade-off between following regulation
signals and their degradation costs in real-time. Existing battery control
strategies either do not consider mismatch penalties in pay-for-performance
markets, or cannot accurately account for battery cycle aging mechanism during
operation. This paper derives an online control policy that minimizes a battery
owner's operating cost for providing frequency regulation in a
pay-for-performance market. The proposed policy considers an accurate
electrochemical battery cycle aging model, and is applicable to most types of
battery cells. It has a threshold structure, and achieves near-optimal
performance with respect to an offline controller that has complete future
information. We explicitly characterize this gap and show it is independent of
the duration of operation. Simulation results with both synthetic and real
regulation traces are conducted to illustrate the theoretical results.
",Mathematics,Computer Science
"Local Feature Descriptor Learning with Adaptive Siamese Network   Although the recent progress in the deep neural network has led to the
development of learnable local feature descriptors, there is no explicit answer
for estimation of the necessary size of a neural network. Specifically, the
local feature is represented in a low dimensional space, so the neural network
should have more compact structure. The small networks required for local
feature descriptor learning may be sensitive to initial conditions and learning
parameters and more likely to become trapped in local minima. In order to
address the above problem, we introduce an adaptive pruning Siamese
Architecture based on neuron activation to learn local feature descriptors,
making the network more computationally efficient with an improved recognition
rate over more complex networks. Our experiments demonstrate that our learned
local feature descriptors outperform the state-of-art methods in patch
matching.
",Computer Science; Statistics,Computer Science; Statistics
"Vandermonde Matrices with Nodes in the Unit Disk and the Large Sieve   We derive bounds on the extremal singular values and the condition number of
NxK, with N>=K, Vandermonde matrices with nodes in the unit disk. The
mathematical techniques we develop to prove our main results are inspired by a
link---first established by by Selberg [1] and later extended by Moitra
[2]---between the extremal singular values of Vandermonde matrices with nodes
on the unit circle and large sieve inequalities. Our main conceptual
contribution lies in establishing a connection between the extremal singular
values of Vandermonde matrices with nodes in the unit disk and a novel large
sieve inequality involving polynomials in z \in C with |z|<=1. Compared to
Bazán's upper bound on the condition number [3], which, to the best of our
knowledge, constitutes the only analytical result---available in the
literature---on the condition number of Vandermonde matrices with nodes in the
unit disk, our bound not only takes a much simpler form, but is also sharper
for certain node configurations. Moreover, the bound we obtain can be evaluated
consistently in a numerically stable fashion, whereas the evaluation of
Bazán's bound requires the solution of a linear system of equations which has
the same condition number as the Vandermonde matrix under consideration and can
therefore lead to numerical instability in practice. As a byproduct, our
result---when particularized to the case of nodes on the unit circle---slightly
improves upon the Selberg-Moitra bound.
",Computer Science; Mathematics,Mathematics
"Separation of the charge density wave and superconducting states by an intermediate semimetal phase in pressurized TaTe2   In layered transition metal dichalcogenides (LTMDCs) that display both charge
density waves (CDWs) and superconductivity, the superconducting state generally
emerges directly on suppression of the CDW state. Here, however, we report a
different observation for pressurized TaTe2, a non-superconducting CDW-bearing
LTMDC at ambient pressure. We find that a superconducting state does not occur
in TaTe2 after the full suppression of its CDW state, which we observe at about
3 GPa, but, rather, a non-superconducting semimetal state is observed. At a
higher pressure, ~21 GPa, where both the semimetal state and the corresponding
positive magnetoresistance effect are destroyed, superconductivity finally
emerges and remains present up to ~50 GPa, the high pressure limit of our
measurements. Our pressure-temperature phase diagram for TaTe2 demonstrates
that the CDW and the superconducting phases in TaTe2 do not directly transform
one to the other, but rather are separated by a semimetal state, - the first
experimental case where the CDW and superconducting states are separated by an
intermediate phase in LTMDC systems.
",Physics,Physics
"Measuring High-Energy Spectra with HAWC   The High-Altitude Water-Cherenkov (HAWC) experiment is a TeV $\gamma$-ray
observatory located \unit[4100]{m} above sea level on the Sierra Negra mountain
in Puebla, Mexico. The detector consists of 300 water-filled tanks, each
instrumented with 4 photomultiplier tubes that utilize the water-Cherenkov
technique to detect atmospheric air showers produced by cosmic $\gamma$ rays.
Construction of HAWC was completed in March of 2015. The experiment's wide
instantaneous field of view (\unit[2]{sr}) and high duty cycle (> 95\%) make it
a powerful survey instrument sensitive to pulsars, supernova remnants, and
other $\gamma$-ray sources. The mechanisms of particle acceleration at these
sources can be studied by analyzing their high-energy spectra. To this end, we
have developed an event-by-event energy-reconstruction algorithm using an
artificial neural network to estimate energies of primary $\gamma$ rays at
HAWC. We will present the details of this technique and its performance as well
as the current progress toward using it to measure energy spectra of
$\gamma$-ray sources.
",Physics,Physics
"AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks   New types of machine learning hardware in development and entering the market
hold the promise of revolutionizing deep learning in a manner as profound as
GPUs. However, existing software frameworks and training algorithms for deep
learning have yet to evolve to fully leverage the capability of the new wave of
silicon. We already see the limitations of existing algorithms for models that
exploit structured input via complex and instance-dependent control flow, which
prohibits minibatching. We present an asynchronous model-parallel (AMP)
training algorithm that is specifically motivated by training on networks of
interconnected devices. Through an implementation on multi-core CPUs, we show
that AMP training converges to the same accuracy as conventional synchronous
training algorithms in a similar number of epochs, but utilizes the available
hardware more efficiently even for small minibatch sizes, resulting in
significantly shorter overall training times. Our framework opens the door for
scaling up a new class of deep learning models that cannot be efficiently
trained today.
",Computer Science; Statistics,Computer Science; Statistics
"ELDAR, a new method to identify AGN in multi-filter surveys: the ALHAMBRA test-case   We present ELDAR, a new method that exploits the potential of medium- and
narrow-band filter surveys to securely identify active galactic nuclei (AGN)
and determine their redshifts. Our methodology improves on traditional
approaches by looking for AGN emission lines expected to be identified against
the continuum, thanks to the width of the filters. To assess its performance,
we apply ELDAR to the data of the ALHAMBRA survey, which covered an effective
area of $2.38\,{\rm deg}^2$ with 20 contiguous medium-band optical filters down
to F814W$\simeq 24.5$. Using two different configurations of ELDAR in which we
require the detection of at least 2 and 3 emission lines, respectively, we
extract two catalogues of type-I AGN. The first is composed of 585 sources
($79\,\%$ of them spectroscopically-unknown) down to F814W$=22.5$ at $z_{\rm
phot}>1$, which corresponds to a surface density of $209\,{\rm deg}^{-2}$. In
the second, the 494 selected sources ($83\,\%$ of them
spectroscopically-unknown) reach F814W$=23$ at $z_{\rm phot}>1.5$, for a
corresponding number density of $176\,{\rm deg}^{-2}$. Then, using samples of
spectroscopically-known AGN in the ALHAMBRA fields, for the two catalogues we
estimate a completeness of $73\,\%$ and $67\,\%$, and a redshift precision of
$1.01\,\%$ and $0.86\,\%$ (with outliers fractions of $8.1\,\%$ and $5.8\,\%$).
At $z>2$, where our selection performs best, we reach $85\,\%$ and $77\,\%$
completeness and we find no contamination from galaxies.
",Physics,Physics
"Non-cocompact Group Actions and $π_1$-Semistability at Infinity   A finitely presented 1-ended group $G$ has {\it semistable fundamental group
at infinity} if $G$ acts geometrically on a simply connected and locally
compact ANR $Y$ having the property that any two proper rays in $Y$ are
properly homotopic. This property of $Y$ captures a notion of connectivity at
infinity stronger than ""1-ended"", and is in fact a feature of $G$, being
independent of choices. It is a fundamental property in the homotopical study
of finitely presented groups. While many important classes of groups have been
shown to have semistable fundamental group at infinity, the question of whether
every $G$ has this property has been a recognized open question for nearly
forty years. In this paper we attack the problem by considering a proper {\it
but non-cocompact} action of a group $J$ on such an $Y$. This $J$ would
typically be a subgroup of infinite index in the geometrically acting
over-group $G$; for example $J$ might be infinite cyclic or some other subgroup
whose semistability properties are known. We divide the semistability property
of $G$ into a $J$-part and a ""perpendicular to $J$"" part, and we analyze how
these two parts fit together. Among other things, this analysis leads to a
proof (in a companion paper) that a class of groups previously considered to be
likely counter examples do in fact have the semistability property.
",Mathematics,Mathematics
"Non-Ergodic Delocalization in the Rosenzweig-Porter Model   We consider the Rosenzweig-Porter model $H = V + \sqrt{T}\, \Phi$, where $V$
is a $N \times N$ diagonal matrix, $\Phi$ is drawn from the $N \times N$
Gaussian Orthogonal Ensemble, and $N^{-1} \ll T \ll 1$. We prove that the
eigenfunctions of $H$ are typically supported in a set of approximately $NT$
sites, thereby confirming the existence of a previously conjectured non-ergodic
delocalized phase. Our proof is based on martingale estimates along the
characteristic curves of the stochastic advection equation satisfied by the
local resolvent of the Brownian motion representation of $H$.
",Physics,Mathematics
"A Grouping Genetic Algorithm for Joint Stratification and Sample Allocation Designs   Predicting the cheapest sample size for the optimal stratification in
multivariate survey design is a problem in cases where the population frame is
large. A solution exists that iteratively searches for the minimum sample size
necessary to meet accuracy constraints in partitions of atomic strata created
by the Cartesian product of auxiliary variables into larger strata. The optimal
stratification can be found by testing all possible partitions. However the
number of possible partitions grows exponentially with the number of initial
strata. There are alternative ways of modelling this problem, one of the most
natural is using Genetic Algorithms (GA). These evolutionary algorithms use
recombination, mutation and selection to search for optimal solutions. They
often converge on optimal or near-optimal solution more quickly than exact
methods. We propose a new GA approach to this problem using grouping genetic
operators instead of traditional operators. The results show a significant
improvement in solution quality for similar computational effort, corresponding
to large monetary savings.
",Statistics,Statistics
"Axion detection via Topological Casimir Effect   We propose a new table-top experimental configuration for the direct
detection of dark matter axions with mass in the $(10^{-6} \rm eV - 10^{-2} \rm
eV)$ range using non-perturbative effects in a system with non-trivial spatial
topology. Different from most experimental setups found in literature on direct
dark matter axion detection, which relies on $\dot{\theta}$ or
$\vec{\nabla}\theta$, we found that our system is in principle sensitive to a
static $\theta\geq 10^{-14}$ and can also be used to set limit on the
fundamental constant $\theta_{\rm QED}$ which becomes the fundamental
observable parameter of the Maxwell system if some conditions are met.
Connection with Witten effect when the induced electric charge $e'$ is
proportional to $\theta$ and the magnetic monopole becomes the dyon with
non-vanishing $e'=-e \frac{\theta}{2\pi}$ is also discussed.
",Physics,Physics
"Convergence of ground state solutions for nonlinear Schrödinger equations on graphs   We consider the nonlinear Schrödinger equation $-\Delta u+(\lambda
a(x)+1)u=|u|^{p-1}u$ on a locally finite graph $G=(V,E)$. We prove via the
Nehari method that if $a(x)$ satisfies certain assumptions, for any
$\lambda>1$, the equation admits a ground state solution $u_\lambda$. Moreover,
as $\lambda\rightarrow \infty$, the solution $u_\lambda$ converges to a
solution of the Dirichlet problem $-\Delta u+u=|u|^{p-1}u$ which is defined on
the potential well $\Omega$. We also provide a numerical experiment which
solves the equation on a finite graph to illustrate our results.
",Mathematics,Mathematics
"Noncommutative Knörrer type equivalences via noncommutative resolutions of singularities   We construct Knörrer type equivalences outside of the hypersurface case,
namely, between singularity categories of cyclic quotient surface singularities
and certain finite dimensional local algebras. This generalises Knörrer's
equivalence for singularities of Dynkin type A (between Krull dimensions $2$
and $0$) and yields many new equivalences between singularity categories of
finite dimensional algebras.
Our construction uses noncommutative resolutions of singularities, relative
singularity categories, and an idea of Hille & Ploog yielding strongly
quasi-hereditary algebras which we describe explicitly by building on Wemyss's
work on reconstruction algebras. Moreover, K-theory gives obstructions to
generalisations of our main result.
",Mathematics,Mathematics
"Real-Time Adaptive Image Compression   We present a machine learning-based approach to lossy image compression which
outperforms all existing codecs, while running in real-time.
Our algorithm typically produces files 2.5 times smaller than JPEG and JPEG
2000, 2 times smaller than WebP, and 1.7 times smaller than BPG on datasets of
generic images across all quality levels. At the same time, our codec is
designed to be lightweight and deployable: for example, it can encode or decode
the Kodak dataset in around 10ms per image on GPU.
Our architecture is an autoencoder featuring pyramidal analysis, an adaptive
coding module, and regularization of the expected codelength. We also
supplement our approach with adversarial training specialized towards use in a
compression setting: this enables us to produce visually pleasing
reconstructions for very low bitrates.
",Computer Science; Statistics,Computer Science
"Comparison of the h-index for Different Fields of Research Using Bootstrap Methodology   An important disadvantage of the h-index is that typically it cannot take
into account the specific field of research of a researcher. Usually sample
point estimates of the average and median h-index values for the various fields
are reported that are highly variable and dependent of the specific samples and
it would be useful to provide confidence intervals of prediction accuracy. In
this paper we apply the non-parametric bootstrap technique for constructing
confidence intervals for the h-index for different fields of research. In this
way no specific assumptions about the distribution of the empirical hindex are
required as well as no large samples since that the methodology is based on
resampling from the initial sample. The results of the analysis showed
important differences between the various fields. The performance of the
bootstrap intervals for the mean and median h-index for most fields seems to be
rather satisfactory as revealed by the performed simulation.
",Computer Science; Statistics,Statistics
"Ultracold atoms in multiple-radiofrequency dressed adiabatic potentials   We present the first experimental demonstration of a multiple-radiofrequency
dressed potential for the configurable magnetic confinement of ultracold atoms.
We load cold $^{87}$Rb atoms into a double well potential with an adjustable
barrier height, formed by three radiofrequencies applied to atoms in a static
quadrupole magnetic field. Our multiple-radiofrequency approach gives precise
control over the double well characteristics, including the depth of individual
wells and the height of the barrier, and enables reliable transfer of atoms
between the available trapping geometries. We have characterised the
multiple-radiofrequency dressed system using radiofrequency spectroscopy,
finding good agreement with the eigenvalues numerically calculated using
Floquet theory. This method creates trapping potentials that can be
reconfigured by changing the amplitudes, polarizations and frequencies of the
applied dressing fields, and easily extended with additional dressing
frequencies.
",Physics,Physics
"Phase locking the spin precession in a storage ring   This letter reports the successful use of feedback from a spin polarization
measurement to the revolution frequency of a 0.97 GeV/$c$ bunched and polarized
deuteron beam in the Cooler Synchrotron (COSY) storage ring in order to control
both the precession rate ($\approx 121$ kHz) and the phase of the horizontal
polarization component. Real time synchronization with a radio frequency (rf)
solenoid made possible the rotation of the polarization out of the horizontal
plane, yielding a demonstration of the feedback method to manipulate the
polarization. In particular, the rotation rate shows a sinusoidal function of
the horizontal polarization phase (relative to the rf solenoid), which was
controlled to within a one standard deviation range of $\sigma = 0.21$ rad. The
minimum possible adjustment was 3.7 mHz out of a revolution frequency of 753
kHz, which changes the precession rate by 26 mrad/s. Such a capability meets a
requirement for the use of storage rings to look for an intrinsic electric
dipole moment of charged particles.
",Physics,Physics
"Measuring scientific buzz   Keywords are important for information retrieval. They are used to classify
and sort papers. However, these terms can also be used to study trends within
and across fields. We want to explore the lifecycle of new keywords. How often
do new terms come into existence and how long till they fade out? In this
paper, we present our preliminary analysis where we measure the burstiness of
keywords within the field of AI. We examine 150k keywords in approximately 100k
journal and conference papers. We find that nearly 80\% of the keywords die off
before year one for both journals and conferences but that terms last longer in
journals versus conferences. We also observe time periods of thematic bursts in
AI -- one where the terms are more neuroscience inspired and one more oriented
to computational optimization. This work shows promise of using author keywords
to better understand dynamics of buzz within science.
",Computer Science,Computer Science; Physics
"Subspace Tracking Algorithms for Millimeter Wave MIMO Channel Estimation with Hybrid Beamforming   This paper proposes the use of subspace tracking algorithms for performing
MIMO channel estimation at millimeter wave (mmWave) frequencies. Using a
subspace approach, we develop a protocol enabling the estimation of the right
(resp. left) singular vectors at the transmitter (resp. receiver) side; then,
we adapt the projection approximation subspace tracking with deflation (PASTd)
and the orthogonal Oja (OOJA) algorithms to our framework and obtain two
channel estimation algorithms. The hybrid analog/digital nature of the
beamformer is also explicitly taken into account at the algorithm design stage.
Numerical results show that the proposed estimation algorithms are effective,
and that they perform better than two relevant competing alternatives available
in the open literature.
",Computer Science,Computer Science
"Effects of geometrical frustration on ferromagnetism in the Hubbard model on the Shastry-Sutherland lattice   The small-cluster exact-diagonalization calculations and the projector
quantum Monte Carlo method are used to examine the competing effects of
geometrical frustration and interaction on ferromagnetism in the Hubbard model
on the Shastry-Sutherland lattice. It is shown that the geometrical frustration
stabilizes the ferromagnetic state at high electron concentrations ($n \gtrsim
7/4$), where strong correlations between ferromagnetism and the shape of the
noninteracting density of states are observed. In particular, it is found that
ferromagnetism is stabilized only for these values of frustration parameters,
which lead to the single peaked noninterating density of states at the band
edge. Once, two or more peaks appear in the noninteracting density of states at
the band egde the ferromagnetic state is suppressed. This opens a new route
towards the understanding of ferromagnetism in strongly correlated systems.
",Physics,Physics
"Semiparametrical Gaussian Processes Learning of Forward Dynamical Models for Navigating in a Circular Maze   This paper presents a problem of model learning for the purpose of learning
how to navigate a ball to a goal state in a circular maze environment with two
degrees of freedom. The motion of the ball in the maze environment is
influenced by several non-linear effects such as dry friction and contacts,
which are difficult to model physically. We propose a semiparametric model to
estimate the motion dynamics of the ball based on Gaussian Process Regression
equipped with basis functions obtained from physics first principles. The
accuracy of this semiparametric model is shown not only in estimation but also
in prediction at n-steps ahead and its compared with standard algorithms for
model learning. The learned model is then used in a trajectory optimization
algorithm to compute ball trajectories. We propose the system presented in the
paper as a benchmark problem for reinforcement and robot learning, for its
interesting and challenging dynamics and its relative ease of reproducibility.
",Computer Science; Statistics,Computer Science; Statistics
"A prismatic classifying space   A qualgebra $G$ is a set having two binary operations that satisfy
compatibility conditions which are modeled upon a group under conjugation and
multiplication. We develop a homology theory for qualgebras and describe a
classifying space for it. This space is constructed from $G$-colored prisms
(products of simplices) and simultaneously generalizes (and includes)
simplicial classifying spaces for groups and cubical classifying spaces for
quandles. Degenerate cells of several types are added to the regular prismatic
cells; by duality, these correspond to ""non-rigid"" Reidemeister moves and their
higher dimensional analogues. Coupled with $G$-coloring techniques, our
homology theory yields invariants of knotted trivalent graphs in $\mathbb{R}^3$
and knotted foams in $\mathbb{R}^4$. We re-interpret these invariants as
homotopy classes of maps from $S^2$ or $S^3$ to the classifying space of $G$.
",Mathematics,Mathematics
"Local Algorithms for Hierarchical Dense Subgraph Discovery   Finding the dense regions of a graph and relations among them is a
fundamental problem in network analysis. Core and truss decompositions reveal
dense subgraphs with hierarchical relations. The incremental nature of
algorithms for computing these decompositions and the need for global
information at each step of the algorithm hinders scalable parallelization and
approximations since the densest regions are not revealed until the end. In a
previous work, Lu et al. proposed to iteratively compute the $h$-indices of
neighbor vertex degrees to obtain the core numbers and prove that the
convergence is obtained after a finite number of iterations. This work
generalizes the iterative $h$-index computation for truss decomposition as well
as nucleus decomposition which leverages higher-order structures to generalize
core and truss decompositions. In addition, we prove convergence bounds on the
number of iterations. We present a framework of local algorithms to obtain the
core, truss, and nucleus decompositions. Our algorithms are local, parallel,
offer high scalability, and enable approximations to explore time and quality
trade-offs. Our shared-memory implementation verifies the efficiency,
scalability, and effectiveness of our local algorithms on real-world networks.
",Computer Science,Computer Science
"Unsupervised Body Part Regression via Spatially Self-ordering Convolutional Neural Networks   Automatic body part recognition for CT slices can benefit various medical
image applications. Recent deep learning methods demonstrate promising
performance, with the requirement of large amounts of labeled images for
training. The intrinsic structural or superior-inferior slice ordering
information in CT volumes is not fully exploited. In this paper, we propose a
convolutional neural network (CNN) based Unsupervised Body part Regression
(UBR) algorithm to address this problem. A novel unsupervised learning method
and two inter-sample CNN loss functions are presented. Distinct from previous
work, UBR builds a coordinate system for the human body and outputs a
continuous score for each axial slice, representing the normalized position of
the body part in the slice. The training process of UBR resembles a
self-organization process: slice scores are learned from inter-slice
relationships. The training samples are unlabeled CT volumes that are abundant,
thus no extra annotation effort is needed. UBR is simple, fast, and accurate.
Quantitative and qualitative experiments validate its effectiveness. In
addition, we show two applications of UBR in network initialization and anomaly
detection.
",Computer Science,Computer Science
"An Inversion-Based Learning Approach for Improving Impromptu Trajectory Tracking of Robots with Non-Minimum Phase Dynamics   This paper presents a learning-based approach for impromptu trajectory
tracking for non-minimum phase systems, i.e., systems with unstable inverse
dynamics. Inversion-based feedforward approaches are commonly used for
improving tracking performance; however, these approaches are not directly
applicable to non-minimum phase systems due to their inherent instability. In
order to resolve the instability issue, existing methods have assumed that the
system model is known and used pre-actuation or inverse approximation
techniques. In this work, we propose an approach for learning a stable,
approximate inverse of a non-minimum phase baseline system directly from its
input-output data. Through theoretical discussions, simulations, and
experiments on two different platforms, we show the stability of our proposed
approach and its effectiveness for high-accuracy, impromptu tracking. Our
approach also shows that including more information in the training, as is
commonly assumed to be useful, does not lead to better performance but may
trigger instability and impact the effectiveness of the overall approach.
",Computer Science,Computer Science
"Parallel mining of time-faded heavy hitters   We present PFDCMSS, a novel message-passing based parallel algorithm for
mining time-faded heavy hitters. The algorithm is a parallel version of the
recently published FDCMSS sequential algorithm. We formally prove its
correctness by showing that the underlying data structure, a sketch augmented
with a Space Saving stream summary holding exactly two counters, is mergeable.
Whilst mergeability of traditional sketches derives immediately from theory, we
show that merging our augmented sketch is non trivial. Nonetheless, the
resulting parallel algorithm is fast and simple to implement. To the best of
our knowledge, PFDCMSS is the first parallel algorithm solving the problem of
mining time-faded heavy hitters on message-passing parallel architectures.
Extensive experimental results confirm that PFDCMSS retains the extreme
accuracy and error bound provided by FDCMSS whilst providing excellent parallel
scalability.
",Computer Science,Computer Science
"Topological conjugacy of topological Markov shifts and Ruelle algebras   We will characterize topologically conjugate two-sided topological Markov
shifts $(\bar{X}_A,\bar{\sigma}_A)$ in terms of the associated asymptotic
Ruelle $C^*$-algebras ${\mathcal{R}}_A$ with its commutative $C^*$-subalgebras
$C(\bar{X}_A)$ and the canonical circle actions. We will also show that
extended Ruelle algebras ${\widetilde{\mathcal{R}}}_A$, which are purely
infinite version of the asymptotic Ruelle algebras, with its commutative
$C^*$-subalgebras $C(\bar{X}_A)$ and the canonical torus actions $\gamma^A$ are
complete invariants for topological conjugacy of two-sided topological Markov
shifts. We then have a computable topological conjugacy invariant, written in
terms of the underlying matrix, of a two-sided topological Markov shift by
using K-theory of the extended Ruelle algebra. The diagonal action of
$\gamma^A$ has a unique KMS-state on ${\widetilde{\mathcal{R}}}_A$, which is an
extension of the Parry measure on $\bar{X}_A$.
",Mathematics,Mathematics
"3D Human Pose Estimation on a Configurable Bed from a Pressure Image   Robots have the potential to assist people in bed, such as in healthcare
settings, yet bedding materials like sheets and blankets can make observation
of the human body difficult for robots. A pressure-sensing mat on a bed can
provide pressure images that are relatively insensitive to bedding materials.
However, prior work on estimating human pose from pressure images has been
restricted to 2D pose estimates and flat beds. In this work, we present two
convolutional neural networks to estimate the 3D joint positions of a person in
a configurable bed from a single pressure image. The first network directly
outputs 3D joint positions, while the second outputs a kinematic model that
includes estimated joint angles and limb lengths. We evaluated our networks on
data from 17 human participants with two bed configurations: supine and seated.
Our networks achieved a mean joint position error of 77 mm when tested with
data from people outside the training set, outperforming several baselines. We
also present a simple mechanical model that provides insight into ambiguity
associated with limbs raised off of the pressure mat, and demonstrate that
Monte Carlo dropout can be used to estimate pose confidence in these
situations. Finally, we provide a demonstration in which a mobile manipulator
uses our network's estimated kinematic model to reach a location on a person's
body in spite of the person being seated in a bed and covered by a blanket.
",Computer Science,Computer Science
"Safer Classification by Synthesis   The discriminative approach to classification using deep neural networks has
become the de-facto standard in various fields. Complementing recent
reservations about safety against adversarial examples, we show that
conventional discriminative methods can easily be fooled to provide incorrect
labels with very high confidence to out of distribution examples. We posit that
a generative approach is the natural remedy for this problem, and propose a
method for classification using generative models. At training time, we learn a
generative model for each class, while at test time, given an example to
classify, we query each generator for its most similar generation, and select
the class corresponding to the most similar one. Our approach is general and
can be used with expressive models such as GANs and VAEs. At test time, our
method accurately ""knows when it does not know,"" and provides resilience to out
of distribution examples while maintaining competitive performance for standard
examples.
",Computer Science; Statistics,Computer Science; Statistics
"Perovskite Substrates Boost the Thermopower of Cobaltate Thin Films at High Temperatures   Transition metal oxides are promising candidates for thermoelectric
applications, because they are stable at high temperature and because strong
electronic correlations can generate large Seebeck coefficients, but their
thermoelectric power factors are limited by the low electrical conductivity. We
report transport measurements on Ca3Co4O9 films on various perovskite
substrates and show that reversible incorporation of oxygen into SrTiO3 and
LaAlO3 substrates activates a parallel conduction channel for p-type carriers,
greatly enhancing the thermoelectric performance of the film-substrate system
at temperatures above 450 °C. Thin-film structures that take advantage of
both electronic correlations and the high oxygen mobility of transition metal
oxides thus open up new perspectives for thermopower generation at high
temperature.
",Physics,Physics
"An Exploration of Approaches to Integrating Neural Reranking Models in Multi-Stage Ranking Architectures   We explore different approaches to integrating a simple convolutional neural
network (CNN) with the Lucene search engine in a multi-stage ranking
architecture. Our models are trained using the PyTorch deep learning toolkit,
which is implemented in C/C++ with a Python frontend. One obvious integration
strategy is to expose the neural network directly as a service. For this, we
use Apache Thrift, a software framework for building scalable cross-language
services. In exploring alternative architectures, we observe that once trained,
the feedforward evaluation of neural networks is quite straightforward.
Therefore, we can extract the parameters of a trained CNN from PyTorch and
import the model into Java, taking advantage of the Java Deeplearning4J library
for feedforward evaluation. This has the advantage that the entire end-to-end
system can be implemented in Java. As a third approach, we can extract the
neural network from PyTorch and ""compile"" it into a C++ program that exposes a
Thrift service. We evaluate these alternatives in terms of performance (latency
and throughput) as well as ease of integration. Experiments show that
feedforward evaluation of the convolutional neural network is significantly
slower in Java, while the performance of the compiled C++ network does not
consistently beat the PyTorch implementation.
",Computer Science,Computer Science
"BMO estimate of lacunary Fourier series on nonabelian discrete groups   We show that the classical equivalence between the BMO norm and the $L^2$
norm of a lacunary Fourier series has an analogue on any discrete group $G$
equipped with a conditionally negative function.
",Mathematics,Mathematics
"Topological Perspectives on Statistical Quantities I   In statistics cumulants are defined to be functions that measure the linear
independence of random variables. In the non-communicative case the Boolean
cumulants can be described as functions that measure deviation of a map between
algebras from being an algebra morphism. In Algebraic topology maps that are
homotopic to being algebra morphisms are studied using the theory of $A_\infty$
algebras. In this paper we will explore the link between these two points of
views on maps between algebras that are not algebra maps.
",Mathematics,Mathematics
"Planning with Multiple Biases   Recent work has considered theoretical models for the behavior of agents with
specific behavioral biases: rather than making decisions that optimize a given
payoff function, the agent behaves inefficiently because its decisions suffer
from an underlying bias. These approaches have generally considered an agent
who experiences a single behavioral bias, studying the effect of this bias on
the outcome.
In general, however, decision-making can and will be affected by multiple
biases operating at the same time. How do multiple biases interact to produce
the overall outcome? Here we consider decisions in the presence of a pair of
biases exhibiting an intuitively natural interaction: present bias -- the
tendency to value costs incurred in the present too highly -- and sunk-cost
bias -- the tendency to incorporate costs experienced in the past into one's
plans for the future.
We propose a theoretical model for planning with this pair of biases, and we
show how certain natural behavioral phenomena can arise in our model only when
agents exhibit both biases. As part of our model we differentiate between
agents that are aware of their biases (sophisticated) and agents that are
unaware of them (naive). Interestingly, we show that the interaction between
the two biases is quite complex: in some cases, they mitigate each other's
effects while in other cases they might amplify each other. We obtain a number
of further results as well, including the fact that the planning problem in our
model for an agent experiencing and aware of both biases is computationally
hard in general, though tractable under more relaxed assumptions.
",Computer Science; Physics,Computer Science; Physics
"Mean field limits for nonlinear spatially extended hawkes processes with exponential memory kernels   We consider spatially extended systems of interacting nonlinear Hawkes
processes modeling large systems of neurons placed in Rd and study the
associated mean field limits. As the total number of neurons tends to infinity,
we prove that the evolution of a typical neuron, attached to a given spatial
position, can be described by a nonlinear limit differential equation driven by
a Poisson random measure. The limit process is described by a neural field
equation. As a consequence, we provide a rigorous derivation of the neural
field equation based on a thorough mean field analysis.
",Mathematics,Quantitative Biology
"Well-posedness and dispersive decay of small data solutions for the Benjamin-Ono equation   This article represents a first step toward understanding the long time
dynamics of solutions for the Benjamin-Ono equation. While this problem is
known to be both completely integrable and globally well-posed in $L^2$, much
less seems to be known concerning its long time dynamics. Here, we prove that
for small localized data the solutions have (nearly) dispersive dynamics almost
globally in time. An additional objective is to revisit the $L^2$ theory for
the Benjamin-Ono equation and provide a simpler, self-contained approach.
",Mathematics,Mathematics
"Effective holographic theory of charge density waves   We use Gauge/Gravity duality to write down an effective low energy
holographic theory of charge density waves. We consider a simple gravity model
which breaks translations spontaneously in the dual field theory in a
homogeneous manner, capturing the low energy dynamics of phonons coupled to
conserved currents. We first focus on the leading two-derivative action, which
leads to excited states with non-zero strain. We show that including subleading
quartic derivative terms leads to dynamical instabilities of AdS$_2$
translation invariant states and to stable phases breaking translations
spontaneously. We compute analytically the real part of the electric
conductivity. The model allows to construct Lifshitz-like hyperscaling
violating quantum critical ground states breaking translations spontaneously.
At these critical points, the real part of the dc conductivity can be metallic
or insulating.
",Physics,Physics
"Conformer-selection by matter-wave interference   We establish that matter-wave interference at near-resonant ultraviolet
optical gratings can be used to spatially separate individual conformers of
complex molecules. Our calculations show that the conformational purity of the
prepared beam can be close to 100% and that all molecules remain in their
electronic ground state. The proposed technique is independent of the dipole
moment and the spin of the molecule and thus paves the way for
structure-sensitive experiments with hydrocarbons and biomolecules, such as
neurotransmitters and hormones, which evaded conformer-pure isolation so far
",Physics,Physics
"An Architecture for Embedded Systems Supporting Assisted Living   The rise in life expectancy is one of the great achievements of the twentieth
century. This phenomenon originates a still increasing interest in Ambient
Assisted Living (AAL) technological solutions that may support people in their
daily routines allowing an independent and safe lifestyle as long as possible.
AAL systems generally acquire data from the field and reason on them and the
context to accomplish their tasks. Very often, AAL systems are vertical
solutions, thus making hard their reuse and adaptation to different domains
with respect to the ones for which they have been developed. In this paper we
propose an architectural solution that allows the acquisition level of an ALL
system to be easily built, configured, and extended without affecting the
reasoning level of the system. We experienced our proposal in a fall detection
system.
",Computer Science,Computer Science
"Visual Entailment: A Novel Task for Fine-Grained Image Understanding   Existing visual reasoning datasets such as Visual Question Answering (VQA),
often suffer from biases conditioned on the question, image or answer
distributions. The recently proposed CLEVR dataset addresses these limitations
and requires fine-grained reasoning but the dataset is synthetic and consists
of similar objects and sentence structures across the dataset.
In this paper, we introduce a new inference task, Visual Entailment (VE) -
consisting of image-sentence pairs whereby a premise is defined by an image,
rather than a natural language sentence as in traditional Textual Entailment
tasks. The goal of a trained VE model is to predict whether the image
semantically entails the text. To realize this task, we build a dataset SNLI-VE
based on the Stanford Natural Language Inference corpus and Flickr30k dataset.
We evaluate various existing VQA baselines and build a model called Explainable
Visual Entailment (EVE) system to address the VE task. EVE achieves up to 71%
accuracy and outperforms several other state-of-the-art VQA based models.
Finally, we demonstrate the explainability of EVE through cross-modal attention
visualizations. The SNLI-VE dataset is publicly available at
this https URL necla-ml/SNLI-VE.
",Computer Science,Computer Science
"Social Media Analysis For Organizations: Us Northeastern Public And State Libraries Case Study   Social networking sites such as Twitter have provided a great opportunity for
organizations such as public libraries to disseminate information for public
relations purposes. However, there is a need to analyze vast amounts of social
media data. This study presents a computational approach to explore the content
of tweets posted by nine public libraries in the northeastern United States of
America. In December 2017, this study extracted more than 19,000 tweets from
the Twitter accounts of seven state libraries and two urban public libraries.
Computational methods were applied to collect the tweets and discover
meaningful themes. This paper shows how the libraries have used Twitter to
represent their services and provides a starting point for different
organizations to evaluate the themes of their public tweets.
",Computer Science; Statistics,Computer Science
"Symmetries and regularity for holomorphic maps between balls   Let $f:{\mathbb B}^n \to {\mathbb B}^N$ be a holomorphic map. We study
subgroups $\Gamma_f \subseteq {\rm Aut}({\mathbb B}^n)$ and $T_f \subseteq {\rm
Aut}({\mathbb B}^N)$. When $f$ is proper, we show both these groups are Lie
subgroups. When $\Gamma_f$ contains the center of ${\bf U}(n)$, we show that
$f$ is spherically equivalent to a polynomial. When $f$ is minimal we show that
there is a homomorphism $\Phi:\Gamma_f \to T_f$ such that $f$ is equivariant
with respect to $\Phi$. To do so, we characterize minimality via the triviality
of a third group $H_f$. We relate properties of ${\rm Ker}(\Phi)$ to older
results on invariant proper maps between balls. When $f$ is proper but
completely non-rational, we show that either both $\Gamma_f$ and $T_f$ are
finite or both are noncompact.
",Mathematics,Mathematics
"Question Answering through Transfer Learning from Large Fine-grained Supervision Data   We show that the task of question answering (QA) can significantly benefit
from the transfer learning of models trained on a different large, fine-grained
QA dataset. We achieve the state of the art in two well-studied QA datasets,
WikiQA and SemEval-2016 (Task 3A), through a basic transfer learning technique
from SQuAD. For WikiQA, our model outperforms the previous best model by more
than 8%. We demonstrate that finer supervision provides better guidance for
learning lexical and syntactic information than coarser supervision, through
quantitative results and visual analysis. We also show that a similar transfer
learning procedure achieves the state of the art on an entailment task.
",Computer Science,Computer Science; Statistics
"Virtual-to-Real: Learning to Control in Visual Semantic Segmentation   Collecting training data from the physical world is usually time-consuming
and even dangerous for fragile robots, and thus, recent advances in robot
learning advocate the use of simulators as the training platform.
Unfortunately, the reality gap between synthetic and real visual data prohibits
direct migration of the models trained in virtual worlds to the real world.
This paper proposes a modular architecture for tackling the virtual-to-real
problem. The proposed architecture separates the learning model into a
perception module and a control policy module, and uses semantic image
segmentation as the meta representation for relating these two modules. The
perception module translates the perceived RGB image to semantic image
segmentation. The control policy module is implemented as a deep reinforcement
learning agent, which performs actions based on the translated image
segmentation. Our architecture is evaluated in an obstacle avoidance task and a
target following task. Experimental results show that our architecture
significantly outperforms all of the baseline methods in both virtual and real
environments, and demonstrates a faster learning curve than them. We also
present a detailed analysis for a variety of variant configurations, and
validate the transferability of our modular architecture.
",Computer Science,Computer Science
"Learning Fast and Slow: PROPEDEUTICA for Real-time Malware Detection   In this paper, we introduce and evaluate PROPEDEUTICA, a novel methodology
and framework for efficient and effective real-time malware detection,
leveraging the best of conventional machine learning (ML) and deep learning
(DL) algorithms. In PROPEDEUTICA, all software processes in the system start
execution subjected to a conventional ML detector for fast classification. If a
piece of software receives a borderline classification, it is subjected to
further analysis via more performance expensive and more accurate DL methods,
via our newly proposed DL algorithm DEEPMALWARE. Further, we introduce delays
to the execution of software subjected to deep learning analysis as a way to
""buy time"" for DL analysis and to rate-limit the impact of possible malware in
the system. We evaluated PROPEDEUTICA with a set of 9,115 malware samples and
877 commonly used benign software samples from various categories for the
Windows OS. Our results show that the false positive rate for conventional ML
methods can reach 20%, and for modern DL methods it is usually below 6%.
However, the classification time for DL can be 100X longer than conventional ML
methods. PROPEDEUTICA improved the detection F1-score from 77.54% (conventional
ML method) to 90.25%, and reduced the detection time by 54.86%. Further, the
percentage of software subjected to DL analysis was approximately 40% on
average. Further, the application of delays in software subjected to ML reduced
the detection time by approximately 10%. Finally, we found and discussed a
discrepancy between the detection accuracy offline (analysis after all traces
are collected) and on-the-fly (analysis in tandem with trace collection). Our
insights show that conventional ML and modern DL-based malware detectors in
isolation cannot meet the needs of efficient and effective malware detection:
high accuracy, low false positive rate, and short classification time.
",Computer Science; Statistics,Computer Science
"Accelerated Primal-Dual Proximal Block Coordinate Updating Methods for Constrained Convex Optimization   Block Coordinate Update (BCU) methods enjoy low per-update computational
complexity because every time only one or a few block variables would need to
be updated among possibly a large number of blocks. They are also easily
parallelized and thus have been particularly popular for solving problems
involving large-scale dataset and/or variables. In this paper, we propose a
primal-dual BCU method for solving linearly constrained convex program in
multi-block variables. The method is an accelerated version of a primal-dual
algorithm proposed by the authors, which applies randomization in selecting
block variables to update and establishes an $O(1/t)$ convergence rate under
weak convexity assumption. We show that the rate can be accelerated to
$O(1/t^2)$ if the objective is strongly convex. In addition, if one block
variable is independent of the others in the objective, we then show that the
algorithm can be modified to achieve a linear rate of convergence. The
numerical experiments show that the accelerated method performs stably with a
single set of parameters while the original method needs to tune the parameters
for different datasets in order to achieve a comparable level of performance.
",Computer Science; Mathematics; Statistics,Computer Science; Statistics
"Evidence accumulation in a Laplace domain decision space   Evidence accumulation models of simple decision-making have long assumed that
the brain estimates a scalar decision variable corresponding to the
log-likelihood ratio of the two alternatives. Typical neural implementations of
this algorithmic cognitive model assume that large numbers of neurons are each
noisy exemplars of the scalar decision variable. Here we propose a neural
implementation of the diffusion model in which many neurons construct and
maintain the Laplace transform of the distance to each of the decision bounds.
As in classic findings from brain regions including LIP, the firing rate of
neurons coding for the Laplace transform of net accumulated evidence grows to a
bound during random dot motion tasks. However, rather than noisy exemplars of a
single mean value, this approach makes the novel prediction that firing rates
grow to the bound exponentially, across neurons there should be a distribution
of different rates. A second set of neurons records an approximate inversion of
the Laplace transform, these neurons directly estimate net accumulated
evidence. In analogy to time cells and place cells observed in the hippocampus
and other brain regions, the neurons in this second set have receptive fields
along a ""decision axis."" This finding is consistent with recent findings from
rodent recordings. This theoretical approach places simple evidence
accumulation models in the same mathematical language as recent proposals for
representing time and space in cognitive models for memory.
",Quantitative Biology,Quantitative Biology
"Imaginary time, shredded propagator method for large-scale GW calculations   The GW method is a many-body approach capable of providing quasiparticle
bands for realistic systems spanning physics, chemistry, and materials science.
Despite its power, GW is not routinely applied to large complex materials due
to its computational expense. We perform an exact recasting of the GW
polarizability and the self-energy as Laplace integrals over imaginary time
propagators. We then ""shred"" the propagators (via energy windowing) and
approximate them in a controlled manner by using Gauss-Laguerre quadrature and
discrete variable methods to treat the imaginary time propagators in real
space. The resulting cubic scaling GW method has a sufficiently small prefactor
to outperform standard quartic scaling methods on small systems (>=10 atoms)
and also represents a substantial improvement over several other cubic methods
tested. This approach is useful for evaluating quantum mechanical response
function involving large sums containing energy (difference) denominators.
",Physics,Physics
"Optical properties of a four-layer waveguiding nanocomposite structure in near-IR regime   The theoretical study of the optical properties of TE- and TM- modes in a
four-layer structure composed of the magneto-optical yttrium iron garnet
guiding layer on a dielectric substrate covered by planar nanocomposite guiding
multilayer is presented. The dispersion equation is obtained taking into
account the bigyrotropic properties of yttrium-iron garnet, and an original
algorithm for the guided modes identification is proposed. The dispersion
spectra are analyzed and the energy flux distributions across the structure are
calculated. The fourfold difference between the partial power fluxes within the
waveguide layers is achieved in the wavelength range of 200 nm.
",Physics,Physics
"Precision of Evaluation Methods in White Light Interferometry: Correlogram Correlation Method   In this paper we promote a method for the evaluation of a surface topography
which we call the correlogram correlation method. Employing a theoretical
analysis as well as numerical simulations the method is proven to be the most
accurate among available evaluation algorithms in the common case of
uncorrelated noise. Examples illustrate the superiority of the correlogram
correlation method over the common envelope and phase methods.
",Physics,Physics
"The Global Optimization Geometry of Low-Rank Matrix Optimization   This paper considers general rank-constrained optimization problems that
minimize a general objective function $f(X)$ over the set of rectangular
$n\times m$ matrices that have rank at most $r$. To tackle the rank constraint
and also to reduce the computational burden, we factorize $X$ into $UV^T$ where
$U$ and $V$ are $n\times r$ and $m\times r$ matrices, respectively, and then
optimize over the small matrices $U$ and $V$. We characterize the global
optimization geometry of the nonconvex factored problem and show that the
corresponding objective function satisfies the robust strict saddle property as
long as the original objective function $f$ satisfies restricted strong
convexity and smoothness properties, ensuring global convergence of many local
search algorithms (such as noisy gradient descent) in polynomial time for
solving the factored problem. We also provide a comprehensive analysis for the
optimization geometry of a matrix factorization problem where we aim to find
$n\times r$ and $m\times r$ matrices $U$ and $V$ such that $UV^T$ approximates
a given matrix $X^\star$. Aside from the robust strict saddle property, we show
that the objective function of the matrix factorization problem has no spurious
local minima and obeys the strict saddle property not only for the
exact-parameterization case where $rank(X^\star) = r$, but also for the
over-parameterization case where $rank(X^\star) < r$ and the
under-parameterization case where $rank(X^\star) > r$. These geometric
properties imply that a number of iterative optimization algorithms (such as
gradient descent) converge to a global solution with random initialization.
",Computer Science; Mathematics,Computer Science; Mathematics
"Flux-flow and vortex-glass phase in iron pnictide BaFe$_{2-x}$Ni$_x$As$_2$ single crystals with $T_c$ $\sim$ 20 K   We analysed the flux-flow region of isofield magneto resistivity data
obtained on three crystals of BaFe$_{2-x}$Ni$_x$As$_2$ with $T_c$$\sim$20 K for
three different geometries relative to the angle formed between the applied
magnetic field and the c-axis of the crystals. The field dependent activation
energy, $U_0$, was obtained from the TAFF and modified vortex-glass models,
which were compared with the values of $U_0$ obtained from flux-creep available
in the literature. We observed that the $U_0$ obtained from the TAFF model show
deviations among the different crystals, while the correspondent glass lines
obtained from the vortex glass model are virtually coincident. It is shown that
the data is well explained by the modified vortex glass model, allowing to
extract values of $T_g$, the glass transition temperature, and $T^*$, a
temperature which scales with the mean field critical temperature $T_c(H)$. The
resulting glass lines obey the anisotropic Ginzburg-Landau theory and are well
fitted by a theory developed in the literature by considering the effect of
disorder.
",Physics,Physics
"Exponential growth of homotopy groups of suspended finite complexes   We study the asymptotic behavior of the homotopy groups of simply connected
finite $p$-local complexes, and define a space to be locally hyperbolic if its
homotopy groups have exponential growth. Under some certain conditions related
to the functorial decomposition of loop suspension, we prove that the suspended
finite complexes are locally hyperbolic if suitable but accessible information
of the homotopy groups is known. In particular, we prove that Moore spaces are
locally hyperbolic, and other candidates are also given.
",Mathematics,Mathematics
"Enhancing TCP End-to-End Performance in Millimeter-Wave Communications   Recently, millimeter-wave (mmWave) communications have received great
attention due to the availability of large spectrum resources. Nevertheless,
their impact on TCP performance has been overlooked, which is observed that the
said TCP performance collapse occurs owing to the significant difference in
signal quality between LOS and NLOS links. We propose a novel TCP design for
mmWave communications, a mmWave performance enhancing proxy (mmPEP), enabling
not only to overcome TCP performance collapse but also exploit the properties
of mmWave channels. The base station installs the TCP proxy to operate the two
functionalities called Ack management and batch retransmission. Specifically,
the proxy sends the said early-Ack to the server not to decrease its sending
rate even in the NLOS status. In addition, when a packet-loss is detected, the
proxy retransmits not only lost packets but also the certain number of the
following packets expected to be lost too. It is verified by ns-3 simulation
that compared with benchmark, mmPEP enhances the end-to-end rate and packet
delivery ratio by maintaining high sending rate with decreasing the loss
recovery time.
",Computer Science,Computer Science
"On the Power of Symmetric Linear Programs   We consider families of symmetric linear programs (LPs) that decide a
property of graphs (or other relational structures) in the sense that, for each
size of graph, there is an LP defining a polyhedral lift that separates the
integer points corresponding to graphs with the property from those
corresponding to graphs without the property. We show that this is equivalent,
with at most polynomial blow-up in size, to families of symmetric Boolean
circuits with threshold gates. In particular, when we consider polynomial-size
LPs, the model is equivalent to definability in a non-uniform version of
fixed-point logic with counting (FPC). Known upper and lower bounds for FPC
apply to the non-uniform version. In particular, this implies that the class of
graphs with perfect matchings has polynomial-size symmetric LPs while we obtain
an exponential lower bound for symmetric LPs for the class of Hamiltonian
graphs. We compare and contrast this with previous results (Yannakakis 1991)
showing that any symmetric LPs for the matching and TSP polytopes have
exponential size. As an application, we establish that for random, uniformly
distributed graphs, polynomial-size symmetric LPs are as powerful as general
Boolean circuits. We illustrate the effect of this on the well-studied
planted-clique problem.
",Computer Science,Computer Science
"The GENIUS Approach to Robust Mendelian Randomization Inference   Mendelian randomization (MR) is a popular instrumental variable (IV)
approach. A key IV identification condition known as the exclusion restriction
requires no direct effect of an IV on the outcome not through the exposure
which is unrealistic in most MR analyses. As a result, possible violation of
the exclusion restriction can seldom be ruled out in such studies. To address
this concern, we introduce a new class of IV estimators which are robust to
violation of the exclusion restriction under a large collection of data
generating mechanisms consistent with parametric models commonly assumed in the
MR literature. Our approach named ""MR G-Estimation under No Interaction with
Unmeasured Selection"" (MR GENIUS) may be viewed as a modification to Robins'
G-estimation approach that is robust to both additive unmeasured confounding
and violation of the exclusion restriction assumption. We also establish that
estimation with MR GENIUS may also be viewed as a robust generalization of the
well-known Lewbel estimator for a triangular system of structural equations
with endogeneity. Specifically, we show that unlike Lewbel estimation, MR
GENIUS is under fairly weak conditions also robust to unmeasured confounding of
the effects of the genetic IVs, another possible violation of a key IV
Identification condition. Furthermore, while Lewbel estimation involves
specification of linear models both for the outcome and the exposure, MR GENIUS
generally does not require specification of a structural model for the direct
effect of invalid IVs on the outcome, therefore allowing the latter model to be
unrestricted. Finally, unlike Lewbel estimation, MR GENIUS is shown to equally
apply for binary, discrete or continuous exposure and outcome variables and can
be used under prospective sampling, or retrospective sampling such as in a
case-control study.
",Statistics,Statistics
"Optimal Frequency Ranges for Sub-Microsecond Precision Pulsar Timing   Precision pulsar timing requires optimization against measurement errors and
astrophysical variance from the neutron stars themselves and the interstellar
medium. We investigate optimization of arrival time precision as a function of
radio frequency and bandwidth. We find that increases in bandwidth that reduce
the contribution from receiver noise are countered by the strong chromatic
dependence of interstellar effects and intrinsic pulse-profile evolution. The
resulting optimal frequency range is therefore telescope and pulsar dependent.
We demonstrate the results for five pulsars included in current pulsar timing
arrays and determine that they are not optimally observed at current center
frequencies. For those objects, we find that better choices of total bandwidth
as well as center frequency can improve the arrival-time precision. Wideband
receivers centered at somewhat higher frequencies with respect to the currently
adopted receivers can reduce required overall integration times and provide
significant improvements in arrival time uncertainty by a factor of ~sqrt(2) in
most cases, assuming a fixed integration time. We also discuss how timing
programs can be extended to pulsars with larger dispersion measures through the
use of higher-frequency observations.
",Physics,Physics
"Chern classes and Gromov--Witten theory of projective bundles   We prove that the Gromov--Witten theory (GWT) of a projective bundle can be
determined by the Chern classes and the GWT of the base. It completely answers
a question raised in a previous paper (arXiv:1607.00740). Its consequences
include that the GWT of the blow-up of X at a smooth subvariety Z is uniquely
determined by GWT of X, Z plus some topological data.
",Mathematics,Mathematics
"Photonic topological pumping through the edges of a dynamical four-dimensional quantum Hall system   When a two-dimensional electron gas is exposed to a perpendicular magnetic
field and an in-plane electric field, its conductance becomes quantized in the
transverse in-plane direction: this is known as the quantum Hall (QH) effect.
This effect is a result of the nontrivial topology of the system's electronic
band structure, where an integer topological invariant known as the first Chern
number leads to the quantization of the Hall conductance. Interestingly, it was
shown that the QH effect can be generalized mathematically to four spatial
dimensions (4D), but this effect has never been realized for the obvious reason
that experimental systems are bound to three spatial dimensions. In this work,
we harness the high tunability and control offered by photonic waveguide arrays
to experimentally realize a dynamically-generated 4D QH system using a 2D array
of coupled optical waveguides. The inter-waveguide separation is constructed
such that the propagation of light along the device samples over
higher-dimensional momenta in the directions orthogonal to the two physical
dimensions, thus realizing a 2D topological pump. As a result, the device's
band structure is associated with 4D topological invariants known as second
Chern numbers which support a quantized bulk Hall response with a 4D symmetry.
In a finite-sized system, the 4D topological bulk response is carried by
localized edges modes that cross the sample as a function of of the modulated
auxiliary momenta. We directly observe this crossing through photon pumping
from edge-to-edge and corner-to-corner of our system. These are equivalent to
the pumping of charge across a 4D system from one 3D hypersurface to the
opposite one and from one 2D hyperedge to another, and serve as first
experimental realization of higher-dimensional topological physics.
",Physics,Physics
"Reynolds number dependence of the structure functions in homogeneous turbulence   We compare the predictions of stochastic closure theory (SCT) with
experimental measurements of homogeneous turbulence made in the Variable
Density Turbulence Tunnel (VDTT) at the Max Planck Institute for Dynamics and
Self-Organization in Gottingen. While the general form of SCT contains
infinitely many free parameters, the data permit us to reduce the number to
seven, only three of which are active over the entire inertial range. Of these
three, one parameter characterizes the variance of the mean field noise in SCT
and another characterizes the rate in the large deviations of the mean. The
third parameter is the decay exponent of the Fourier variables in the Fourier
expansion of the noise, which characterizes the smoothness of the turbulent
velocity. SCT compares favorably with velocity structure functions measured in
the experiment. We considered even-order structure functions ranging in order
from two to eight as well as the third-order structure functions at five
Taylor-Reynolds numbers (Rl) between 110 and 1450. The comparisons highlight
several advantages of the SCT, which include explicit predictions for the
structure functions at any scale and for any Reynolds number. We observed that
finite-Rl corrections, for instance, are important even at the highest Reynolds
numbers produced in the experiments. SCT gives us the correct basis function to
express all the moments of the velocity differences in turbulence in Fourier
space. The SCT produces the coefficients of the series and so determines the
statistical quantities that characterize the small scales in turbulence. It
also characterizes the random force acting on the fluid in the stochastic
Navier-Stokes equation, as described in the paper.
",Physics,Physics
"The modularity of action and perception revisited using control theory and active inference   The assumption that action and perception can be investigated independently
is entrenched in theories, models and experimental approaches across the brain
and mind sciences. In cognitive science, this has been a central point of
contention between computationalist and 4Es (enactive, embodied, extended and
embedded) theories of cognition, with the former embracing the ""classical
sandwich"", modular, architecture of the mind and the latter actively denying
this separation can be made. In this work we suggest that the modular
independence of action and perception strongly resonates with the separation
principle of control theory and furthermore that this principle provides formal
criteria within which to evaluate the implications of the modularity of action
and perception. We will also see that real-time feedback with the environment,
often considered necessary for the definition of 4Es ideas, is not however a
sufficient condition to avoid the ""classical sandwich"". Finally, we argue that
an emerging framework in the cognitive and brain sciences, active inference,
extends ideas derived from control theory to the study of biological systems
while disposing of the separation principle, describing non-modular models of
behaviour strongly aligned with 4Es theories of cognition.
",Quantitative Biology,Quantitative Biology
"Numerical assessment of the percolation threshold using complement networks   Models of percolation processes on networks currently assume locally
tree-like structures at low densities, and are derived exactly only in the
thermodynamic limit. Finite size effects and the presence of short loops in
real systems however cause a deviation between the empirical percolation
threshold $p_c$ and its model-predicted value $\pi_c$. Here we show the
existence of an empirical linear relation between $p_c$ and $\pi_c$ across a
large number of real and model networks. Such a putatively universal relation
can then be used to correct the estimated value of $\pi_c$. We further show how
to obtain a more precise relation using the concept of the complement graph, by
investigating on the connection between the percolation threshold of a network,
$p_c$, and that of its complement, $\bar{p}_c$.
",Computer Science,Physics
"Network topology of neural systems supporting avalanche dynamics predicts stimulus propagation and recovery   Many neural systems display avalanche behavior characterized by uninterrupted
sequences of neuronal firing whose distributions of size and durations are
heavy-tailed. Theoretical models of such systems suggest that these dynamics
support optimal information transmission and storage. However, the unknown role
of network structure precludes an understanding of how variations in network
topology manifest in neural dynamics and either support or impinge upon
information processing. Here, using a generalized spiking model, we develop a
mechanistic understanding of how network topology supports information
processing through network dynamics. First, we show how network topology
determines network dynamics by analytically and numerically demonstrating that
network topology can be designed to propagate stimulus patterns for long
durations. We then identify strongly connected cycles as empirically observable
network motifs that are prevalent in such networks. Next, we show that within a
network, mathematical intuitions from network control theory are tightly linked
with dynamics initiated by node-specific stimulation and can identify stimuli
that promote long-lasting cascades. Finally, we use these network-based metrics
and control-based stimuli to demonstrate that long-lasting cascade dynamics
facilitate delayed recovery of stimulus patterns from network activity, as
measured by mutual information. Collectively, our results provide evidence that
cortical networks are structured with architectural motifs that support
long-lasting propagation and recovery of a few crucial patterns of stimulation,
especially those consisting of activity in highly controllable neurons.
Broadly, our results imply that avalanching neural networks could contribute to
cognitive faculties that require persistent activation of neuronal patterns,
such as working memory or attention.
",Quantitative Biology,Quantitative Biology
"Mixing of odd- and even-frequency pairings in strongly correlated electron systems under magnetic field   Even- and odd-frequency superconductivity coexist due to broken time-reversal
symmetry under magnetic field. In order to describe this mixing, we extend the
linearized Eliashberg equation for the spin and charge fluctuation mechanism in
strongly correlated electron systems. We apply this extended Eliashberg
equation to the odd-frequency superconductivity on a quasi-one-dimensional
isosceles triangular lattice under in-plane magnetic field and examine the
effect of the even-frequency component.
",Physics,Physics
"Asymptotic Properties of the Maximum Likelihood Estimator in Regime Switching Econometric Models   Markov regime switching models have been widely used in numerous empirical
applications in economics and finance. However, the asymptotic distribution of
the maximum likelihood estimator (MLE) has not been proven for some empirically
popular Markov regime switching models. In particular, the asymptotic
distribution of the MLE has been unknown for models in which some elements of
the transition probability matrix have the value of zero, as is commonly
assumed in empirical applications with models with more than two regimes. This
also includes models in which the regime-specific density depends on both the
current and the lagged regimes such as the seminal model of Hamilton (1989) and
switching ARCH model of Hamilton and Susmel (1994). This paper shows the
asymptotic normality of the MLE and consistency of the asymptotic covariance
matrix estimate of these models.
",Mathematics; Statistics,Mathematics; Statistics
"Differences Among Noninformative Stopping Rules Are Often Relevant to Bayesian Decisions   L.J. Savage once hoped to show that ""the superficially incompatible systems
of ideas associated on the one hand with [subjective Bayesianism] and on the
other hand with [classical statistics]...lend each other mutual support and
clarification."" By 1972, however, he had largely ""lost faith in the devices"" of
classical statistics. One aspect of those ""devices"" that he found objectionable
is that differences among the ""stopping rules"" that are used to decide when to
end an experiment which are ""noninformative"" from a Bayesian perspective can
affect decisions made using a classical approach. Two experiments that produce
the same data using different stopping rules seem to differ only in the
intentions of the experimenters regarding whether or not they would have
carried on if the data had been different, which seem irrelevant to the
evidential import of the data and thus to facts about what actions the data
warrant.
I argue that classical and Bayesian ideas about stopping rules do in fact
""lend each other"" the kind of ""mutual support and clarification"" that Savage
had originally hoped to find. They do so in a kind of case that is common in
scientific practice, in which those who design an experiment have different
interests from those who will make decisions in light of its results. I show
that, in cases of this kind, Bayesian principles provide qualified support for
the classical statistical practice of ""penalizing"" ""biased"" stopping rules.
However, they require this practice in a narrower range of circumstances than
classical principles do, and for different reasons. I argue that classical
arguments for this practice are compelling in precisely the class of cases in
which Bayesian principles also require it, and thus that we should regard
Bayesian principles as clarifying classical statistical ideas about stopping
rules rather than the reverse.
",Mathematics; Statistics,Computer Science; Statistics
"SySeVR: A Framework for Using Deep Learning to Detect Software Vulnerabilities   The detection of software vulnerabilities (or vulnerabilities for short) is
an important problem that has yet to be tackled, as manifested by many
vulnerabilities reported on a daily basis. This calls for machine learning
methods to automate vulnerability detection. Deep learning is attractive for
this purpose because it does not require human experts to manually define
features. Despite the tremendous success of deep learning in other domains, its
applicability to vulnerability detection is not systematically understood. In
order to fill this void, we propose the first systematic framework for using
deep learning to detect vulnerabilities. The framework, dubbed Syntax-based,
Semantics-based, and Vector Representations (SySeVR), focuses on obtaining
program representations that can accommodate syntax and semantic information
pertinent to vulnerabilities. Our experiments with 4 software products
demonstrate the usefulness of the framework: we detect 15 vulnerabilities that
are not reported in the National Vulnerability Database. Among these 15
vulnerabilities, 7 are unknown and have been reported to the vendors, and the
other 8 have been ""silently"" patched by the vendors when releasing newer
versions of the products.
",Statistics,Computer Science
"On Data-Dependent Random Features for Improved Generalization in Supervised Learning   The randomized-feature approach has been successfully employed in large-scale
kernel approximation and supervised learning. The distribution from which the
random features are drawn impacts the number of features required to
efficiently perform a learning task. Recently, it has been shown that employing
data-dependent randomization improves the performance in terms of the required
number of random features. In this paper, we are concerned with the
randomized-feature approach in supervised learning for good generalizability.
We propose the Energy-based Exploration of Random Features (EERF) algorithm
based on a data-dependent score function that explores the set of possible
features and exploits the promising regions. We prove that the proposed score
function with high probability recovers the spectrum of the best fit within the
model class. Our empirical results on several benchmark datasets further verify
that our method requires smaller number of random features to achieve a certain
generalization error compared to the state-of-the-art while introducing
negligible pre-processing overhead. EERF can be implemented in a few lines of
code and requires no additional tuning parameters.
",Computer Science; Statistics,Computer Science; Statistics
"On the commutative center of Moufang loops   We construct two infinite series of Moufang loops of exponent $3$ whose
commutative center (i.e. the set of elements that commute with all elements of
the loop) is not a normal subloop. In particular, we obtain examples of such
loops of orders $3^8$ and $3^{11}$ one of which can be defined as the Moufang
triplication of the free Burnside group $B(3,3)$.
",Mathematics,Mathematics
"Bayesian analysis of 210Pb dating   In many studies of environmental change of the past few centuries, 210Pb
dating is used to obtain chronologies for sedimentary sequences. One of the
most commonly used approaches to estimate the ages of depths in a sequence is
to assume a constant rate of supply (CRS) or influx of `unsupported' 210Pb from
the atmosphere, together with a constant or varying amount of `supported'
210Pb. Current 210Pb dating models do not use a proper statistical framework
and thus provide poor estimates of errors. Here we develop a new model for
210Pb dating, where both ages and values of supported and unsupported 210Pb
form part of the parameters. We apply our model to a case study from Canada as
well as to some simulated examples. Our model can extend beyond the current CRS
approach, deal with asymmetric errors and mix 210Pb with other types of dating,
thus obtaining more robust, realistic and statistically better defined
estimates.
",Statistics,Physics
"k*-Nearest Neighbors: From Global to Local   The weighted k-nearest neighbors algorithm is one of the most fundamental
non-parametric methods in pattern recognition and machine learning. The
question of setting the optimal number of neighbors as well as the optimal
weights has received much attention throughout the years, nevertheless this
problem seems to have remained unsettled. In this paper we offer a simple
approach to locally weighted regression/classification, where we make the
bias-variance tradeoff explicit. Our formulation enables us to phrase a notion
of optimal weights, and to efficiently find these weights as well as the
optimal number of neighbors efficiently and adaptively, for each data point
whose value we wish to estimate. The applicability of our approach is
demonstrated on several datasets, showing superior performance over standard
locally weighted methods.
",Computer Science; Statistics,Computer Science; Statistics
"On a registration-based approach to sensor network localization   We consider a registration-based approach for localizing sensor networks from
range measurements. This is based on the assumption that one can find
overlapping cliques spanning the network. That is, for each sensor, one can
identify geometric neighbors for which all inter-sensor ranges are known. Such
cliques can be efficiently localized using multidimensional scaling. However,
since each clique is localized in some local coordinate system, we are required
to register them in a global coordinate system. In other words, our approach is
based on transforming the localization problem into a problem of registration.
In this context, the main contributions are as follows. First, we describe an
efficient method for partitioning the network into overlapping cliques. Second,
we study the problem of registering the localized cliques, and formulate a
necessary rigidity condition for uniquely recovering the global sensor
coordinates. In particular, we present a method for efficiently testing
rigidity, and a proposal for augmenting the partitioned network to enforce
rigidity. A recently proposed semidefinite relaxation of global registration is
used for registering the cliques. We present simulation results on random and
structured sensor networks to demonstrate that the proposed method compares
favourably with state-of-the-art methods in terms of run-time, accuracy, and
scalability.
",Computer Science; Mathematics,Computer Science
"Regularization, sparse recovery, and median-of-means tournaments   A regularized risk minimization procedure for regression function estimation
is introduced that achieves near optimal accuracy and confidence under general
conditions, including heavy-tailed predictor and response variables. The
procedure is based on median-of-means tournaments, introduced by the authors in
[8]. It is shown that the new procedure outperforms standard regularized
empirical risk minimization procedures such as lasso or slope in heavy-tailed
problems.
",Mathematics; Statistics,Statistics
"Isomorphism between Differential and Moment Invariants under Affine Transform   The invariant is one of central topics in science, technology and
engineering. The differential invariant is essential in understanding or
describing some important phenomena or procedures in mathematics, physics,
chemistry, biology or computer science etc. The derivation of differential
invariants is usually difficult or complicated. This paper reports a discovery
that under the affine transform, differential invariants have similar
structures with moment invariants up to a scalar function of transform
parameters. If moment invariants are known, relative differential invariants
can be obtained by the substitution of moments by derivatives with the same
order. Whereas moment invariants can be calculated by multiple integrals, this
method provides a simple way to derive differential invariants without the need
to resolve any equation system. Since the definition of moments on different
manifolds or in different dimension of spaces is well established, differential
invariants on or in them will also be well defined. Considering that moments
have a strong background in mathematics and physics, this technique offers a
new view angle to the inner structure of invariants. Projective differential
invariants can also be found in this way with a screening process.
",Computer Science,Mathematics
"Taming Non-stationary Bandits: A Bayesian Approach   We consider the multi armed bandit problem in non-stationary environments.
Based on the Bayesian method, we propose a variant of Thompson Sampling which
can be used in both rested and restless bandit scenarios. Applying discounting
to the parameters of prior distribution, we describe a way to systematically
reduce the effect of past observations. Further, we derive the exact expression
for the probability of picking sub-optimal arms. By increasing the exploitative
value of Bayes' samples, we also provide an optimistic version of the
algorithm. Extensive empirical analysis is conducted under various scenarios to
validate the utility of proposed algorithms. A comparison study with various
state-of-the-arm algorithms is also included.
",Computer Science; Statistics,Statistics
"Non-resonant secular dynamics of trans-Neptunian objects perturbed by a distant super-Earth   We use a secular model to describe the non-resonant dynamics of
trans-Neptunian objects in the presence of an external ten-earth-mass
perturber. The secular dynamics is analogous to an ""eccentric Kozai mechanism""
but with both an inner component (the four giant planets) and an outer one (the
eccentric distant perturber). By the means of Poincaré sections, the cases of
a non-inclined or inclined outer planet are successively studied, making the
connection with previous works. In the inclined case, the problem is reduced to
two degrees of freedom by assuming a non-precessing argument of perihelion for
the perturbing body.
The size of the perturbation is typically ruled by the semi-major axis of the
small body: we show that the classic integrable picture is still valid below
about 70 AU, but it is progressively destroyed when we get closer to the
external perturber. In particular, for a>150 AU, large-amplitude orbital flips
become possible, and for a>200 AU, the Kozai libration islands are totally
submerged by the chaotic sea. Numerous resonance relations are highlighted. The
most large and persistent ones are associated to apsidal alignments or
anti-alignments with the orbit of the distant perturber.
",Physics,Physics
"Quantum sensors for the generating functional of interacting quantum field theories   Difficult problems described in terms of interacting quantum fields evolving
in real time or out of equilibrium are abound in condensed-matter and
high-energy physics. Addressing such problems via controlled experiments in
atomic, molecular, and optical physics would be a breakthrough in the field of
quantum simulations. In this work, we present a quantum-sensing protocol to
measure the generating functional of an interacting quantum field theory and,
with it, all the relevant information about its in or out of equilibrium
phenomena. Our protocol can be understood as a collective interferometric
scheme based on a generalization of the notion of Schwinger sources in quantum
field theories, which make it possible to probe the generating functional. We
show that our scheme can be realized in crystals of trapped ions acting as
analog quantum simulators of self-interacting scalar quantum field theories.
",Physics,Physics
"Unifying Map and Landmark Based Representations for Visual Navigation   This works presents a formulation for visual navigation that unifies map
based spatial reasoning and path planning, with landmark based robust plan
execution in noisy environments. Our proposed formulation is learned from data
and is thus able to leverage statistical regularities of the world. This allows
it to efficiently navigate in novel environments given only a sparse set of
registered images as input for building representations for space. Our
formulation is based on three key ideas: a learned path planner that outputs
path plans to reach the goal, a feature synthesis engine that predicts features
for locations along the planned path, and a learned goal-driven closed loop
controller that can follow plans given these synthesized features. We test our
approach for goal-driven navigation in simulated real world environments and
report performance gains over competitive baseline approaches.
",Computer Science,Computer Science
"On the Role of Text Preprocessing in Neural Network Architectures: An Evaluation Study on Text Categorization and Sentiment Analysis   Text preprocessing is often the first step in the pipeline of a Natural
Language Processing (NLP) system, with potential impact in its final
performance. Despite its importance, text preprocessing has not received much
attention in the deep learning literature. In this paper we investigate the
impact of simple text preprocessing decisions (particularly tokenizing,
lemmatizing, lowercasing and multiword grouping) on the performance of a
standard neural text classifier. We perform an extensive evaluation on standard
benchmarks from text categorization and sentiment analysis. While our
experiments show that a simple tokenization of input text is generally
adequate, they also highlight significant degrees of variability across
preprocessing techniques. This reveals the importance of paying attention to
this usually-overlooked step in the pipeline, particularly when comparing
different models. Finally, our evaluation provides insights into the best
preprocessing practices for training word embeddings.
",Computer Science,Computer Science
"The Principle of Similitude in Biology: From Allometry to the Formulation of Dimensionally Homogenous `Laws'   Meaningful laws of nature must be independent of the units employed to
measure the variables. The principle of similitude (Rayleigh 1915) or
dimensional homogeneity, states that only commensurable quantities (ones having
the same dimension) may be compared, therefore, meaningful laws of nature must
be homogeneous equations in their various units of measurement, a result which
was formalized in the $\rm \Pi$ theorem (Vaschy 1892; Buckingham 1914).
However, most relations in allometry do not satisfy this basic requirement,
including the `3/4 Law' (Kleiber 1932) that relates the basal metabolic rate
and body mass, which it is sometimes claimed to be the most fundamental
biological rate (Brown et al. 2004) and the closest to a law in life sciences
(West \& Brown 2004). Using the $\rm \Pi$ theorem, here we show that it is
possible to construct a unique homogeneous equation for the metabolic rates, in
agreement with data in the literature. We find that the variations in the
dependence of the metabolic rates on body mass are secondary, coming from
variations in the allometric dependence of the heart frequencies. This includes
not only different classes of animals (mammals, birds, invertebrates) but also
different exercise conditions (basal and maximal). Our results demonstrate that
most of the differences found in the allometric exponents (White et al. 2007)
are due to compare incommensurable quantities and that our dimensionally
homogenous formula, unify these differences into a single formulation. We
discuss the ecological implications of this new formulation in the context of
the Malthusian's, Fenchel's and the total energy consumed in a lifespan
relations.
",Physics,Physics
"Facial Recognition Enabled Smart Door Using Microsoft Face API   Privacy and Security are two universal rights and, to ensure that in our
daily life we are secure, a lot of research is going on in the field of home
security, and IoT is the turning point for the industry, where we connect
everyday objects to share data for our betterment. Facial recognition is a
well-established process in which the face is detected and identified out of
the image. We aim to create a smart door, which secures the gateway on the
basis of who we are. In our proof of concept of a smart door we have used a
live HD camera on the front side of setup attached to a display monitor
connected to the camera to show who is standing in front of the door, also the
whole system will be able to give voice outputs by processing text them on the
Raspberry Pi ARM processor used and show the answers as output on the screen.
We are using a set of electromagnets controlled by the microcontroller, which
will act as a lock. So a person can open the smart door with the help of facial
recognition and at the same time also be able to interact with it. The facial
recognition is done by Microsoft face API but our state of the art desktop
application operating over Microsoft Visual Studio IDE reduces the
computational time by detecting the face out of the photo and giving that as
the output to Microsoft Face API, which is hosted over Microsoft Azure cloud
support.
",Computer Science,Computer Science
"An Exploratory Study on the Implementation and Adoption of ERP Solutions for Businesses   Enterprise Resource Planning (ERP) systems have been covered in both
mainstream Information Technology (IT) periodicals, and in academic literature,
as a result of extensive adoption by organisations in the last two decades.
Some of the past studies have reported operational efficiency and other gains,
while other studies have pointed out the challenges. ERP systems continue to
evolve, moving into the cloud hosted sphere, and being implemented by
relatively smaller and regional companies. This project has carried out an
exploratory study into the use of ERP systems, within Hawke's Bay New Zealand.
ERP systems make up a major investment and undertaking by those companies.
Therefore, research and lessons learned in this area are very important. In
addition to a significant initial literature review, this project has conducted
a survey on the local users' experience with Microsoft Dynamics NAV (a popular
ERP brand). As a result, this study will contribute new and relevant
information to the literature on business information systems and to ERP
systems, in particular.
",Computer Science,Computer Science
"Aggregation and Resource Scheduling in Machine-type Communication Networks: A Stochastic Geometry Approach   Data aggregation is a promising approach to enable massive machine-type
communication (mMTC). This paper focuses on the aggregation phase where a
massive number of machine-type devices (MTDs) transmit to aggregators. By using
non-orthogonal multiple access (NOMA) principles, we allow several MTDs to
share the same orthogonal channel in our proposed hybrid access scheme. We
develop an analytical framework based on stochastic geometry to investigate the
system performance in terms of average success probability and average number
of simultaneously served MTDs, under imperfect successive interference
cancellation (SIC) at the aggregators, for two scheduling schemes: random
resource scheduling (RRS) and channel-aware resource scheduling (CRS). We
identify the power constraints on the MTDs sharing the same channel to attain a
fair coexistence with purely orthogonal multiple access (OMA) setups, then
power control coefficients are found so that these MTDs perform with similar
reliability. We show that under high access demand, the hybrid scheme with CRS
outperforms the OMA setup by simultaneously serving more MTDs with reduced
power consumption.
",Computer Science; Statistics,Computer Science
"Electrostatic gyrokinetic simulation of global tokamak boundary plasma and the generation of nonlinear intermittent turbulence   Boundary plasma physics plays an important role in tokamak confinement, but
is difficult to simulate in a gyrokinetic code due to the scale-inseparable
nonlocal multi-physics in magnetic separatrix and open magnetic field geometry.
Neutral particles are also an important part of the boundary plasma physics. In
the present paper, noble electrostatic gyrokinetic techniques to simulate the
flux-driven, low-beta electrostatic boundary plasma is reported. Gyrokinetic
ions and drift-kinetic electrons are utilized without scale-separation between
the neoclassical and turbulence dynamics. It is found that the nonlinear
intermittent turbulence is a natural gyrokinetic phenomenon in the boundary
plasma in the vicinity of the magnetic separatrix surface and in the scrape-off
layer.
",Physics,Physics
"Deep Learning Interior Tomography for Region-of-Interest Reconstruction   Interior tomography for the region-of-interest (ROI) imaging has advantages
of using a small detector and reducing X-ray radiation dose. However, standard
analytic reconstruction suffers from severe cupping artifacts due to existence
of null space in the truncated Radon transform. Existing penalized
reconstruction methods may address this problem but they require extensive
computations due to the iterative reconstruction. Inspired by the recent deep
learning approaches to low-dose and sparse view CT, here we propose a deep
learning architecture that removes null space signals from the FBP
reconstruction. Experimental results have shown that the proposed method
provides near-perfect reconstruction with about 7-10 dB improvement in PSNR
over existing methods in spite of significantly reduced run-time complexity.
",Computer Science; Statistics,Computer Science; Statistics
"Stochastic Constraint Programming as Reinforcement Learning   Stochastic Constraint Programming (SCP) is an extension of Constraint
Programming (CP) used for modelling and solving problems involving constraints
and uncertainty. SCP inherits excellent modelling abilities and filtering
algorithms from CP, but so far it has not been applied to large problems.
Reinforcement Learning (RL) extends Dynamic Programming to large stochastic
problems, but is problem-specific and has no generic solvers. We propose a
hybrid combining the scalability of RL with the modelling and constraint
filtering methods of CP. We implement a prototype in a CP system and
demonstrate its usefulness on SCP problems.
",Computer Science,Computer Science
"Fractional Topological Elasticity and Fracton Order   We analyze the ""higher rank"" gauge theories, that capture some of the
phenomenology of the Fracton order. It is shown that these theories loose gauge
invariance when arbitrarily weak and smooth curvature is introduced. We propose
a resolution to this problem by introducing a theory invariant under
area-preserving diffeomorphisms, which reduce to the ""higher rank"" gauge
transformations upon linearization around a flat background. The proposed
theory is \emph{geometric} in nature and is interpreted as a theory of
\emph{fractional topological elasticity}. This theory exhibits the Fracton
phenomenology. We explore the conservation laws, topological excitations,
linear response, various kinematical constraints, and canonical structure of
the theory. Finally, we emphasize that the very structure of Riemann-Cartan
geometry, which we use to formulate the theory, encodes the Fracton
phenomenology, suggesting that the Fracton order itself is \emph{geometric} in
nature.
",Physics,Mathematics
"Evidence for OH or H2O on the surface of 433 Eros and 1036 Ganymed   Water and hydroxyl, once thought to be found only in the primitive airless
bodies that formed beyond roughly 2.5-3 AU, have recently been detected on the
Moon and Vesta, which both have surfaces dominated by evolved, non-primitive
compositions. In both these cases, the water/OH is thought to be exogenic,
either brought in via impacts with comets or hydrated asteroids or created via
solar wind interactions with silicates in the regolith or both. Such exogenic
processes should also be occurring on other airless body surfaces. To test this
hypothesis, we used the NASA Infrared Telescope Facility (IRTF) to measure
reflectance spectra (2.0 to 4.1 {\mu}m) of two large near-Earth asteroids
(NEAs) with compositions generally interpreted as anhydrous: 433 Eros and 1036
Ganymed. OH is detected on both of these bodies in the form of absorption
features near 3 {\mu}m. The spectra contain a component of thermal emission at
longer wavelengths, from which we estimate thermal of 167+/- 98 J m-2s-1/2K-1
for Eros (consistent with previous estimates) and 214+/- 80 J m-2s-1/2K-1 for
Ganymed, the first reported measurement of thermal inertia for this object.
These observations demonstrate that processes responsible for water/OH creation
on large airless bodies also act on much smaller bodies.
",Physics,Physics
"Lattice Boltzmann simulation of viscous fingering of immiscible displacement in a channel using an improved wetting scheme   An improved wetting boundary implementation strategy is proposed based on
lattice Boltzmann color-gradient model in this paper. In this strategy, an
extra interface force condition is demonstrated based on the diffuse interface
assumption and is employed in contact line region. It has been validated by
three benchmark problems: static droplet wetting on a flat surface and a curved
surface, and dynamic capillary filling. Good performances are shown in all
three cases. Relied on the strict validation to our scheme, the viscous
fingering phenomenon of immiscible fluids displacement in a two-dimensional
channel has been restudied in this paper. High viscosity ratio, wide range
contact angle, accurate moving contact line and mutual independence between
surface tension and viscosity are the obvious advantages of our model. We find
the linear relationship between the contact angle and displacement velocity or
variation of finger length. When the viscosity ratio is smaller than 20, the
displacement velocity is increasing with increasing viscosity ratio and
reducing capillary number, and when the viscosity ratio is larger than 20, the
displacement velocity tends to a specific constant. A similar conclusion is
obtained on the variation of finger length.
",Physics,Physics
"Automated Problem Identification: Regression vs Classification via Evolutionary Deep Networks   Regression or classification? This is perhaps the most basic question faced
when tackling a new supervised learning problem. We present an Evolutionary
Deep Learning (EDL) algorithm that automatically solves this by identifying the
question type with high accuracy, along with a proposed deep architecture.
Typically, a significant amount of human insight and preparation is required
prior to executing machine learning algorithms. For example, when creating deep
neural networks, the number of parameters must be selected in advance and
furthermore, a lot of these choices are made based upon pre-existing knowledge
of the data such as the use of a categorical cross entropy loss function.
Humans are able to study a dataset and decide whether it represents a
classification or a regression problem, and consequently make decisions which
will be applied to the execution of the neural network. We propose the
Automated Problem Identification (API) algorithm, which uses an evolutionary
algorithm interface to TensorFlow to manipulate a deep neural network to decide
if a dataset represents a classification or a regression problem. We test API
on 16 different classification, regression and sentiment analysis datasets with
up to 10,000 features and up to 17,000 unique target values. API achieves an
average accuracy of $96.3\%$ in identifying the problem type without hardcoding
any insights about the general characteristics of regression or classification
problems. For example, API successfully identifies classification problems even
with 1000 target values. Furthermore, the algorithm recommends which loss
function to use and also recommends a neural network architecture. Our work is
therefore a step towards fully automated machine learning.
",Computer Science; Statistics,Computer Science; Statistics
"Astronomy of Cholanaikkan tribe of Kerala   Cholanaikkans are a diminishing tribe of India. With a population of less
than 200 members, this tribe living in the reserved forests about 80 km from
Kozhikode, it is one of the most isolated tribes. A programme of the Government
of Kerala brings some of them to Kozhikode once a year. We studied various
aspects of the tribe during such a visit in 2016. We report their science and
technology.
",Physics,Physics
"Efficient Estimation for Dimension Reduction with Censored Data   We propose a general index model for survival data, which generalizes many
commonly used semiparametric survival models and belongs to the framework of
dimension reduction. Using a combination of geometric approach in
semiparametrics and martingale treatment in survival data analysis, we devise
estimation procedures that are feasible and do not require
covariate-independent censoring as assumed in many dimension reduction methods
for censored survival data. We establish the root-$n$ consistency and
asymptotic normality of the proposed estimators and derive the most efficient
estimator in this class for the general index model. Numerical experiments are
carried out to demonstrate the empirical performance of the proposed estimators
and an application to an AIDS data further illustrates the usefulness of the
work.
",Mathematics; Statistics,Mathematics; Statistics
"A Note on Kaldi's PLDA Implementation   Some explanations to Kaldi's PLDA implementation to make formula derivation
easier to catch.
",Statistics,Mathematics
"Entanglement entropy and computational complexity of the Anderson impurity model out of equilibrium I: quench dynamics   We study the growth of entanglement entropy in density matrix renormalization
group calculations of the real-time quench dynamics of the Anderson impurity
model. We find that with appropriate choice of basis, the entropy growth is
logarithmic in both the interacting and noninteracting single-impurity models.
The logarithmic entropy growth is understood from a noninteracting chain model
as a critical behavior separating regimes of linear growth and saturation of
entropy, corresponding respectively to an overlapping and gapped energy spectra
of the set of bath states. We find that with an appropriate choices of basis
(energy-ordered bath orbitals), logarithmic entropy growth is the generic
behavior of quenched impurity models. A noninteracting calculation of a
double-impurity Anderson model supports the conclusion in the multi-impurity
case. The logarithmic growth of entanglement entropy enables studies of quench
dynamics to very long times.
",Physics,Physics
"Multi-objective optimization to explicitly account for model complexity when learning Bayesian Networks   Bayesian Networks have been widely used in the last decades in many fields,
to describe statistical dependencies among random variables. In general,
learning the structure of such models is a problem with considerable
theoretical interest that still poses many challenges. On the one hand, this is
a well-known NP-complete problem, which is practically hardened by the huge
search space of possible solutions. On the other hand, the phenomenon of
I-equivalence, i.e., different graphical structures underpinning the same set
of statistical dependencies, may lead to multimodal fitness landscapes further
hindering maximum likelihood approaches to solve the task. Despite all these
difficulties, greedy search methods based on a likelihood score coupled with a
regularization term to account for model complexity, have been shown to be
surprisingly effective in practice. In this paper, we consider the formulation
of the task of learning the structure of Bayesian Networks as an optimization
problem based on a likelihood score. Nevertheless, our approach do not adjust
this score by means of any of the complexity terms proposed in the literature;
instead, it accounts directly for the complexity of the discovered solutions by
exploiting a multi-objective optimization procedure. To this extent, we adopt
NSGA-II and define the first objective function to be the likelihood of a
solution and the second to be the number of selected arcs. We thoroughly
analyze the behavior of our method on a wide set of simulated data, and we
discuss the performance considering the goodness of the inferred solutions both
in terms of their objective functions and with respect to the retrieved
structure. Our results show that NSGA-II can converge to solutions
characterized by better likelihood and less arcs than classic approaches,
although paradoxically frequently characterized by a lower similarity to the
target network.
",Statistics,Computer Science; Statistics
"Theoretical Accuracy in Cosmological Growth Estimation   We elucidate the importance of the consistent treatment of gravity-model
specific non-linearities when estimating the growth of cosmological structures
from redshift space distortions (RSD). Within the context of standard
perturbation theory (SPT), we compare the predictions of two theoretical
templates with redshift space data from COLA (COmoving Lagrangian Acceleration)
simulations in the normal branch of DGP gravity (nDGP) and General Relativity
(GR). Using COLA for these comparisons is validated using a suite of full
N-body simulations for the same theories. The two theoretical templates
correspond to the standard general relativistic perturbation equations and
those same equations modelled within nDGP. Gravitational clustering non-linear
effects are accounted for by modelling the power spectrum up to one loop order
and redshift space clustering anisotropy is modelled using the Taruya,
Nishimichi and Saito (TNS) RSD model. Using this approach, we attempt to
recover the simulation's fiducial logarithmic growth parameter $f$. By
assigning the simulation data with errors representing an idealised survey with
a volume of $10\mbox{Gpc}^3/h^3$, we find the GR template is unable to recover
fiducial $f$ to within 1$\sigma$ at $z=1$ when we match the data up to $k_{\rm
max}=0.195h$/Mpc. On the other hand, the DGP template recovers the fiducial
value within $1\sigma$. Further, we conduct the same analysis for sets of mock
data generated for generalised models of modified gravity using SPT, where
again we analyse the GR template's ability to recover the fiducial value. We
find that for models with enhanced gravitational non-linearity, the theoretical
bias of the GR template becomes significant for stage IV surveys. Thus, we show
that for the future large data volume galaxy surveys, the self-consistent
modelling of non-GR gravity scenarios will be crucial in constraining theory
parameters.
",Physics,Physics
"An Ensemble Boosting Model for Predicting Transfer to the Pediatric Intensive Care Unit   Our work focuses on the problem of predicting the transfer of pediatric
patients from the general ward of a hospital to the pediatric intensive care
unit. Using data collected over 5.5 years from the electronic health records of
two medical facilities, we develop classifiers based on adaptive boosting and
gradient tree boosting. We further combine these learned classifiers into an
ensemble model and compare its performance to a modified pediatric early
warning score (PEWS) baseline that relies on expert defined guidelines. To
gauge model generalizability, we perform an inter-facility evaluation where we
train our algorithm on data from one facility and perform evaluation on a
hidden test dataset from a separate facility. We show that improvements are
witnessed over the PEWS baseline in accuracy (0.77 vs. 0.69), sensitivity (0.80
vs. 0.68), specificity (0.74 vs. 0.70) and AUROC (0.85 vs. 0.73).
",Computer Science; Statistics,Computer Science; Statistics
"Pronunciation recognition of English phonemes /\textipa{@}/, /æ/, /\textipa{A}:/ and /\textipa{2}/ using Formants and Mel Frequency Cepstral Coefficients   The Vocal Joystick Vowel Corpus, by Washington University, was used to study
monophthongs pronounced by native English speakers. The objective of this study
was to quantitatively measure the extent at which speech recognition methods
can distinguish between similar sounding vowels. In particular, the phonemes
/\textipa{@}/, /{\ae}/, /\textipa{A}:/ and /\textipa{2}/ were analysed. 748
sound files from the corpus were used and subjected to Linear Predictive Coding
(LPC) to compute their formants, and to Mel Frequency Cepstral Coefficients
(MFCC) algorithm, to compute the cepstral coefficients. A Decision Tree
Classifier was used to build a predictive model that learnt the patterns of the
two first formants measured in the data set, as well as the patterns of the 13
cepstral coefficients. An accuracy of 70\% was achieved using formants for the
mentioned phonemes. For the MFCC analysis an accuracy of 52 \% was achieved and
an accuracy of 71\% when /\textipa{@}/ was ignored. The results obtained show
that the studied algorithms are far from mimicking the ability of
distinguishing subtle differences in sounds like human hearing does.
",Computer Science,Computer Science
"Ubiquitous quasi-Fuchsian surfaces in cusped hyperbolic 3-manifolds   This paper proves that every finite volume hyperbolic 3-manifold M contains a
ubiquitous collection of closed, immersed, quasi-Fuchsian surfaces. These
surfaces are ubiquitous in the sense that their preimages in the universal
cover separate any pair of disjoint, non-asymptotic geodesic planes. The proof
relies in a crucial way on the corresponding theorem of Kahn and Markovic for
closed 3-manifolds. As a corollary of this result and a companion statement
about surfaces with cusps, we recover Wise's theorem that the fundamental group
of M acts freely and cocompactly on a CAT(0) cube complex.
",Mathematics,Mathematics
"Verifiable Light-Weight Monitoring for Certificate Transparency Logs   Trust in publicly verifiable Certificate Transparency (CT) logs is reduced
through cryptography, gossip, auditing, and monitoring. The role of a monitor
is to observe each and every log entry, looking for suspicious certificates
that interest the entity running the monitor. While anyone can run a monitor,
it requires continuous operation and copies of the logs to be inspected. This
has lead to the emergence of monitoring-as-a-service: a trusted party runs the
monitor and provides registered subjects with selective certificate
notifications, e.g., ""notify me of all foo.com certificates"". We present a
CT/bis extension for verifiable light-weight monitoring that enables subjects
to verify the correctness of such notifications, reducing the trust that is
placed in these monitors. Our extension supports verifiable monitoring of
wild-card domains and piggybacks on CT's existing gossip-audit security model.
",Computer Science,Computer Science
"Fixed-Parameter Tractable Sampling for RNA Design with Multiple Target Structures   The design of multi-stable RNA molecules has important applications in
biology, medicine, and biotechnology. Synthetic design approaches profit
strongly from effective in-silico methods, which can tremendously impact their
cost and feasibility. We revisit a central ingredient of most in-silico design
methods: the sampling of sequences for the design of multi-target structures,
possibly including pseudoknots. For this task, we present the efficient, tree
decomposition-based algorithm. Our fixed parameter tractable approach is
underpinned by establishing the P-hardness of uniform sampling. Modeling the
problem as a constraint network, our program supports generic
Boltzmann-weighted sampling for arbitrary additive RNA energy models; this
enables the generation of RNA sequences meeting specific goals like expected
free energies or \GCb-content. Finally, we empirically study general properties
of the approach and generate biologically relevant multi-target
Boltzmann-weighted designs for a common design benchmark. Generating seed
sequences with our program, we demonstrate significant improvements over the
previously best multi-target sampling strategy (uniform sampling).Our software
is freely available at: this https URL .
",Quantitative Biology,Computer Science; Statistics
"K-means Algorithm over Compressed Binary Data   We consider a network of binary-valued sensors with a fusion center. The
fusion center has to perform K-means clustering on the binary data transmitted
by the sensors. In order to reduce the amount of data transmitted within the
network, the sensors compress their data with a source coding scheme based on
binary sparse matrices. We propose to apply the K-means algorithm directly over
the compressed data without reconstructing the original sensors measurements,
in order to avoid potentially complex decoding operations. We provide
approximated expressions of the error probabilities of the K-means steps in the
compressed domain. From these expressions, we show that applying the K-means
algorithm in the compressed domain enables to recover the clusters of the
original domain. Monte Carlo simulations illustrate the accuracy of the
obtained approximated error probabilities, and show that the coding rate needed
to perform K-means clustering in the compressed domain is lower than the rate
needed to reconstruct all the measurements.
",Computer Science; Mathematics,Computer Science
"Motif and Hypergraph Correlation Clustering   Motivated by applications in social and biological network analysis, we
introduce a new form of agnostic clustering termed~\emph{motif correlation
clustering}, which aims to minimize the cost of clustering errors associated
with both edges and higher-order network structures. The problem may be
succinctly described as follows: Given a complete graph $G$, partition the
vertices of the graph so that certain predetermined `important' subgraphs
mostly lie within the same cluster, while `less relevant' subgraphs are allowed
to lie across clusters. Our contributions are as follows: We first introduce
several variants of motif correlation clustering and then show that these
clustering problems are NP-hard. We then proceed to describe polynomial-time
clustering algorithms that provide constant approximation guarantees for the
problems at hand. Despite following the frequently used LP relaxation and
rounding procedure, the algorithms involve a sophisticated and carefully
designed neighborhood growing step that combines information about both edge
and motif structures. We conclude with several examples illustrating the
performance of the developed algorithms on synthetic and real networks.
",Computer Science,Computer Science; Statistics
"Surface Normals in the Wild   We study the problem of single-image depth estimation for images in the wild.
We collect human annotated surface normals and use them to train a neural
network that directly predicts pixel-wise depth. We propose two novel loss
functions for training with surface normal annotations. Experiments on NYU
Depth and our own dataset demonstrate that our approach can significantly
improve the quality of depth estimation in the wild.
",Computer Science,Computer Science; Statistics
"Extensions of Operators, Liftings of Monads and Distributive Laws   In a previous study, the algebraic formulation of the First Fundamental
Theorem of Calculus (FFTC) is shown to allow extensions of differential and
Rota-Baxter operators on the one hand, and to give rise to liftings of monads
and comonads, and mixed distributive laws on the other. Generalizing the FFTC,
we consider in this paper a class of constraints between a differential
operator and a Rota-Baxter operator. For a given constraint, we show that the
existences of extensions of differential and Rota-Baxter operators, of liftings
of monads and comonads, and of mixed distributive laws are equivalent. We
further give a classification of the constraints satisfying these equivalent
conditions.
",Mathematics,Mathematics
"Realisability of Pomsets via Communicating Automata   Pomsets are a model of concurrent computations introduced by Pratt. They can
provide a syntax-oblivious description of semantics of coordination models
based on asynchronous message-passing, such as Message Sequence Charts (MSCs).
In this paper, we study conditions that ensure a specification expressed as a
set of pomsets can be faithfully realised via communicating automata. Our main
contributions are (i) the definition of a realisability condition accounting
for termination soundness, (ii) conditions for global specifications with
""multi-threaded"" participants, and (iii) the definition of realisability
conditions that can be decided directly over pomsets. A positive by-product of
our approach is the efficiency gain in the verification of the realisability
conditions obtained when restricting to specific classes of choreographies
characterisable in term of behavioural types.
",Computer Science,Computer Science
"FO model checking of geometric graphs   Over the past two decades the main focus of research into first-order (FO)
model checking algorithms has been on sparse relational structures -
culminating in the FPT algorithm by Grohe, Kreutzer and Siebertz for FO model
checking of nowhere dense classes of graphs. On contrary to that, except the
case of locally bounded clique-width only little is currently known about FO
model checking of dense classes of graphs or other structures. We study the FO
model checking problem for dense graph classes definable by geometric means
(intersection and visibility graphs). We obtain new nontrivial FPT results,
e.g., for restricted subclasses of circular-arc, circle, box, disk, and
polygon-visibility graphs. These results use the FPT algorithm by Gajarský et
al. for FO model checking of posets of bounded width. We also complement the
tractability results by related hardness reductions.
",Computer Science,Computer Science; Mathematics
"Matter-wave solutions in the Bose-Einstein condensates with the harmonic and Gaussian potentials   We study exact solutions of the quasi-one-dimensional Gross-Pitaevskii (GP)
equation with the (space, time)-modulated potential and nonlinearity and the
time-dependent gain or loss term in Bose-Einstein condensates. In particular,
based on the similarity transformation, we report several families of exact
solutions of the GP equation in the combination of the harmonic and Gaussian
potentials, in which some physically relevant solutions are described. The
stability of the obtained matter-wave solutions is addressed numerically such
that some stable solutions are found. Moreover, we also analyze the parameter
regimes for the stable solutions. These results may raise the possibility of
relative experiments and potential applications.
",Physics; Mathematics,Physics; Mathematics
"On the area of constrained polygonal linkages   We study configuration spaces of linkages whose underlying graph are polygons
with diagonal constrains, or more general, partial two-trees. We show that
(with an appropriate definition) the oriented area is a Bott-Morse function on
the configuration space. Its critical points are described and Bott-Morse
indices are computed. This paper is a generalization of analogous results for
polygonal linkages (obtained earlier by G. Khimshiashvili, G. Panina, and A.
Zhukova).
",Mathematics,Mathematics
"Evolution of the Kondo lattice electronic structure above the transport coherence temperature   The temperature-dependent evolution of the Kondo lattice is a long-standing
topic of theoretical and experimental investigation and yet it lacks a truly
microscopic description of the relation of the basic $f$-$d$ hybridization
processes to the fundamental temperature scales of Kondo screening and
Fermi-liquid lattice coherence. Here, the temperature-dependence of $f$-$d$
hybridized band dispersions and Fermi-energy $f$ spectral weight in the Kondo
lattice system CeCoIn$_5$ is investigated using $f$-resonant angle-resolved
photoemission (ARPES) with sufficient detail to allow direct comparison to
first principles dynamical mean field theory (DMFT) calculations containing
full realism of crystalline electric field states. The ARPES results, for two
orthogonal (001) and (100) cleaved surfaces and three different $f$-$d$
hybridization scenarios, with additional microscopic insight provided by DMFT,
reveal $f$ participation in the Fermi surface at temperatures much higher than
the lattice coherence temperature, $T^*\approx$ 45 K, commonly believed to be
the onset for such behavior. The identification of a $T$-dependent crystalline
electric field degeneracy crossover in the DMFT theory $below$ $T^*$ is
specifically highlighted.
",Physics,Physics
"Possible heights of graph transformation groups   In the following text we prove that for all finite $p\geq0$ there exists a
topological graph $X$ such that $\{p,p+1,p+2,\ldots\}\cup\{+\infty\}$ is the
collection of all possible heights for transformation groups with phase space
$X$. Moreover for all topological graph $X$ with $p$ as height of
transformation group $(Homeo(X),X)$, $\{p,p+1,p+2,\ldots\}\cup\{+\infty\}$
again is the collection of all possible heights for transformation groups with
phase space $X$.
",Mathematics,Mathematics
"The Supernova -- Supernova Remnant Connection   Many aspects of the progenitor systems, environments, and explosion dynamics
of the various subtypes of supernovae are difficult to investigate at
extragalactic distances where they are observed as unresolved sources.
Alternatively, young supernova remnants in our own galaxy and in the Large and
Small Magellanic Clouds offer opportunities to resolve, measure, and track
expanding stellar ejecta in fine detail, but the handful that are known exhibit
widely different properties that reflect the diversity of their parent
explosions and local circumstellar and interstellar environments. A way of
complementing both supernova and supernova remnant research is to establish
strong empirical links between the two separate stages of stellar explosions.
Here we briefly review recent progress in the development of
supernova---supernova remnant connections, paying special attention to
connections made through the study of ""middle-aged"" (10-100 yr) supernovae and
young (< 1000 yr) supernova remnants. We highlight how this approach can
uniquely inform several key areas of supernova research, including the origins
of explosive mixing, high-velocity jets, and the formation of dust in the
ejecta.
",Physics,Physics
"UV Detector based on InAlN/GaN-on-Si HEMT Stack with Photo-to-Dark Current Ratio > 107   We demonstrate an InAlN/GaN-on-Si HEMT based UV detector with photo to dark
current ratio > 107. Ti/Al/Ni/Au metal stack was evaporated and rapid thermal
annealed for Ohmic contacts to the 2D electron gas (2DEG) at the InAlN/GaN
interface while the channel + barrier was recess etched to a depth of 20 nm to
pinch-off the 2DEG between Source-Drain pads. Spectral responsivity (SR) of 34
A/W at 367 nm was measured at 5 V in conjunction with very high photo to dark
current ratio of > 10^7. The photo to dark current ratio at a fixed bias was
found to be decreasing with increase in recess length of the PD. The fabricated
devices were found to exhibit a UV-to-visible rejection ratio of >103 with a
low dark current < 32 pA at 5 V. Transient measurements showed rise and fall
times in the range of 3-4 ms. The gain mechanism was investigated and carrier
lifetimes were estimated which matched well with those reported elsewhere.
",Physics,Physics
"Double-sided probing by map of Asplund's distances using Logarithmic Image Processing in the framework of Mathematical Morphology   We establish the link between Mathematical Morphology and the map of
Asplund's distances between a probe and a grey scale function, using the
Logarithmic Image Processing scalar multiplication. We demonstrate that the map
is the logarithm of the ratio between a dilation and an erosion of the function
by a structuring function: the probe. The dilations and erosions are mappings
from the lattice of the images into the lattice of the positive functions.
Using a flat structuring element, the expression of the map of Asplund's
distances can be simplified with a dilation and an erosion of the image; these
mappings stays in the lattice of the images. We illustrate our approach by an
example of pattern matching with a non-flat structuring function.
",Computer Science; Mathematics,Mathematics
"Comparison of Flow Scheduling Policies for Mix of Regular and Deadline Traffic in Datacenter Environments   Datacenters are the main infrastructure on top of which cloud computing
services are offered. Such infrastructure may be shared by a large number of
tenants and applications generating a spectrum of datacenter traffic. Delay
sensitive applications and applications with specific Service Level Agreements
(SLAs), generate deadline constrained flows, while other applications initiate
flows that are desired to be delivered as early as possible. As a result,
datacenter traffic is a mix of two types of flows: deadline and regular. There
are several scheduling policies for either traffic type with focus on
minimizing completion times or deadline miss rate. In this report, we apply
several scheduling policies to mix traffic scenario while varying the ratio of
regular to deadline traffic. We consider FCFS (First Come First Serve), SRPT
(Shortest Remaining Processing Time) and Fair Sharing as deadline agnostic
approaches and a combination of Earliest Deadline First (EDF) with either FCFS
or SRPT as deadline-aware schemes. In addition, for the latter, we consider
both cases of prioritizing deadline traffic (Deadline First) and prioritizing
regular traffic (Deadline Last). We study both light-tailed and heavy-tailed
flow size distributions and measure mean, median and tail flow completion times
(FCT) for regular flows along with Deadline Miss Rate (DMR) and average
lateness for deadline flows. We also consider two operation regimes of
lightly-loaded (low utilization) and heavily-loaded (high utilization). We find
that performance of deadline-aware schemes is highly dependent on fraction of
deadline traffic. With light-tailed flow sizes, we find that FCFS performs
better in terms of tail times and average lateness while SRPT performs better
in average times and deadline miss rate. For heavy-tailed flow sizes, except
for tail times, SRPT performs better in all other metrics.
",Computer Science,Computer Science
"Magnetic behavior of new compounds, Gd3RuSn6 and Tb3RuSn6   We report temperature (T) dependence of dc magnetization, electrical
resistivity (rho(T)), and heat-capacity of rare-earth (R) compounds, Gd3RuSn6
and Tb3RuSn6, which are found to crystallize in the Yb3CoSn6-type orthorhombic
structure (space group: Cmcm). The results establish that there is an onset of
antiferromagnetic order near (T_N) 19 and 25 K respectively. In addition, we
find that there is another magnetic transition for both the cases around 14 and
17 K respectively. In the case of the Gd compound, the spin-scattering
contribution to rho is found to increase below 75 K as the material is cooled
towards T_N, thereby resulting in a minimum in the plot of rho(T) unexpected
for Gd based systems. Isothermal magnetization at 1.8 K reveals an upward
curvature around 50 kOe. Isothermal magnetoresistance plots show interesting
anomalies in the magnetically ordered state. There are sign reversals in the
plot of isothermal entropy change versus T in the magnetically ordered state,
indicating subtle changes in the spin reorientation with T. The results reveal
that these compounds exhibit interesting magnetic properties.
",Physics,Physics
"Stage 4 validation of the Satellite Image Automatic Mapper lightweight computer program for Earth observation Level 2 product generation, Part 1 Theory   The European Space Agency (ESA) defines an Earth Observation (EO) Level 2
product as a multispectral (MS) image corrected for geometric, atmospheric,
adjacency and topographic effects, stacked with its scene classification map
(SCM), whose legend includes quality layers such as cloud and cloud-shadow. No
ESA EO Level 2 product has ever been systematically generated at the ground
segment. To contribute toward filling an information gap from EO big data to
the ESA EO Level 2 product, an original Stage 4 validation (Val) of the
Satellite Image Automatic Mapper (SIAM) lightweight computer program was
conducted by independent means on an annual Web-Enabled Landsat Data (WELD)
image composite time-series of the conterminous U.S. The core of SIAM is a one
pass prior knowledge based decision tree for MS reflectance space
hyperpolyhedralization into static color names presented in literature in
recent years. For the sake of readability this paper is split into two. The
present Part 1 Theory provides the multidisciplinary background of a priori
color naming in cognitive science, from linguistics to computer vision. To cope
with dictionaries of MS color names and land cover class names that do not
coincide and must be harmonized, an original hybrid guideline is proposed to
identify a categorical variable pair relationship. An original quantitative
measure of categorical variable pair association is also proposed. The
subsequent Part 2 Validation discusses Stage 4 Val results collected by an
original protocol for wall-to-wall thematic map quality assessment without
sampling where the test and reference map legends can differ. Conclusions are
that the SIAM-WELD maps instantiate a Level 2 SCM product whose legend is the 4
class taxonomy of the FAO Land Cover Classification System at the Dichotomous
Phase Level 1 vegetation/nonvegetation and Level 2 terrestrial/aquatic.
",Computer Science,Computer Science
"A Dynamic Programming Principle for Distribution-Constrained Optimal Stopping   We consider an optimal stopping problem where a constraint is placed on the
distribution of the stopping time. Reformulating the problem in terms of
so-called measure-valued martingales allows us to transform the marginal
constraint into an initial condition and view the problem as a stochastic
control problem; we establish the corresponding dynamic programming principle.
",Mathematics,Computer Science; Mathematics
"Precision Prediction for the Cosmological Density Distribution   The distribution of matter in the universe is, to first order, lognormal.
Improving this approximation requires characterization of the third moment
(skewness) of the log density field. Thus, using Millennium Simulation
phenomenology and building on previous work, we present analytic fits for the
mean, variance, and skewness of the log density field $A$. We further show that
a Generalized Extreme Value (GEV) distribution accurately models $A$; we submit
that this GEV behavior is the result of strong intrapixel correlations, without
which the smoothed distribution would tend (by the Central Limit Theorem)
toward a Gaussian. Our GEV model yields cumulative distribution functions
accurate to within 1.7 per cent for near-concordance cosmologies, over a range
of redshifts and smoothing scales.
",Physics,Physics
"An Optics-Based Approach to Thermal Management of Photovoltaics: Selective-Spectral and Radiative Cooling   For commercial one-sun solar modules, up to 80% of the incoming sunlight may
be dissipated as heat, potentially raising the temperature 20 C - 30 C higher
than the ambient. In the long term, extreme self-heating erodes efficiency and
shortens lifetime, thereby dramatically reducing the total energy output.
Therefore, it is critically important to develop effective and practical (and
preferably passive) cooling methods to reduce operating temperature of PV
modules. In this paper, we explore two fundamental (but often overlooked)
origins of PV self-heating, namely, sub-bandgap absorption and imperfect
thermal radiation. The analysis suggests that we redesign the optical
properties of the solar module to eliminate parasitic absorption
(selective-spectral cooling) and enhance thermal emission (radiative cooling).
Our Comprehensive opto-electro-thermal simulation shows that the proposed
techniques would cool the one-sun and low-concentrated terrestrial solar
modules up to 10 C and 20 C, respectively. This self-cooling would
substantially extend the lifetime for solar modules, with The corresponding
increase in energy yields and reduced LCOE.
",Physics,Physics
"Functional geometry of protein-protein interaction networks   Motivation: Protein-protein interactions (PPIs) are usually modelled as
networks. These networks have extensively been studied using graphlets, small
induced subgraphs capturing the local wiring patterns around nodes in networks.
They revealed that proteins involved in similar functions tend to be similarly
wired. However, such simple models can only represent pairwise relationships
and cannot fully capture the higher-order organization of protein interactions,
including protein complexes. Results: To model the multi-sale organization of
these complex biological systems, we utilize simplicial complexes from
computational geometry. The question is how to mine these new representations
of PPI networks to reveal additional biological information. To address this,
we define simplets, a generalization of graphlets to simplicial complexes. By
using simplets, we define a sensitive measure of similarity between simplicial
complex network representations that allows for clustering them according to
their data types better than clustering them by using other state-of-the-art
measures, e.g., spectral distance, or facet distribution distance. We model
human and baker's yeast PPI networks as simplicial complexes that capture PPIs
and protein complexes as simplices. On these models, we show that our newly
introduced simplet-based methods cluster proteins by function better than the
clustering methods that use the standard PPI networks, uncovering the new
underlying functional organization of the cell. We demonstrate the existence of
the functional geometry in the PPI data and the superiority of our
simplet-based methods to effectively mine for new biological information hidden
in the complexity of the higher order organization of PPI networks.
",Quantitative Biology,Computer Science; Physics
"A new algorithm for fast generalized DFTs   We give an new arithmetic algorithm to compute the generalized Discrete
Fourier Transform (DFT) over finite groups $G$. The new algorithm uses
$O(|G|^{\omega/2 + o(1)})$ operations to compute the generalized DFT over
finite groups of Lie type, including the linear, orthogonal, and symplectic
families and their variants, as well as all finite simple groups of Lie type.
Here $\omega$ is the exponent of matrix multiplication, so the exponent
$\omega/2$ is optimal if $\omega = 2$. Previously, ""exponent one"" algorithms
were known for supersolvable groups and the symmetric and alternating groups.
No exponent one algorithms were known (even under the assumption $\omega = 2$)
for families of linear groups of fixed dimension, and indeed the previous
best-known algorithm for $SL_2(F_q)$ had exponent $4/3$ despite being the focus
of significant effort. We unconditionally achieve exponent at most $1.19$ for
this group, and exponent one if $\omega = 2$. Our algorithm also yields an
improved exponent for computing the generalized DFT over general finite groups
$G$, which beats the longstanding previous best upper bound, for any $\omega$.
In particular, assuming $\omega = 2$, we achieve exponent $\sqrt{2}$, while the
previous best was $3/2$.
",Computer Science; Mathematics,Computer Science
"Spectral up- and downshifting of Akhmediev breathers under wind forcing   We experimentally and numerically investigate the effect of wind forcing on
the spectral dynamics of Akhmediev breathers, a wave-type known to model the
modulation instability. We develop the wind model to the same order in
steepness as the higher order modifcation of the nonlinear Schroedinger
equation, also referred to as the Dysthe equation. This results in an
asymmetric wind term in the higher order, in addition to the leading order wind
forcing term. The derived model is in good agreement with laboratory
experiments within the range of the facility's length. We show that the leading
order forcing term amplifies all frequencies equally and therefore induces only
a broadening of the spectrum while the asymmetric higher order term in the
model enhances higher frequencies more than lower ones. Thus, the latter term
induces a permanent upshift of the spectral mean. On the other hand, in
contrast to the direct effect of wind forcing, wind can indirectly lead to
frequency downshifts, due to dissipative effects such as wave breaking, or
through amplification of the intrinsic spectral asymmetry of the Dysthe
equation. Furthermore, the definitions of the up- and downshift in terms of
peak- and mean frequencies, that are critical to relate our work to previous
results, are highlighted and discussed.
",Physics,Physics
"Uncharted Forest a Technique for Exploratory Data Analysis   Exploratory data analysis is crucial for developing and understanding
classification models from high-dimensional datasets. We explore the utility of
a new unsupervised tree ensemble called uncharted forest for visualizing class
associations, sample-sample associations, class heterogeneity, and
uninformative classes for provenance studies. The uncharted forest algorithm
can be used to partition data using random selections of variables and metrics
based on statistical spread. After each tree is grown, a tally of the samples
that arrive at every terminal node is maintained. Those tallies are stored in
single sample association matrix and a likelihood measure for each sample being
partitioned with one another can be made. That matrix may be readily viewed as
a heat map, and the probabilities can be quantified via new metrics that
account for class or cluster membership. We display the advantages and
limitations of using this technique by applying it to two classification
datasets and three provenance study datasets. Two of the metrics presented in
this paper are also compared with widely used metrics from two algorithms that
have variance-based clustering mechanisms.
",Statistics,Statistics
"Solvable Hydrodynamics of Quantum Integrable Systems   The conventional theory of hydrodynamics describes the evolution in time of
chaotic many-particle systems from local to global equilibrium. In a quantum
integrable system, local equilibrium is characterized by a local generalized
Gibbs ensemble or equivalently a local distribution of pseudo-momenta. We study
time evolution from local equilibria in such models by solving a certain
kinetic equation, the ""Bethe-Boltzmann"" equation satisfied by the local
pseudo-momentum density. Explicit comparison with density matrix
renormalization group time evolution of a thermal expansion in the XXZ model
shows that hydrodynamical predictions from smooth initial conditions can be
remarkably accurate, even for small system sizes. Solutions are also obtained
in the Lieb-Liniger model for free expansion into vacuum and collisions between
clouds of particles, which model experiments on ultracold one-dimensional Bose
gases.
",Physics,Physics
"Differentially Private Learning of Undirected Graphical Models using Collective Graphical Models   We investigate the problem of learning discrete, undirected graphical models
in a differentially private way. We show that the approach of releasing noisy
sufficient statistics using the Laplace mechanism achieves a good trade-off
between privacy, utility, and practicality. A naive learning algorithm that
uses the noisy sufficient statistics ""as is"" outperforms general-purpose
differentially private learning algorithms. However, it has three limitations:
it ignores knowledge about the data generating process, rests on uncertain
theoretical foundations, and exhibits certain pathologies. We develop a more
principled approach that applies the formalism of collective graphical models
to perform inference over the true sufficient statistics within an
expectation-maximization framework. We show that this learns better models than
competing approaches on both synthetic data and on real human mobility data
used as a case study.
",Computer Science; Statistics,Computer Science; Statistics
"ChaLearn Looking at People: A Review of Events and Resources   This paper reviews the historic of ChaLearn Looking at People (LAP) events.
We started in 2011 (with the release of the first Kinect device) to run
challenges related to human action/activity and gesture recognition. Since then
we have regularly organized events in a series of competitions covering all
aspects of visual analysis of humans. So far we have organized more than 10
international challenges and events in this field. This paper reviews
associated events, and introduces the ChaLearn LAP platform where public
resources (including code, data and preprints of papers) related to the
organized events are available. We also provide a discussion on perspectives of
ChaLearn LAP activities.
",Computer Science,Computer Science
"Minimal surfaces near short geodesics in hyperbolic $3$-manifolds   If $M$ is a finite volume complete hyperbolic $3$-manifold, the quantity
$\mathcal A_1(M)$ is defined as the infimum of the areas of closed minimal
surfaces in $M$. In this paper we study the continuity property of the
functional $\mathcal A_1$ with respect to the geometric convergence of
hyperbolic manifolds. We prove that it is lower semi-continuous and even
continuous if $\mathcal A_1(M)$ is realized by a minimal surface satisfying
some hypotheses. Understanding the interaction between minimal surfaces and
short geodesics in $M$ is the main theme of this paper
",Mathematics,Mathematics
"The Two-fold Role of Observables in Classical and Quantum Kinematics   Observables have a dual nature in both classical and quantum kinematics: they
are at the same time \emph{quantities}, allowing to separate states by means of
their numerical values, and \emph{generators of transformations}, establishing
relations between different states. In this work, we show how this two-fold
role of observables constitutes a key feature in the conceptual analysis of
classical and quantum kinematics, shedding a new light on the distinguishing
feature of the quantum at the kinematical level. We first take a look at the
algebraic description of both classical and quantum observables in terms of
Jordan-Lie algebras and show how the two algebraic structures are the precise
mathematical manifestation of the two-fold role of observables. Then, we turn
to the geometric reformulation of quantum kinematics in terms of Kähler
manifolds. A key achievement of this reformulation is to show that the two-fold
role of observables is the constitutive ingredient defining what an observable
is. Moreover, it points to the fact that, from the restricted point of view of
the transformational role of observables, classical and quantum kinematics
behave in exactly the same way. Finally, we present Landsman's general
framework of Poisson spaces with transition probability, which highlights with
unmatched clarity that the crucial difference between the two kinematics lies
in the way the two roles of observables are related to each other.
",Physics,Mathematics
"Learning Aided Optimization for Energy Harvesting Devices with Outdated State Information   This paper considers utility optimal power control for energy harvesting
wireless devices with a finite capacity battery. The distribution information
of the underlying wireless environment and harvestable energy is unknown and
only outdated system state information is known at the device controller. This
scenario shares similarity with Lyapunov opportunistic optimization and online
learning but is different from both. By a novel combination of Zinkevich's
online gradient learning technique and the drift-plus-penalty technique from
Lyapunov opportunistic optimization, this paper proposes a learning-aided
algorithm that achieves utility within $O(\epsilon)$ of the optimal, for any
desired $\epsilon>0$, by using a battery with an $O(1/\epsilon)$ capacity. The
proposed algorithm has low complexity and makes power investment decisions
based on system history, without requiring knowledge of the system state or its
probability distribution.
",Computer Science,Computer Science; Statistics
"Evidence for universality in the initial planetesimal mass function   Planetesimals may form from the gravitational collapse of dense particle
clumps initiated by the streaming instability. We use simulations of
aerodynamically coupled gas-particle mixtures to investigate whether the
properties of planetesimals formed in this way depend upon the sizes of the
particles that participate in the instability. Based on three high resolution
simulations that span a range of dimensionless stopping time $6 \times 10^{-3}
\leq \tau \leq 2$ no statistically significant differences in the initial
planetesimal mass function are found. The mass functions are fit by a
power-law, ${\rm d}N / {\rm d}M_p \propto M_p^{-p}$, with $p=1.5-1.7$ and
errors of $\Delta p \approx 0.1$. Comparing the particle density fields prior
to collapse, we find that the high wavenumber power spectra are similarly
indistinguishable, though the large-scale geometry of structures induced via
the streaming instability is significantly different between all three cases.
We interpret the results as evidence for a near-universal slope to the mass
function, arising from the small-scale structure of streaming-induced
turbulence.
",Physics,Physics
"Parity-Forbidden Transitions and Their Impacts on the Optical Absorption Properties of Lead-Free Metal Halide Perovskites and Double Perovskites   Using density-functional theory calculations, we analyze the optical
absorption properties of lead (Pb)-free metal halide perovskites
(AB$^{2+}$X$_3$) and double perovskites (AB$^+$B$^{3+}$X$_6$) (A = Cs or
monovalent organic ion, B$^{2+}$ = non-Pb divalent metal, B$^+$ = monovalent
metal, B$^{3+}$ = trivalent metal, X = halogen). We show that, if B$^{2+}$ is
not Sn or Ge, Pb-free metal halide perovskites exhibit poor optical absorptions
because of their indirect bandgap nature. Among the nine possible types of
Pb-free metal halide double perovskites, six have direct bandgaps. Of these six
types, four show inversion symmetry-induced parity-forbidden or weak
transitions between band edges, making them not ideal for thin-film solar cell
application. Only one type of Pb-free double perovskite shows optical
absorption and electronic properties suitable for solar cell applications,
namely those with B$^+$ = In, Tl and B$^{3+}$ = Sb, Bi. Our results provide
important insights for designing new metal halide perovskites and double
perovskites for optoelectronic applications.
",Physics,Physics
"Bootstrap confidence sets for spectral projectors of sample covariance   Let $X_{1},\ldots,X_{n}$ be i.i.d. sample in $\mathbb{R}^{p}$ with zero mean
and the covariance matrix $\mathbf{\Sigma}$. The problem of recovering the
projector onto an eigenspace of $\mathbf{\Sigma}$ from these observations
naturally arises in many applications. Recent technique from [Koltchinskii,
Lounici, 2015] helps to study the asymptotic distribution of the distance in
the Frobenius norm $\| \mathbf{P}_r - \widehat{\mathbf{P}}_r \|_{2}$ between
the true projector $\mathbf{P}_r$ on the subspace of the $r$-th eigenvalue and
its empirical counterpart $\widehat{\mathbf{P}}_r$ in terms of the effective
rank of $\mathbf{\Sigma}$. This paper offers a bootstrap procedure for building
sharp confidence sets for the true projector $\mathbf{P}_r$ from the given
data. This procedure does not rely on the asymptotic distribution of $\|
\mathbf{P}_r - \widehat{\mathbf{P}}_r \|_{2}$ and its moments. It could be
applied for small or moderate sample size $n$ and large dimension $p$. The main
result states the validity of the proposed procedure for finite samples with an
explicit error bound for the error of bootstrap approximation. This bound
involves some new sharp results on Gaussian comparison and Gaussian
anti-concentration in high-dimensional spaces. Numeric results confirm a good
performance of the method in realistic examples.
",Mathematics; Statistics,Mathematics; Statistics
"Six operations formalism for generalized operads   This paper shows that generalizations of operads equipped with their
respective bar/cobar dualities are related by a six operations formalism
analogous to that of classical contexts in algebraic geometry. As a consequence
of our constructions, we prove intertwining theorems which govern derived
Koszul duality of push-forwards and pull-backs.
",Mathematics,Mathematics
"Beyond black-boxes in Bayesian inverse problems and model validation: applications in solid mechanics of elastography   The present paper is motivated by one of the most fundamental challenges in
inverse problems, that of quantifying model discrepancies and errors. While
significant strides have been made in calibrating model parameters, the
overwhelming majority of pertinent methods is based on the assumption of a
perfect model. Motivated by problems in solid mechanics which, as all problems
in continuum thermodynamics, are described by conservation laws and
phenomenological constitutive closures, we argue that in order to quantify
model uncertainty in a physically meaningful manner, one should break open the
black-box forward model. In particular we propose formulating an undirected
probabilistic model that explicitly accounts for the governing equations and
their validity. This recasts the solution of both forward and inverse problems
as probabilistic inference tasks where the problem's state variables should not
only be compatible with the data but also with the governing equations as well.
Even though the probability densities involved do not contain any black-box
terms, they live in much higher-dimensional spaces. In combination with the
intractability of the normalization constant of the undirected model employed,
this poses significant challenges which we propose to address with a
linearly-scaling, double-layer of Stochastic Variational Inference. We
demonstrate the capabilities and efficacy of the proposed model in synthetic
forward and inverse problems (with and without model error) in elastography.
",Statistics,Physics; Statistics
"Characterization of Thermal Neutron Beam Monitors   Neutron beam monitors with high efficiency, low gamma sensitivity, high time
and space resolution are required in neutron beam experiments to continuously
diagnose the delivered beam. In this work, commercially available neutron beam
monitors have been characterized using the R2D2 beamline at IFE (Norway) and
using a Be-based neutron source. For the gamma sensitivity measurements
different gamma sources have been used. The evaluation of the monitors
includes, the study of their efficiency, attenuation, scattering and
sensitivity to gamma. In this work we report the results of this
characterization.
",Physics,Physics
"Bayesian inference in Y-linked two-sex branching processes with mutations: ABC approach   A Y-linked two-sex branching process with mutations and blind choice of males
is a suitable model for analyzing the evolution of the number of carriers of an
allele and its mutations of a Y-linked gene. Considering a two-sex monogamous
population, in this model each female chooses her partner from among the male
population without caring about his type (i.e., the allele he carries). In this
work, we deal with the problem of estimating the main parameters of such model
developing the Bayesian inference in a parametric framework. Firstly, we
consider, as sample scheme, the observation of the total number of females and
males up to some generation as well as the number of males of each genotype at
last generation. Later, we introduce the information of the mutated males only
in the last generation obtaining in this way a second sample scheme. For both
samples, we apply the Approximate Bayesian Computation (ABC) methodology to
approximate the posterior distributions of the main parameters of this model.
The accuracy of the procedure based on these samples is illustrated and
discussed by way of simulated examples.
",Statistics; Quantitative Biology,Statistics
"Waldschmidt constants for Stanley-Reisner ideals of a class of graphs   In the present note we study Waldschmidt constants of Stanley-Reisner ideals
of a hypergraph and a graph with vertices forming a bipyramid over a planar
n-gon. The case of the hypergraph has been studied by Bocci and Franci. We
reprove their main result. The case of the graph is new. Interestingly, both
cases provide series of ideals with Waldschmidt constants descending to 1. It
would be interesting to known if there are bounded ascending sequences of
Waldschmidt constants.
",Mathematics,Mathematics
"$L^p$ estimates for the Bergman projection on some Reinhardt domains   We obtain $L^p$ regularity for the Bergman projection on some Reinhardt
domains. We start with a bounded initial domain $\Omega$ with some symmetry
properties and generate successor domains in higher {dimensions}. We prove: If
the Bergman kernel on $\Omega$ satisfies appropriate estimates, then the
Bergman projection on the successor is $L^p$ bounded. For example, the Bergman
projection on successors of strictly pseudoconvex initial domains is bounded on
$L^p$ for $1<p<\infty$. The successor domains need not have smooth boundary nor
be strictly pseudoconvex.
",Mathematics,Mathematics
"Randomizing growing networks with a time-respecting null model   Complex networks are often used to represent systems that are not static but
grow with time: people make new friendships, new papers are published and refer
to the existing ones, and so forth. To assess the statistical significance of
measurements made on such networks, we propose a randomization methodology---a
time-respecting null model---that preserves both the network's degree sequence
and the time evolution of individual nodes' degree values. By preserving the
temporal linking patterns of the analyzed system, the proposed model is able to
factor out the effect of the system's temporal patterns on its structure. We
apply the model to the citation network of Physical Review scholarly papers and
the citation network of US movies. The model reveals that the two datasets are
strikingly different with respect to their degree-degree correlations, and we
discuss the important implications of this finding on the information provided
by paradigmatic node centrality metrics such as indegree and Google's PageRank.
The randomization methodology proposed here can be used to assess the
significance of any structural property in growing networks, which could bring
new insights into the problems where null models play a critical role, such as
the detection of communities and network motifs.
",Computer Science; Physics,Computer Science; Statistics
"Self-Taught Support Vector Machine   In this paper, a new approach for classification of target task using limited
labeled target data as well as enormous unlabeled source data is proposed which
is called self-taught learning. The target and source data can be drawn from
different distributions. In the previous approaches, covariate shift assumption
is considered where the marginal distributions p(x) change over domains and the
conditional distributions p(y|x) remain the same. In our approach, we propose a
new objective function which simultaneously learns a common space T(.) where
the conditional distributions over domains p(T(x)|y) remain the same and learns
robust SVM classifiers for target task using both source and target data in the
new representation. Hence, in the proposed objective function, the hidden label
of the source data is also incorporated. We applied the proposed approach on
Caltech-256, MSRC+LMO datasets and compared the performance of our algorithm to
the available competing methods. Our method has a superior performance to the
successful existing algorithms.
",Computer Science; Statistics,Computer Science; Statistics
"Block Mean Approximation for Efficient Second Order Optimization   Advanced optimization algorithms such as Newton method and AdaGrad benefit
from second order derivative or second order statistics to achieve better
descent directions and faster convergence rates. At their heart, such
algorithms need to compute the inverse or inverse square root of a matrix whose
size is quadratic of the dimensionality of the search space. For high
dimensional search spaces, the matrix inversion or inversion of square root
becomes overwhelming which in turn demands for approximate methods. In this
work, we propose a new matrix approximation method which divides a matrix into
blocks and represents each block by one or two numbers. The method allows
efficient computation of matrix inverse and inverse square root. We apply our
method to AdaGrad in training deep neural networks. Experiments show
encouraging results compared to the diagonal approximation.
",Statistics,Computer Science; Statistics
"Quantitative analysis of the influence of keV He ion bombardment on exchange bias layer systems   The mechanism of ion bombardment induced magnetic patterning of exchange bias
layer systems for creating engineered magnetic stray field landscapes is still
unclear. We compare results from vectorial magneto-optic Kerr effect
measurements to a recently proposed model with time dependent rotatable
magnetic anisotropy. Results show massive reduction of rotational magnetic
anisotropy compared to all other magnetic anisotropies. We disprove the
assumption of comparable weakening of all magnetic anisotropies and show that
ion bombardment mainly influences smaller grains in the antiferromagnet.
",Physics,Physics
"Regularization for Deep Learning: A Taxonomy   Regularization is one of the crucial ingredients of deep learning, yet the
term regularization has various definitions, and regularization methods are
often studied separately from each other. In our work we present a systematic,
unifying taxonomy to categorize existing methods. We distinguish methods that
affect data, network architectures, error terms, regularization terms, and
optimization procedures. We do not provide all details about the listed
methods; instead, we present an overview of how the methods can be sorted into
meaningful categories and sub-categories. This helps revealing links and
fundamental similarities between them. Finally, we include practical
recommendations both for users and for developers of new regularization
methods.
",Computer Science; Statistics,Computer Science; Statistics
"Gaschütz Lemma for Compact Groups   We prove the Gaschütz Lemma holds for all metrisable compact groups.
",Mathematics,Mathematics
"Dual SVM Training on a Budget   We present a dual subspace ascent algorithm for support vector machine
training that respects a budget constraint limiting the number of support
vectors. Budget methods are effective for reducing the training time of kernel
SVM while retaining high accuracy. To date, budget training is available only
for primal (SGD-based) solvers. Dual subspace ascent methods like sequential
minimal optimization are attractive for their good adaptation to the problem
structure, their fast convergence rate, and their practical speed. By
incorporating a budget constraint into a dual algorithm, our method enjoys the
best of both worlds. We demonstrate considerable speed-ups over primal budget
training methods.
",Statistics,Computer Science
"An Evolutionary Game for User Access Mode Selection in Fog Radio Access Networks   The fog radio access network (F-RAN) is a promising paradigm for the fifth
generation wireless communication systems to provide high spectral efficiency
and energy efficiency. Characterizing users to select an appropriate
communication mode among fog access point (F-AP), and device-to-device (D2D) in
F-RANs is critical for performance optimization. Using evolutionary game
theory, we investigate the dynamics of user access mode selection in F-RANs.
Specifically, the competition among groups of potential users space is
formulated as a dynamic evolutionary game, and the evolutionary equilibrium is
the solution to this game. Stochastic geometry tool is used to derive the
proposals' payoff expressions for both F-AP and D2D users by taking into
account the different nodes locations, cache sizes as well as the delay cost.
The analytical results obtained from the game model are evaluated via
simulations, which show that the evolutionary game based access mode selection
algorithm can reach a much higher payoff than the max rate based algorithm.
",Computer Science,Computer Science
"Two-component domain decomposition scheme with overlapping subdomains for parabolic equations   An iteration-free method of domain decomposition is considered for
approximate solving a boundary value problem for a second-order parabolic
equation. A standard approach to constructing domain decomposition schemes is
based on a partition of unity for the domain under the consideration. Here a
new general approach is proposed for constructing domain decomposition schemes
with overlapping subdomains based on indicator functions of subdomains. The
basic peculiarity of this method is connected with a representation of the
problem operator as the sum of two operators, which are constructed for two
separate subdomains with the subtraction of the operator that is associated
with the intersection of the subdomains. There is developed a two-component
factorized scheme, which can be treated as a generalization of the standard
Alternating Direction Implicit (ADI) schemes to the case of a special
three-component splitting. There are obtained conditions for the unconditional
stability of regionally additive schemes constructed using indicator functions
of subdomains. Numerical results are presented for a model two-dimensional
problem.
",Computer Science,Mathematics
"Nondestructive testing of grating imperfections using grating-based X-ray phase-contrast imaging   We reported the usage of grating-based X-ray phase-contrast imaging in
nondestructive testing of grating imperfections. It was found that
electroplating flaws could be easily detected by conventional absorption
signal, and in particular, we observed that the grating defects resulting from
uneven ultraviolet exposure could be clearly discriminated with phase-contrast
signal. The experimental results demonstrate that grating-based X-ray
phase-contrast imaging, with a conventional low-brilliance X-ray source, a
large field of view and a reasonable compact setup, which simultaneously yields
phase- and attenuation-contrast signal of the sample, can be ready-to-use in
fast nondestructive testing of various imperfections in gratings and other
similar photoetching products.
",Physics,Physics
"Heterogeneous elastic plates with in-plane modulation of the target curvature and applications to thin gel sheets   We rigorously derive a Kirchhoff plate theory, via $\Gamma$-convergence, from
a three-di\-men\-sio\-nal model that describes the finite elasticity of an
elastically heterogeneous, thin sheet. The heterogeneity in the elastic
properties of the material results in a spontaneous strain that depends on both
the thickness and the plane variables $x'$. At the same time, the spontaneous
strain is $h$-close to the identity, where $h$ is the small parameter
quantifying the thickness. The 2D Kirchhoff limiting model is constrained to
the set of isometric immersions of the mid-plane of the plate into
$\mathbb{R}^3$, with a corresponding energy that penalizes deviations of the
curvature tensor associated with a deformation from a $x'$-dependent target
curvature tensor. A discussion on the 2D minimizers is provided in the case
where the target curvature tensor is piecewise constant. Finally, we apply the
derived plate theory to the modeling of swelling-induced shape changes in
heterogeneous thin gel sheets.
",Physics; Mathematics,Physics
"Machine Learning for Structured Clinical Data   Research is a tertiary priority in the EHR, where the priorities are patient
care and billing. Because of this, the data is not standardized or formatted in
a manner easily adapted to machine learning approaches. Data may be missing for
a large variety of reasons ranging from individual input styles to differences
in clinical decision making, for example, which lab tests to issue. Few
patients are annotated at a research quality, limiting sample size and
presenting a moving gold standard. Patient progression over time is key to
understanding many diseases but many machine learning algorithms require a
snapshot, at a single time point, to create a usable vector form. Furthermore,
algorithms that produce black box results do not provide the interpretability
required for clinical adoption. This chapter discusses these challenges and
others in applying machine learning techniques to the structured EHR (i.e.
Patient Demographics, Family History, Medication Information, Vital Signs,
Laboratory Tests, Genetic Testing). It does not cover feature extraction from
additional sources such as imaging data or free text patient notes but the
approaches discussed can include features extracted from these sources.
",Computer Science,Statistics
"The growth of carbon chains in IRC+10216 mapped with ALMA   Linear carbon chains are common in various types of astronomical molecular
sources. Possible formation mechanisms involve both bottom-up and top-down
routes. We have carried out a combined observational and modeling study of the
formation of carbon chains in the C-star envelope IRC+10216, where the
polymerization of acetylene and hydrogen cyanide induced by ultraviolet photons
can drive the formation of linear carbon chains of increasing length. We have
used ALMA to map the emission of 3 mm rotational lines of the hydrocarbon
radicals C2H, C4H, and C6H, and the CN-containing species CN, C3N, HC3N, and
HC5N with an angular resolution of 1"". The spatial distribution of all these
species is a hollow, 5-10"" wide, spherical shell located at a radius of 10-20""
from the star, with no appreciable emission close to the star. Our observations
resolve the broad shell of carbon chains into thinner sub-shells which are 1-2""
wide and not fully concentric, indicating that the mass loss process has been
discontinuous and not fully isotropic. The radial distributions of the species
mapped reveal subtle differences: while the hydrocarbon radicals have very
similar radial distributions, the CN-containing species show more diverse
distributions, with HC3N appearing earlier in the expansion and the radical CN
extending later than the rest of the species. The observed morphology can be
rationalized by a chemical model in which the growth of polyynes is mainly
produced by rapid gas-phase chemical reactions of C2H and C4H radicals with
unsaturated hydrocarbons, while cyanopolyynes are mainly formed from polyynes
in gas-phase reactions with CN and C3N radicals.
",Physics,Physics
"V-cycle multigrid algorithms for discontinuous Galerkin methods on non-nested polytopic meshes   In this paper we analyse the convergence properties of V-cycle multigrid
algorithms for the numerical solution of the linear system of equations arising
from discontinuous Galerkin discretization of second-order elliptic partial
differential equations on polytopal meshes. Here, the sequence of spaces that
stands at the basis of the multigrid scheme is possibly non nested and is
obtained based on employing agglomeration with possible edge/face coarsening.
We prove that the method converges uniformly with respect to the granularity of
the grid and the polynomial approximation degree p, provided that the number of
smoothing steps, which depends on p, is chosen sufficiently large.
",Computer Science,Mathematics
"MC$^2$: Multi-wavelength and dynamical analysis of the merging galaxy cluster ZwCl 0008.8+5215: An older and less massive Bullet Cluster   We analyze a rich dataset including Subaru/SuprimeCam, HST/ACS and WFC3,
Keck/DEIMOS, Chandra/ACIS-I, and JVLA/C and D array for the merging galaxy
cluster ZwCl 0008.8+5215. With a joint Subaru/HST weak gravitational lensing
analysis, we identify two dominant subclusters and estimate the masses to be
M$_{200}=\text{5.7}^{+\text{2.8}}_{-\text{1.8}}\times\text{10}^{\text{14}}\,\text{M}_{\odot}$
and 1.2$^{+\text{1.4}}_{-\text{0.6}}\times10^{14}$ M$_{\odot}$. We estimate the
projected separation between the two subclusters to be
924$^{+\text{243}}_{-\text{206}}$ kpc. We perform a clustering analysis on
confirmed cluster member galaxies and estimate the line of sight velocity
difference between the two subclusters to be 92$\pm$164 km s$^{-\text{1}}$. We
further motivate, discuss, and analyze the merger scenario through an analysis
of the 42 ks of Chandra/ACIS-I and JVLA/C and D polarization data. The X-ray
surface brightness profile reveals a remnant core reminiscent of the Bullet
Cluster. The X-ray luminosity in the 0.5-7.0 keV band is
1.7$\pm$0.1$\times$10$^{\text{44}}$ erg s$^{-\text{1}}$ and the X-ray
temperature is 4.90$\pm$0.13 keV. The radio relics are polarized up to 40$\%$.
We implement a Monte Carlo dynamical analysis and estimate the merger velocity
at pericenter to be 1800$^{+\text{400}}_{-\text{300}}$ km s$^{-\text{1}}$. ZwCl
0008.8+5215 is a low-mass version of the Bullet Cluster and therefore may prove
useful in testing alternative models of dark matter. We do not find significant
offsets between dark matter and galaxies, as the uncertainties are large with
the current lensing data. Furthermore, in the east, the BCG is offset from
other luminous cluster galaxies, which poses a puzzle for defining dark matter
-- galaxy offsets.
",Physics,Physics
"Phase diagram of a generalized off-diagonal Aubry-André model with p-wave pairing   Off-diagonal Aubry-André (AA) model has recently attracted a great deal
of attention as they provide condensed matter realization of topological
phases. We numerically study a generalized off-diagonal AA model with p-wave
superfluid pairing in the presence of both commensurate and incommensurate
hopping modulations. The phase diagram as functions of the modulation strength
of incommensurate hopping and the strength of the p-wave pairing is obtained by
using the multifractal analysis. We show that with the appearance of the p-wave
pairing, the system exhibits mobility-edge phases and critical phases with
various number of topologically protected zero-energy modes. Predicted
topological nature of these exotic phases can be realized in a cold atomic
system of incommensurate bichromatic optical lattice with induced p-wave
superfluid pairing by using a Raman laser in proximity to a molecular
Bose-Einstein condensation.
",Physics,Physics
"GSAE: an autoencoder with embedded gene-set nodes for genomics functional characterization   Bioinformatics tools have been developed to interpret gene expression data at
the gene set level, and these gene set based analyses improve the biologists'
capability to discover functional relevance of their experiment design. While
elucidating gene set individually, inter gene sets association is rarely taken
into consideration. Deep learning, an emerging machine learning technique in
computational biology, can be used to generate an unbiased combination of gene
set, and to determine the biological relevance and analysis consistency of
these combining gene sets by leveraging large genomic data sets. In this study,
we proposed a gene superset autoencoder (GSAE), a multi-layer autoencoder model
with the incorporation of a priori defined gene sets that retain the crucial
biological features in the latent layer. We introduced the concept of the gene
superset, an unbiased combination of gene sets with weights trained by the
autoencoder, where each node in the latent layer is a superset. Trained with
genomic data from TCGA and evaluated with their accompanying clinical
parameters, we showed gene supersets' ability of discriminating tumor subtypes
and their prognostic capability. We further demonstrated the biological
relevance of the top component gene sets in the significant supersets. Using
autoencoder model and gene superset at its latent layer, we demonstrated that
gene supersets retain sufficient biological information with respect to tumor
subtypes and clinical prognostic significance. Superset also provides high
reproducibility on survival analysis and accurate prediction for cancer
subtypes.
",Statistics; Quantitative Biology,Statistics
"The Hidden Binary Search Tree:A Balanced Rotation-Free Search Tree in the AVL RAM Model   In this paper we generalize the definition of ""Search Trees"" (ST) to enable
reference values other than the key of prior inserted nodes. The idea builds on
the assumption an $n$-node AVL (or Red-Black) requires to assure $O(\log_2n)$
worst-case search time, namely, a single comparison between two keys takes
constant time. This means the size of each key in bits is fixed to $B=c\log_2
n$ ($c\geq1$) once $n$ is determined, otherwise the $O(1)$-time comparison
assumption does not hold. Based on this we calculate \emph{ideal} reference
values from the mid-point of the interval $0..2^B$. This idea follows
`recursively' to assure each node along the search path is provided a reference
value that guarantees an overall logarithmic time. Because the search tree
property works only when keys are compared to reference values and these values
are calculated only during searches, we term the data structure as the Hidden
Binary Search Tree (HBST). We show elementary functions to maintain the HSBT
height $O(B)=O(\log_2n)$. This result requires no special order on the input --
as does BST -- nor self-balancing procedures, as do AVL and Red-Black.
",Computer Science,Computer Science
"Generalized orderless pooling performs implicit salient matching   Most recent CNN architectures use average pooling as a final feature encoding
step. In the field of fine-grained recognition, however, recent global
representations like bilinear pooling offer improved performance. In this
paper, we generalize average and bilinear pooling to ""alpha-pooling"", allowing
for learning the pooling strategy during training. In addition, we present a
novel way to visualize decisions made by these approaches. We identify parts of
training images having the highest influence on the prediction of a given test
image. It allows for justifying decisions to users and also for analyzing the
influence of semantic parts. For example, we can show that the higher capacity
VGG16 model focuses much more on the bird's head than, e.g., the lower-capacity
VGG-M model when recognizing fine-grained bird categories. Both contributions
allow us to analyze the difference when moving between average and bilinear
pooling. In addition, experiments show that our generalized approach can
outperform both across a variety of standard datasets.
",Computer Science,Computer Science; Statistics
"Time- and spatially-resolved magnetization dynamics driven by spin-orbit torques   Current-induced spin-orbit torques (SOTs) represent one of the most effective
ways to manipulate the magnetization in spintronic devices. The orthogonal
torque-magnetization geometry, the strong damping, and the large domain wall
velocities inherent to materials with strong spin-orbit coupling make SOTs
especially appealing for fast switching applications in nonvolatile memory and
logic units. So far, however, the timescale and evolution of the magnetization
during the switching process have remained undetected. Here, we report the
direct observation of SOT-driven magnetization dynamics in Pt/Co/AlO$_x$ dots
during current pulse injection. Time-resolved x-ray images with 25 nm spatial
and 100 ps temporal resolution reveal that switching is achieved within the
duration of a sub-ns current pulse by the fast nucleation of an inverted domain
at the edge of the dot and propagation of a tilted domain wall across the dot.
The nucleation point is deterministic and alternates between the four dot
quadrants depending on the sign of the magnetization, current, and external
field. Our measurements reveal how the magnetic symmetry is broken by the
concerted action of both damping-like and field-like SOT and show that
reproducible switching events can be obtained for over $10^{12}$ reversal
cycles.
",Physics,Physics
"Spatio-temporal intermittency of the turbulent energy cascade   In incompressible and periodic statistically stationary turbulence, exchanges
of turbulent energy across scales and space are characterised by very intense
and intermittent spatio-temporal fluctuations around zero of the
time-derivative term, the spatial turbulent transport of fluctuating energy,
and the pressure-velocity term. These fluctuations are correlated with each
other and with the intense intermittent fluctuations of the interscale energy
transfer rate. These correlations are caused by the sweeping effect, the link
between non-linearity and non-locality, and also relate to geometrical
alignments between the two-point fluctuating pressure force difference and the
two-point fluctuating velocity difference in the case of the correlation
between the interscale transfer rate and the pressure-velocity term. All these
processes are absent from the spatio-temporal average picture of the turbulence
cascade in statistically stationary and homogeneous turbulence.
",Physics,Physics
"Dynamical Mass Generation in Pseudo Quantum Electrodynamics with Four-Fermion Interactions   We describe dynamical symmetry breaking in a system of massless Dirac
fermions with both electromagnetic and four-fermion interactions in (2+1)
dimensions. The former is described by the Pseudo Quantum Electrodynamics
(PQED) and the latter is given by the so-called Gross-Neveu action. We apply
the Hubbard-Stratonovich transformation and the large$-N_f$ expansion in our
model to obtain a Yukawa action. Thereafter, the presence of a symmetry broken
phase is inferred from the non-perturbative Schwinger-Dyson equation for the
electron propagator. This is the physical solution whenever the fine-structure
constant is larger than a critical value $\alpha_c(D N_f)$. In particular, we
obtain the critical coupling constant $\alpha_c\approx 0.36$ for $D N_f=8$.,
where $D=2,4$ corresponds to the SU(2) and SU(4) cases, respectively, and $N_f$
is the flavor number. Our results show a decreasing of the critical coupling
constant in comparison with the case of pure electromagnetic interaction, thus
yielding a more favorable scenario for the occurrence of dynamical symmetry
breaking. For two-dimensional materials,in application in condensed matter
systems, it implies an energy gap at the Dirac points or valleys of the
honeycomb lattice.
",Physics,Physics
"An upper bound on transport   The linear growth of operators in local quantum systems leads to an effective
lightcone even if the system is non-relativistic. We show that consistency of
diffusive transport with this lightcone places an upper bound on the
diffusivity: $D \lesssim v^2 \tau_\text{eq}$. The operator growth velocity $v$
defines the lightcone and $\tau_\text{eq}$ is the local equilibration
timescale, beyond which the dynamics of conserved densities is diffusive. We
verify that the bound is obeyed in various weakly and strongly interacting
theories. In holographic models this bound establishes a relation between the
hydrodynamic and leading non-hydrodynamic quasinormal modes of planar black
holes. Our bound relates transport data --- including the electrical
resistivity and the shear viscosity --- to the local equilibration time, even
in the absence of a quasiparticle description. In this way, the bound sheds
light on the observed $T$-linear resistivity of many unconventional metals, the
shear viscosity of the quark-gluon plasma and the spin transport of unitary
fermions.
",Physics,Physics
"Development of a passive Rehabilitation Robot for the wrist joint through the implementation of an Arduino UNO microcontroller   In this research was implemented the use of an Arduino UNO R3 microcontroller
to control the movements of a prototype robotic functional developed to perform
rehabilitation exercises in the wrist joint; This device can be used to assist
the physiatrist to rehabilitate the tendinitis, synovitis, rheumatoid arthritis
and for pre-operative and post-operative therapy in this joint. During the
design stage of the functional prototype, the methodology of the industrial
design process was used from a concurrent engineering approach, through which
anthropometric studies could be performed related to the dimensions and angles
of movement of the wrist joint in the population Venezuelan from the
information collected, the design proposal was elaborated, and the use of CAD
programs defined the different forms, geometries and materials of the
components of the rehabilitation device, which were later analyzed using the
finite element method for the determination The tensional state of efforts and
safety factors through the use of CAE programs. In addition, a software was
developed for the acquisition, registration, reproduction and execution of the
different movements produced during the rehabilitation therapy. Through the
research developed, a device was designed that will help the rehabilitation of
the wrist joint allowing the combination of dorsal-palmar flexion and
ulnar-radial movements to recover the joint function of various pathologies
presented in the Venezuelan population.
",Computer Science; Physics,Computer Science
"Short Laws for Finite Groups and Residual Finiteness Growth   We prove that for every $n \in \mathbb{N}$ and $\delta>0$ there exists a word
$w_n \in F_2$ of length $n^{2/3} \log(n)^{3+\delta}$ which is a law for every
finite group of order at most $n$. This improves upon the main result of [A.
Thom, About the length of laws for finite groups, Isr. J. Math.]. As an
application we prove a new lower bound on the residual finiteness growth of
non-abelian free groups.
",Mathematics,Mathematics
"Analysis of a remarkable singularity in a nonlinear DDE   In this work we investigate the dynamics of the nonlinear DDE
(delay-differential equation)
x''(t)+x(t-T)+x(t)^3=0
where T is the delay. For T=0 this system is conservative and exhibits no
limit cycles. For T>0, no matter how small, an infinite number of limit cycles
exist, their amplitudes going to infinity in the limit as T approaches zero.
We investigate this situation in three ways: 1) Harmonic Balance, 2)
Melnikov's integral, and 3) Adding damping to regularize the singularity.
",Physics; Mathematics,Mathematics
"Topologically protected Dirac plasmons in graphene   Topological optical states exhibit unique immunity to defects and the ability
to propagate without losses rendering them ideal for photonic applications.A
powerful class of such states is based on time-reversal symmetry breaking of
the optical response.However, existing proposals either involve sophisticated
and bulky structural designs or can only operate in the microwave regime. Here,
we propose and provide a theoretical proof-of-principle demonstration for
highly confined topologically protected optical states to be realized at
infrared frequencies in a simple 2D material structure-a periodically patterned
graphene monolayer-subject to a magnetic field below 1 tesla. In our graphene
honeycomb superlattice structures plasmons exhibit substantial nonreciprocal
behavior at the superlattice junctions, leading to the emergence of
topologically protected edge states and localized bulk modes enabled by the
strong magneto-optical response of this material, which leads to
time-reversal-symmetry breaking already at moderate static magnetic fields. The
proposed approach is simple and robust for realizing topologically nontrivial
2D optical states, not only in graphene, but also in other 2D atomic layers,
and could pave the way for realizing fast, nanoscale, defect-immune devices for
integrated photonics applications.
",Physics,Physics
"High-power closed-cycle $^4$He cryostat with top-loading sample exchange   We report on the development of a versatile cryogen-free laboratory cryostat
based upon a commercial pulse tube cryocooler. It provides enough cooling power
for continuous recondensation of circulating $^4$He gas at a condensation
pressure of approximately 250~mbar. Moreover, the cryostat allows for exchange
of different cryostat-inserts as well as fast and easy ""wet"" top-loading of
samples directly into the 1 K pot with a turn-over time of less than 75~min.
Starting from room temperature and using a $^4$He cryostat-insert, a base
temperature of 1.0~K is reached within approximately seven hours and a cooling
power of 250~mW is established at 1.24~K.
",Physics,Physics
"Perturbation theory for cosmologies with non-linear structure   The next generation of cosmological surveys will operate over unprecedented
scales, and will therefore provide exciting new opportunities for testing
general relativity. The standard method for modelling the structures that these
surveys will observe is to use cosmological perturbation theory for linear
structures on horizon-sized scales, and Newtonian gravity for non-linear
structures on much smaller scales. We propose a two-parameter formalism that
generalizes this approach, thereby allowing interactions between large and
small scales to be studied in a self-consistent and well-defined way. This uses
both post-Newtonian gravity and cosmological perturbation theory, and can be
used to model realistic cosmological scenarios including matter, radiation and
a cosmological constant. We find that the resulting field equations can be
written as a hierarchical set of perturbation equations. At leading-order,
these equations allow us to recover a standard set of Friedmann equations, as
well as a Newton-Poisson equation for the inhomogeneous part of the Newtonian
energy density in an expanding background. For the perturbations in the
large-scale cosmology, however, we find that the field equations are sourced by
both non-linear and mode-mixing terms, due to the existence of small-scale
structures. These extra terms should be expected to give rise to new
gravitational effects, through the mixing of gravitational modes on small and
large scales - effects that are beyond the scope of standard linear
cosmological perturbation theory. We expect our formalism to be useful for
accurately modelling gravitational physics in universes that contain non-linear
structures, and for investigating the effects of non-linear gravity in the era
of ultra-large-scale surveys.
",Physics,Physics
"Airy structures and symplectic geometry of topological recursion   We propose a new approach to the topological recursion of Eynard-Orantin
based on the notion of Airy structure, which we introduce in the paper. We
explain why Airy structure is a more fundamental object than the one of the
spectral curve. We explain how the concept of quantization of Airy structure
leads naturally to the formulas of topological recursion as well as their
generalizations. The notion of spectral curve is also considered in a more
general framework of Poisson surfaces endowed with foliation. We explain how
the deformation theory of spectral curves is related to Airy structures. Few
other topics (e.g. the Holomorphic Anomaly Equation) are also discussed from
the general point of view of Airy structures.
",Mathematics,Mathematics
"Skeleton-based Action Recognition of People Handling Objects   In visual surveillance systems, it is necessary to recognize the behavior of
people handling objects such as a phone, a cup, or a plastic bag. In this
paper, to address this problem, we propose a new framework for recognizing
object-related human actions by graph convolutional networks using human and
object poses. In this framework, we construct skeletal graphs of reliable human
poses by selectively sampling the informative frames in a video, which include
human joints with high confidence scores obtained in pose estimation. The
skeletal graphs generated from the sampled frames represent human poses related
to the object position in both the spatial and temporal domains, and these
graphs are used as inputs to the graph convolutional networks. Through
experiments over an open benchmark and our own data sets, we verify the
validity of our framework in that our method outperforms the state-of-the-art
method for skeleton-based action recognition.
",Computer Science,Computer Science
"Block Compressive Sensing of Image and Video with Nonlocal Lagrangian Multiplier and Patch-based Sparse Representation   Although block compressive sensing (BCS) makes it tractable to sense
large-sized images and video, its recovery performance has yet to be
significantly improved because its recovered images or video usually suffer
from blurred edges, loss of details, and high-frequency oscillatory artifacts,
especially at a low subrate. This paper addresses these problems by designing a
modified total variation technique that employs multi-block gradient
processing, a denoised Lagrangian multiplier, and patch-based sparse
representation. In the case of video, the proposed recovery method is able to
exploit both spatial and temporal similarities. Simulation results confirm the
improved performance of the proposed method for compressive sensing of images
and video in terms of both objective and subjective qualities.
",Computer Science,Computer Science
"A GAMP Based Low Complexity Sparse Bayesian Learning Algorithm   In this paper, we present an algorithm for the sparse signal recovery problem
that incorporates damped Gaussian generalized approximate message passing
(GGAMP) into Expectation-Maximization (EM)-based sparse Bayesian learning
(SBL). In particular, GGAMP is used to implement the E-step in SBL in place of
matrix inversion, leveraging the fact that GGAMP is guaranteed to converge with
appropriate damping. The resulting GGAMP-SBL algorithm is much more robust to
arbitrary measurement matrix $\boldsymbol{A}$ than the standard damped GAMP
algorithm while being much lower complexity than the standard SBL algorithm. We
then extend the approach from the single measurement vector (SMV) case to the
temporally correlated multiple measurement vector (MMV) case, leading to the
GGAMP-TSBL algorithm. We verify the robustness and computational advantages of
the proposed algorithms through numerical experiments.
",Computer Science; Statistics,Computer Science; Statistics
"Parabolic equations with natural growth approximated by nonlocal equations   In this paper we study several aspects related with solutions of nonlocal
problems whose prototype is $$ u_t =\displaystyle \int_{\mathbb{R}^N} J(x-y)
\big( u(y,t) -u(x,t) \big) \mathcal G\big( u(y,t) -u(x,t) \big) dy \qquad
\mbox{ in } \, \Omega \times (0,T)\,, $$ being $ u (x,t)=0 \mbox{ in }
(\mathbb{R}^N\setminus \Omega )\times (0,T)\,$ and $ u(x,0)=u_0 (x) \mbox{ in }
\Omega$. We take, as the most important instance, $\mathcal G (s) \sim 1+
\frac{\mu}{2} \frac{s}{1+\mu^2 s^2 }$ with $\mu\in \mathbb{R}$ as well as $u_0
\in L^1 (\Omega)$, $J$ is a smooth symmetric function with compact support and
$\Omega$ is either a bounded smooth subset of $\mathbb{R}^N$, with nonlocal
Dirichlet boundary condition, or $\mathbb{R}^N$ itself.
The results deal with existence, uniqueness, comparison principle and
asymptotic behavior. Moreover we prove that if the kernel rescales in a
suitable way, the unique solution of the above problem converges to a solution
of the deterministic Kardar-Parisi-Zhang equation.
",Mathematics,Mathematics
"Fast-slow asymptotic for semi-analytical ignition criteria in FitzHugh-Nagumo system   We study the problem of initiation of excitation waves in the FitzHugh-Nagumo
model. Our approach follows earlier works and is based on the idea of
approximating the boundary between basins of attraction of propagating waves
and of the resting state as the stable manifold of a critical solution. Here,
we obtain analytical expressions for the essential ingredients of the theory by
singular perturbation using two small parameters, the separation of time scales
of the activator and inhibitor, and the threshold in the activator's kinetics.
This results in a closed analytical expression for the strength-duration curve.
",Physics,Physics
"Geometric theories of patch and Lawson topologies   We give geometric characterisations of patch and Lawson topologies in the
context of predicative point-free topology using the constructive notion of
located subset. We present the patch topology of a stably locally compact
formal topology by a geometric theory whose models are the points of the given
topology that are located, and the Lawson topology of a continuous lattice by a
geometric theory whose models are the located subsets of the given lattice. We
also give a predicative presentation of the frame of perfect nuclei on a stably
locally compact formal topology, and show that it is essentially the same as
our geometric presentation of the patch topology. Moreover, the construction of
Lawson topologies naturally induces a monad on the category of compact regular
formal topologies, which is shown to be isomorphic to the Vietoris monad.
",Computer Science; Mathematics,Mathematics
"Entanglement Entropy in Excited States of the Quantum Lifshitz Model   We investigate the entanglement properties of an infinite class of excited
states in the quantum Lifshitz model (QLM). The presence of a conformal quantum
critical point in the QLM makes it unusually tractable for a model above one
spatial dimension, enabling the ground state entanglement entropy for an
arbitrary domain to be expressed in terms of geometrical and topological
quantities. Here we extend this result to excited states and find that the
entanglement can be naturally written in terms of quantities which we dub
""entanglement propagator amplitudes"" (EPAs). EPAs are geometrical probabilities
that we explicitly calculate and interpret. A comparison of lattice and
continuum results demonstrates that EPAs are universal. This work shows that
the QLM is an example of a 2+1d field theory where the universal behavior of
excited-state entanglement may be computed analytically.
",Physics,Physics
"Mechanisms of dimensionality reduction and decorrelation in deep neural networks   Deep neural networks are widely used in various domains. However, the nature
of computations at each layer of the deep networks is far from being well
understood. Increasing the interpretability of deep neural networks is thus
important. Here, we construct a mean-field framework to understand how compact
representations are developed across layers, not only in deterministic deep
networks with random weights but also in generative deep networks where an
unsupervised learning is carried out. Our theory shows that the deep
computation implements a dimensionality reduction while maintaining a finite
level of weak correlations between neurons for possible feature extraction.
Mechanisms of dimensionality reduction and decorrelation are unified in the
same framework. This work may pave the way for understanding how a sensory
hierarchy works.
",Computer Science; Statistics,Quantitative Biology
"Detection of an Optical Counterpart to the ALFALFA Ultra-compact High Velocity Cloud AGC 249525   We report on the detection at $>$98% confidence of an optical counterpart to
AGC 249525, an Ultra-Compact High Velocity Cloud (UCHVC) discovered by the
ALFALFA blind neutral hydrogen survey. UCHVCs are compact, isolated HI clouds
with properties consistent with their being nearby low-mass galaxies, but
without identified counterparts in extant optical surveys. Analysis of the
resolved stellar sources in deep $g$- and $i$-band imaging from the WIYN pODI
camera reveals a clustering of possible Red Giant Branch stars associated with
AGC 249525 at a distance of 1.64$\pm$0.45 Mpc. Matching our optical detection
with the HI synthesis map of AGC 249525 from Adams et al. (2016) shows that the
stellar overdensity is exactly coincident with the highest-density HI contour
from that study. Combining our optical photometry and the HI properties of this
object yields an absolute magnitude of $-7.1 \leq M_V \leq -4.5$, a stellar
mass between $2.2\pm0.6\times10^4 M_{\odot}$ and $3.6\pm1.0\times10^5
M_{\odot}$, and an HI to stellar mass ratio between 9 and 144. This object has
stellar properties within the observed range of gas-poor Ultra-Faint Dwarfs in
the Local Group, but is gas-dominated.
",Physics,Physics
"Design of Real-time Semantic Segmentation Decoder for Automated Driving   Semantic segmentation remains a computationally intensive algorithm for
embedded deployment even with the rapid growth of computation power. Thus
efficient network design is a critical aspect especially for applications like
automated driving which requires real-time performance. Recently, there has
been a lot of research on designing efficient encoders that are mostly task
agnostic. Unlike image classification and bounding box object detection tasks,
decoders are computationally expensive as well for semantic segmentation task.
In this work, we focus on efficient design of the segmentation decoder and
assume that an efficient encoder is already designed to provide shared features
for a multi-task learning system. We design a novel efficient non-bottleneck
layer and a family of decoders which fit into a small run-time budget using
VGG10 as efficient encoder. We demonstrate in our dataset that experimentation
with various design choices led to an improvement of 10\% from a baseline
performance.
",Computer Science; Statistics,Computer Science
"Unified description of dynamics of a repulsive two-component Fermi gas   We study a binary spin-mixture of a zero-temperature repulsively interacting
$^6$Li atoms using both the atomic-orbital and the density functional
approaches. The gas is initially prepared in a configuration of two magnetic
domains and we determine the frequency of the spin-dipole oscillations which
are emerging after the repulsive barrier, initially separating the domains, is
removed. We find, in agreement with recent experiment (G. Valtolina et al.,
arXiv:1605.07850 (2016)), the occurrence of a ferromagnetic instability in an
atomic gas while the interaction strength between different spin states is
increased, after which the system becomes ferromagnetic. The ferromagnetic
instability is preceded by the softening of the spin-dipole mode.
",Physics,Physics
"Signaling on the Continuous Spectrum of Nonlinear Optical fiber   This paper studies different signaling techniques on the continuous spectrum
(CS) of nonlinear optical fiber defined by nonlinear Fourier transform. Three
different signaling techniques are proposed and analyzed based on the
statistics of the noise added to CS after propagation along the nonlinear
optical fiber. The proposed methods are compared in terms of error performance,
distance reach, and complexity. Furthermore, the effect of chromatic dispersion
on the data rate and noise in nonlinear spectral domain is investigated. It is
demonstrated that, for a given sequence of CS symbols, an optimal bandwidth (or
symbol rate) can be determined so that the temporal duration of the propagated
signal at the end of the fiber is minimized. In effect, the required guard
interval between the subsequently transmitted data packets in time is minimized
and the effective data rate is significantly enhanced. Moreover, by selecting
the proper signaling method and design criteria a reach distance of 7100 km is
reported by only singling on the CS at a rate of 9.6 Gbps.
",Computer Science; Physics,Physics
"Quasiparticle entropy in superconductor/normal metal/superconductor proximity junctions in the diffusive limit   We discuss the quasiparticle entropy and heat capacity of a dirty
superconductor-normal metal-superconductor junction. In the case of short
junctions, the inverse proximity effect extending in the superconducting banks
plays a crucial role in determining the thermodynamic quantities. In this case,
commonly used approximations can violate thermodynamic relations between
supercurrent and quasiparticle entropy. We provide analytical and numerical
results as a function of different geometrical parameters. Quantitative
estimates for the heat capacity can be relevant for the design of caloritronic
devices or radiation sensor applications.
",Physics,Physics
"Convergence rates of least squares regression estimators with heavy-tailed errors   We study the performance of the Least Squares Estimator (LSE) in a general
nonparametric regression model, when the errors are independent of the
covariates but may only have a $p$-th moment ($p\geq 1$). In such a
heavy-tailed regression setting, we show that if the model satisfies a standard
`entropy condition' with exponent $\alpha \in (0,2)$, then the $L_2$ loss of
the LSE converges at a rate \begin{align*}
\mathcal{O}_{\mathbf{P}}\big(n^{-\frac{1}{2+\alpha}} \vee
n^{-\frac{1}{2}+\frac{1}{2p}}\big). \end{align*} Such a rate cannot be improved
under the entropy condition alone.
This rate quantifies both some positive and negative aspects of the LSE in a
heavy-tailed regression setting. On the positive side, as long as the errors
have $p\geq 1+2/\alpha$ moments, the $L_2$ loss of the LSE converges at the
same rate as if the errors are Gaussian. On the negative side, if
$p<1+2/\alpha$, there are (many) hard models at any entropy level $\alpha$ for
which the $L_2$ loss of the LSE converges at a strictly slower rate than other
robust estimators.
The validity of the above rate relies crucially on the independence of the
covariates and the errors. In fact, the $L_2$ loss of the LSE can converge
arbitrarily slowly when the independence fails.
The key technical ingredient is a new multiplier inequality that gives sharp
bounds for the `multiplier empirical process' associated with the LSE. We
further give an application to the sparse linear regression model with
heavy-tailed covariates and errors to demonstrate the scope of this new
inequality.
",Mathematics; Statistics,Mathematics; Statistics
"Algebras of Quasi-Plücker Coordinates are Koszul   Motivated by the theory of quasi-determinants, we study non-commutative
algebras of quasi-Plücker coordinates. We prove that these algebras provide
new examples of non-homogeneous quadratic Koszul algebras by showing that their
quadratic duals have quadratic Gröbner bases.
",Mathematics,Mathematics
"Submolecular-resolution non-invasive imaging of interfacial water with atomic force microscopy   Scanning probe microscopy (SPM) has been extensively applied to probe
interfacial water in many interdisciplinary fields but the disturbance of the
probes on the hydrogen-bonding structure of water has remained an intractable
problem. Here we report submolecular-resolution imaging of the water clusters
on a NaCl(001) surface within the nearly non-invasive region by a qPlus-based
noncontact atomic force microscopy. Comparison with theoretical simulations
reveals that the key lies in probing the weak high-order electrostatic force
between the quadrupole-like CO-terminated tip and the polar water molecules at
large tip-water distances. This interaction allows the imaging and structural
determination of the weakly bonded water clusters and even of their metastable
states without inducing any disturbance. This work may open up new possibility
of studying the intrinsic structure and electrostatics of ice or water on bulk
insulating surfaces, ion hydration and biological water with atomic precision.
",Physics,Physics
"One-Shot Learning of Multi-Step Tasks from Observation via Activity Localization in Auxiliary Video   Due to burdensome data requirements, learning from demonstration often falls
short of its promise to allow users to quickly and naturally program robots.
Demonstrations are inherently ambiguous and incomplete, making correct
generalization to unseen situations difficult without a large number of
demonstrations in varying conditions. By contrast, humans are often able to
learn complex tasks from a single demonstration (typically observations without
action labels) by leveraging context learned over a lifetime. Inspired by this
capability, our goal is to enable robots to perform one-shot learning of
multi-step tasks from observation by leveraging auxiliary video data as
context. Our primary contribution is a novel system that achieves this goal by:
(1) using a single user-segmented demonstration to define the primitive actions
that comprise a task, (2) localizing additional examples of these actions in
unsegmented auxiliary videos via a metalearning-based approach, (3) using these
additional examples to learn a reward function for each action, and (4)
performing reinforcement learning on top of the inferred reward functions to
learn action policies that can be combined to accomplish the task. We
empirically demonstrate that a robot can learn multi-step tasks more
effectively when provided auxiliary video, and that performance greatly
improves when localizing individual actions, compared to learning from
unsegmented videos.
",Computer Science; Statistics,Computer Science
"Recursion for the smallest eigenvalue density of $β$-Wishart-Laguerre ensemble   The statistics of the smallest eigenvalue of Wishart-Laguerre ensemble is
important from several perspectives. The smallest eigenvalue density is
typically expressible in terms of determinants or Pfaffians. These results are
of utmost significance in understanding the spectral behavior of
Wishart-Laguerre ensembles and, among other things, unveil the underlying
universality aspects in the asymptotic limits. However, obtaining exact and
explicit expressions by expanding determinants or Pfaffians becomes impractical
if large dimension matrices are involved. For the real matrices ($\beta=1$)
Edelman has provided an efficient recurrence scheme to work out exact and
explicit results for the smallest eigenvalue density which does not involve
determinants or matrices. Very recently, an analogous recurrence scheme has
been obtained for the complex matrices ($\beta=2$). In the present work we
extend this to $\beta$-Wishart-Laguerre ensembles for the case when exponent
$\alpha$ in the associated Laguerre weight function, $\lambda^\alpha
e^{-\beta\lambda/2}$, is a non-negative integer, while $\beta$ is positive
real. This also gives access to the smallest eigenvalue density of fixed trace
$\beta$-Wishart-Laguerre ensemble, as well as moments for both cases. Moreover,
comparison with earlier results for the smallest eigenvalue density in terms of
certain hypergeometric function of matrix argument results in an effective way
of evaluating these explicitly. Exact evaluations for large values of $n$ (the
matrix dimension) and $\alpha$ also enable us to compare with Tracy-Widom
density and large deviation results of Katzav and Castillo. We also use our
result to obtain the density of the largest of the proper delay times which are
eigenvalues of the Wigner-Smith matrix and are relevant to the problem of
quantum chaotic scattering.
",Statistics,Physics
"Interpretable Low-Dimensional Regression via Data-Adaptive Smoothing   We consider the problem of estimating a regression function in the common
situation where the number of features is small, where interpretability of the
model is a high priority, and where simple linear or additive models fail to
provide adequate performance. To address this problem, we present Maximum
Variance Total Variation denoising (MVTV), an approach that is conceptually
related both to CART and to the more recent CRISP algorithm, a state-of-the-art
alternative method for interpretable nonlinear regression. MVTV divides the
feature space into blocks of constant value and fits the value of all blocks
jointly via a convex optimization routine. Our method is fully data-adaptive,
in that it incorporates highly robust routines for tuning all hyperparameters
automatically. We compare our approach against CART and CRISP via both a
complexity-accuracy tradeoff metric and a human study, demonstrating that that
MVTV is a more powerful and interpretable method.
",Statistics,Computer Science; Statistics
"Bivariate Causal Discovery and its Applications to Gene Expression and Imaging Data Analysis   The mainstream of research in genetics, epigenetics and imaging data analysis
focuses on statistical association or exploring statistical dependence between
variables. Despite their significant progresses in genetic research,
understanding the etiology and mechanism of complex phenotypes remains elusive.
Using association analysis as a major analytical platform for the complex data
analysis is a key issue that hampers the theoretic development of genomic
science and its application in practice. Causal inference is an essential
component for the discovery of mechanical relationships among complex
phenotypes. Many researchers suggest making the transition from association to
causation. Despite its fundamental role in science, engineering and
biomedicine, the traditional methods for causal inference require at least
three variables. However, quantitative genetic analysis such as QTL, eQTL,
mQTL, and genomic-imaging data analysis requires exploring the causal
relationships between two variables. This paper will focus on bivariate causal
discovery. We will introduce independence of cause and mechanism (ICM) as a
basic principle for causal inference, algorithmic information theory and
additive noise model (ANM) as major tools for bivariate causal discovery.
Large-scale simulations will be performed to evaluate the feasibility of the
ANM for bivariate causal discovery. To further evaluate their performance for
causal inference, the ANM will be applied to the construction of gene
regulatory networks. Also, the ANM will be applied to trait-imaging data
analysis to illustrate three scenarios: presence of both causation and
association, presence of association while absence of causation, and presence
of causation, while lack of association between two variables.
",Quantitative Biology,Statistics
"The Frechet distribution: Estimation and Application an Overview   In this article, we consider the problem of estimating the parameters of the
Fréchet distribution from both frequentist and Bayesian points of view. First
we briefly describe different frequentist approaches, namely, maximum
likelihood, method of moments, percentile estimators, L-moments, ordinary and
weighted least squares, maximum product of spacings, maximum goodness-of-fit
estimators and compare them with respect to mean relative estimates, mean
squared errors and the 95\% coverage probability of the asymptotic confidence
intervals using extensive numerical simulations. Next, we consider the Bayesian
inference approach using reference priors. The Metropolis-Hasting algorithm is
used to draw Markov Chain Monte Carlo samples, and they have in turn been used
to compute the Bayes estimates and also to construct the corresponding credible
intervals. Five real data sets related to the minimum flow of water on
Piracicaba river in Brazil are used to illustrate the applicability of the
discussed procedures.
",Statistics,Mathematics; Statistics
"On Estimating Multi-Attribute Choice Preferences using Private Signals and Matrix Factorization   Revealed preference theory studies the possibility of modeling an agent's
revealed preferences and the construction of a consistent utility function.
However, modeling agent's choices over preference orderings is not always
practical and demands strong assumptions on human rationality and
data-acquisition abilities. Therefore, we propose a simple generative choice
model where agents are assumed to generate the choice probabilities based on
latent factor matrices that capture their choice evaluation across multiple
attributes. Since the multi-attribute evaluation is typically hidden within the
agent's psyche, we consider a signaling mechanism where agents are provided
with choice information through private signals, so that the agent's choices
provide more insight about his/her latent evaluation across multiple
attributes. We estimate the choice model via a novel multi-stage matrix
factorization algorithm that minimizes the average deviation of the factor
estimates from choice data. Simulation results are presented to validate the
estimation performance of our proposed algorithm.
",Statistics,Computer Science; Statistics
"Multi-time correlators in continuous measurement of qubit observables   We consider multi-time correlators for output signals from linear detectors,
continuously measuring several qubit observables at the same time. Using the
quantum Bayesian formalism, we show that for unital (symmetric) evolution in
the absence of phase backaction, an $N$-time correlator can be expressed as a
product of two-time correlators when $N$ is even. For odd $N$, there is a
similar factorization, which also includes a single-time average. Theoretical
predictions agree well with experimental results for two detectors, which
simultaneously measure non-commuting qubit observables.
",Physics,Physics
"Formal Methods for Adaptive Control of Dynamical Systems   We develop a method to control discrete-time systems with constant but
initially unknown parameters from linear temporal logic (LTL) specifications.
We introduce the notions of (non-deterministic) parametric and adaptive
transition systems and show how to use tools from formal methods to compute
adaptive control strategies for finite systems. For infinite systems, we first
compute abstractions in the form of parametric finite quotient transition
systems and then apply the techniques for finite systems. Unlike traditional
adaptive control methods, our approach is correct by design, does not require a
reference model, and can deal with a much wider range of systems and
specifications. Illustrative case studies are included.
",Computer Science; Mathematics,Computer Science
"An integral formula for the powered sum of the independent, identically and normally distributed random variables   The distribution of the sum of r-th power of standard normal random variables
is a generalization of the chi-squared distribution. In this paper, we
represent the probability density function of the random variable by an
one-dimensional absolutely convergent integral with the characteristic
function. Our integral formula is expected to be applied for evaluation of the
density function. Our integral formula is based on the inversion formula, and
we utilize a summation method. We also discuss on our formula in the view point
of hyperfunctions.
",Mathematics,Mathematics
"Improved Bounds for Online Dominating Sets of Trees   The online dominating set problem is an online variant of the minimum
dominating set problem, which is one of the most important NP-hard problems on
graphs. This problem is defined as follows: Given an undirected graph $G = (V,
E)$, in which $V$ is a set of vertices and $E$ is a set of edges. We say that a
set $D \subseteq V$ of vertices is a {\em dominating set} of $G$ if for each $v
\in V \setminus D$, there exists a vertex $u \in D$ such that $\{ u, v \} \in
E$. The vertices are revealed to an online algorithm one by one over time. When
a vertex is revealed, edges between the vertex and vertices revealed in the
past are also revealed. A revelaed subtree is connected at any time.
Immediately after the revelation of each vertex, an online algorithm can choose
vertices which were already revealed irrevocably and must maintain a dominating
set of a graph revealed so far. The cost of an algorithm on a given tree is the
number of vertices chosen by it, and its objective is to minimize the cost.
Eidenbenz (Technical report, Institute of Theoretical Computer Science, ETH
Zürich, 2002) and Boyar et al.\ (SWAT 2016) studied the case in which given
graphs are trees. They designed a deterministic online algorithm whose
competitive ratio is at most three, and proved that a lower bound on the
competitive ratio of any deterministic algorithm is two. In this paper, we also
focus on trees. We establish a matching lower bound for any deterministic
algorithm. Moreover, we design a randomized online algorithm whose competitive
ratio is at most $5/2 = 2.5$, and show that the competitive ratio of any
randomized algorithm is at least $4/3 \approx 1.333$.
",Computer Science,Computer Science
"Multilingual Hierarchical Attention Networks for Document Classification   Hierarchical attention networks have recently achieved remarkable performance
for document classification in a given language. However, when multilingual
document collections are considered, training such models separately for each
language entails linear parameter growth and lack of cross-language transfer.
Learning a single multilingual model with fewer parameters is therefore a
challenging but potentially beneficial objective. To this end, we propose
multilingual hierarchical attention networks for learning document structures,
with shared encoders and/or shared attention mechanisms across languages, using
multi-task learning and an aligned semantic space as input. We evaluate the
proposed models on multilingual document classification with disjoint label
sets, on a large dataset which we provide, with 600k news documents in 8
languages, and 5k labels. The multilingual models outperform monolingual ones
in low-resource as well as full-resource settings, and use fewer parameters,
thus confirming their computational efficiency and the utility of
cross-language transfer.
",Computer Science,Computer Science
"Magnetic resonance of rubidium atoms passing through a multi-layered transmission magnetic grating   We measured the magnetic resonance of rubidium atoms passing through periodic
magnetic fields generated by two types of multilayered transmission magnetic
grating. One of the gratings reported here was assembled by stacking four
layers of magnetic films so that the direction of magnetization alternated at
each level. The other grating was assembled so that the magnetization at each
level was aligned. For both types of grating, the experimental results were in
good agreement with our calculations. We studied the feasibility of extending
the frequency band of the grating and narrowing its resonance linewidth by
performing calculations. For magnetic resonance precision spectroscopy, we
conclude that the multi-layered transmission magnetic grating can generate
periodic fields with narrower linewidths at higher frequencies when a larger
number of layers is assembled at a shorter period length. Moreover, the
frequency band of this type of grating can potentially achieve frequencies of
up to hundreds of PHz.
",Physics,Physics
"OGLE-2013-BLG-1761Lb: A Massive Planet Around an M/K Dwarf   We report the discovery and the analysis of the planetary microlensing event,
OGLE-2013-BLG-1761. There are some degenerate solutions in this event because
the planetary anomaly is only sparsely sampled. But the detailed light curve
analysis ruled out all stellar binary models and shows that the lens to be a
planetary system. There is the so-called close/wide degeneracy in the solutions
with the planet/host mass ratio of $q \sim (7.5 \pm 1.5) \times 10^{-3}$ and $q
\sim (9.3 \pm 2.9) \times 10^{-3}$ with the projected separation in Einstein
radius units of $s = 0.95$ (close) and $s = 1.19$ (wide), respectively. The
microlens parallax effect is not detected but the finite source effect is
detected. Our Bayesian analysis indicates that the lens system is located at
$D_{\rm L}=6.9_{-1.2}^{+1.0} \ {\rm kpc}$ away from us and the host star is an
M/K-dwarf with the mass of $M_{\rm L}=0.33_{-0.18}^{+0.32} \ M_{\odot}$ orbited
by a super-Jupiter mass planet with the mass of $m_{\rm P}=2.8_{-1.5}^{+2.5} \
M_{\rm Jup}$ at the projected separation of $a_{\perp}=1.8_{-0.5}^{+0.5} \ {\rm
AU}$. The preference of the large lens distance in the Bayesian analysis is due
to the relatively large observed source star radius. The distance and other
physical parameters can be constrained by the future high resolution imaging by
ground large telescopes or HST. If the estimated lens distance is correct, this
planet provides another sample for testing the claimed deficit of planets in
the Galactic bulge.
",Physics,Physics
"Implicit Cooperative Positioning in Vehicular Networks   Absolute positioning of vehicles is based on Global Navigation Satellite
Systems (GNSS) combined with on-board sensors and high-resolution maps. In
Cooperative Intelligent Transportation Systems (C-ITS), the positioning
performance can be augmented by means of vehicular networks that enable
vehicles to share location-related information. This paper presents an Implicit
Cooperative Positioning (ICP) algorithm that exploits the Vehicle-to-Vehicle
(V2V) connectivity in an innovative manner, avoiding the use of explicit V2V
measurements such as ranging. In the ICP approach, vehicles jointly localize
non-cooperative physical features (such as people, traffic lights or inactive
cars) in the surrounding areas, and use them as common noisy reference points
to refine their location estimates. Information on sensed features are fused
through V2V links by a consensus procedure, nested within a message passing
algorithm, to enhance the vehicle localization accuracy. As positioning does
not rely on explicit ranging information between vehicles, the proposed ICP
method is amenable to implementation with off-the-shelf vehicular communication
hardware. The localization algorithm is validated in different traffic
scenarios, including a crossroad area with heterogeneous conditions in terms of
feature density and V2V connectivity, as well as a real urban area by using
Simulation of Urban MObility (SUMO) for traffic data generation. Performance
results show that the proposed ICP method can significantly improve the vehicle
location accuracy compared to the stand-alone GNSS, especially in harsh
environments, such as in urban canyons, where the GNSS signal is highly
degraded or denied.
",Computer Science; Statistics,Computer Science
"The OGLE Collection of Variable Stars. Over 450 000 Eclipsing and Ellipsoidal Binary Systems Toward the Galactic Bulge   We present a collection of 450 598 eclipsing and ellipsoidal binary systems
detected in the OGLE fields toward the Galactic bulge. The collection consists
of binary systems of all types: detached, semi-detached, and contact eclipsing
binaries, RS CVn stars, cataclysmic variables, HW Vir binaries, double periodic
variables, and even planetary transits. For all stars we provide the I- and
V-band time-series photometry obtained during the OGLE-II, OGLE-III, and
OGLE-IV surveys. We discuss methods used to identify binary systems in the OGLE
data and present several objects of particular interest.
",Physics,Physics
"Atmospheric thermal tides and planetary spin I. The complex interplay between stratification and rotation   Thermal atmospheric tides can torque telluric planets away from spin-orbit
synchronous rotation, as observed in the case of Venus. They thus participate
to determine the possible climates and general circulations of the atmospheres
of these planets. In this work, we write the equations governing the dynamics
of thermal tides in a local vertically-stratified section of a rotating
planetary atmosphere by taking into account the effects of the complete
Coriolis acceleration on tidal waves. This allows us to derive analytically the
tidal torque and the tidally dissipated energy, which we use to discuss the
possible regimes of tidal dissipation and examine the key role played by
stratification.
In agreement with early studies, we find that the frequency dependence of the
thermal atmospheric tidal torque in the vicinity of synchronization can be
approximated by a Maxwell model. This behaviour corresponds to weakly stably
stratified or convective fluid layers, as observed in ADLM2016a. A strong
stable stratification allows gravity waves to propagate, which makes the tidal
torque become negligible. The transition is continuous between these two
regimes. The traditional approximation appears to be valid in thin atmospheres
and in regimes where the rotation frequency is dominated by the forcing or the
buoyancy frequencies.
Depending on the stability of their atmospheres with respect to convection,
observed exoplanets can be tidally driven toward synchronous or asynchronous
final rotation rates. The domain of applicability of the traditional
approximation is rigorously constrained by calculations.
",Physics,Physics
"Single crystal polarized neutron diffraction study of the magnetic structure of HoFeO$_3$   Polarised neutron diffraction measurements have been made on HoFeO$_3$ single
crystals magnetised in both the [001] and [100] directions ($Pbnm$ setting).
The polarisation dependencies of Bragg reflection intensities were measured
both with a high field of H = 9 T parallel to [001] at T = 70 K and with the
lower field H = 0.5 T parallel to [100] at T = 5, 15, 25~K. A Fourier
projection of magnetization induced parallel to [001], made using the $hk0$
reflections measured in 9~T, indicates that almost all of it is due to
alignment of Ho moments. Further analysis of the asymmetries of general
reflections in these data showed that although, at 70~K, 9~T applied parallel
to [001] hardly perturbs the antiferromagnetic order of the Fe sublattices, it
induces significant antiferromagnetic order of the Ho sublattices in the
$x\mhyphen y$ plane, with the antiferromagnetic components of moment having the
same order of magnitude as the induced ferromagnetic ones. Strong intensity
asymmetries measured in the low temperature $\Gamma_2$ structure with a lower
field, 0.5 T $\parallel$ [100] allowed the variation of the ordered components
of the Ho and Fe moments to be followed. Their absolute orientations, in the
180\degree\ domain stabilised by the field were determined relative to the
distorted perovskite structure,. This relationship fixes the sign of the
Dzyalshinski-Moriya (D-M) interaction which leads to the weak ferromagnetism.
Our results indicate that the combination of strong y-axis anisotropy of the Ho
moments and Ho-Fe exchange interactions breaks the centrosymmetry of the
structure and could lead to ferroelectric polarization.
",Physics,Physics
"Index coding with erroneous side information   In this paper, new index coding problems are studied, where each receiver has
erroneous side information. Although side information is a crucial part of
index coding, the existence of erroneous side information has not yet been
considered. We study an index code with receivers that have erroneous side
information symbols in the error-free broadcast channel, which is called an
index code with side information errors (ICSIE). The encoding and decoding
procedures of the ICSIE are proposed, based on the syndrome decoding. Then, we
derive the bounds on the optimal codelength of the proposed index code with
erroneous side information. Furthermore, we introduce a special graph for the
proposed index coding problem, called a $\delta_s$-cycle whose properties are
similar to those of the cycle in the conventional index coding problem.
Properties of the ICSIE are also discussed in the $\delta_s$-cycle and clique.
Finally, the proposed ICSIE is generalized to an index code for the scenario
having both additive channel errors and side information errors, called a
generalized error correcting index code (GECIC).
",Computer Science,Computer Science
"A probabilistic approach to emission-line galaxy classification   We invoke a Gaussian mixture model (GMM) to jointly analyse two traditional
emission-line classification schemes of galaxy ionization sources: the
Baldwin-Phillips-Terlevich (BPT) and $\rm W_{H\alpha}$ vs. [NII]/H$\alpha$
(WHAN) diagrams, using spectroscopic data from the Sloan Digital Sky Survey
Data Release 7 and SEAGal/STARLIGHT datasets. We apply a GMM to empirically
define classes of galaxies in a three-dimensional space spanned by the $\log$
[OIII]/H$\beta$, $\log$ [NII]/H$\alpha$, and $\log$ EW(H${\alpha}$), optical
parameters. The best-fit GMM based on several statistical criteria suggests a
solution around four Gaussian components (GCs), which are capable to explain up
to 97 per cent of the data variance. Using elements of information theory, we
compare each GC to their respective astronomical counterpart. GC1 and GC4 are
associated with star-forming galaxies, suggesting the need to define a new
starburst subgroup. GC2 is associated with BPT's Active Galaxy Nuclei (AGN)
class and WHAN's weak AGN class. GC3 is associated with BPT's composite class
and WHAN's strong AGN class. Conversely, there is no statistical evidence --
based on four GCs -- for the existence of a Seyfert/LINER dichotomy in our
sample. Notwithstanding, the inclusion of an additional GC5 unravels it. The
GC5 appears associated to the LINER and Passive galaxies on the BPT and WHAN
diagrams respectively. Subtleties aside, we demonstrate the potential of our
methodology to recover/unravel different objects inside the wilderness of
astronomical datasets, without lacking the ability to convey physically
interpretable results. The probabilistic classifications from the GMM analysis
are publicly available within the COINtoolbox
(this https URL\_Catalogue/).
",Physics; Statistics,Physics
"Secure uniform random number extraction via incoherent strategies   To guarantee the security of uniform random numbers generated by a quantum
random number generator, we study secure extraction of uniform random numbers
when the environment of a given quantum state is controlled by the third party,
the eavesdropper. Here we restrict our operations to incoherent strategies that
are composed of the measurement on the computational basis and incoherent
operations (or incoherence-preserving operations). We show that the maximum
secure extraction rate is equal to the relative entropy of coherence. By
contrast, the coherence of formation gives the extraction rate when a certain
constraint is imposed on eavesdropper's operations. The condition under which
the two extraction rates coincide is then determined. Furthermore, we find that
the exponential decreasing rate of the leaked information is characterized by
Rényi relative entropies of coherence. These results clarify the power of
incoherent strategies in random number generation, and can be applied to
guarantee the quality of random numbers generated by a quantum random number
generator.
",Computer Science,Computer Science; Mathematics
"Estimating functional time series by moving average model fitting   Functional time series have become an integral part of both functional data
and time series analysis. Important contributions to methodology, theory and
application for the prediction of future trajectories and the estimation of
functional time series parameters have been made in the recent past. This paper
continues this line of research by proposing a first principled approach to
estimate invertible functional time series by fitting functional moving average
processes. The idea is to estimate the coefficient operators in a functional
linear filter. To do this a functional Innovations Algorithm is utilized as a
starting point to estimate the corresponding moving average operators via
suitable projections into principal directions. In order to establish
consistency of the proposed estimators, asymptotic theory is developed for
increasing subspaces of these principal directions. For practical purposes,
several strategies to select the number of principal directions to include in
the estimation procedure as well as the choice of order of the functional
moving average process are discussed. Their empirical performance is evaluated
through simulations and an application to vehicle traffic data.
",Statistics,Mathematics; Statistics
"Zermelo deformation of Finsler metrics by Killing vector fields   We show how geodesics, Jacobi vector fields and flag curvature of a Finsler
metric behave under Zermelo deformation with respect to a Killing vector field.
We also show that Zermelo deformation with respect to a Killing vector field of
a locally symmetric Finsler metric is also locally symmetric.
",Mathematics,Mathematics
"Variational Inference of Disentangled Latent Concepts from Unlabeled Observations   Disentangled representations, where the higher level data generative factors
are reflected in disjoint latent dimensions, offer several benefits such as
ease of deriving invariant representations, transferability to other tasks,
interpretability, etc. We consider the problem of unsupervised learning of
disentangled representations from large pool of unlabeled observations, and
propose a variational inference based approach to infer disentangled latent
factors. We introduce a regularizer on the expectation of the approximate
posterior over observed data that encourages the disentanglement. We also
propose a new disentanglement metric which is better aligned with the
qualitative disentanglement observed in the decoder's output. We empirically
observe significant improvement over existing methods in terms of both
disentanglement and data likelihood (reconstruction quality).
",Computer Science; Statistics,Statistics
"More on cyclic amenability of the Lau product of Banach algebras defined by a Banach algebra morphism   For two Banach algebras $A$ and $B$, the $T$-Lau product $A\times_T B$, was
recently introduced and studied for some bounded homomorphism $T:B\to A$ with
$\|T\|\leq 1$. Here, we give general nessesary and sufficent conditions for
$A\times_T B$ to be (approximately) cyclic amenable. In particular, we extend
some recent results on (approximate) cyclic amenability of direct product
$A\oplus B$ and $T$-Lau product $A\times_T B$ and answer a question on cyclic
amenability of $A\times_T B$.
",Mathematics,Mathematics
"A Deep Learning Approach for Population Estimation from Satellite Imagery   Knowing where people live is a fundamental component of many decision making
processes such as urban development, infectious disease containment, evacuation
planning, risk management, conservation planning, and more. While bottom-up,
survey driven censuses can provide a comprehensive view into the population
landscape of a country, they are expensive to realize, are infrequently
performed, and only provide population counts over broad areas. Population
disaggregation techniques and population projection methods individually
address these shortcomings, but also have shortcomings of their own. To jointly
answer the questions of ""where do people live"" and ""how many people live
there,"" we propose a deep learning model for creating high-resolution
population estimations from satellite imagery. Specifically, we train
convolutional neural networks to predict population in the USA at a
$0.01^{\circ} \times 0.01^{\circ}$ resolution grid from 1-year composite
Landsat imagery. We validate these models in two ways: quantitatively, by
comparing our model's grid cell estimates aggregated at a county-level to
several US Census county-level population projections, and qualitatively, by
directly interpreting the model's predictions in terms of the satellite image
inputs. We find that aggregating our model's estimates gives comparable results
to the Census county-level population projections and that the predictions made
by our model can be directly interpreted, which give it advantages over
traditional population disaggregation methods. In general, our model is an
example of how machine learning techniques can be an effective tool for
extracting information from inherently unstructured, remotely sensed data to
provide effective solutions to social problems.
",Computer Science,Computer Science; Statistics
"Sequential detection of low-rank changes using extreme eigenvalues   We study the problem of detecting an abrupt change to the signal covariance
matrix. In particular, the covariance changes from a ""white"" identity matrix to
an unknown spiked or low-rank matrix. Two sequential change-point detection
procedures are presented, based on the largest and the smallest eigenvalues of
the sample covariance matrix. To control false-alarm-rate, we present an
accurate theoretical approximation to the average-run-length (ARL) and expected
detection delay (EDD) of the detection, leveraging the extreme eigenvalue
distributions from random matrix theory and by capturing a non-negligible
temporal correlation in the sequence of scan statistics due to the sliding
window approach. Real data examples demonstrate the good performance of our
method for detecting behavior change of a swarm.
",Mathematics; Statistics,Computer Science; Statistics
"Efficient Computation of Feedback Control for Constrained Systems   A method is presented for solving the discrete-time finite-horizon Linear
Quadratic Regulator (LQR) problem subject to auxiliary linear equality
constraints, such as fixed end-point constraints. The method explicitly
determines an affine relationship between the control and state variables, as
in standard Riccati recursion, giving rise to feedback control policies that
account for constraints. Since the linearly-constrained LQR problem arises
commonly in robotic trajectory optimization, having a method that can
efficiently compute these solutions is important. We demonstrate some of the
useful properties and interpretations of said control policies, and we compare
the computation time of our method against existing methods.
",Computer Science,Computer Science
"Modeling Oral Multispecies Biofilm Recovery After Antibacterial Treatment   Recovery of multispecies oral biofilms is investigated following treatment by
chlorhexidine gluconate (CHX), iodine-potassium iodide (IPI) and Sodium
hypochlorite (NaOCl) both experimentally and theoretically. Experimentally,
biofilms taken from two donors were exposed to the three antibacterial
solutions (irrigants) for 10 minutes, respectively. We observe that (a) live
bacterial cell ratios decline for a week after the exposure and the trend
reverses beyond a week; after fifteen weeks, live bacterial cell ratios in
biofilms fully return to their pretreatment levels; (b) NaOCl is shown as the
strongest antibacterial agent for the oral biofilms; (c) multispecies oral
biofilms from different donors showed no difference in their susceptibility to
all the bacterial solutions. Guided by the experiment, a mathematical model for
biofilm dynamics is developed, accounting for multiple bacterial phenotypes,
quorum sensing, and growth factor proteins, to describe the nonlinear time
evolutionary behavior of the biofilms. The model captures time evolutionary
dynamics of biofilms before and after antibacterial treatment very well. It
reveals the crucial role played by quorum sensing molecules and growth factors
in biofilm recovery and verifies that the source of biofilms has a minimal to
their recovery. The model is also applied to describe the state of biofilms of
various ages treated by CHX, IPI and NaOCl, taken from different donors. Good
agreement with experimental data predicted by the model is obtained as well,
confirming its applicability to modeling biofilm dynamics in general.
",Quantitative Biology,Quantitative Biology
"The structure of rationally factorized Lax type flows and their analytical integrability   The work is devoted to constructing a wide class of differential-functional
dynamical systems, whose rich algebraic structure makes their integrability
analytically effective. In particular, there is analyzed in detail the operator
Lax type equations for factorized seed elements, there is proved an important
theorem about their operator factorization and the related analytical solution
scheme to the corresponding nonlinear differential-functional dynamical
systems.
",Physics,Mathematics
"Frank-Wolfe Optimization for Symmetric-NMF under Simplicial Constraint   Symmetric nonnegative matrix factorization has found abundant applications in
various domains by providing a symmetric low-rank decomposition of nonnegative
matrices. In this paper we propose a Frank-Wolfe (FW) solver to optimize the
symmetric nonnegative matrix factorization problem under a simplicial
constraint, which has recently been proposed for probabilistic clustering.
Compared with existing solutions, this algorithm is simple to implement, and
has no hyperparameters to be tuned. Building on the recent advances of FW
algorithms in nonconvex optimization, we prove an $O(1/\varepsilon^2)$
convergence rate to $\varepsilon$-approximate KKT points, via a tight bound
$\Theta(n^2)$ on the curvature constant, which matches the best known result in
unconstrained nonconvex setting using gradient methods. Numerical results
demonstrate the effectiveness of our algorithm. As a side contribution, we
construct a simple nonsmooth convex problem where the FW algorithm fails to
converge to the optimum. This result raises an interesting question about
necessary conditions of the success of the FW algorithm on convex problems.
",Computer Science; Mathematics; Statistics,Computer Science; Mathematics
"Converse passivity theorems   Passivity is an imperative concept and a widely utilized tool in the analysis
and control of interconnected systems. It naturally arises in the modelling of
physical systems involving passive elements and dynamics. While many theorems
on passivity are known in the theory of robust control, very few converse
passivity results exist. This paper establishes various versions of converse
passivity theorems for nonlinear feedback systems. In particular, open-loop
passivity is shown to be necessary to ensure closed-loop passivity from an
input-output perspective. Moreover, the stability of the feedback
interconnection of a specific system with an arbitrary passive system is shown
to imply passivity of the system itself.
",Mathematics,Computer Science
"Parameter Sharing Deep Deterministic Policy Gradient for Cooperative Multi-agent Reinforcement Learning   Deep reinforcement learning for multi-agent cooperation and competition has
been a hot topic recently. This paper focuses on cooperative multi-agent
problem based on actor-critic methods under local observations settings. Multi
agent deep deterministic policy gradient obtained state of art results for some
multi-agent games, whereas, it cannot scale well with growing amount of agents.
In order to boost scalability, we propose a parameter sharing deterministic
policy gradient method with three variants based on neural networks, including
actor-critic sharing, actor sharing and actor sharing with partially shared
critic. Benchmarks from rllab show that the proposed method has advantages in
learning speed and memory efficiency, well scales with growing amount of
agents, and moreover, it can make full use of reward sharing and
exchangeability if possible.
",Computer Science,Computer Science; Statistics
"Private Learning on Networks: Part II   This paper considers a distributed multi-agent optimization problem, with the
global objective consisting of the sum of local objective functions of the
agents. The agents solve the optimization problem using local computation and
communication between adjacent agents in the network. We present two randomized
iterative algorithms for distributed optimization. To improve privacy, our
algorithms add ""structured"" randomization to the information exchanged between
the agents. We prove deterministic correctness (in every execution) of the
proposed algorithms despite the information being perturbed by noise with
non-zero mean. We prove that a special case of a proposed algorithm (called
function sharing) preserves privacy of individual polynomial objective
functions under a suitable connectivity condition on the network topology.
",Computer Science; Mathematics,Computer Science; Statistics
"Antiferromagnetic structure and electronic properties of BaCr2As2 and BaCrFeAs2   The chromium arsenides BaCr2As2 and BaCrFeAs2 with ThCr2Si2 type structure
(space group I4/mmm; also adopted by '122' iron arsenide superconductors) have
been suggested as mother compounds for possible new superconductors. DFT-based
calculations of the electronic structure evidence metallic antiferromagnetic
ground states for both compounds. By powder neutron diffraction we confirm for
BaCr2As2 a robust ordering in the antiferromagnetic G-type structure at T_N =
580 K with mu_Cr = 1.9 mu_B at T = 2K. Anomalies in the lattice parameters
point to magneto-structural coupling effects. In BaCrFeAs2 the Cr and Fe atoms
randomly occupy the transition-metal site and G-type order is found below 265 K
with mu_Cr/Fe = 1.1 mu_B. 57Fe Moessbauer spectroscopy demonstrates that only a
small ordered moment is associated with the Fe atoms, in agreement with
electronic structure calculations with mu_Fe ~ 0. The temperature dependence of
the hyperfine field does not follow that of the total moments. Both compounds
are metallic but show large enhancements of the linear specific heat
coefficient gamma with respect to the band structure values. The metallic state
and the electrical transport in BaCrFeAs2 is dominated by the atomic disorder
of Cr and Fe and partial magnetic disorder of Fe. Our results indicate that
Neel-type order is unfavorable for the Fe moments and thus it is destabilized
with increasing iron content.
",Physics,Physics
"Experimental Determination of the Structural Coefficient of Restitution of a Bouncing Asteroid Lander   The structural coefficient of restitution describes the kinetic energy
dissipation upon low-velocity (~0.1 m/s) impact of a small asteroid lander,
MASCOT, against a hard, ideally elastic plane surface. It is a crucial
worst-case input for mission analysis for landing MACOT on a 1km asteroid in
2018. We conducted pendulum tests and describe their analysis and the results.
",Physics,Physics
"Throughput-Optimal Broadcast in Wireless Networks with Point-to-Multipoint Transmissions   We consider the problem of efficient packet dissemination in wireless
networks with point-to-multi-point wireless broadcast channels. We propose a
dynamic policy, which achieves the broadcast capacity of the network. This
policy is obtained by first transforming the original multi-hop network into a
precedence-relaxed virtual single-hop network and then finding an optimal
broadcast policy for the relaxed network. The resulting policy is shown to be
throughput-optimal for the original wireless network using a sample-path
argument. We also prove the NP-completeness of the finite-horizon broadcast
problem, which is in contrast with the polynomial time solvability of the
problem with point-to-point channels. Illustrative simulation results
demonstrate the efficacy of the proposed broadcast policy in achieving the full
broadcast capacity with low delay.
",Computer Science; Mathematics,Computer Science
"Why Do Neural Dialog Systems Generate Short and Meaningless Replies? A Comparison between Dialog and Translation   This paper addresses the question: Why do neural dialog systems generate
short and meaningless replies? We conjecture that, in a dialog system, an
utterance may have multiple equally plausible replies, causing the deficiency
of neural networks in the dialog application. We propose a systematic way to
mimic the dialog scenario in a machine translation system, and manage to
reproduce the phenomenon of generating short and less meaningful sentences in
the translation setting, showing evidence of our conjecture.
",Computer Science,Computer Science
"Effects of sampling skewness of the importance-weighted risk estimator on model selection   Importance-weighting is a popular and well-researched technique for dealing
with sample selection bias and covariate shift. It has desirable
characteristics such as unbiasedness, consistency and low computational
complexity. However, weighting can have a detrimental effect on an estimator as
well. In this work, we empirically show that the sampling distribution of an
importance-weighted estimator can be skewed. For sample selection bias
settings, and for small sample sizes, the importance-weighted risk estimator
produces overestimates for datasets in the body of the sampling distribution,
i.e. the majority of cases, and large underestimates for data sets in the tail
of the sampling distribution. These over- and underestimates of the risk lead
to suboptimal regularization parameters when used for importance-weighted
validation.
",Statistics,Statistics
"Multiplicities of bifurcation sets of Pham singularities   The local multiplicities of the Maxwell sets in the spaces of versal
deformations of Pham holomorphic function singularities are calculated. A
similar calculation for some other bifurcation sets (generalized Stokes' sets)
defined by more complicated relations between the critical values is given.
Aplications to the complexity of algorithms enumerating topologically distinct
morsifications of complicated real function singularities are discussed.
",Mathematics,Mathematics
"The Hadamard Determinant Inequality - Extensions to Operators on a Hilbert Space   A generalization of classical determinant inequalities like Hadamard's
inequality and Fischer's inequality is studied. For a version of the
inequalities originally proved by Arveson for positive operators in von Neumann
algebras with a tracial state, we give a different proof. We also improve and
generalize to the setting of finite von Neumann algebras, some `Fischer-type'
inequalities by Matic for determinants of perturbed positive-definite matrices.
In the process, a conceptual framework is established for viewing these
inequalities as manifestations of Jensen's inequality in conjunction with the
theory of operator monotone and operator convex functions on $[0,\infty)$. We
place emphasis on documenting necessary and sufficient conditions for equality
to hold.
",Mathematics,Mathematics
"Commutativity theorems for groups and semigroups   In this note we prove a selection of commutativity theorems for various
classes of semigroups. For instance, if in a separative or completely regular
semigroup $S$ we have $x^p y^p = y^p x^p$ and $x^q y^q = y^q x^q$ for all
$x,y\in S$ where $p$ and $q$ are relatively prime, then $S$ is commutative. In
a separative or inverse semigroup $S$, if there exist three consecutive
integers $i$ such that $(xy)^i = x^i y^i$ for all $x,y\in S$, then $S$ is
commutative. Finally, if $S$ is a separative or inverse semigroup satisfying
$(xy)^3=x^3y^3$ for all $x,y\in S$, and if the cubing map $x\mapsto x^3$ is
injective, then $S$ is commutative.
",Mathematics,Mathematics
"Effective One-Dimensional Coupling in the Highly-Frustrated Square-Lattice Itinerant Magnet CaCo$_{\mathrm{2}-y}$As$_{2}$   Inelastic neutron scattering measurements on the itinerant antiferromagnet
(AFM) CaCo$_{\mathrm{2}-y}$As$_{2}$ at a temperature of 8 K reveal two
orthogonal planes of scattering perpendicular to the Co square lattice in
reciprocal space, demonstrating the presence of effective one-dimensional spin
interactions. These results are shown to arise from near-perfect bond
frustration within the $J_1$-$J_2$ Heisenberg model on a square lattice with
ferromagnetic $J_1$, and hence indicate that the extensive previous
experimental and theoretical study of the $J_1$-$J_2$ Heisenberg model on
local-moment square spin lattices should be expanded to include itinerant spin
systems.
",Physics,Physics
"The effect of surface tension on steadily translating bubbles in an unbounded Hele-Shaw cell   New numerical solutions to the so-called selection problem for one and two
steadily translating bubbles in an unbounded Hele-Shaw cell are presented. Our
approach relies on conformal mapping which, for the two-bubble problem,
involves the Schottky-Klein prime function associated with an annulus. We show
that a countably infinite number of solutions exist for each fixed value of
dimensionless surface tension, with the bubble shapes becoming more exotic as
the solution branch number increases. Our numerical results suggest that a
single solution is selected in the limit that surface tension vanishes, with
the scaling between the bubble velocity and surface tension being different to
the well-studied problems for a bubble or a finger propagating in a channel
geometry.
",Physics; Mathematics,Physics
"Stochastic evolution equations for large portfolios of stochastic volatility models   We consider a large market model of defaultable assets in which the asset
price processes are modelled as Heston-type stochastic volatility models with
default upon hitting a lower boundary. We assume that both the asset prices and
their volatilities are correlated through systemic Brownian motions. We are
interested in the loss process that arises in this setting and we prove the
existence of a large portfolio limit for the empirical measure process of this
system. This limit evolves as a measure valued process and we show that it will
have a density given in terms of a solution to a stochastic partial
differential equation of filtering type in the two-dimensional half-space, with
a Dirichlet boundary condition. We employ Malliavin calculus to establish the
existence of a regular density for the volatility component, and an
approximation by models of piecewise constant volatilities combined with a
kernel smoothing technique to obtain existence and regularity for the full
two-dimensional filtering problem. We are able to establish good regularity
properties for solutions, however uniqueness remains an open problem.
",Mathematics,Quantitative Finance
"On the Taylor coefficients of a subclass of meromorphic univalent functions   Let $\mathcal{V}_p(\lambda)$ be the collection of all functions $f$ defined
in the unit disc $\ID$ having a simple pole at $z=p$ where $0<p<1$ and analytic
in $\ID\setminus\{p\}$ with $f(0)=0=f'(0)-1$ and satisfying the differential
inequality $|(z/f(z))^2 f'(z)-1|< \lambda $ for $z\in \ID$, $0<\lambda\leq 1$.
Each $f\in\mathcal{V}_p(\lambda)$ has the following Taylor expansion:
$$
f(z)=z+\sum_{n=2}^{\infty}a_n(f) z^n, \quad |z|<p.
$$
In \cite{BF-3}, we conjectured that
$$
|a_n(f)|\leq \frac{1-(\lambda p^2)^n}{p^{n-1}(1-\lambda p^2)}\quad
\mbox{for}\quad n\geq3. $$ In the present article, we first obtain a
representation formula for functions in the class $\mathcal{V}_p(\lambda)$.
Using this representation, we prove the aforementioned conjecture for $n=3,4,5$
whenever $p$ belongs to certain subintervals of $(0,1)$. Also we determine non
sharp bounds for $|a_n(f)|,\,n\geq 3$ and for $|a_{n+1}(f)-a_n(f)/p|,\,n\geq
2$.
",Mathematics,Mathematics
"Radio detection of Extensive Air Showers (ECRS 2016)   Detection of the mostly geomagnetically generated radio emission of
cosmic-ray air showers provides an alternative to air-Cherenkov and
air-fluorescence detection, since it is not limited to clear nights. Like these
established methods, the radio signal is sensitive to the calorimetric energy
and the position of the maximum of the electromagnetic shower component. This
makes antenna arrays an ideal extension for particle-detector arrays above a
threshold energy of about 100 PeV of the primary cosmic-ray particles. In the
last few years the digital radio technique for cosmic-ray air showers again
made significant progress, and there now is a consistent picture of the
emission mechanisms confirmed by several measurements. Recent results by the
antenna arrays AERA and Tunka-Rex confirm that the absolute accuracy for the
shower energy is as good as the other detection techniques. Moreover, the
sensitivity to the shower maximum of the radio signal has been confirmed in
direct comparison to air-Cherenkov measurements by Tunka-Rex. The dense antenna
array LOFAR can already compete with the established techniques in accuracy for
cosmic-ray mass-composition. In the future, a new generation of radio
experiments might drive the field: either by providing extremely large exposure
for inclined cosmic-ray or neutrino showers or, like the SKA core in Australia
with its several 10,000 antennas, by providing extremely detailed measurements.
",Physics,Physics
"IMLS-SLAM: scan-to-model matching based on 3D data   The Simultaneous Localization And Mapping (SLAM) problem has been well
studied in the robotics community, especially using mono, stereo cameras or
depth sensors. 3D depth sensors, such as Velodyne LiDAR, have proved in the
last 10 years to be very useful to perceive the environment in autonomous
driving, but few methods exist that directly use these 3D data for odometry. We
present a new low-drift SLAM algorithm based only on 3D LiDAR data. Our method
relies on a scan-to-model matching framework. We first have a specific sampling
strategy based on the LiDAR scans. We then define our model as the previous
localized LiDAR sweeps and use the Implicit Moving Least Squares (IMLS) surface
representation. We show experiments with the Velodyne HDL32 with only 0.40%
drift over a 4 km acquisition without any loop closure (i.e., 16 m drift after
4 km). We tested our solution on the KITTI benchmark with a Velodyne HDL64 and
ranked among the best methods (against mono, stereo and LiDAR methods) with a
global drift of only 0.69%.
",Computer Science,Computer Science
"AdS4 backgrounds with N>16 supersymmetries in 10 and 11 dimensions   We explore all warped $AdS_4\times_w M^{D-4}$ backgrounds with the most
general allowed fluxes that preserve more than 16 supersymmetries in $D=10$-
and $11$-dimensional supergravities. After imposing the assumption that either
the internal space $M^{D-4}$ is compact without boundary or the isometry
algebra of the background decomposes into that of AdS$_4$ and that of
$M^{D-4}$, we find that there are no such backgrounds in IIB supergravity.
Similarly in IIA supergravity, there is a unique such background with 24
supersymmetries locally isometric to $AdS_4\times \mathbb{CP}^3$, and in $D=11$
supergravity all such backgrounds are locally isometric to the maximally
supersymmetric $AdS_4\times S^7$ solution.
",Mathematics,Mathematics
"Occupation times for the finite buffer fluid queue with phase-type ON-times   In this short communication we study a fluid queue with a finite buffer. The
performance measure we are interested in is the occupation time over a finite
time period, i.e., the fraction of time the workload process is below some
fixed target level. We construct an alternating sequence of sojourn times
$D_1,U_1,...$ where the pairs $(D_i,U_i)_{i\in\mathbb{N}}$ are i.i.d. random
vectors. We use this sequence to determine the distribution function of the
occupation time in terms of its double transform.
",Mathematics,Computer Science; Mathematics
"A Dictionary Approach to Identifying Transient RFI   As radio telescopes become more sensitive, the damaging effects of radio
frequency interference (RFI) become more apparent. Near radio telescope arrays,
RFI sources are often easily removed or replaced; the challenge lies in
identifying them. Transient (impulsive) RFI is particularly difficult to
identify. We propose a novel dictionary-based approach to transient RFI
identification. RFI events are treated as sequences of sub-events, drawn from
particular labelled classes. We demonstrate an automated method of extracting
and labelling sub-events using a dataset of transient RFI. A dictionary of
labels may be used in conjunction with hidden Markov models to identify the
sources of RFI events reliably. We attain improved classification accuracy over
traditional approaches such as SVMs or a naïve kNN classifier. Finally, we
investigate why transient RFI is difficult to classify. We show that cluster
separation in the principal components domain is influenced by the mains supply
phase for certain sources.
",Physics,Physics
"Machine vs Machine: Minimax-Optimal Defense Against Adversarial Examples   Recently, researchers have discovered that the state-of-the-art object
classifiers can be fooled easily by small perturbations in the input
unnoticeable to human eyes. It is also known that an attacker can generate
strong adversarial examples if she knows the classifier parameters. Conversely,
a defender can robustify the classifier by retraining if she has access to the
adversarial examples. We explain and formulate this adversarial example problem
as a two-player continuous zero-sum game, and demonstrate the fallacy of
evaluating a defense or an attack as a static problem. To find the best
worst-case defense against whitebox attacks, we propose a continuous minimax
optimization algorithm. We demonstrate the minimax defense with two types of
attack classes -- gradient-based and neural network-based attacks. Experiments
with the MNIST and the CIFAR-10 datasets demonstrate that the defense found by
numerical minimax optimization is indeed more robust than non-minimax defenses.
We discuss directions for improving the result toward achieving robustness
against multiple types of attack classes.
",Computer Science; Statistics,Computer Science
"The 2017 DAVIS Challenge on Video Object Segmentation   We present the 2017 DAVIS Challenge on Video Object Segmentation, a public
dataset, benchmark, and competition specifically designed for the task of video
object segmentation. Following the footsteps of other successful initiatives,
such as ILSVRC and PASCAL VOC, which established the avenue of research in the
fields of scene classification and semantic segmentation, the DAVIS Challenge
comprises a dataset, an evaluation methodology, and a public competition with a
dedicated workshop co-located with CVPR 2017. The DAVIS Challenge follows up on
the recent publication of DAVIS (Densely-Annotated VIdeo Segmentation), which
has fostered the development of several novel state-of-the-art video object
segmentation techniques. In this paper we describe the scope of the benchmark,
highlight the main characteristics of the dataset, define the evaluation
metrics of the competition, and present a detailed analysis of the results of
the participants to the challenge.
",Computer Science,Computer Science
"Geometric Rescaling Algorithms for Submodular Function Minimization   We present a new class of polynomial-time algorithms for submodular function
minimization (SFM), as well as a unified framework to obtain strongly
polynomial SFM algorithms. Our new algorithms are based on simple iterative
methods for the minimum-norm problem, such as the conditional gradient and the
Fujishige-Wolfe algorithms. We exhibit two techniques to turn simple iterative
methods into polynomial-time algorithms.
Firstly, we use the geometric rescaling technique, which has recently gained
attention in linear programming. We adapt this technique to SFM and obtain a
weakly polynomial bound $O((n^4\cdot EO + n^5)\log (n L))$.
Secondly, we exhibit a general combinatorial black-box approach to turn any
strongly polynomial $\varepsilon L$-approximate SFM oracle into a strongly
polynomial exact SFM algorithm. This framework can be applied to a wide range
of combinatorial and continuous algorithms, including pseudo-polynomial ones.
In particular, we can obtain strongly polynomial algorithms by a repeated
application of the conditional gradient or of the Fujishige-Wolfe algorithm.
Combined with the geometric rescaling technique, the black-box approach
provides a $O((n^5\cdot EO + n^6)\log^2 n)$ algorithm. Finally, we show that
one of the techniques we develop in the paper can also be combined with the
cutting-plane method of Lee, Sidford, and Wong, yielding a simplified variant
of their $O(n^3 \log^2 n \cdot EO + n^4\log^{O(1)} n)$ algorithm.
",Computer Science; Mathematics,Computer Science
"Nondegeneracy and the Jacobi fields of rotationally symmetric solutions to the Cahn-Hillard equation   In this paper we study rotationally symmetric solutions of the Cahn-Hilliard
equation in $\mathbb R^3$ constructed by the authors. These solutions form a
one parameter family analog to the family of Delaunay surfaces and in fact the
zero level sets of their blowdowns approach these surfaces. Presently we go a
step further and show that their stability properties are inherited from the
stability properties of the Delaunay surfaces. Our main result states that the
rotationally symmetric solutions are non degenerate and that they have exactly
$6$ Jacobi fields of temperate growth coming from the natural invariances of
the problem (3 translations and 2 rotations) and the variation of the Delaunay
parameter.
",Mathematics,Mathematics
"Co-location Epidemic Tracking on London Public Transports Using Low Power Mobile Magnetometer   The public transports provide an ideal means to enable contagious diseases
transmission. This paper introduces a novel idea to detect co-location of
people in such environment using just the ubiquitous geomagnetic field sensor
on the smart phone. Essentially, given that all passengers must share the same
journey between at least two consecutive stations, we have a long window to
match the user trajectory. Our idea was assessed over a painstakingly survey of
over 150 kilometres of travelling distance, covering different parts of London,
using the overground trains, the underground tubes and the buses.
",Computer Science,Computer Science
"A minimally-dissipative low-Mach number solver for complex reacting flows in OpenFOAM   Large eddy simulation (LES) has become the de-facto computational tool for
modeling complex reacting flows, especially in gas turbine applications.
However, readily usable general-purpose LES codes for complex geometries are
typically academic or proprietary/commercial in nature. The objective of this
work is to develop and disseminate an open source LES tool for low-Mach number
turbulent combustion using the OpenFOAM framework. In particular, a
collocated-mesh approach suited for unstructured grid formulation is provided.
Unlike other fluid dynamics models, LES accuracy is intricately linked to
so-called primary and secondary conservation properties of the numerical
discretization schemes. This implies that although the solver only evolves
equations for mass, momentum, and energy, the implied discrete equation for
kinetic energy (square of velocity) should be minimally-dissipative. Here, a
specific spatial and temporal discretization is imposed such that this kinetic
energy dissipation is minimized. The method is demonstrated using manufactured
solutions approach on regular and skewed meshes, a canonical flow problem, and
a turbulent sooting flame in a complex domain relevant to gas turbines
applications.
",Physics,Physics
"Spelling Correction as a Foreign Language   In this paper, we reformulated the spell correction problem as a machine
translation task under the encoder-decoder framework. This reformulation
enabled us to use a single model for solving the problem that is traditionally
formulated as learning a language model and an error model. This model employs
multi-layer recurrent neural networks as an encoder and a decoder. We
demonstrate the effectiveness of this model using an internal dataset, where
the training data is automatically obtained from user logs. The model offers
competitive performance as compared to the state of the art methods but does
not require any feature engineering nor hand tuning between models.
",Computer Science,Computer Science
"Astrophysical uncertainties on the local dark matter distribution and direct detection experiments   The differential event rate in Weakly Interacting Massive Particle (WIMP)
direct detection experiments depends on the local dark matter density and
velocity distribution. Accurate modelling of the local dark matter distribution
is therefore required to obtain reliable constraints on the WIMP particle
physics properties. Data analyses typically use a simple Standard Halo Model
which might not be a good approximation to the real Milky Way (MW) halo. We
review observational determinations of the local dark matter density, circular
speed and escape speed and also studies of the local dark matter distribution
in simulated MW-like galaxies. We discuss the effects of the uncertainties in
these quantities on the energy spectrum and its time and direction dependence.
Finally we conclude with an overview of various methods for handling these
astrophysical uncertainties.
",Physics,Physics
"The Externalities of Exploration and How Data Diversity Helps Exploitation   Online learning algorithms, widely used to power search and content
optimization on the web, must balance exploration and exploitation, potentially
sacrificing the experience of current users for information that will lead to
better decisions in the future. Recently, concerns have been raised about
whether the process of exploration could be viewed as unfair, placing too much
burden on certain individuals or groups. Motivated by these concerns, we
initiate the study of the externalities of exploration - the undesirable side
effects that the presence of one party may impose on another - under the linear
contextual bandits model. We introduce the notion of a group externality,
measuring the extent to which the presence of one population of users impacts
the rewards of another. We show that this impact can in some cases be negative,
and that, in a certain sense, no algorithm can avoid it. We then study
externalities at the individual level, interpreting the act of exploration as
an externality imposed on the current user of a system by future users. This
drives us to ask under what conditions inherent diversity in the data makes
explicit exploration unnecessary. We build on a recent line of work on the
smoothed analysis of the greedy algorithm that always chooses the action that
currently looks optimal, improving on prior results to show that a greedy
approach almost matches the best possible Bayesian regret rate of any other
algorithm on the same problem instance whenever the diversity conditions hold,
and that this regret is at most $\tilde{O}(T^{1/3})$. Returning to group-level
effects, we show that under the same conditions, negative group externalities
essentially vanish under the greedy algorithm. Together, our results uncover a
sharp contrast between the high externalities that exist in the worst case, and
the ability to remove all externalities if the data is sufficiently diverse.
",Statistics,Computer Science; Statistics
"Analytical Approach for Calculating Chemotaxis Sensitivity Function   We consider the chemotaxis problem for a one-dimensional system. To analyze
the interaction of bacteria and attractant we use a modified Keller-Segel model
which accounts attractant absorption. To describe the system we use the
chemotaxis sensitivity function, which characterizes nonuniformity of bacteria
distribution. In particular, we investigate how the chemotaxis sensitivity
function depends on the concentration of attractant at the boundary of the
system. It is known that in the system without absorption the chemotaxis
sensitivity function has a bell shape maximum. Here we show that attractant
absorption and special boundary conditions for bacteria can cause the
appearance of an additional maximum in the chemotaxis sensitivity function. The
value of this maximum is determined by the intensity of absorption.
",Physics,Physics
"Real intersection homology   We present a definition of intersection homology for real algebraic varieties
that is analogous to Goresky and MacPherson's original definition of
intersection homology for complex varieties.
",Mathematics,Mathematics
"Trace Expressiveness of Timed and Probabilistic Automata   Automata expressiveness is an essential feature in understanding which of the
formalisms available should be chosen for modelling a particular problem.
Probabilistic and stochastic automata are suitable for modelling systems
exhibiting probabilistic behavior and their expressiveness has been studied
relative to non-probabilistic transition systems and Markov chains. In this
paper, we consider previous formalisms of Timed, Probabilistic and Stochastic
Timed Automata, we present our new model of Timed Automata with Polynomial
Delay, we introduce a measure of expressiveness for automata we call trace
expressiveness and we characterize the expressiveness of these models relative
to each other under this new measure.
",Computer Science,Computer Science; Mathematics
"Lee-Carter method for forecasting mortality for Peruvian Population   In this article, we have modeled mortality rates of Peruvian female and male
populations during the period of 1950-2017 using the Lee-Carter (LC) model. The
stochastic mortality model was introduced by Lee and Carter (1992) and has been
used by many authors for fitting and forecasting the human mortality rates. The
Singular Value Decomposition (SVD) approach is used for estimation of the
parameters of the LC model. Utilizing the best fitted auto regressive
integrated moving average (ARIMA) model we forecast the values of the time
dependent parameter of the LC model for the next thirty years. The forecasted
values of life expectancy at different age group with $95\%$ confidence
intervals are also reported for the next thirty years. In this research we use
the data, obtained from the Peruvian National Institute of Statistics (INEI).
",Quantitative Finance,Statistics
"Analysis of the Gibbs Sampler for Gaussian hierarchical models via multigrid decomposition   We study the convergence properties of the Gibbs Sampler in the context of
posterior distributions arising from Bayesian analysis of Gaussian hierarchical
models. We consider centred and non-centred parameterizations as well as their
hybrids including the full family of partially non-centred parameterizations.
We develop a novel methodology based on multi-grid decompositions to derive
analytic expressions for the convergence rates of the algorithm for an
arbitrary number of layers in the hierarchy, while previous work was typically
limited to the two-level case. Our work gives a complete understanding for the
three-level symmetric case and this gives rise to approximations for the
non-symmetric case. We also give analogous, if less explicit, results for
models of arbitrary level. This theory gives rise to simple and
easy-to-implement guidelines for the practical implementation of Gibbs samplers
on conditionally Gaussian hierarchical models.
",Statistics,Statistics
"Geometric Analysis of Synchronization in Neuronal Networks with Global Inhibition and Coupling Delays   We study synaptically coupled neuronal networks to identify the role of
coupling delays in network's synchronized behaviors. We consider a network of
excitable, relaxation oscillator neurons where two distinct populations, one
excitatory and one inhibitory, are coupled and interact with each other. The
excitatory population is uncoupled, while the inhibitory population is tightly
coupled. A geometric singular perturbation analysis yields existence and
stability conditions for synchronization states under different firing patterns
between the two populations, along with formulas for the periods of such
synchronous solutions. Our results demonstrate that the presence of coupling
delays in the network promotes synchronization. Numerical simulations are
conducted to supplement and validate analytical results. We show the results
carry over to a model for spindle sleep rhythms in thalamocortical networks,
one of the biological systems which motivated our study. The analysis helps to
explain how coupling delays in either excitatory or inhibitory synapses
contribute to producing synchronized rhythms.
",Physics,Quantitative Biology
"Iterative bidding in electricity markets: rationality and robustness   This paper studies an electricity market consisting of an independent system
operator (ISO) and a group of generators. The goal is to solve the DC optimal
power flow (DC-OPF) problem: have the generators collectively meet the power
demand while minimizing the aggregate generation cost and respecting line flow
limits in the network. The ISO by itself cannot solve the DC-OPF problem as
generators are strategic and do not share their cost functions. Instead, each
generator submits to the ISO a bid, consisting of the price per unit of
electricity at which it is willing to provide power. Based on the bids, the ISO
decides how much production to allocate to each generator to minimize the total
payment while meeting the load and satisfying the line limits. We provide a
provably correct, decentralized iterative scheme, termed BID ADJUSTMENT
ALGORITHM, for the resulting Bertrand competition game. Regarding convergence,
we show that the algorithm takes the generators' bids to any desired
neighborhood of the efficient Nash equilibrium at a linear convergence rate. As
a consequence, the optimal production of the generators converges to the
optimizer of the DC-OPF problem. Regarding robustness, we show that the
algorithm is robust to affine perturbations in the bid adjustment scheme and
that there is no incentive for any individual generator to deviate from the
algorithm by using an alternative bid update scheme. We also establish the
algorithm robustness to collusion, i.e., we show that, as long as each bus with
generation has a generator following the strategy, there is no incentive for
any group of generators to share information with the intent of tricking the
system to obtain a higher payoff. Simulations illustrate our results.
",Computer Science; Mathematics,Computer Science
"r-BTN: Cross-domain Face Composite and Synthesis from Limited Facial Patches   We start by asking an interesting yet challenging question, ""If an eyewitness
can only recall the eye features of the suspect, such that the forensic artist
can only produce a sketch of the eyes (e.g., the top-left sketch shown in Fig.
1), can advanced computer vision techniques help generate the whole face
image?"" A more generalized question is that if a large proportion (e.g., more
than 50%) of the face/sketch is missing, can a realistic whole face
sketch/image still be estimated. Existing face completion and generation
methods either do not conduct domain transfer learning or can not handle large
missing area. For example, the inpainting approach tends to blur the generated
region when the missing area is large (i.e., more than 50%). In this paper, we
exploit the potential of deep learning networks in filling large missing region
(e.g., as high as 95% missing) and generating realistic faces with
high-fidelity in cross domains. We propose the recursive generation by
bidirectional transformation networks (r-BTN) that recursively generates a
whole face/sketch from a small sketch/face patch. The large missing area and
the cross domain challenge make it difficult to generate satisfactory results
using a unidirectional cross-domain learning structure. On the other hand, a
forward and backward bidirectional learning between the face and sketch domains
would enable recursive estimation of the missing region in an incremental
manner (Fig. 1) and yield appealing results. r-BTN also adopts an adversarial
constraint to encourage the generation of realistic faces/sketches. Extensive
experiments have been conducted to demonstrate the superior performance from
r-BTN as compared to existing potential solutions.
",Computer Science,Computer Science
"Memory effects, transient growth, and wave breakup in a model of paced atrium   The mechanisms underlying cardiac fibrillation have been investigated for
over a century, but we are still finding surprising results that change our
view of this phenomenon. The present study focuses on the transition from
normal rhythm to atrial fibrillation associated with a gradual increase in the
pacing rate. While some of our findings are consistent with existing
experimental, numerical, and theoretical studies of this problem, one result
appears to contradict the accepted picture. Specifically we show that, in a
two-dimensional model of paced homogeneous atrial tissue, transition from
discordant alternans to conduction block, wave breakup, reentry, and spiral
wave chaos is associated with transient growth of finite amplitude disturbances
rather than a conventional instability. It is mathematically very similar to
subcritical, or bypass, transition from laminar fluid flow to turbulence, which
allows many of the tools developed in the context of fluid turbulence to be
used for improving our understanding of cardiac arrhythmias.
",Physics,Physics
"It's Time to Consider ""Time"" when Evaluating Recommender-System Algorithms [Proposal]   In this position paper, we question the current practice of calculating
evaluation metrics for recommender systems as single numbers (e.g. precision
p=.28 or mean absolute error MAE = 1.21). We argue that single numbers express
only average effectiveness over a usually rather long period (e.g. a year or
even longer), which provides only a vague and static view of the data. We
propose that recommender-system researchers should instead calculate metrics
for time-series such as weeks or months, and plot the results in e.g. a line
chart. This way, results show how algorithms' effectiveness develops over time,
and hence the results allow drawing more meaningful conclusions about how an
algorithm will perform in the future. In this paper, we explain our reasoning,
provide an example to illustrate our reasoning and present suggestions for what
the community should do next.
",Computer Science,Computer Science
"Eigendecompositions of Transfer Operators in Reproducing Kernel Hilbert Spaces   Transfer operators such as the Perron--Frobenius or Koopman operator play an
important role in the global analysis of complex dynamical systems. The
eigenfunctions of these operators can be used to detect metastable sets, to
project the dynamics onto the dominant slow processes, or to separate
superimposed signals. We extend transfer operator theory to reproducing kernel
Hilbert spaces and show that these operators are related to Hilbert space
representations of conditional distributions, known as conditional mean
embeddings in the machine learning community. Moreover, numerical methods to
compute empirical estimates of these embeddings are akin to data-driven methods
for the approximation of transfer operators such as extended dynamic mode
decomposition and its variants. One main benefit of the presented kernel-based
approaches is that these methods can be applied to any domain where a
similarity measure given by a kernel is available. We illustrate the results
with the aid of guiding examples and highlight potential applications in
molecular dynamics as well as video and text data analysis.
",Computer Science; Statistics,Computer Science; Statistics
"A statistical model for aggregating judgments by incorporating peer predictions   We propose a probabilistic model to aggregate the answers of respondents
answering multiple-choice questions. The model does not assume that everyone
has access to the same information, and so does not assume that the consensus
answer is correct. Instead, it infers the most probable world state, even if
only a minority vote for it. Each respondent is modeled as receiving a signal
contingent on the actual world state, and as using this signal to both
determine their own answer and predict the answers given by others. By
incorporating respondent's predictions of others' answers, the model infers
latent parameters corresponding to the prior over world states and the
probability of different signals being received in all possible world states,
including counterfactual ones. Unlike other probabilistic models for
aggregation, our model applies to both single and multiple questions, in which
case it estimates each respondent's expertise. The model shows good
performance, compared to a number of other probabilistic models, on data from
seven studies covering different types of expertise.
",Statistics,Computer Science; Statistics
"Reverse approximation of gradient flows as Minimizing Movements: a conjecture by De Giorgi   We consider the Cauchy problem for the gradient flow \begin{equation}
\label{eq:81}
\tag{$\star$}
u'(t)=-\nabla\phi(u(t)),\quad t\ge 0;\quad
u(0)=u_0, \end{equation} generated by a continuously differentiable function
$\phi:\mathbb H \to \mathbb R$ in a Hilbert space $\mathbb H$ and study the
reverse approximation of solutions to ($\star$) by the De Giorgi Minimizing
Movement approach. We prove that if $\mathbb H$ has finite dimension and $\phi$
is quadratically bounded from below (in particular if $\phi$ is Lipschitz) then
for every solution $u$ to ($\star$) (which may have an infinite number of
solutions) there exist perturbations $\phi_\tau:\mathbb H \to \mathbb R \
(\tau>0)$ converging to $\phi$ in the Lipschitz norm such that $u$ can be
approximated by the Minimizing Movement scheme generated by the recursive
minimization of $\Phi(\tau,U,V):=\frac 1{2\tau}|V-U|^2+ \phi_\tau(V)$:
\begin{equation}
\label{eq:abstract}
\tag{$\star\star$}
U_\tau^n\in \operatorname{argmin}_{V\in \mathbb H}
\Phi(\tau,U_\tau^{n-1},V)\quad n\in\mathbb N, \quad U_\tau^0:=u_0.
\end{equation} We show that the piecewise constant interpolations with time
step $\tau > 0$ of all possible selections of solutions
$(U_\tau^n)_{n\in\mathbb N}$ to ($\star\star$) will converge to $u$ as
$\tau\downarrow 0$. This result solves a question raised by Ennio De Giorgi. We
also show that even if $\mathbb H$ has infinite dimension the above
approximation holds for the distinguished class of minimal solutions to
($\star$), that generate all the other solutions to ($\star$) by time
reparametrization.
",Mathematics,Mathematics
"A case study of hurdle and generalized additive models in astronomy: the escape of ionizing radiation   The dark ages of the Universe end with the formation of the first generation
of stars residing in primeval galaxies. These objects were the first to produce
ultraviolet ionizing photons in a period when the cosmic gas changed from a
neutral state to an ionized one, known as Epoch of Reionization (EoR). A
pivotal aspect to comprehend the EoR is to probe the intertwined relationship
between the fraction of ionizing photons capable to escape dark haloes, also
known as the escape fraction ($f_{esc}$), and the physical properties of the
galaxy. This work develops a sound statistical model suitable to account for
such non-linear relationships and the non-Gaussian nature of $f_{esc}$. This
model simultaneously estimates the probability that a given primordial galaxy
starts the ionizing photon production and estimates the mean level of the
$f_{esc}$ once it is triggered. The model was employed in the First Billion
Years simulation suite, from which we show that the baryonic fraction and the
rate of ionizing photons appear to have a larger impact on $f_{esc}$ than
previously thought. A naive univariate analysis of the same problem would
suggest smaller effects for these properties and a much larger impact for the
specific star formation rate, which is lessened after accounting for other
galaxy properties and non-linearities in the statistical model.
",Statistics,Physics
"Improving the upper bound on the length of the shortest reset words   We improve the best known upper bound on the length of the shortest reset
words of synchronizing automata. The new bound is slightly better than $114 n^3
/ 685 + O(n^2)$. The Černý conjecture states that $(n-1)^2$ is an upper
bound. So far, the best general upper bound was $(n^3-n)/6-1$ obtained by
J.-E.~Pin and P.~Frankl in 1982. Despite a number of efforts, it remained
unchanged for about 35 years.
To obtain the new upper bound we utilize avoiding words. A word is avoiding
for a state $q$ if after reading the word the automaton cannot be in $q$. We
obtain upper bounds on the length of the shortest avoiding words, and using the
approach of Trahtman from 2011 combined with the well known Frankl theorem from
1982, we improve the general upper bound on the length of the shortest reset
words. For all the bounds, there exist polynomial algorithms finding a word of
length not exceeding the bound.
",Computer Science,Computer Science
"A Theory of Solvability for Lossless Power Flow Equations -- Part II: Conditions for Radial Networks   This two-part paper details a theory of solvability for the power flow
equations in lossless power networks. In Part I, we derived a new formulation
of the lossless power flow equations, which we term the fixed-point power flow.
The model is parameterized by several graph-theoretic matrices -- the power
network stiffness matrices -- which quantify the internal coupling strength of
the network. In Part II, we leverage the fixed-point power flow to study power
flow solvability. For radial networks, we derive parametric conditions which
guarantee the existence and uniqueness of a high-voltage power flow solution,
and construct examples for which the conditions are also necessary. Our
conditions (i) imply convergence of the fixed-point power flow iteration, (ii)
unify and extend recent results on solvability of decoupled power flow, (iii)
directly generalize the textbook two-bus system results, and (iv) provide new
insights into how the structure and parameters of the grid influence power flow
solvability.
",Mathematics,Computer Science
"An accurate approximation formula for gamma function   In this paper, we present a very accurate approximation for gamma function:
\begin{equation*} \Gamma \left( x+1\right) \thicksim \sqrt{2\pi x}\left(
\dfrac{x}{e}\right) ^{x}\left( x\sinh \frac{1}{x}\right) ^{x/2}\exp \left(
\frac{7}{324}\frac{1}{ x^{3}\left( 35x^{2}+33\right) }\right) =W_{2}\left(
x\right) \end{equation*} as $x\rightarrow \infty $, and prove that the function
$x\mapsto \ln \Gamma \left( x+1\right) -\ln W_{2}\left( x\right) $ is strictly
decreasing and convex from $\left( 1,\infty \right) $ onto $\left( 0,\beta
\right) $, where \begin{equation*} \beta =\frac{22\,025}{22\,032}-\ln
\sqrt{2\pi \sinh 1}\approx 0.00002407. \end{equation*}
",Mathematics,Mathematics
"Nonlinear probability. A theory with incompatible stochastic variables   In 1991 J.F. Aarnes introduced the concept of quasi-measures in a compact
topological space $\Omega$ and established the connection between quasi-states
on $C (\Omega)$ and quasi-measures in $\Omega$. This work solved the linearity
problem of quasi-states on $C^*$-algebras formulated by R.V. Kadison in 1965.
The answer is that a quasi-state need not be linear, so a quasi-state need not
be a state. We introduce nonlinear measures in a space $\Omega$ which is a
generalization of a measurable space. In this more general setting we are still
able to define integration and establish a representation theorem for the
corresponding functionals. A probabilistic language is choosen since we feel
that the subject should be of some interest to probabilists. In particular we
point out that the theory allows for incompatible stochastic variables. The
need for incompatible variables is well known in quantum mechanics, but the
need seems natural also in other contexts as we try to explain by a questionary
example.
Keywords and phrases: Epistemic probability, Integration with respect to mea-
sures and other set functions, Banach algebras of continuous functions, Set
func- tions and measures on topological spaces, States, Logical foundations of
quantum mechanics.
",Mathematics; Statistics,Mathematics
"A Compressed Sensing Approach for Distribution Matching   In this work, we formulate the fixed-length distribution matching as a
Bayesian inference problem. Our proposed solution is inspired from the
compressed sensing paradigm and the sparse superposition (SS) codes. First, we
introduce sparsity in the binary source via position modulation (PM). We then
present a simple and exact matcher based on Gaussian signal quantization. At
the receiver, the dematcher exploits the sparsity in the source and performs
low-complexity dematching based on generalized approximate message-passing
(GAMP). We show that GAMP dematcher and spatial coupling lead to asymptotically
optimal performance, in the sense that the rate tends to the entropy of the
target distribution with vanishing reconstruction error in a proper limit.
Furthermore, we assess the performance of the dematcher on practical
Hadamard-based operators. A remarkable feature of our proposed solution is the
possibility to: i) perform matching at the symbol level (nonbinary); ii)
perform joint channel coding and matching.
",Statistics,Computer Science; Statistics
"Node Centralities and Classification Performance for Characterizing Node Embedding Algorithms   Embedding graph nodes into a vector space can allow the use of machine
learning to e.g. predict node classes, but the study of node embedding
algorithms is immature compared to the natural language processing field
because of a diverse nature of graphs. We examine the performance of node
embedding algorithms with respect to graph centrality measures that
characterize diverse graphs, through systematic experiments with four node
embedding algorithms, four or five graph centralities, and six datasets.
Experimental results give insights into the properties of node embedding
algorithms, which can be a basis for further research on this topic.
",Computer Science; Statistics,Computer Science; Statistics
"On the essential self-adjointness of singular sub-Laplacians   We prove a general essential self-adjointness criterion for sub-Laplacians on
complete sub-Riemannian manifolds, defined with respect to singular measures.
As a consequence, we show that the intrinsic sub-Laplacian (i.e. defined w.r.t.
Popp's measure) is essentially self-adjoint on the equiregular connected
components of a sub-Riemannian manifold. This result holds under mild
regularity assumptions on the singular region, and when the latter does not
contain characteristic points.
",Mathematics,Mathematics
"Back to the Future: an Even More Nearly Optimal Cardinality Estimation Algorithm   We describe a new cardinality estimation algorithm that is extremely
space-efficient. It applies one of three novel estimators to the compressed
state of the Flajolet-Martin-85 coupon collection process. In an
apples-to-apples empirical comparison against compressed HyperLogLog sketches,
the new algorithm simultaneously wins on all three dimensions of the
time/space/accuracy tradeoff. Our prototype uses the zstd compression library,
and produces sketches that are smaller than the entropy of HLL, so no possible
implementation of compressed HLL can match its space efficiency. The paper's
technical contributions include analyses and simulations of the three new
estimators, accurate values for the entropies of FM85 and HLL, and a
non-trivial method for estimating a double asymptotic limit via simulation.
",Computer Science,Mathematics; Statistics
"Locally free actions of groupoids and proper topological correspondences   Let $(G,\alpha)$ and $(H,\beta)$ be locally compact Hausdorff groupoids with
Haar systems, and let $(X,\lambda)$ be a topological correspondence from
$(G,\alpha)$ to $(H,\beta)$ which induce the ${C}^*$-correspondence
$\mathcal{H}(X)\colon {C}^*(G,\alpha)\to {C}^*(H,\beta)$. We give sufficient
topological conditions which when satisfied the ${C}^*$-correspondence
$\mathcal{H}(X)$ is proper, that is, the ${C}^*$-algebra ${C}^*(G,\alpha)$ acts
on the Hilbert ${C}^*(H,\beta)$-module ${H}(X)$ via the comapct operators. Thus
a proper topological correspondence produces an element in
${KK}({C}^*(G,\alpha),{C}^*(H,\beta))$.
",Mathematics,Mathematics
"Bootstrapped synthetic likelihood   Approximate Bayesian computation (ABC) and synthetic likelihood (SL)
techniques have enabled the use of Bayesian inference for models that may be
simulated, but for which the likelihood cannot be evaluated pointwise at values
of an unknown parameter $\theta$. The main idea in ABC and SL is to, for
different values of $\theta$ (usually chosen using a Monte Carlo algorithm),
build estimates of the likelihood based on simulations from the model
conditional on $\theta$. The quality of these estimates determines the
efficiency of an ABC/SL algorithm. In standard ABC/SL, the only means to
improve an estimated likelihood at $\theta$ is to simulate more times from the
model conditional on $\theta$, which is infeasible in cases where the simulator
is computationally expensive. In this paper we describe how to use
bootstrapping as a means for improving SL estimates whilst using fewer
simulations from the model, and also investigate its use in ABC. Further, we
investigate the use of the bag of little bootstraps as a means for applying
this approach to large datasets, yielding Monte Carlo algorithms that
accurately approximate posterior distributions whilst only simulating
subsamples of the full data. Examples of the approach applied to i.i.d.,
temporal and spatial data are given.
",Computer Science; Statistics,Statistics
"From arteries to boreholes: Transient response of a poroelastic cylinder to fluid injection   The radially outward flow of fluid through a porous medium occurs in many
practical problems, from transport across vascular walls to the pressurisation
of boreholes in the subsurface. When the driving pressure is non-negligible
relative to the stiffness of the solid structure, the poromechanical coupling
between the fluid and the solid can control both the steady-state and the
transient mechanics of the system. Very large pressures or very soft materials
lead to large deformations of the solid skeleton, which introduce kinematic and
constitutive nonlinearity that can have a nontrivial impact on these mechanics.
Here, we study the transient response of a poroelastic cylinder to sudden fluid
injection. We consider the impacts of kinematic and constitutive nonlinearity,
both separately and in combination, and we highlight the central role of
driving method in the evolution of the response. We show that the various
facets of nonlinearity may either accelerate or decelerate the transient
response relative to linear poroelasticity, depending on the boundary
conditions and the initial geometry, and that an imposed fluid pressure leads
to a much faster response than an imposed fluid flux.
",Physics,Physics
"Signal-based Bayesian Seismic Monitoring   Detecting weak seismic events from noisy sensors is a difficult perceptual
task. We formulate this task as Bayesian inference and propose a generative
model of seismic events and signals across a network of spatially distributed
stations. Our system, SIGVISA, is the first to directly model seismic
waveforms, allowing it to incorporate a rich representation of the physics
underlying the signal generation process. We use Gaussian processes over
wavelet parameters to predict detailed waveform fluctuations based on
historical events, while degrading smoothly to simple parametric envelopes in
regions with no historical seismicity. Evaluating on data from the western US,
we recover three times as many events as previous work, and reduce mean
location errors by a factor of four while greatly increasing sensitivity to
low-magnitude events.
",Computer Science; Physics,Statistics
"Symmetric Rank Covariances: a Generalised Framework for Nonparametric Measures of Dependence   The need to test whether two random vectors are independent has spawned a
large number of competing measures of dependence. We are interested in
nonparametric measures that are invariant under strictly increasing
transformations, such as Kendall's tau, Hoeffding's D, and the more recently
discovered Bergsma--Dassios sign covariance. Each of these measures exhibits
symmetries that are not readily apparent from their definitions. Making these
symmetries explicit, we define a new class of multivariate nonparametric
measures of dependence that we refer to as Symmetric Rank Covariances. This new
class generalises all of the above measures and leads naturally to multivariate
extensions of the Bergsma--Dassios sign covariance. Symmetric Rank Covariances
may be estimated unbiasedly using U-statistics for which we prove results on
computational efficiency and large-sample behavior. The algorithms we develop
for their computation include, to the best of our knowledge, the first
efficient algorithms for the well-known Hoeffding's D statistic in the
multivariate setting.
",Mathematics; Statistics,Computer Science; Statistics
"Nearly-tight VC-dimension and pseudodimension bounds for piecewise linear neural networks   We prove new upper and lower bounds on the VC-dimension of deep neural
networks with the ReLU activation function. These bounds are tight for almost
the entire range of parameters. Letting $W$ be the number of weights and $L$ be
the number of layers, we prove that the VC-dimension is $O(W L \log(W))$, and
provide examples with VC-dimension $\Omega( W L \log(W/L) )$. This improves
both the previously known upper bounds and lower bounds. In terms of the number
$U$ of non-linear units, we prove a tight bound $\Theta(W U)$ on the
VC-dimension. All of these bounds generalize to arbitrary piecewise linear
activation functions, and also hold for the pseudodimensions of these function
classes.
Combined with previous results, this gives an intriguing range of
dependencies of the VC-dimension on depth for networks with different
non-linearities: there is no dependence for piecewise-constant, linear
dependence for piecewise-linear, and no more than quadratic dependence for
general piecewise-polynomial.
",Computer Science,Computer Science; Statistics
"Sketched Subspace Clustering   The immense amount of daily generated and communicated data presents unique
challenges in their processing. Clustering, the grouping of data without the
presence of ground-truth labels, is an important tool for drawing inferences
from data. Subspace clustering (SC) is a relatively recent method that is able
to successfully classify nonlinearly separable data in a multitude of settings.
In spite of their high clustering accuracy, SC methods incur prohibitively high
computational complexity when processing large volumes of high-dimensional
data. Inspired by random sketching approaches for dimensionality reduction, the
present paper introduces a randomized scheme for SC, termed Sketch-SC, tailored
for large volumes of high-dimensional data. Sketch-SC accelerates the
computationally heavy parts of state-of-the-art SC approaches by compressing
the data matrix across both dimensions using random projections, thus enabling
fast and accurate large-scale SC. Performance analysis as well as extensive
numerical tests on real data corroborate the potential of Sketch-SC and its
competitive performance relative to state-of-the-art scalable SC approaches.
",Computer Science; Statistics,Computer Science
"Deep Incremental Boosting   This paper introduces Deep Incremental Boosting, a new technique derived from
AdaBoost, specifically adapted to work with Deep Learning methods, that reduces
the required training time and improves generalisation. We draw inspiration
from Transfer of Learning approaches to reduce the start-up time to training
each incremental Ensemble member. We show a set of experiments that outlines
some preliminary results on some common Deep Learning datasets and discuss the
potential improvements Deep Incremental Boosting brings to traditional Ensemble
methods in Deep Learning.
",Computer Science; Statistics,Computer Science; Statistics
"A representation theorem for stochastic processes with separable covariance functions, and its implications for emulation   Many applications require stochastic processes specified on two- or
higher-dimensional domains; spatial or spatial-temporal modelling, for example.
In these applications it is attractive, for conceptual simplicity and
computational tractability, to propose a covariance function that is separable;
e.g., the product of a covariance function in space and one in time. This paper
presents a representation theorem for such a proposal, and shows that all
processes with continuous separable covariance functions are second-order
identical to the product of second-order uncorrelated processes. It discusses
the implications of separable or nearly separable prior covariances for the
statistical emulation of complicated functions such as computer codes, and
critically reexamines the conventional wisdom concerning emulator structure,
and size of design.
",Mathematics; Statistics,Computer Science; Mathematics; Statistics
"Matrix KP: tropical limit and Yang-Baxter maps   We study soliton solutions of matrix Kadomtsev-Petviashvili (KP) equations in
a tropical limit, in which their support at fixed time is a planar graph and
polarizations are attached to its constituting lines. There is a subclass of
""pure line soliton solutions"" for which we find that, in this limit, the
distribution of polarizations is fully determined by a Yang-Baxter map. For a
vector KP equation, this map is given by an R-matrix, whereas it is a
non-linear map in case of a more general matrix KP equation. We also consider
the corresponding Korteweg-deVries (KdV) reduction. Furthermore, exploiting the
fine structure of soliton interactions in the tropical limit, we obtain a new
solution of the tetrahedron (or Zamolodchikov) equation. Moreover, a solution
of the functional tetrahedron equation arises from the parameter-dependence of
the vector KP R-matrix.
",Physics,Mathematics
"Design of Improved Quasi-Cyclic Protograph-Based Raptor-Like LDPC Codes for Short Block-Lengths   Protograph-based Raptor-like low-density parity-check codes (PBRL codes) are
a recently proposed family of easily encodable and decodable rate-compatible
LDPC (RC-LDPC) codes. These codes have an excellent iterative decoding
threshold and performance across all design rates. PBRL codes designed thus
far, for both long and short block-lengths, have been based on optimizing the
iterative decoding threshold of the protograph of the RC code family at various
design rates.
In this work, we propose a design method to obtain better quasi-cyclic (QC)
RC-LDPC codes with PBRL structure for short block-lengths (of a few hundred
bits). We achieve this by maximizing an upper bound on the minimum distance of
any QC-LDPC code that can be obtained from the protograph of a PBRL ensemble.
The obtained codes outperform the original PBRL codes at short block-lengths by
significantly improving the error floor behavior at all design rates.
Furthermore, we identify a reduction in complexity of the design procedure,
facilitated by the general structure of a PBRL ensemble.
",Computer Science,Computer Science
"Some results on Ricatti Equations, Floquet Theory and Applications   In this paper, we present two new results to the classical Floquet theory,
which provides the Floquet multipliers for two classes of the planar periodic
system. One these results provides the Floquet multipliers independently of the
solution of system. To demonstrate the application of these analytical results,
we consider a cholera epidemic model with phage dynamics and seasonality
incorporated.
",Mathematics,Mathematics
"DroidStar: Callback Typestates for Android Classes   Event-driven programming frameworks, such as Android, are based on components
with asynchronous interfaces. The protocols for interacting with these
components can often be described by finite-state machines we dub *callback
typestates*. Callback typestates are akin to classical typestates, with the
difference that their outputs (callbacks) are produced asynchronously. While
useful, these specifications are not commonly available, because writing them
is difficult and error-prone.
Our goal is to make the task of producing callback typestates significantly
easier. We present a callback typestate assistant tool, DroidStar, that
requires only limited user interaction to produce a callback typestate. Our
approach is based on an active learning algorithm, L*. We improved the
scalability of equivalence queries (a key component of L*), thus making active
learning tractable on the Android system.
We use DroidStar to learn callback typestates for Android classes both for
cases where one is already provided by the documentation, and for cases where
the documentation is unclear. The results show that DroidStar learns callback
typestates accurately and efficiently. Moreover, in several cases, the
synthesized callback typestates uncovered surprising and undocumented
behaviors.
",Computer Science,Computer Science
"A framework for Multi-A(rmed)/B(andit) testing with online FDR control   We propose an alternative framework to existing setups for controlling false
alarms when multiple A/B tests are run over time. This setup arises in many
practical applications, e.g. when pharmaceutical companies test new treatment
options against control pills for different diseases, or when internet
companies test their default webpages versus various alternatives over time.
Our framework proposes to replace a sequence of A/B tests by a sequence of
best-arm MAB instances, which can be continuously monitored by the data
scientist. When interleaving the MAB tests with an an online false discovery
rate (FDR) algorithm, we can obtain the best of both worlds: low sample
complexity and any time online FDR control. Our main contributions are: (i) to
propose reasonable definitions of a null hypothesis for MAB instances; (ii) to
demonstrate how one can derive an always-valid sequential p-value that allows
continuous monitoring of each MAB test; and (iii) to show that using rejection
thresholds of online-FDR algorithms as the confidence levels for the MAB
algorithms results in both sample-optimality, high power and low FDR at any
point in time. We run extensive simulations to verify our claims, and also
report results on real data collected from the New Yorker Cartoon Caption
contest.
",Computer Science; Statistics,Computer Science; Statistics
"Distributed Functional Observers for LTI Systems   We study the problem of designing distributed functional observers for LTI
systems. Specifically, we consider a setting consisting of a state vector that
evolves over time according to a dynamical process. A set of nodes distributed
over a communication network wish to collaboratively estimate certain functions
of the state. We first show that classical existence conditions for the design
of centralized functional observers do not directly translate to the
distributed setting, due to the coupling that exists between the dynamics of
the functions of interest and the diverse measurements at the various nodes.
Accordingly, we design transformations that reveal such couplings and identify
portions of the corresponding dynamics that are locally detectable at each
sensor node. We provide sufficient conditions on the network, along with state
estimate update and exchange rules for each node, that guarantee asymptotic
reconstruction of the functions at each sensor node.
",Mathematics,Computer Science
"The First Detection of Gravitational Waves   This article deals with the first detection of gravitational waves by the
advanced Laser Interferometer Gravitational Wave Observatory (LIGO) detectors
on 14 September 2015, where the signal was generated by two stellar mass black
holes with masses 36 $ M_{\odot}$ and 29 $ M_{\odot}$ that merged to form a 62
$ M_{\odot}$ black hole, releasing 3 $M_{\odot}$ energy in gravitational waves,
almost 1.3 billion years ago. We begin by providing a brief overview of
gravitational waves, their sources and the gravitational wave detectors. We
then describe in detail the first detection of gravitational waves from a
binary black hole merger. We then comment on the electromagnetic follow up of
the detection event with various telescopes. Finally, we conclude with the
discussion on the tests of gravity and fundamental physics with the first
gravitational wave detection event.
",Physics,Physics
"Gas dynamics in strong centrifugal fields   Dynamics of waves generated by scopes in gas centrifuges (GC) for isotope
separation is considered. The centrifugal acceleration in the GC reaches values
of the order of $10^6$g. The centrifugal and Coriolis forces modify essentially
the conventional sound waves. Three families of the waves with different
polarisation and dispersion exist in these conditions. Dynamics of the flow in
the model GC Iguasu is investigated numerically. Comparison of the results of
the numerical modelling of the wave dynamics with the analytical predictions is
performed. New phenomena of the resonances in the GC is found. The resonances
occur for the waves polarized along the rotational axis having the smallest
dumping due to the viscosity.
",Physics,Physics
"Adversarial Networks for the Detection of Aggressive Prostate Cancer   Semantic segmentation constitutes an integral part of medical image analyses
for which breakthroughs in the field of deep learning were of high relevance.
The large number of trainable parameters of deep neural networks however
renders them inherently data hungry, a characteristic that heavily challenges
the medical imaging community. Though interestingly, with the de facto standard
training of fully convolutional networks (FCNs) for semantic segmentation being
agnostic towards the `structure' of the predicted label maps, valuable
complementary information about the global quality of the segmentation lies
idle. In order to tap into this potential, we propose utilizing an adversarial
network which discriminates between expert and generated annotations in order
to train FCNs for semantic segmentation. Because the adversary constitutes a
learned parametrization of what makes a good segmentation at a global level, we
hypothesize that the method holds particular advantages for segmentation tasks
on complex structured, small datasets. This holds true in our experiments: We
learn to segment aggressive prostate cancer utilizing MRI images of 152
patients and show that the proposed scheme is superior over the de facto
standard in terms of the detection sensitivity and the dice-score for
aggressive prostate cancer. The achieved relative gains are shown to be
particularly pronounced in the small dataset limit.
",Computer Science,Statistics
"A Framework for Dynamic Stability Analysis of Power Systems with Volatile Wind Power   We propose a framework employing stochastic differential equations to
facilitate the long-term stability analysis of power grids with intermittent
wind power generations. This framework takes into account the discrete dynamics
which play a critical role in the long-term stability analysis, incorporates
the model of wind speed with different probability distributions, and also
develops an approximation methodology (by a deterministic hybrid model) for the
stochastic hybrid model to reduce the computational burden brought about by the
uncertainty of wind power. The theoretical and numerical studies show that a
deterministic hybrid model can provide an accurate trajectory approximation and
stability assessments for the stochastic hybrid model under mild conditions. In
addition, we discuss the critical cases that the deterministic hybrid model
fails and discover that these cases are caused by a violation of the proposed
sufficient conditions. Such discussion complements the proposed framework and
methodology and also reaffirms the importance of the stochastic hybrid model
when the system operates close to its stability limit.
",Computer Science,Computer Science; Physics
"Type Safe Redis Queries: A Case Study of Type-Level Programming in Haskell   Redis is an in-memory data structure store, often used as a database, with a
Haskell interface Hedis. Redis is dynamically typed --- a key can be discarded
and re-associated to a value of a different type, and a command, when fetching
a value of a type it does not expect, signals a runtime error. We develop a
domain-specific language that, by exploiting Haskell type-level programming
techniques including indexed monad, type-level literals and closed type
families, keeps track of types of values in the database and statically
guarantees that type errors cannot happen for a class of Redis programs.
",Computer Science,Computer Science
"Extragalactic source population studies at very high energies in the Cherenkov Telescope Array era   The Cherenkov Telescope Array (CTA) is the next generation ground-based
$\gamma$-ray observatory. It will provide an order of magnitude better
sensitivity and an extended energy coverage, 20 GeV - 300 TeV, relative to
current Imaging Atmospheric Cherenkov Telescopes (IACTs). IACTs, despite
featuring an excellent sensitivity, are characterized by a limited field of
view that makes the blind search of new sources very time inefficient.
Fortunately, the $\textit{Fermi}$-LAT collaboration recently released a new
catalog of 1,556 sources detected in the 10 GeV - 2 TeV range by the Large Area
Telescope (LAT) in the first 7 years of its operation (the 3FHL catalog). This
catalog is currently the most appropriate description of the sky that will be
energetically accessible to CTA. Here, we discuss a detailed analysis of the
extragalactic source population (mostly blazars) that will be studied in the
near future by CTA. This analysis is based on simulations built from the
expected array configurations and information reported in the 3FHL catalog.
These results show the improvements that CTA will provide on the extragalactic
TeV source population studies, which will be carried out by Key Science
Projects as well as dedicated proposals.
",Physics,Physics
"Actions Speak Louder Than Goals: Valuing Player Actions in Soccer   Assessing the impact of the individual actions performed by soccer players
during games is a crucial aspect of the player recruitment process.
Unfortunately, most traditional metrics fall short in addressing this task as
they either focus on rare events like shots and goals alone or fail to account
for the context in which the actions occurred. This paper introduces a novel
advanced soccer metric for valuing any type of individual player action on the
pitch, be it with or without the ball. Our metric values each player action
based on its impact on the game outcome while accounting for the circumstances
under which the action happened. When applied to on-the-ball actions like
passes, dribbles, and shots alone, our metric identifies Argentine forward
Lionel Messi, French teenage star Kylian Mbappé, and Belgian winger Eden
Hazard as the most effective players during the 2016/2017 season.
",Statistics,Computer Science
"Rapidly star-forming galaxies adjacent to quasars at redshifts exceeding 6   The existence of massive ($10^{11}$ solar masses) elliptical galaxies by
redshift z~4 (when the Universe was 1.5 billion years old) necessitates the
presence of galaxies with star-formation rates exceeding 100 solar masses per
year at z>6 (corresponding to an age of the Universe of less than 1 billion
years). Surveys have discovered hundreds of galaxies at these early cosmic
epochs, but their star-formation rates are more than an order of magnitude
lower. The only known galaxies with very high star-formation rates at z>6 are,
with only one exception, the host galaxies of quasars, but these galaxies also
host accreting supermassive (more than $10^9$ solar masses) black holes, which
probably affect the properties of the galaxies. Here we report observations of
an emission line of singly ionized carbon ([CII] at a wavelength of 158
micrometres) in four galaxies at z>6 that are companions of quasars, with
velocity offsets of less than 600 kilometers per second and linear offsets of
less than 600 kiloparsecs. The discovery of these four galaxies was
serendipitous; they are close to their companion quasars and appear bright in
the far-infrared. On the basis of the [CII] measurements, we estimate
star-formation rates in the companions of more than 100 solar masses per year.
These sources are similar to the host galaxies of the quasars in [CII]
brightness, linewidth and implied dynamical masses, but do not show evidence
for accreting supermassive black holes. Similar systems have previously been
found at lower redshift. We find such close companions in four out of
twenty-five z>6 quasars surveyed, a fraction that needs to be accounted for in
simulations. If they are representative of the bright end of the [CII]
luminosity function, then they can account for the population of massive
elliptical galaxies at z~4 in terms of cosmic space density.
",Physics,Physics
"Robust Recovery of Missing Data in Electricity Distribution Systems   The advanced operation of future electricity distribution systems is likely
to require significant observability of the different parameters of interest
(e.g., demand, voltages, currents, etc.). Ensuring completeness of data is,
therefore, paramount. In this context, an algorithm for recovering missing
state variable observations in electricity distribution systems is presented.
The proposed method exploits the low rank structure of the state variables via
a matrix completion approach while incorporating prior knowledge in the form of
second order statistics. Specifically, the recovery method combines nuclear
norm minimization with Bayesian estimation. The performance of the new
algorithm is compared to the information-theoretic limits and tested trough
simulations using real data of an urban low voltage distribution system. The
impact of the prior knowledge is analyzed when a mismatched covariance is used
and for a Markovian sampling that introduces structure in the observation
pattern. Numerical results demonstrate that the proposed algorithm is robust
and outperforms existing state of the art algorithms.
",Computer Science,Computer Science; Statistics
"OAuthGuard: Protecting User Security and Privacy with OAuth 2.0 and OpenID Connect   Millions of users routinely use Google to log in to websites supporting OAuth
2.0 or OpenID Connect; the security of OAuth 2.0 and OpenID Connect is
therefore of critical importance. As revealed in previous studies, in practice
RPs often implement OAuth 2.0 incorrectly, and so many real-world OAuth 2.0 and
OpenID Connect systems are vulnerable to attack. However, users of such flawed
systems are typically unaware of these issues, and so are at risk of attacks
which could result in unauthorised access to the victim user's account at an
RP. In order to address this threat, we have developed OAuthGuard, an OAuth 2.0
and OpenID Connect vulnerability scanner and protector, that works with RPs
using Google OAuth 2.0 and OpenID Connect services. It protects user security
and privacy even when RPs do not implement OAuth 2.0 or OpenID Connect
correctly. We used OAuthGuard to survey the 1000 top-ranked websites supporting
Google sign-in for the possible presence of five OAuth 2.0 or OpenID Connect
security and privacy vulnerabilities, of which one has not previously been
described in the literature. Of the 137 sites in our study that employ Google
Sign-in, 69 were found to suffer from at least one serious vulnerability.
OAuthGuard was able to protect user security and privacy for 56 of these 69
RPs, and for the other 13 was able to warn users that they were using an
insecure implementation.
",Computer Science,Computer Science
"Effect of Particle Number Conservation on the Berry Phase Resulting from Transport of a Bound Quasiparticle around a Superfluid Vortex   Motivated by understanding Majorana zero modes in topological superfluids in
particle-number conserving framework beyond the present framework, we study the
effect of particle number conservation on the Berry phase resulting from
transport of a bound quasiparticle around a superfluid vortex. We find that
particle-number non-conserving calculations based on Bogoliubov-de Gennes (BdG)
equations are unable to capture the correct physics when the quasiparticle is
within the penetration depth of the vortex core where the superfluid velocity
is non-zero. Particle number conservation is crucial for deriving the correct
Berry phase in this context, and the Berry phase takes non-universal values
depending on the system parameters and the external trap imposed to bind the
quasiparticle. Of particular relevance to Majorana physics are the findings
that superfluid condensate affects the part of the Berry phase not accounted
for in the standard BdG framework, and that the superfluid many-body ground
state of odd number of fermions involves superfluid condensate deformation due
to the presence of the bound quasiparticle - an effect which is beyond the
description of the BdG equations.
",Physics,Physics
"Micrometer-Sized Water Ice Particles for Planetary Science Experiments: Influence of Surface Structure on Collisional Properties   Models and observations suggest that ice-particle aggregation at and beyond
the snowline dominates the earliest stages of planet-formation, which therefore
is subject to many laboratory studies. However, the pressure-temperature
gradients in proto-planetary disks mean that the ices are constantly processed,
undergoing phase changes between different solid phases and the gas phase. Open
questions remain as to whether the properties of the icy particles themselves
dictate collision outcomes and therefore how effectively collision experiments
reproduce conditions in pro- toplanetary environments. Previous experiments
often yielded apparently contradictory results on collision outcomes, only
agreeing in a temperature dependence setting in above $\approx$ 210 K. By
exploiting the unique capabilities of the NIMROD neutron scattering instrument,
we characterized the bulk and surface structure of icy particles used in
collision experiments, and studied how these structures alter as a function of
temperature at a constant pressure of around 30 mbar. Our icy grains, formed
under liquid nitrogen, undergo changes in the crystalline ice-phase,
sublimation, sintering and surface pre-melting as they are heated from 103 to
247 K. An increase in the thickness of the diffuse surface layer from $\approx$
10 to $\approx$ 30 {\AA} ($\approx$ 2.5 to 12 bilayers) proves increased
molecular mobility at temperatures above $\approx$ 210 K. As none of the other
changes tie-in with the temperature trends in collisional outcomes, we conclude
that the surface pre-melting phenomenon plays a key role in collision
experiments at these temperatures. Consequently, the pressure-temperature
environment, may have a larger influence on collision outcomes than previously
thought.
",Physics,Physics
"Confidence Interval Estimators for MOS Values   For the quantification of QoE, subjects often provide individual rating
scores on certain rating scales which are then aggregated into Mean Opinion
Scores (MOS). From the observed sample data, the expected value is to be
estimated. While the sample average only provides a point estimator, confidence
intervals (CI) are an interval estimate which contains the desired expected
value with a given confidence level. In subjective studies, the number of
subjects performing the test is typically small, especially in lab
environments. The used rating scales are bounded and often discrete like the
5-point ACR rating scale. Therefore, we review statistical approaches in the
literature for their applicability in the QoE domain for MOS interval
estimation (instead of having only a point estimator, which is the MOS). We
provide a conservative estimator based on the SOS hypothesis and binomial
distributions and compare its performance (CI width, outlier ratio of CI
violating the rating scale bounds) and coverage probability with well known CI
estimators. We show that the provided CI estimator works very well in practice
for MOS interval estimators, while the commonly used studentized CIs suffer
from a positive outlier ratio, i.e., CIs beyond the bounds of the rating scale.
As an alternative, bootstrapping, i.e., random sampling of the subjective
ratings with replacement, is an efficient CI estimator leading to typically
smaller CIs, but lower coverage than the proposed estimator.
",Computer Science,Statistics
"Rechargeable redox flow batteries: Maximum current density with electrolyte flow reactant penetration in a serpentine flow structure   Rechargeable redox flow batteries with serpentine flow field designs have
been demonstrated to deliver higher current density and power density in medium
and large-scale stationary energy storage applications. Nevertheless, the
fundamental mechanisms involved with improved current density in flow batteries
with flow field designs have not been understood. Here we report a maximum
current density concept associated with stoichiometric availability of
electrolyte reactant flow penetration through the porous electrode that can be
achieved in a flow battery system with a ""zero-gap""serpentine flow field
architecture. This concept can explain a higher current density achieved within
allowing reactions of all species soluble in the electrolyte. Further
validations with experimental data are confirmed by an example of a vanadium
flow battery with a serpentine flow structure over carbon paper electrode.
",Physics,Physics
"Controllability of temporal networks: An analysis using higher-order networks   The control of complex networks is a significant challenge, especially when
the network topology of the system to be controlled is dynamic. Addressing this
challenge, here we introduce a novel approach which allows exploring the
controllability of temporal networks. Studying six empirical data sets, we
particularly show that order correlations in the sequence of interactions can
both increase or decrease the time needed to achieve full controllability.
Counter-intuitively, we find that this effect can be opposite than the effect
of order correlations on other dynamical processes. Specifically, we show that
order correlations that speed up a diffusion process in a given system can slow
down the control of the same system, and vice-versa. Building on the
higher-order graphical modeling framework introduced in recent works, we
further demonstrate that spectral properties of higher-order network topologies
can be used to analytically explain this phenomenon.
",Computer Science; Physics,Computer Science; Physics
"Adaptive Multilevel Monte Carlo Approximation of Distribution Functions   We analyse a multilevel Monte Carlo method for the approximation of
distribution functions of univariate random variables. Since, by assumption,
the target distribution is not known explicitly, approximations have to be
used. We provide an asymptotic analysis of the error and the cost of the
algorithm. Furthermore we construct an adaptive version of the algorithm that
does not require any a priori knowledge on weak or strong convergence rates. We
apply the adaptive algorithm to smooth path-independent and path-dependent
functionals and to stopped exit times of SDEs.
",Mathematics; Statistics,Mathematics; Statistics
"The composition of Solar system asteroids and Earth/Mars moons, and the Earth-Moon composition similarity   [abridged] In the typical giant-impact scenario for the Moon formation most
of the Moon's material originates from the impactor. Any Earth-impactor
composition difference should, therefore, correspond to a comparable Earth-Moon
composition difference. Analysis of Moon rocks shows a close Earth-Moon
composition similarity, posing a challenge for the giant-impact scenario, given
that impactors were thought to significantly differ in composition from the
planets they impact. Here we use a large set of 140 simulations to show that
the composition of impactors could be very similar to that of the planets they
impact; in $4.9\%$-$18.2\%$ ($1.9\%$-$6.7\%$) of the cases the resulting
composition of the Moon is consistent with the observations of
$\Delta^{17}O<15$ ($\Delta^{17}O<6$ ppm). These findings suggest that the
Earth-Moon composition similarity could be resolved as to arise from the
primordial Earth-impactor composition similarity. Note that although we find
the likelihood for the suggested competing model of very high mass-ratio
impacts (producing significant Earth-impactor composition mixing) is comparable
($<6.7\%$), this scenario also requires additional fine-tuned requirements of a
very fast spinning Earth. Using the same simulations we also explore the
composition of giant-impact formed Mars-moons as well as Vesta-like asteroids.
We find that the Mars-moon composition difference should be large, but smaller
than expected if the moons are captured asteroids. Finally, we find that the
left-over planetesimals ('asteroids') in our simulations are frequently
scattered far away from their initial positions, thus potentially explaining
the mismatch between the current position and composition of the Vesta
asteroid.
",Physics,Physics
"Do metric fluctuations affect the Higgs dynamics during inflation?   We show that the dynamics of the Higgs field during inflation is not affected
by metric fluctuations if the Higgs is an energetically subdominant light
spectator. For Standard Model parameters we find that couplings between Higgs
and metric fluctuations are suppressed by $\mathcal{O}(10^{-7})$. They are
negligible compared to both pure Higgs terms in the effective potential and the
unavoidable non-minimal Higgs coupling to background scalar curvature. The
question of the electroweak vacuum instability during high energy scale
inflation can therefore be studied consistently using the Jordan frame action
in a Friedmann--Lemaître--Robertson--Walker metric, where the Higgs-curvature
coupling enters as an effective mass contribution. Similar results apply for
other light spectator scalar fields during inflation.
",Physics,Physics
"Volume growth in the component of fibered twists   For a Liouville domain $W$ whose boundary admits a periodic Reeb flow, we can
consider the connected component $[\tau] \in \pi_0(\text{Symp}^c(\widehat W))$
of fibered twists. In this paper, we investigate an entropy-type invariant,
called the slow volume growth, of the component $[\tau]$ and give a uniform
lower bound of the growth using wrapped Floer homology. We also show that
$[\tau]$ has infinite order in $\pi_0(\text{Symp}^c(\widehat W))$ if there is
an admissible Lagrangian $L$ in $W$ whose wrapped Floer homology is infinite
dimensional. We apply our results to fibered twists coming from the Milnor
fibers of $A_k$-type singularities and complements of a symplectic hypersurface
in a real symplectic manifold. They admit so-called real Lagrangians, and we
can explicitly compute wrapped Floer homology groups using a version of
Morse-Bott spectral sequences.
",Mathematics,Mathematics
"Testing Global Constraints   Every Constraint Programming (CP) solver exposes a library of constraints for
solving combinatorial problems. In order to be useful, CP solvers need to be
bug-free. Therefore the testing of the solver is crucial to make developers and
users confident. We present a Java library allowing any JVM based solver to
test that the implementations of the individual constraints are correct. The
library can be used in a test suite executed in a continuous integration tool
or it can also be used to discover minimalist instances violating some
properties (arc-consistency, etc) in order to help the developer to identify
the origin of the problem using standard debuggers.
",Computer Science,Computer Science
"Spectral analysis of jet turbulence   Informed by LES data and resolvent analysis of the mean flow, we examine the
structure of turbulence in jets in the subsonic, transonic, and supersonic
regimes. Spectral (frequency-space) proper orthogonal decomposition is used to
extract energy spectra and decompose the flow into energy-ranked coherent
structures. The educed structures are generally well predicted by the resolvent
analysis. Over a range of low frequencies and the first few azimuthal mode
numbers, these jets exhibit a low-rank response characterized by
Kelvin-Helmholtz (KH) type wavepackets associated with the annular shear layer
up to the end of the potential core and that are excited by forcing in the
very-near-nozzle shear layer. These modes too the have been experimentally
observed before and predicted by quasi-parallel stability theory and other
approximations--they comprise a considerable portion of the total turbulent
energy. At still lower frequencies, particularly for the axisymmetric mode, and
again at high frequencies for all azimuthal wavenumbers, the response is not
low rank, but consists of a family of similarly amplified modes. These modes,
which are primarily active downstream of the potential core, are associated
with the Orr mechanism. They occur also as sub-dominant modes in the range of
frequencies dominated by the KH response. Our global analysis helps tie
together previous observations based on local spatial stability theory, and
explains why quasi-parallel predictions were successful at some frequencies and
azimuthal wavenumbers, but failed at others.
",Physics,Physics
"Testing FLUKA on neutron activation of Si and Ge at nuclear research reactor using gamma spectroscopy   Samples of two characteristic semiconductor sensor materials, silicon and
germanium, have been irradiated with neutrons produced at the RP-10 Nuclear
Research Reactor at 4.5 MW. Their radionuclides photon spectra have been
measured with high resolution gamma spectroscopy, quantifying four
radioisotopes ($^{28}$Al, $^{29}$Al for Si and $^{75}$Ge and $^{77}$Ge for Ge).
We have compared the radionuclides production and their emission spectrum data
with Monte Carlo simulation results from FLUKA. Thus we have tested FLUKA's low
energy neutron library (ENDF/B-VIIR) and decay photon scoring with respect to
the activation of these semiconductors. We conclude that FLUKA is capable of
predicting relative photon peak amplitudes, with gamma intensities greater than
1%, of produced radionuclides with an average uncertainty of 13%. This work
allows us to estimate the corresponding systematic error on neutron activation
simulation studies of these sensor materials.
",Physics,Physics
"Perturbed Proximal Descent to Escape Saddle Points for Non-convex and Non-smooth Objective Functions   We consider the problem of finding local minimizers in non-convex and
non-smooth optimization. Under the assumption of strict saddle points, positive
results have been derived for first-order methods. We present the first known
results for the non-smooth case, which requires different analysis and a
different algorithm.
",Computer Science; Statistics,Mathematics; Statistics
"The Parameterized Complexity of Positional Games   We study the parameterized complexity of several positional games. Our main
result is that Short Generalized Hex is W[1]-complete parameterized by the
number of moves. This solves an open problem from Downey and Fellows'
influential list of open problems from 1999. Previously, the problem was
thought of as a natural candidate for AW[*]-completeness. Our main tool is a
new fragment of first-order logic where universally quantified variables only
occur in inequalities. We show that model-checking on arbitrary relational
structures for a formula in this fragment is W[1]-complete when parameterized
by formula size. We also consider a general framework where a positional game
is represented as a hypergraph and two players alternately pick vertices. In a
Maker-Maker game, the first player to have picked all the vertices of some
hyperedge wins the game. In a Maker-Breaker game, the first player wins if she
picks all the vertices of some hyperedge, and the second player wins otherwise.
In an Enforcer-Avoider game, the first player wins if the second player picks
all the vertices of some hyperedge, and the second player wins otherwise. Short
Maker-Maker is AW[*]-complete, whereas Short Maker-Breaker is W[1]-complete and
Short Enforcer-Avoider co-W[1]-complete parameterized by the number of moves.
This suggests a rough parameterized complexity categorization into positional
games that are complete for the first level of the W-hierarchy when the winning
configurations only depend on which vertices one player has been able to pick,
but AW[*]-completeness when the winning condition depends on which vertices
both players have picked. However, some positional games where the board and
the winning configurations are highly structured are fixed-parameter tractable.
We give another example of such a game, Short k-Connect, which is
fixed-parameter tractable when parameterized by the number of moves.
",Computer Science,Computer Science
"An Assessment of Data Transfer Performance for Large-Scale Climate Data Analysis and Recommendations for the Data Infrastructure for CMIP6   We document the data transfer workflow, data transfer performance, and other
aspects of staging approximately 56 terabytes of climate model output data from
the distributed Coupled Model Intercomparison Project (CMIP5) archive to the
National Energy Research Supercomputing Center (NERSC) at the Lawrence Berkeley
National Laboratory required for tracking and characterizing extratropical
storms, a phenomena of importance in the mid-latitudes. We present this
analysis to illustrate the current challenges in assembling multi-model data
sets at major computing facilities for large-scale studies of CMIP5 data.
Because of the larger archive size of the upcoming CMIP6 phase of model
intercomparison, we expect such data transfers to become of increasing
importance, and perhaps of routine necessity. We find that data transfer rates
using the ESGF are often slower than what is typically available to US
residences and that there is significant room for improvement in the data
transfer capabilities of the ESGF portal and data centers both in terms of
workflow mechanics and in data transfer performance. We believe performance
improvements of at least an order of magnitude are within technical reach using
current best practices, as illustrated by the performance we achieved in
transferring the complete raw data set between two high performance computing
facilities. To achieve these performance improvements, we recommend: that
current best practices (such as the Science DMZ model) be applied to the data
servers and networks at ESGF data centers; that sufficient financial and human
resources be devoted at the ESGF data centers for systems and network
engineering tasks to support high performance data movement; and that
performance metrics for data transfer between ESGF data centers and major
computing facilities used for climate data analysis be established, regularly
tested, and published.
",Computer Science; Physics,Computer Science; Physics
"An arithmetic site of Connes-Consani type for imaginary quadratic fields with class number 1   We construct, for imaginary quadratic number fields with class number 1, an
arithmetic site of Connes-Consani type. The main difficulty here is that the
constructions of Connes and Consani and part of their results strongly rely on
the natural order existing on real numbers which is compatible with basic
arithmetic operations. Of course nothing of this sort exists in the case of
imaginary quadratic number fields with class number 1. We first define what we
call arithmetic site for such number fields, we then calculate the points of
those arithmetic sites and we express them in terms of the adèles class space
considered by Connes to give a spectral interpretation of zeroes of Hecke L
functions of number fields. We get therefore that for a fixed imaginary
quadratic number field with class number 1, that the points of our arithmetic
site are related to the zeroes of the Dedekind zeta function of the number
field considered and to the zeroes of some Hecke L functions. We then study the
relation between the spectrum of the ring of integers of the number field and
the arithmetic site. Finally we construct the square of the arithmetic site.
",Mathematics,Mathematics
"Products of random walks on finite groups with moderate growth   In this article, we consider products of random walks on finite groups with
moderate growth and discuss their cutoffs in the total variation. Based on
several comparison techniques, we are able to identify the total variation
cutoff of discrete time lazy random walks with the Hellinger distance cutoff of
continuous time random walks. Along with the cutoff criterion for Laplace
transforms, we derive a series of equivalent conditions on the existence of
cutoffs, including the existence of pre-cutoffs, Peres' product condition and a
formula generated by the graph diameters. For illustration, we consider
products of Heisenberg groups and randomized products of finite cycles.
",Mathematics,Mathematics
"Predicting shim gaps in aircraft assembly with machine learning and sparse sensing   A modern aircraft may require on the order of thousands of custom shims to
fill gaps between structural components in the airframe that arise due to
manufacturing tolerances adding up across large structures. These shims are
necessary to eliminate gaps, maintain structural performance, and minimize
pull-down forces required to bring the aircraft into engineering nominal
configuration for peak aerodynamic efficiency. Gap filling is a time-consuming
process, involving either expensive by-hand inspection or computations on vast
quantities of measurement data from increasingly sophisticated metrology
equipment. Either case amounts to significant delays in production, with much
of the time spent in the critical path of aircraft assembly. This work presents
an alternative strategy for predictive shimming, based on machine learning and
sparse sensing to first learn gap distributions from historical data, and then
design optimized sparse sensing strategies to streamline data collection and
processing. This new approach is based on the assumption that patterns exist in
shim distributions across aircraft, which may be mined and used to reduce the
burden of data collection and processing in future aircraft. Specifically,
robust principal component analysis is used to extract low-dimensional patterns
in the gap measurements while rejecting outliers. Next, optimized sparse
sensors are obtained that are most informative about the dimensions of a new
aircraft in these low-dimensional principal components. We demonstrate the
success of the proposed approach, called PIXel Identification Despite
Uncertainty in Sensor Technology (PIXI-DUST), on historical production data
from 54 representative Boeing commercial aircraft. Our algorithm successfully
predicts $99\%$ of shim gaps within the desired measurement tolerance using
$3\%$ of the laser scan points typically required; all results are
cross-validated.
",Statistics,Computer Science
"Identifiability of Nonparametric Mixture Models and Bayes Optimal Clustering   Motivated by problems in data clustering, we establish general conditions
under which families of nonparametric mixture models are identifiable, by
introducing a novel framework involving clustering overfitted \emph{parametric}
(i.e. misspecified) mixture models. These identifiability conditions generalize
existing conditions in the literature, and are flexible enough to include for
example mixtures of Gaussian mixtures. In contrast to the recent literature on
estimating nonparametric mixtures, we allow for general nonparametric mixture
components, and instead impose regularity assumptions on the underlying mixing
measure. As our primary application, we apply these results to partition-based
clustering, generalizing the notion of a Bayes optimal partition from classical
parametric model-based clustering to nonparametric settings. Furthermore, this
framework is constructive so that it yields a practical algorithm for learning
identified mixtures, which is illustrated through several examples on real
data. The key conceptual device in the analysis is the convex, metric geometry
of probability measures on metric spaces and its connection to the Wasserstein
convergence of mixing measures. The result is a flexible framework for
nonparametric clustering with formal consistency guarantees.
",Statistics,Computer Science; Statistics
"Efficient Modelling & Forecasting with range based volatility models and application   This paper considers an alternative method for fitting CARR models using
combined estimating functions (CEF) by showing its usefulness in applications
in economics and quantitative finance. The associated information matrix for
corresponding new estimates is derived to calculate the standard errors. A
simulation study is carried out to demonstrate its superiority relative to
other two competitors: linear estimating functions (LEF) and the maximum
likelihood (ML). Results show that CEF estimates are more efficient than LEF
and ML estimates when the error distribution is mis-specified. Taking a real
data set from financial economics, we illustrate the usefulness and
applicability of the CEF method in practice and report reliable forecast values
to minimize the risk in the decision making process.
",Mathematics; Statistics,Statistics
"A momentum conserving $N$-body scheme with individual timesteps   $N$-body simulations study the dynamics of $N$ particles under the influence
of mutual long-distant forces such as gravity. In practice, $N$-body codes will
violate Newton's third law if they use either an approximate Poisson solver or
individual timesteps. In this study, we construct a novel $N$-body scheme by
combining a fast multipole method (FMM) based Poisson solver and a time
integrator using a hierarchical Hamiltonian splitting (HHS) technique. We test
our implementation for collision-less systems using several problems in
galactic dynamics. As a result of the momentum conserving nature of these two
key components, the new $N$-body scheme is also momentum conserving. Moreover,
we can fully utilize the $\mathcal O(\textit N)$ complexity of FMM with the
integrator. With the restored force symmetry, we can improve both angular
momentum conservation and energy conservation substantially. The new scheme
will be suitable for many applications in galactic dynamics and structure
formation. Our implementation, in the code Taichi, is publicly available at
this https URL.
",Physics,Physics
"On biconservative surfaces in Euclidean spaces   In this paper, we study biconservative surfaces with parallel normalized mean
curvature vector in $\mathbb{E}^4$. We obtain complete local classification in
$\mathbb{E}^4$ for a biconservative PNMCV surface. We also give an example to
show the existence of PNMCV biconservative surfaces in $\mathbb{E}^4$.
",Mathematics,Mathematics
"In-Hand Object Stabilization by Independent Finger Control   Grip control during robotic in-hand manipulation is usually modeled as part
of a monolithic task, relying on complex controllers specialized for specific
situations. Such approaches do not generalize well and are difficult to apply
to novel manipulation tasks. Here, we propose a modular object stabilization
method based on a proposition that explains how humans achieve grasp stability.
In this bio-mimetic approach, independent tactile grip stabilization
controllers ensure that slip does not occur locally at the engaged robot
fingers. Such local slip is predicted from the tactile signals of each
fingertip sensor i.e., BioTac and BioTac SP by Syntouch. We show that stable
grasps emerge without any form of central communication when such independent
controllers are engaged in the control of multi-digit robotic hands. These
grasps are resistant to external perturbations while being capable of
stabilizing a large variety of objects.
",Computer Science,Computer Science
"Spherical Planetary Robot for Rugged Terrain Traversal   Wheeled planetary rovers such as the Mars Exploration Rovers (MERs) and Mars
Science Laboratory (MSL) have provided unprecedented, detailed images of the
Mars surface. However, these rovers are large and are of high-cost as they need
to carry sophisticated instruments and science laboratories. We propose the
development of low-cost planetary rovers that are the size and shape of
cantaloupes and that can be deployed from a larger rover. The rover named
SphereX is 2 kg in mass, is spherical, holonomic and contains a hopping
mechanism to jump over rugged terrain. A small low-cost rover complements a
larger rover, particularly to traverse rugged terrain or roll down a canyon,
cliff or crater to obtain images and science data. While it may be a one-way
journey for these small robots, they could be used tactically to obtain
high-reward science data. The robot is equipped with a pair of stereo cameras
to perform visual navigation and has room for a science payload. In this paper,
we analyze the design and development of a laboratory prototype. The results
show a promising pathway towards development of a field system.
",Computer Science; Physics,Computer Science
"Reliable counting of weakly labeled concepts by a single spiking neuron model   Making an informed, correct and quick decision can be life-saving. It's
crucial for animals during an escape behaviour or for autonomous cars during
driving. The decision can be complex and may involve an assessment of the
amount of threats present and the nature of each threat. Thus, we should expect
early sensory processing to supply classification information fast and
accurately, even before relying the information to higher brain areas or more
complex system components downstream. Today, advanced convolutional artificial
neural networks can successfully solve visual detection and classification
tasks and are commonly used to build complex decision making systems. However,
in order to perform well on these tasks they require increasingly complex,
""very deep"" model structure, which is costly in inference run-time, energy
consumption and number of training samples, only trainable on cloud-computing
clusters. A single spiking neuron has been shown to be able to solve
recognition tasks for homogeneous Poisson input statistics, a commonly used
model for spiking activity in the neocortex. When modeled as leaky integrate
and fire with gradient decent learning algorithm it was shown to posses a
variety of complex computational capabilities. Here we improve its
implementation. We also account for more natural stimulus generated inputs that
deviate from this homogeneous Poisson spiking. The improved gradient-based
local learning rule allows for significantly better and stable generalization.
We also show that with its improved capabilities it can count weakly labeled
concepts by applying our model to a problem of multiple instance learning (MIL)
with counting where labels are only available for collections of concepts. In
this counting MNIST task the neuron exploits the improved implementation and
outperforms conventional ConvNet architecture under similar condtions.
",Quantitative Biology,Statistics
"Intersection of conjugate solvable subgroups in symmetric groups   It is shown that for a solvable subgroup $G$ of an almost simple group $S$
which socle is isomorphic to $A_n$ $ (n\ge5)$ there are $x,y,z,t \in S$ such
that $G \cap G^x \cap G^y \cap G^z \cap G^t =1.$
",Mathematics,Mathematics
"On problems in the calculus of variations in increasingly elongated domains   We consider minimization problems in the calculus of variations set in a
sequence of domains the size of which tends to infinity in certain directions
and such that the data only depend on the coordinates in the directions that
remain constant. We study the asymptotic behavior of minimizers in various
situations and show that they converge in an appropriate sense toward
minimizers of a related energy functional in the constant directions.
",Mathematics,Mathematics
"Comparing People with Bibliometrics   Bibliometric indicators, citation counts and/or download counts are
increasingly being used to inform personnel decisions such as hiring or
promotions. These statistics are very often misused. Here we provide a guide to
the factors which should be considered when using these so-called quantitative
measures to evaluate people. Rules of thumb are given for when begin to use
bibliometric measures when comparing otherwise similar candidates.
",Computer Science; Physics,Computer Science
"Variants on the Berz sublinearity theorem   We consider variants on the classical Berz sublinearity theorem, using only
DC, the Axiom of Dependent Choices, rather than AC, the Axiom of Choice which
Berz used. We consider thinned versions, in which conditions are imposed on
only part of the domain of the function -- results of quantifier-weakening
type. There are connections with classical results on subadditivity. We close
with a discussion of the extensive related literature.
",Mathematics,Mathematics
"Raw Waveform-based Speech Enhancement by Fully Convolutional Networks   This study proposes a fully convolutional network (FCN) model for raw
waveform-based speech enhancement. The proposed system performs speech
enhancement in an end-to-end (i.e., waveform-in and waveform-out) manner, which
dif-fers from most existing denoising methods that process the magnitude
spectrum (e.g., log power spectrum (LPS)) only. Because the fully connected
layers, which are involved in deep neural networks (DNN) and convolutional
neural networks (CNN), may not accurately characterize the local information of
speech signals, particularly with high frequency components, we employed fully
convolutional layers to model the waveform. More specifically, FCN consists of
only convolutional layers and thus the local temporal structures of speech
signals can be efficiently and effectively preserved with relatively few
weights. Experimental results show that DNN- and CNN-based models have limited
capability to restore high frequency components of waveforms, thus leading to
decreased intelligibility of enhanced speech. By contrast, the proposed FCN
model can not only effectively recover the waveforms but also outperform the
LPS-based DNN baseline in terms of short-time objective intelligibility (STOI)
and perceptual evaluation of speech quality (PESQ). In addition, the number of
model parameters in FCN is approximately only 0.2% compared with that in both
DNN and CNN.
",Computer Science; Statistics,Computer Science
"Motivic zeta functions and infinite cyclic covers   We associate with an infinite cyclic cover of a punctured neighborhood of a
simple normal crossing divisor on a complex quasi-projective manifold (assuming
certain finiteness conditions are satisfied) a rational function in $K_0({\rm
Var}^{\hat \mu}_{\mathbb{C}})[\mathbb{L}^{-1}]$, which we call {\it motivic
infinite cyclic zeta function}, and show its birational invariance. Our
construction is a natural extension of the notion of {\it motivic infinite
cyclic covers} introduced by the authors, and as such, it generalizes the
Denef-Loeser motivic Milnor zeta function of a complex hypersurface singularity
germ.
",Mathematics,Mathematics
"Interstellar communication. VII. Benchmarking inscribed matter probes   We have explored the optimal frequency of interstellar photon communications
and benchmarked other particles as information carriers in previous papers of
this series. We now compare the latency and bandwidth of sending probes with
inscribed matter. Durability requirements such as shields against dust and
radiation, as well as data duplication, add negligible weight overhead at
velocities <0.2c. Probes may arrive in full, while most of a photon beam is
lost to diffraction. Probes can be more energy efficient per bit, and can have
higher bandwidth, compared to classical communication, unless a photon receiver
is placed in a stellar gravitational lens. The probe's advantage dominates by
order of magnitude for long distances (kpc) and low velocities (<0.1c) at the
cost of higher latency.
",Physics,Physics
"Gap Acceptance During Lane Changes by Large-Truck Drivers-An Image-Based Analysis   This paper presents an analysis of rearward gap acceptance characteristics of
drivers of large trucks in highway lane change scenarios. The range between the
vehicles was inferred from camera images using the estimated lane width
obtained from the lane tracking camera as the reference. Six-hundred lane
change events were acquired from a large-scale naturalistic driving data set.
The kinematic variables from the image-based gap analysis were filtered by the
weighted linear least squares in order to extrapolate them at the lane change
time. In addition, the time-to-collision and required deceleration were
computed, and potential safety threshold values are provided. The resulting
range and range rate distributions showed directional discrepancies, i.e., in
left lane changes, large trucks are often slower than other vehicles in the
target lane, whereas they are usually faster in right lane changes. Video
observations have confirmed that major motivations for changing lanes are
different depending on the direction of move, i.e., moving to the left (faster)
lane occurs due to a slower vehicle ahead or a merging vehicle on the
right-hand side, whereas right lane changes are frequently made to return to
the original lane after passing.
",Computer Science,Computer Science
"Approximate Value Iteration for Risk-aware Markov Decision Processes   We consider large-scale Markov decision processes (MDPs) with a risk measure
of variability in cost, under the risk-aware MDPs paradigm. Previous studies
showed that risk-aware MDPs, based on a minimax approach to handling risk, can
be solved using dynamic programming for small to medium sized problems.
However, due to the ""curse of dimensionality"", MDPs that model real-life
problems are typically prohibitively large for such approaches. In this paper,
we employ an approximate dynamic programming approach, and develop a family of
simulation-based algorithms to approximately solve large-scale risk-aware MDPs.
In parallel, we develop a unified convergence analysis technique to derive
sample complexity bounds for this new family of algorithms.
",Computer Science; Mathematics,Computer Science; Statistics
"Northern sky Galactic Cosmic Ray anisotropy between 10-1000 TeV with the Tibet Air Shower Array   We report the analysis of the $10-1000$ TeV large-scale sidereal anisotropy
of Galactic cosmic rays (GCRs) with the data collected by the Tibet Air Shower
Array from October, 1995 to February, 2010. In this analysis, we improve the
energy estimate and extend the declination range down to $-30^{\circ}$. We find
that the anisotropy maps above 100 TeV are distinct from that at multi-TeV
band. The so-called ""tail-in"" and ""loss-cone"" features identified at low
energies get less significant and a new component appears at $\sim100$ TeV. The
spatial distribution of the GCR intensity with an excess (7.2$\sigma$
pre-trial, 5.2$\sigma$ post-trial) and a deficit ($-5.8\sigma$ pre-trial) are
observed in the 300 TeV anisotropy map, in a good agreement with IceCube's
results at 400 TeV. Combining the Tibet results in the northern sky with
IceCube's results in the southern sky, we establish a full-sky picture of the
anisotropy in hundreds of TeV band. We further find that the amplitude of the
first order anisotropy increases sharply above $\sim100$ TeV, indicating a new
component of the anisotropy. All these results may shed new light on
understanding the origin and propagation of GCRs.
",Physics,Physics
"The occurrence of transverse and longitudinal electric currents in the classical plasma under the action of N transverse electromagnetic waves   Classical plasma with arbitrary degree of degeneration of electronic gas is
considered. In plasma N (N>2) collinear electromagnatic waves are propagated.
It is required to find the response of plasma to these waves. Distribution
function in square-law approximation on quantities of two small parameters from
Vlasov equation is received. The formula for electric current calculation is
deduced. It is demonstrated that the nonlinearity account leads to occurrence
of the longitudinal electric current directed along a wave vector. This
longitudinal current is orthogonal to the known transversal current received at
the linear analysis. The case of small values of wave number is considered.
",Physics,Physics
"Superluminal transmission of phase modulation information by a long macroscopic pulse propagating through interstellar space   A method of transmitting information in interstellar space at superluminal,
or $> c$, speeds is proposed. The information is encoded as phase modulation of
an electromagnetic wave of constant intensity, i.e. fluctuations in the rate of
energy transport plays no role in the communication, and no energy is
transported at speed $>$ c. Of course, such a constant wave can ultimately last
only the duration of its enveloping wave packet. However, as a unique feature
of this paper, we assume the source is sufficiently steady to be capable of
emitting wave packets, or pulses, of size much larger than the separation
between sender and receiver. Therefore, if a pre-existing and enduring wave
envelope already connects the two sides, the subluminal nature of the
envelope's group velocity will no longer slow down the communication, which is
now limited by the speed at which information encoded as phase modulation
propagates through the plasma, i.e. the phase velocity $v_p > c$. The method
involves no sharp structure in either time or frequency. As a working example,
we considered two spaceships separated by 1 lt-s in the local hot bubble.
Provided the bandwidth of the extra Fourier modes generated by the phase
modulation is much smaller than the carrier wave frequency, the radio
communication of a message, encoded as a specific alignment between the carrier
wave phase and the anomalous (modulated) phase, can take place at a speed in
excess of light by a few parts in 10$^{11}$ at $\nu\approx 1$~GHz, and higher
at smaller $\nu$.
",Physics,Physics
"Self-consistent dynamical model of the Broad Line Region   We develope a self-consistent description of the Broad Line Region based on
the concept of the failed wind powered by the radiation pressure acting on
dusty accretion disk atmosphere in Keplerian motion. The material raised high
above the disk is illuminated, dust evaportes, and the matter falls back
towards the disk. This material is the source of emission lines. The model
predicts the inner and outer radius of the region, the cloud dynamics under the
dust radiation pressure and, subsequently, just the gravitational field of the
central black hole, which results in assymetry between the rise and fall.
Knowledge of the dynamics allows to predict the shapes of the emission lines as
functions of the basic parameters of an active nucleus: black hole mass,
accretion rate, black hole spin (or accretion efficiency) and the viewing angle
with respect to the symmetry axis. Here we show preliminary results based on
analytical approximations to the cloud motion.
",Physics,Physics
"Small Telescope Exoplanet Transit Surveys: XO   The XO project aims at detecting transiting exoplanets around bright stars
from the ground using small telescopes. The original configuration of XO
(McCullough et al. 2005) has been changed and extended as described here. The
instrumental setup consists of three identical units located at different
sites, each composed of two lenses equipped with CCD cameras mounted on the
same mount. We observed two strips of the sky covering an area of 520 deg$^2$
for twice nine months. We build lightcurves for ~20,000 stars up to magnitude
R~12.5 using a custom-made photometric data reduction pipeline. The photometric
precision is around 1-2% for most stars, and the large quantity of data allows
us to reach a millimagnitude precision when folding the lightcurves on
timescales that are relevant to exoplanetary transits. We search for periodic
signals and identify several hundreds of variable stars and a few tens of
transiting planet candidates. Follow-up observations are underway to confirm or
reject these candidates. We found two close-in gas giant planets so far, in
line with the expected yield.
",Physics,Physics
"Reservoir Computing for Detection of Steady State in Performance Tests of Compressors   Fabrication of devices in industrial plants often includes undergoing quality
assurance tests or tests that seek to determine some attributes or capacities
of the device. For instance, in testing refrigeration compressors, we want to
find the true refrigeration capacity of the compressor being tested. Such test
(also called an episode) may take up to four hours, being an actual hindrance
to applying it to the total number of compressors produced. This work seeks to
reduce the time spent on such industrial trials by employing Recurrent Neural
Networks (RNNs) as dynamical models for detecting when a test is entering the
so-called steady-state region. Specifically, we use Reservoir Computing (RC)
networks which simplify the learning of RNNs by speeding up training time and
showing convergence to a global optimum. Also, this work proposes a
self-organized subspace projection method for RC networks which uses
information from the beginning of the episode to define a cluster to which the
episode belongs to. This assigned cluster defines a particular binary input
that shifts the operating point of the reservoir to a subspace of trajectories
for the duration of the episode. This new method is shown to turn the RC model
robust in performance with respect to varying combination of reservoir
parameters, such as spectral radius and leak rate, when compared to a standard
RC network.
",Computer Science,Computer Science; Statistics
"Science with e-ASTROGAM (A space mission for MeV-GeV gamma-ray astrophysics)   e-ASTROGAM (enhanced ASTROGAM) is a breakthrough Observatory space mission,
with a detector composed by a Silicon tracker, a calorimeter, and an
anticoincidence system, dedicated to the study of the non-thermal Universe in
the photon energy range from 0.3 MeV to 3 GeV - the lower energy limit can be
pushed to energies as low as 150 keV for the tracker, and to 30 keV for
calorimetric detection. The mission is based on an advanced space-proven
detector technology, with unprecedented sensitivity, angular and energy
resolution, combined with polarimetric capability. Thanks to its performance in
the MeV-GeV domain, substantially improving its predecessors, e-ASTROGAM will
open a new window on the non-thermal Universe, making pioneering observations
of the most powerful Galactic and extragalactic sources, elucidating the nature
of their relativistic outflows and their effects on the surroundings. With a
line sensitivity in the MeV energy range one to two orders of magnitude better
than previous generation instruments, e-ASTROGAM will determine the origin of
key isotopes fundamental for the understanding of supernova explosion and the
chemical evolution of our Galaxy. The mission will provide unique data of
significant interest to a broad astronomical community, complementary to
powerful observatories such as LIGO-Virgo-GEO600-KAGRA, SKA, ALMA, E-ELT, TMT,
LSST, JWST, Athena, CTA, IceCube, KM3NeT, and LISA.
",Physics,Physics
"Optimisation de la QoS dans un r{é}seau de radio cognitive en utilisant la m{é}taheuristique SFLA (Shuffled Frog Leaping Algorithm)   This work proposes a study of quality of service (QoS) in cognitive radio
networks. This study is based on a stochastic optimization method called
shuffled frog leaping algorithm (SFLA). The interest of the SFLA algorithm is
to guarantee a better solution in a multi-carrier context in order to satisfy
the requirements of the secondary user (SU).
",Computer Science,Computer Science
"A Coupled Lattice Boltzmann Method and Discrete Element Method for Discrete Particle Simulations of Particulate Flows   Discrete particle simulations are widely used to study large-scale
particulate flows in complex geometries where particle-particle and
particle-fluid interactions require an adequate representation but the
computational cost has to be kept low. In this work, we present a novel
coupling approach for such simulations. A lattice Boltzmann formulation of the
generalized Navier-Stokes equations is used to describe the fluid motion. This
promises efficient simulations suitable for high performance computing and,
since volume displacement effects by the solid phase are considered, our
approach is also applicable to non-dilute particulate systems. The discrete
element method is combined with an explicit evaluation of interparticle
lubrication forces to simulate the motion of individual submerged particles.
Drag, pressure and added mass forces determine the momentum transfer by
fluid-particle interactions. A stable coupling algorithm is presented and
discussed in detail. We demonstrate the validity of our approach for dilute as
well as dense systems by predicting the settling velocity of spheres over a
broad range of solid volume fractions in good agreement with semi-empirical
correlations. Additionally, the accuracy of particle-wall interactions in a
viscous fluid is thoroughly tested and established. Our approach can thus be
readily used for various particulate systems and can be extended
straightforward to e.g. non-spherical particles.
",Computer Science; Physics,Physics
"Coverage Analysis of a Vehicular Network Modeled as Cox Process Driven by Poisson Line Process   In this paper, we consider a vehicular network in which the wireless nodes
are located on a system of roads. We model the roadways, which are
predominantly straight and randomly oriented, by a Poisson line process (PLP)
and the locations of nodes on each road as a homogeneous 1D Poisson point
process (PPP). Assuming that each node transmits independently, the locations
of transmitting and receiving nodes are given by two Cox processes driven by
the same PLP. For this setup, we derive the coverage probability of a typical
receiver, which is an arbitrarily chosen receiving node, assuming independent
Nakagami-$m$ fading over all wireless channels. Assuming that the typical
receiver connects to its closest transmitting node in the network, we first
derive the distribution of the distance between the typical receiver and the
serving node to characterize the desired signal power. We then characterize
coverage probability for this setup, which involves two key technical
challenges. First, we need to handle several cases as the serving node can
possibly be located on any line in the network and the corresponding
interference experienced at the typical receiver is different in each case.
Second, conditioning on the serving node imposes constraints on the spatial
configuration of lines, which require careful analysis of the conditional
distribution of the lines. We address these challenges in order to accurately
characterize the interference experienced at the typical receiver. We then
derive an exact expression for coverage probability in terms of the derivative
of Laplace transform of interference power distribution. We analyze the trends
in coverage probability as a function of the network parameters: line density
and node density. We also study the asymptotic behavior of this model and
compare the coverage performance with that of a homogeneous 2D PPP model with
the same node density.
",Computer Science,Computer Science
"A Liouville theorem for indefinite fractional diffusion equations and its application to existence of solutions   In this work we obtain a Liouville theorem for positive, bounded solutions of
the equation $$ (-\Delta)^s u= h(x_N)f(u) \quad \hbox{in }\mathbb{R}^{N} $$
where $(-\Delta)^s$ stands for the fractional Laplacian with $s\in (0,1)$, and
the functions $h$ and $f$ are nondecreasing. The main feature is that the
function $h$ changes sign in $\mathbb{R}$, therefore the problem is sometimes
termed as indefinite. As an application we obtain a priori bounds for positive
solutions of some boundary value problems, which give existence of such
solutions by means of bifurcation methods.
",Mathematics,Mathematics
"Bursting dynamics of viscous film without circular symmetry: the effect of confinement   We experimentally investigate the bursting dynamics of confined liquid film
suspended in air and find a viscous dynamics distinctly different from the
non-confined counterpart, due to lack of circular symmetry in the shape of
expanding hole: the novel confined-viscous bursting proceeds at a constant
speed and a rim formed at the bursting tip does not grow. We find a
confined-viscous to confined-inertial crossover, as well as a
nonconfined-inertial to confined-inertial crossover, at which bursting speed
does not change although the circular symmetry in the hole shape breaks
dynamically.
",Physics,Physics
"Manin's conjecture for a class of singular cubic hypersurfaces   Let $n$ be a positive multiple of $4$. We establish an asymptotic formula for
the number of rational points of bounded height on singular cubic hypersurfaces
$S_n$ defined by $$ x^3=(y_1^2 + \cdots + y_n^2)z . $$ This result is new in
two aspects: first, it can be viewed as a modest start on the study of density
of rational points on those singular cubic hypersurfaces which are not covered
by the classical theorems of Davenport or Heath-Brown; second, it proves
Manin's conjecture for singular cubic hypersurfaces $S_n$ defined above.
",Mathematics,Mathematics
"Luminous Efficiency Estimates of Meteors -I. Uncertainty analysis   The luminous efficiency of meteors is poorly known, but critical for
determining the meteoroid mass. We present an uncertainty analysis of the
luminous efficiency as determined by the classical ablation equations, and
suggest a possible method for determining the luminous efficiency of real
meteor events. We find that a two-term exponential fit to simulated lag data is
able to reproduce simulated luminous efficiencies reasonably well.
",Physics,Physics
"Eternal inflation and the quantum birth of cosmic structure   We consider the eternal inflation scenario of the slow-roll/chaotic type with
the additional element of an objective collapse of the wave function. The
incorporation of this new agent to the traditional inflationary setting might
represent a possible solution to the quantum measurement problem during
inflation, a subject that has not reached a consensus among the community.
Specifically, it could provide an explanation for the generation of the
primordial anisotropies and inhomogeneities, starting from a perfectly
symmetric background and invoking symmetric dynamics. We adopt the continuous
spontaneous localization model, in the context of inflation, as the dynamical
reduction mechanism that generates the primordial inhomogeneities. Furthermore,
when enforcing the objective reduction mechanism, the condition for eternal
inflation can be bypassed. In particular, the collapse mechanism incites the
wave function, corresponding to the inflaton, to localize itself around the
zero mode of the field. Then the zero mode will evolve essentially unperturbed,
driving inflation to an end in any region of the Universe where inflation
occurred. Also, our approach achieves a primordial spectrum with an amplitude
and shape consistent with the one that best fits the observational data.
",Physics,Physics
"Vision and Challenges for Knowledge Centric Networking (KCN)   In the creation of a smart future information society, Internet of Things
(IoT) and Content Centric Networking (CCN) break two key barriers for both the
front-end sensing and back-end networking. However, we still observe the
missing piece of the research that dominates the current networking traffic
control and system management, e.g., lacking of the knowledge penetrated into
both sensing and networking to glue them holistically. In this paper, we
envision to leverage emerging machine learning or deep learning techniques to
create aspects of knowledge for facilitating the designs. In particular, we can
extract knowledge from collected data to facilitate reduced data volume,
enhanced system intelligence and interactivity, improved service quality,
communication with better controllability and lower cost. We name such a
knowledge-oriented traffic control and networking management paradigm as the
Knowledge Centric Networking (KCN). This paper presents KCN rationale, KCN
benefits, related works and research opportunities.
",Computer Science,Computer Science
"Anisotropic thermophoresis   Colloidal migration in temperature gradient is referred to as thermophoresis.
In contrast to particles with spherical shape, we show that elongated colloids
may have a thermophoretic response that varies with the colloid orientation.
Remarkably, this can translate into a non-vanishing thermophoretic force in the
direction perpendicular to the temperature gradient. Oppositely to the friction
force, the thermophoretic force of a rod oriented with the temperature gradient
can be larger or smaller than when oriented perpendicular to it. The precise
anisotropic thermophoretic behavior clearly depends on the colloidal rod aspect
ratio, and also on its surface details, which provides an interesting
tunability to the devices constructed based on this principle. By means of
mesoscale hydrodynamic simulations, we characterize this effect for different
types of rod-like colloids.
",Physics,Physics
"Integral field observations of the blue compact galaxy Haro14. Star formation and feedback in dwarf galaxies   (Abridged) Low-luminosity, gas-rich blue compact galaxies (BCG) are ideal
laboratories to investigate many aspects of the star formation in galaxies. We
study the morphology, stellar content, kinematics, and the nebular excitation
and ionization mechanism in the BCG Haro 14 by means of integral field
observations with VIMOS in the VLT. From these data we build maps in continuum
and in the brighter emission lines, produce line-ratio maps, and obtain the
velocity and velocity dispersion fields. We also generate the integrated
spectrum of the major HII regions and young stellar clusters identified in the
maps to determine reliable physical parameters and oxygen abundances. We find
as follows: i) the current star formation in Haro 14 is spatially extended with
the major HII regions placed along a linear structure, elongated in the
north-south direction, and in a horseshoe-like curvilinear feature that extends
about 760 pc eastward; the continuum emission is more concentrated and peaks
close to the galaxy center; ii) two different episodes of star formation are
present: the recent starburst, with ages $\leq$ 6 Myrs and the intermediate-age
clusters, with ages between 10 and 30 Myrs; these stellar components rest on a
several Gyr old underlying host galaxy; iii) the H$\alpha$/H$\beta$ pattern is
inhomogeneous, with excess color values varying from E(B-V)=0.04 up to
E(B-V)=1.09; iv) shocks play a significant role in the galaxy; and v) the
velocity field displays a complicated pattern with regions of material moving
toward us in the east and north galaxy areas. The morphology of Haro 14, its
irregular velocity field, and the presence of shocks speak in favor of a
scenario of triggered star formation. Ages of the knots are consistent with the
ongoing burst being triggered by the collective action of stellar winds and
supernovae originated in the central clusters.
",Physics,Physics
"ILP-based Alleviation of Dense Meander Segments with Prioritized Shifting and Progressive Fixing in PCB Routing   Length-matching is an important technique to bal- ance delays of bus signals
in high-performance PCB routing. Existing routers, however, may generate very
dense meander segments. Signals propagating along these meander segments
exhibit a speedup effect due to crosstalk between the segments of the same
wire, thus leading to mismatch of arrival times even under the same physical
wire length. In this paper, we present a post-processing method to enlarge the
width and the distance of meander segments and hence distribute them more
evenly on the board so that crosstalk can be reduced. In the proposed
framework, we model the sharing of available routing areas after removing dense
meander segments from the initial routing, as well as the generation of relaxed
meander segments and their groups for wire length compensation. This model is
transformed into an ILP problem and solved for a balanced distribution of wire
patterns. In addition, we adjust the locations of long wire segments according
to wire priorities to swap free spaces toward critical wires that need much
length compensation. To reduce the problem space of the ILP model, we also
introduce a progressive fixing technique so that wire patterns are grown
gradually from the edge of the routing toward the center area. Experimental
results show that the proposed method can expand meander segments significantly
even under very tight area constraints, so that the speedup effect can be
alleviated effectively in high- performance PCB designs.
",Computer Science,Computer Science
"Quasiconformal mappings and Hölder continuity   We establish that every $K$-quasiconformal mapping $w$ of the unit ball $\IB$
onto a $C^2$-Jordan domain $\Omega$ is Hölder continuous with constant
$\alpha= 2-\frac{n}{p}$, provided that its weak Laplacean $\Delta w$ is in $
L^p(\IB)$ for some $n/2<p<n$. In particular it is Hölder continuous for every
$0<\alpha<1$ provided that $\Delta w\in L^n(\IB)$.
",Mathematics,Mathematics
"Genetic interactions from first principles   We derive a general statistical model of interactions, starting from
probabilistic principles and elementary requirements. Prevailing interaction
models in biomedical researches diverge both mathematically and practically. In
particular, genetic interaction inquiries are formulated without an obvious
mathematical unity. Our model reveals theoretical properties unnoticed so far,
particularly valuable for genetic interaction mapping, where mechanistic
details are mostly unknown, distribution of gene variants differ between
populations, and genetic susceptibilities are spuriously propagated by linkage
disequilibrium. When applied to data of the largest interaction mapping
experiment on Saccharomyces Cerevisiae to date, our results imply less aversion
to positive interactions, detection of well-documented hubs and partial
remapping of functional regions of the currently known genetic interaction
landscape. Assessment of divergent annotations across functional categories
further suggests that positive interactions have a more important role on
ribosome biogenesis than previously realized. The unity of arguments elaborated
here enables the analysis of dissimilar interaction models and experimental
data with a common framework.
",Statistics,Quantitative Biology
"Bounding the Radius of Convergence of Analytic Functions   Contour integration is a crucial technique in many numeric methods of
interest in physics ranging from differentiation to evaluating functions of
matrices. It is often important to determine whether a given contour contains
any poles or branch cuts, either to make use of these features or to avoid
them. A special case of this problem is that of determining or bounding the
radius of convergence of a function, as this provides a known circle around a
point in which a function remains analytic. We describe a method for
determining whether or not a circular contour of a complex-analytic function
contains any poles. We then build on this to produce a robust method for
bounding the radius of convergence of a complex-analytic function.
",Mathematics,Computer Science; Mathematics
"Adversarial Examples: Attacks and Defenses for Deep Learning   With rapid progress and significant successes in a wide spectrum of
applications, deep learning is being applied in many safety-critical
environments. However, deep neural networks have been recently found vulnerable
to well-designed input samples, called adversarial examples. Adversarial
examples are imperceptible to human but can easily fool deep neural networks in
the testing/deploying stage. The vulnerability to adversarial examples becomes
one of the major risks for applying deep neural networks in safety-critical
environments. Therefore, attacks and defenses on adversarial examples draw
great attention. In this paper, we review recent findings on adversarial
examples for deep neural networks, summarize the methods for generating
adversarial examples, and propose a taxonomy of these methods. Under the
taxonomy, applications for adversarial examples are investigated. We further
elaborate on countermeasures for adversarial examples and explore the
challenges and the potential solutions.
",Computer Science; Statistics,Computer Science; Statistics
"User Interface (UI) Design Issues for the Multilingual Users: A Case Study   A multitude of web and desktop applications are now widely available in
diverse human languages. This paper explores the design issues that are
specifically relevant for multilingual users. It reports on the continued
studies of Information System (IS) issues and users' behaviour across
cross-cultural and transnational boundaries. Taking the BBC website as a model
that is internationally recognised, usability tests were conducted to compare
different versions of the website. The dependant variables derived from the
questionnaire were analysed (via descriptive statistics) to elucidate the
multilingual UI design issues. Using Principal Component Analysis (PCA), five
de-correlated variables were identified which were then used for hypotheses
tests. A modified version of Herzberg's Hygiene-motivational Theory about the
Workplace was applied to assess the components used in the website. Overall, it
was concluded that the English versions of the website gave superior usability
results and this implies the need for deeper study of the problems in usability
of the translated versions.
",Computer Science,Computer Science
"Linear growth of streaming instability in pressure bumps   Streaming instability is a powerful mechanism which concentrates dust grains
in pro- toplanetary discs, eventually up to the stage where they collapse
gravitationally and form planetesimals. Previous studies inferred that it
should be ineffective in viscous discs, too efficient in inviscid discs, and
may not operate in local pressure maxima where solids accumulate. From a linear
analysis of stability, we show that streaming instability behaves differently
inside local pressure maxima. Under the action of the strong differential
advection imposed by the bump, a novel unstable mode develops and grows even
when gas viscosity is large. Hence, pressure bumps are found to be the only
places where streaming instability occurs in viscous discs. This offers a
promising way to conciliate models of planet formation with recent observations
of young discs.
",Physics,Physics
"A punishment voting algorithm based on super categories construction for acoustic scene classification   In acoustic scene classification researches, audio segment is usually split
into multiple samples. Majority voting is then utilized to ensemble the results
of the samples. In this paper, we propose a punishment voting algorithm based
on the super categories construction method for acoustic scene classification.
Specifically, we propose a DenseNet-like model as the base classifier. The base
classifier is trained by the CQT spectrograms generated from the raw audio
segments. Taking advantage of the results of the base classifier, we propose a
super categories construction method using the spectral clustering. Super
classifiers corresponding to the constructed super categories are further
trained. Finally, the super classifiers are utilized to enhance the majority
voting of the base classifier by punishment voting. Experiments show that the
punishment voting obviously improves the performances on both the DCASE2017
Development dataset and the LITIS Rouen dataset.
",Computer Science; Statistics,Computer Science
"Robust Adversarial Reinforcement Learning   Deep neural networks coupled with fast simulation and improved computation
have led to recent successes in the field of reinforcement learning (RL).
However, most current RL-based approaches fail to generalize since: (a) the gap
between simulation and real world is so large that policy-learning approaches
fail to transfer; (b) even if policy learning is done in real world, the data
scarcity leads to failed generalization from training to test scenarios (e.g.,
due to different friction or object masses). Inspired from H-infinity control
methods, we note that both modeling errors and differences in training and test
scenarios can be viewed as extra forces/disturbances in the system. This paper
proposes the idea of robust adversarial reinforcement learning (RARL), where we
train an agent to operate in the presence of a destabilizing adversary that
applies disturbance forces to the system. The jointly trained adversary is
reinforced -- that is, it learns an optimal destabilization policy. We
formulate the policy learning as a zero-sum, minimax objective function.
Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah,
Swimmer, Hopper and Walker2d) conclusively demonstrate that our method (a)
improves training stability; (b) is robust to differences in training/test
conditions; and c) outperform the baseline even in the absence of the
adversary.
",Computer Science,Computer Science; Statistics
"Bounded cohomology and virtually free hyperbolically embedded subgroups   Using a probabilistic argument we show that the second bounded cohomology of
an acylindrically hyperbolic group $G$ (e.g., a non-elementary hyperbolic or
relatively hyperbolic group, non-exceptional mapping class group, ${\rm
Out}(F_n)$, \dots) embeds via the natural restriction maps into the inverse
limit of the second bounded cohomologies of its virtually free subgroups, and
in fact even into the inverse limit of the second bounded cohomologies of its
hyperbolically embedded virtually free subgroups. This result is new and
non-trivial even in the case where $G$ is a (non-free) hyperbolic group. The
corresponding statement fails in general for the third bounded cohomology, even
for surface groups.
",Mathematics,Mathematics
"Weighted Random Walk Sampling for Multi-Relational Recommendation   In the information overloaded web, personalized recommender systems are
essential tools to help users find most relevant information. The most
heavily-used recommendation frameworks assume user interactions that are
characterized by a single relation. However, for many tasks, such as
recommendation in social networks, user-item interactions must be modeled as a
complex network of multiple relations, not only a single relation. Recently
research on multi-relational factorization and hybrid recommender models has
shown that using extended meta-paths to capture additional information about
both users and items in the network can enhance the accuracy of recommendations
in such networks. Most of this work is focused on unweighted heterogeneous
networks, and to apply these techniques, weighted relations must be simplified
into binary ones. However, information associated with weighted edges, such as
user ratings, which may be crucial for recommendation, are lost in such
binarization. In this paper, we explore a random walk sampling method in which
the frequency of edge sampling is a function of edge weight, and apply this
generate extended meta-paths in weighted heterogeneous networks. With this
sampling technique, we demonstrate improved performance on multiple data sets
both in terms of recommendation accuracy and model generation efficiency.
",Computer Science,Computer Science; Statistics
"The Reinhardt Conjecture as an Optimal Control Problem   In 1934, Reinhardt conjectured that the shape of the centrally symmetric
convex body in the plane whose densest lattice packing has the smallest density
is a smoothed octagon. This conjecture is still open. We formulate the
Reinhardt Conjecture as a problem in optimal control theory. The smoothed
octagon is a Pontryagin extremal trajectory with bang-bang control. More
generally, the smoothed regular $6k+2$-gon is a Pontryagin extremal with
bang-bang control. The smoothed octagon is a strict (micro) local minimum to
the optimal control problem. The optimal solution to the Reinhardt problem is a
trajectory without singular arcs. The extremal trajectories that do not meet
the singular locus have bang-bang controls with finitely many switching times.
Finally, we reduce the Reinhardt problem to an optimization problem on a
five-dimensional manifold. (Each point on the manifold is an initial condition
for a potential Pontryagin extremal lifted trajectory.) We suggest that the
Reinhardt conjecture might eventually be fully resolved through optimal control
theory. Some proofs are computer-assisted using a computer algebra system.
",Mathematics,Mathematics
"Search Rank Fraud De-Anonymization in Online Systems   We introduce the fraud de-anonymization problem, that goes beyond fraud
detection, to unmask the human masterminds responsible for posting search rank
fraud in online systems. We collect and study search rank fraud data from
Upwork, and survey the capabilities and behaviors of 58 search rank fraudsters
recruited from 6 crowdsourcing sites. We propose Dolos, a fraud
de-anonymization system that leverages traits and behaviors extracted from
these studies, to attribute detected fraud to crowdsourcing site fraudsters,
thus to real identities and bank accounts. We introduce MCDense, a min-cut
dense component detection algorithm to uncover groups of user accounts
controlled by different fraudsters, and leverage stylometry and deep learning
to attribute them to crowdsourcing site profiles. Dolos correctly identified
the owners of 95% of fraudster-controlled communities, and uncovered fraudsters
who promoted as many as 97.5% of fraud apps we collected from Google Play. When
evaluated on 13,087 apps (820,760 reviews), which we monitored over more than 6
months, Dolos identified 1,056 apps with suspicious reviewer groups. We report
orthogonal evidence of their fraud, including fraud duplicates and fraud
re-posts.
",Computer Science,Computer Science
"On Optimal Weighted-Delay Scheduling in Input-Queued Switches   Motivated by relatively few delay-optimal scheduling results, in comparison
to results on throughput optimality, we investigate an input-queued switch
scheduling problem in which the objective is to minimize a linear function of
the queue-length vector. Theoretical properties of variants of the well-known
MaxWeight scheduling algorithm are established within this context, which
includes showing that these algorithms exhibit optimal heavy-traffic
queue-length scaling. For the case of $2 \times 2$ input-queued switches, we
derive an optimal scheduling policy and establish its theoretical properties,
demonstrating fundamental differences with the variants of MaxWeight
scheduling. Our theoretical results are expected to be of interest more broadly
than input-queued switches. Computational experiments demonstrate and quantify
the benefits of our optimal scheduling policy.
",Mathematics,Computer Science
"Control Problems with Vanishing Lie Bracket Arising from Complete Odd Circulant Evolutionary Games   We study an optimal control problem arising from a generalization of
rock-paper-scissors in which the number of strategies may be selected from any
positive odd number greater than 1 and in which the payoff to the winner is
controlled by a control variable $\gamma$. Using the replicator dynamics as the
equations of motion, we show that a quasi-linearization of the problem admits a
special optimal control form in which explicit dynamics for the controller can
be identified. We show that all optimal controls must satisfy a specific second
order differential equation parameterized by the number of strategies in the
game. We show that as the number of strategies increases, a limiting case
admits a closed form for the open-loop optimal control. In performing our
analysis we show necessary conditions on an optimal control problem that allow
this analytic approach to function.
",Computer Science,Computer Science
"Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning   The learning of domain-invariant representations in the context of domain
adaptation with neural networks is considered. We propose a new regularization
method that minimizes the discrepancy between domain-specific latent feature
representations directly in the hidden activation space. Although some standard
distribution matching approaches exist that can be interpreted as the matching
of weighted sums of moments, e.g. Maximum Mean Discrepancy (MMD), an explicit
order-wise matching of higher order moments has not been considered before. We
propose to match the higher order central moments of probability distributions
by means of order-wise moment differences. Our model does not require
computationally expensive distance and kernel matrix computations. We utilize
the equivalent representation of probability distributions by moment sequences
to define a new distance function, called Central Moment Discrepancy (CMD). We
prove that CMD is a metric on the set of probability distributions on a compact
interval. We further prove that convergence of probability distributions on
compact intervals w.r.t. the new metric implies convergence in distribution of
the respective random variables. We test our approach on two different
benchmark data sets for object recognition (Office) and sentiment analysis of
product reviews (Amazon reviews). CMD achieves a new state-of-the-art
performance on most domain adaptation tasks of Office and outperforms networks
trained with MMD, Variational Fair Autoencoders and Domain Adversarial Neural
Networks on Amazon reviews. In addition, a post-hoc parameter sensitivity
analysis shows that the new approach is stable w.r.t. parameter changes in a
certain interval. The source code of the experiments is publicly available.
",Statistics,Computer Science; Statistics
"A Conic Integer Programming Approach to Constrained Assortment Optimization under the Mixed Multinomial Logit Model   We consider the constrained assortment optimization problem under the mixed
multinomial logit model. Even moderately sized instances of this problem are
challenging to solve directly using standard mixed-integer linear optimization
formulations. This has motivated recent research exploring customized
optimization strategies and approximation techniques. In contrast, we develop a
novel conic quadratic mixed-integer formulation. This new formulation, together
with McCormick inequalities exploiting the capacity constraints, enables the
solution of large instances using commercial optimization software.
",Mathematics,Computer Science
"The Pentagonal Inequality   Given a positive linear combination of five (respectively seven) cosines,
where the angles are positive and sum to pi, the aim of this article is to
express the sharp bound of the combination as a Positive Real Fraction in the
coefficients (hence cosine-free). The method uses algebraic and arithmetic
manipulations with judicious transformations.
",Mathematics,Mathematics
"Non Uniform On Chip Power Delivery Network Synthesis Methodology   In this paper, we proposed a non-uniform power delivery network (PDN)
synthesis methodology. It first constructs initial PDN using uniform approach.
Then preliminary power integrity analysis is performed to derive IR-safe
candidate window. Congestion map is obtained based global route congestion
estimation. A self-adaptive non-uniform PDN synthesis is then performed to
globally and locally optimize PDN over selected regions. The PDN synthesis is
congestion-driven and IR- guarded. Experimental results show significant timing
important in trade-off small PDN length reduction with no EM/IR impact. We
further explored potential power savings using our non-uniform PDN synthesis
methodology.
",Computer Science,Computer Science
"On algebraically integrable domains in Euclidean spaces   Let $D$ be a bounded domain $D$ in $\mathbb R^n $ with infinitely smooth
boundary and $n$ is odd. We prove that if the volume cut off from the domain by
a hyperplane is an algebraic function of the hyperplane, free of real singular
points, then the domain is an ellipsoid. This partially answers a question of
V.I. Arnold: whether odd-dimensional ellipsoids are the only algebraically
integrable domains?
",Mathematics,Mathematics
"Numerical analysis of a nonlinear free-energy diminishing Discrete Duality Finite Volume scheme for convection diffusion equations   We propose a nonlinear Discrete Duality Finite Volume scheme to approximate
the solutions of drift diffusion equations. The scheme is built to preserve at
the discrete level even on severely distorted meshes the energy / energy
dissipation relation. This relation is of paramount importance to capture the
long-time behavior of the problem in an accurate way. To enforce it, the linear
convection diffusion equation is rewritten in a nonlinear form before being
discretized. We establish the existence of positive solutions to the scheme.
Based on compactness arguments, the convergence of the approximate solution
towards a weak solution is established. Finally, we provide numerical evidences
of the good behavior of the scheme when the discretization parameters tend to 0
and when time goes to infinity.
",Mathematics,Mathematics
"The earliest phases of high-mass star formation, as seen in NGC 6334 by \emph{Herschel}   To constrain models of high-mass star formation, the Herschel/HOBYS KP aims
at discovering massive dense cores (MDCs) able to host the high-mass analogs of
low-mass prestellar cores, which have been searched for over the past decade.
We here focus on NGC6334, one of the best-studied HOBYS molecular cloud
complexes.
We used Herschel PACS and SPIRE 70-500mu images of the NGC6334 complex
complemented with (sub)millimeter and mid-infrared data. We built a complete
procedure to extract ~0.1 pc dense cores with the getsources software, which
simultaneously measures their far-infrared to millimeter fluxes. We carefully
estimated the temperatures and masses of these dense cores from their SEDs.
A cross-correlation with high-mass star formation signposts suggests a mass
threshold of 75Msun for MDCs in NGC6334. MDCs have temperatures of 9.5-40K,
masses of 75-1000Msun, and densities of 10^5-10^8cm-3. Their mid-IR emission is
used to separate 6 IR-bright and 10 IR-quiet protostellar MDCs while their 70mu
emission strength, with respect to fitted SEDs, helps identify 16 starless MDC
candidates. The ability of the latter to host high-mass prestellar cores is
investigated here and remains questionable. An increase in mass and density
from the starless to the IR-quiet and IR-bright phases suggests that the
protostars and MDCs simultaneously grow in mass. The statistical lifetimes of
the high-mass prestellar and protostellar core phases, estimated to be
1-7x10^4yr and at most 3x10^5yr respectively, suggest a dynamical scenario of
high-mass star formation.
The present study provides good mass estimates for a statistically
significant sample, covering the earliest phases of high-mass star formation.
High-mass prestellar cores may not exist in NGC6334, favoring a scenario
presented here, which simultaneously forms clouds and high-mass protostars.
",Physics,Physics
"Privacy and Fairness in Recommender Systems via Adversarial Training of User Representations   Latent factor models for recommender systems represent users and items as low
dimensional vectors. Privacy risks of such systems have previously been studied
mostly in the context of recovery of personal information in the form of usage
records from the training data. However, the user representations themselves
may be used together with external data to recover private user information
such as gender and age. In this paper we show that user vectors calculated by a
common recommender system can be exploited in this way. We propose the
privacy-adversarial framework to eliminate such leakage of private information,
and study the trade-off between recommender performance and leakage both
theoretically and empirically using a benchmark dataset. An advantage of the
proposed method is that it also helps guarantee fairness of results, since all
implicit knowledge of a set of attributes is scrubbed from the representations
used by the model, and thus can't enter into the decision making. We discuss
further applications of this method towards the generation of deeper and more
insightful recommendations.
",Statistics,Computer Science
"Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks   Progress in deep learning is slowed by the days or weeks it takes to train
large models. The natural solution of using more hardware is limited by
diminishing returns, and leads to inefficient use of additional resources. In
this paper, we present a large batch, stochastic optimization algorithm that is
both faster than widely used algorithms for fixed amounts of computation, and
also scales up substantially better as more computational resources become
available. Our algorithm implicitly computes the inverse Hessian of each
mini-batch to produce descent directions; we do so without either an explicit
approximation to the Hessian or Hessian-vector products. We demonstrate the
effectiveness of our algorithm by successfully training large ImageNet models
(Inception-V3, Resnet-50, Resnet-101 and Inception-Resnet-V2) with mini-batch
sizes of up to 32000 with no loss in validation error relative to current
baselines, and no increase in the total number of steps. At smaller mini-batch
sizes, our optimizer improves the validation error in these models by 0.8-0.9%.
Alternatively, we can trade off this accuracy to reduce the number of training
steps needed by roughly 10-30%. Our work is practical and easily usable by
others -- only one hyperparameter (learning rate) needs tuning, and
furthermore, the algorithm is as computationally cheap as the commonly used
Adam optimizer.
",Computer Science; Statistics,Computer Science; Statistics
"Why Adaptively Collected Data Have Negative Bias and How to Correct for It   From scientific experiments to online A/B testing, the previously observed
data often affects how future experiments are performed, which in turn affects
which data will be collected. Such adaptivity introduces complex correlations
between the data and the collection procedure. In this paper, we prove that
when the data collection procedure satisfies natural conditions, then sample
means of the data have systematic \emph{negative} biases. As an example,
consider an adaptive clinical trial where additional data points are more
likely to be tested for treatments that show initial promise. Our surprising
result implies that the average observed treatment effects would underestimate
the true effects of each treatment. We quantitatively analyze the magnitude and
behavior of this negative bias in a variety of settings. We also propose a
novel debiasing algorithm based on selective inference techniques. In
experiments, our method can effectively reduce bias and estimation error.
",Computer Science; Statistics,Statistics
"Nonlocal Pertubations of Fractional Choquard Equation   We study the equation \begin{equation} (-\Delta)^{s}u+V(x)u=
(I_{\alpha}*|u|^{p})|u|^{p-2}u+\lambda(I_{\beta}*|u|^{q})|u|^{q-2}u \quad\mbox{
in } \R^{N}, \end{equation} where $I_\gamma(x)=|x|^{-\gamma}$ for any
$\gamma\in (0,N)$, $p, q >0$, $\alpha,\beta\in (0,N)$, $N\geq 3$ and $ \lambda
\in R$. First, the existence of a groundstate solutions using minimization
method on the associated Nehari manifold is obtained. Next, the existence of
least energy sign-changing solutions is investigated by considering the Nehari
nodal set.
",Mathematics,Mathematics
"Mathematics of Isogeny Based Cryptography   These lectures notes were written for a summer school on Mathematics for
post-quantum cryptography in Thiès, Senegal. They try to provide a guide for
Masters' students to get through the vast literature on elliptic curves,
without getting lost on their way to learning isogeny based cryptography. They
are by no means a reference text on the theory of elliptic curves, nor on
cryptography; students are encouraged to complement these notes with some of
the books recommended in the bibliography.
The presentation is divided in three parts, roughly corresponding to the
three lectures given. In an effort to keep the reader interested, each part
alternates between the fundamental theory of elliptic curves, and applications
in cryptography. We often prefer to have the main ideas flow smoothly, rather
than having a rigorous presentation as one would have in a more classical book.
The reader will excuse us for the inaccuracies and the omissions.
",Computer Science,Computer Science
"Amplitude death and resurgence of oscillation in network of mobile oscillators   The phenomenon of amplitude death has been explored using a variety of
different coupling strategies in the last two decades. In most of the work, the
basic coupling arrangement is considered to be static over time, although many
realistic systems exhibit significant changes in the interaction pattern as
time varies. In this article, we study the emergence of amplitude death in a
dynamical network composed of time-varying interaction amidst a collection of
random walkers in a finite region of three dimensional space. We consider an
oscillator for each walker and demonstrate that depending upon the network
parameters and hence the interaction between them, global oscillation in the
network gets suppressed. In this framework, vision range of each oscillator
decides the number of oscillators with which it interacts. In addition, with
the use of an appropriate feedback parameter in the coupling strategy, we
articulate how the suppressed oscillation can be resurrected in the systems'
parameter space. The phenomenon of amplitude death and the resurgence of
oscillation is investigated taking limit cycle and chaotic oscillators for
broad ranges of parameters, like interaction strength k between the entities,
vision range r and the speed of movement v.
",Physics,Physics
"Duality of Graphical Models and Tensor Networks   In this article we show the duality between tensor networks and undirected
graphical models with discrete variables. We study tensor networks on
hypergraphs, which we call tensor hypernetworks. We show that the tensor
hypernetwork on a hypergraph exactly corresponds to the graphical model given
by the dual hypergraph. We translate various notions under duality. For
example, marginalization in a graphical model is dual to contraction in the
tensor network. Algorithms also translate under duality. We show that belief
propagation corresponds to a known algorithm for tensor network contraction.
This article is a reminder that the research areas of graphical models and
tensor networks can benefit from interaction.
",Computer Science; Mathematics; Statistics,Computer Science; Mathematics
"Incremental Eigenpair Computation for Graph Laplacian Matrices: Theory and Applications   The smallest eigenvalues and the associated eigenvectors (i.e., eigenpairs)
of a graph Laplacian matrix have been widely used in spectral clustering and
community detection. However, in real-life applications the number of clusters
or communities (say, $K$) is generally unknown a-priori. Consequently, the
majority of the existing methods either choose $K$ heuristically or they repeat
the clustering method with different choices of $K$ and accept the best
clustering result. The first option, more often, yields suboptimal result,
while the second option is computationally expensive. In this work, we propose
an incremental method for constructing the eigenspectrum of the graph Laplacian
matrix. This method leverages the eigenstructure of graph Laplacian matrix to
obtain the $K$-th smallest eigenpair of the Laplacian matrix given a collection
of all previously computed $K-1$ smallest eigenpairs. Our proposed method
adapts the Laplacian matrix such that the batch eigenvalue decomposition
problem transforms into an efficient sequential leading eigenpair computation
problem. As a practical application, we consider user-guided spectral
clustering. Specifically, we demonstrate that users can utilize the proposed
incremental method for effective eigenpair computation and for determining the
desired number of clusters based on multiple clustering metrics.
",Computer Science; Statistics,Computer Science; Statistics
"Cause-Effect Deep Information Bottleneck For Incomplete Covariates   Estimating the causal effects of an intervention in the presence of
confounding is a frequently occurring problem in applications such as medicine.
The task is challenging since there may be multiple confounding factors, some
of which may be missing, and inferences must be made from high-dimensional,
noisy measurements. In this paper, we propose a decision-theoretic approach to
estimate the causal effects of interventions where a subset of the covariates
is unavailable for some patients during testing. Our approach uses the
information bottleneck principle to perform a discrete, low-dimensional
sufficient reduction of the covariate data to estimate a distribution over
confounders. In doing so, we can estimate the causal effect of an intervention
where only partial covariate information is available. Our results on a causal
inference benchmark and a real application for treating sepsis show that our
method achieves state-of-the-art performance, without sacrificing
interpretability.
",Statistics,Statistics
"Chimera states in complex networks: interplay of fractal topology and delay   Chimera states are an example of intriguing partial synchronization patterns
emerging in networks of identical oscillators. They consist of spatially
coexisting domains of coherent (synchronized) and incoherent (desynchronized)
dynamics. We analyze chimera states in networks of Van der Pol oscillators with
hierarchical connectivities, and elaborate the role of time delay introduced in
the coupling term. In the parameter plane of coupling strength and delay time
we find tongue-like regions of existence of chimera states alternating with
regions of existence of coherent travelling waves. We demonstrate that by
varying the time delay one can deliberately stabilize desired spatio-temporal
patterns in the system.
",Physics,Physics
"Adaptive p-value weighting with power optimality   Weighting the p-values is a well-established strategy that improves the power
of multiple testing procedures while dealing with heterogeneous data. However,
how to achieve this task in an optimal way is rarely considered in the
literature. This paper contributes to fill the gap in the case of
group-structured null hypotheses, by introducing a new class of procedures
named ADDOW (for Adaptive Data Driven Optimal Weighting) that adapts both to
the alternative distribution and to the proportion of true null hypotheses. We
prove the asymptotical FDR control and power optimality among all weighted
procedures of ADDOW, which shows that it dominates all existing procedures in
that framework. Some numerical experiments show that the proposed method
preserves its optimal properties in the finite sample setting when the number
of tests is moderately large.
",Mathematics; Statistics,Mathematics; Statistics
"Coordination game in bidirectional flow   We have introduced evolutionary game dynamics to a one-dimensional
cellular-automaton to investigate evolution and maintenance of cooperative
avoiding behavior of self-driven particles in bidirectional flow. In our model,
there are two kinds of particles, which are right-going particles and
left-going particles. They often face opponent particles, so that they swerve
to the right or left stochastically in order to avoid conflicts. The particles
reinforce their preferences of the swerving direction after their successful
avoidance. The preference is also weakened by memory-loss effect.
Result of our simulation indicates that cooperative avoiding behavior is
achieved, i.e., swerving directions of the particles are unified, when the
density of particles is close to 1/2 and the memory-loss rate is small.
Furthermore, when the right-going particles occupy the majority of the system,
we observe that their flow increases when the number of left-going particles,
which prevent the smooth movement of right-going particles, becomes large. It
is also investigated that the critical memory-loss rate of the cooperative
avoiding behavior strongly depends on the size of the system. Small system can
prolong the cooperative avoiding behavior in wider range of memory-loss rate
than large system.
",Computer Science; Physics,Physics
"Simultaneous Localization and Layout Model Selection in Manhattan Worlds   In this paper, we will demonstrate how Manhattan structure can be exploited
to transform the Simultaneous Localization and Mapping (SLAM) problem, which is
typically solved by a nonlinear optimization over feature positions, into a
model selection problem solved by a convex optimization over higher order
layout structures, namely walls, floors, and ceilings. Furthermore, we show how
our novel formulation leads to an optimization procedure that automatically
performs data association and loop closure and which ultimately produces the
simplest model of the environment that is consistent with the available
measurements. We verify our method on real world data sets collected with
various sensing modalities.
",Computer Science,Computer Science; Statistics
"Studying Magnetic Fields using Low-frequency Pulsar Observations   Low-frequency polarisation observations of pulsars, facilitated by
next-generation radio telescopes, provide powerful probes of astrophysical
plasmas that span many orders of magnitude in magnetic field strength and
scale: from pulsar magnetospheres to intervening magneto-ionic plasmas
including the ISM and the ionosphere. Pulsar magnetospheres with teragauss
field strengths can be explored through their numerous emission phenomena
across multiple frequencies, the mechanism behind which remains elusive.
Precise dispersion and Faraday rotation measurements towards a large number of
pulsars probe the three-dimensional large-scale (and eventually small-scale)
structure of the Galactic magnetic field, which plays a role in many
astrophysical processes, but is not yet well understood, especially towards the
Galactic halo. We describe some results and ongoing work from the Low Frequency
Array (LOFAR) and the Murchison Widefield Array (MWA) radio telescopes in these
areas. These and other pathfinder and precursor telescopes have reinvigorated
low-frequency science and build towards the Square Kilometre Array (SKA), which
will make significant advancements in studies of astrophysical magnetic fields
in the next 50 years.
",Physics,Physics
"Calibrated Boosting-Forest   Excellent ranking power along with well calibrated probability estimates are
needed in many classification tasks. In this paper, we introduce a technique,
Calibrated Boosting-Forest that captures both. This novel technique is an
ensemble of gradient boosting machines that can support both continuous and
binary labels. While offering superior ranking power over any individual
regression or classification model, Calibrated Boosting-Forest is able to
preserve well calibrated posterior probabilities. Along with these benefits, we
provide an alternative to the tedious step of tuning gradient boosting
machines. We demonstrate that tuning Calibrated Boosting-Forest can be reduced
to a simple hyper-parameter selection. We further establish that increasing
this hyper-parameter improves the ranking performance under a diminishing
return. We examine the effectiveness of Calibrated Boosting-Forest on
ligand-based virtual screening where both continuous and binary labels are
available and compare the performance of Calibrated Boosting-Forest with
logistic regression, gradient boosting machine and deep learning. Calibrated
Boosting-Forest achieved an approximately 48% improvement compared to a
state-of-art deep learning model. Moreover, it achieved around 95% improvement
on probability quality measurement compared to the best individual gradient
boosting machine. Calibrated Boosting-Forest offers a benchmark demonstration
that in the field of ligand-based virtual screening, deep learning is not the
universally dominant machine learning model and good calibrated probabilities
can better facilitate virtual screening process.
",Computer Science; Statistics,Statistics
"Herschel survey and modelling of externally-illuminated photoevaporating protoplanetary disks   Protoplanetary disks undergo substantial mass-loss by photoevaporation, a
mechanism which is crucial to their dynamical evolution. However, the processes
regulating the gas energetics have not been well constrained by observations so
far. We aim at studying the processes involved in disk photoevaporation when it
is driven by far-UV photons. We present a unique Herschel survey and new ALMA
observations of four externally-illuminated photoevaporating disks (a.k.a.
proplyds). For the analysis of these data, we developed a 1D model of the
photodissociation region (PDR) of a proplyd, based on the Meudon PDR code and
computed the far infrared line emission. We successfully reproduce most of the
observations and derive key physical parameters, i.e. densities at the disk
surface of about $10^{6}$ cm$^{-3}$ and local gas temperatures of about 1000 K.
Our modelling suggests that all studied disks are found in a transitional
regime resulting from the interplay between several heating and cooling
processes that we identify. These differ from those dominating in classical
PDRs, i.e. grain photo-electric effect and cooling by [OI] and [CII] FIR lines.
This energetic regime is associated to an equilibrium dynamical point of the
photoevaporation flow: the mass-loss rate is self-regulated to set the envelope
column density at a value that maintains the temperature at the disk surface
around 1000 K. From our best-fit models, we estimate mass-loss rates - of the
order of $10^{-7}$ $\mathrm{M}_\odot$/yr - that are in agreement with earlier
spectroscopic observation of ionised gas tracers. This holds only if we assume
an evaporation flow launched from the disk surface at sound speed
(supercritical regime). We have identified the energetic regime regulating
FUV-photoevaporation in proplyds. This regime could be implemented into models
of the dynamical evolution of protoplanetary disks.
",Physics,Physics
"Exact Simulation of the Extrema of Stable Processes   We exhibit an exact simulation algorithm for the supremum of a stable process
over a finite time interval using dominated coupling from the past (DCFTP). We
establish a novel perpetuity equation for the supremum (via the representation
of the concave majorants of Lévy processes) and apply it to construct a
Markov chain in the DCFTP algorithm. We prove that the number of steps taken
backwards in time before the coalescence is detected is finite.
",Statistics,Mathematics
"Multi-task Learning with Gradient Guided Policy Specialization   We present a method for efficient learning of control policies for multiple
related robotic motor skills. Our approach consists of two stages, joint
training and specialization training. During the joint training stage, a neural
network policy is trained with minimal information to disambiguate the motor
skills. This forces the policy to learn a common representation of the
different tasks. Then, during the specialization training stage we selectively
split the weights of the policy based on a per-weight metric that measures the
disagreement among the multiple tasks. By splitting part of the control policy,
it can be further trained to specialize to each task. To update the control
policy during learning, we use Trust Region Policy Optimization with
Generalized Advantage Function (TRPOGAE). We propose a modification to the
gradient update stage of TRPO to better accommodate multi-task learning
scenarios. We evaluate our approach on three continuous motor skill learning
problems in simulation: 1) a locomotion task where three single legged robots
with considerable difference in shape and size are trained to hop forward, 2) a
manipulation task where three robot manipulators with different sizes and joint
types are trained to reach different locations in 3D space, and 3) locomotion
of a two-legged robot, whose range of motion of one leg is constrained in
different ways. We compare our training method to three baselines. The first
baseline uses only joint training for the policy, the second trains independent
policies for each task, and the last randomly selects weights to split. We show
that our approach learns more efficiently than each of the baseline methods.
",Computer Science,Computer Science
"Deep generative models of genetic variation capture mutation effects   The functions of proteins and RNAs are determined by a myriad of interactions
between their constituent residues, but most quantitative models of how
molecular phenotype depends on genotype must approximate this by simple
additive effects. While recent models have relaxed this constraint to also
account for pairwise interactions, these approaches do not provide a tractable
path towards modeling higher-order dependencies. Here, we show how latent
variable models with nonlinear dependencies can be applied to capture
beyond-pairwise constraints in biomolecules. We present a new probabilistic
model for sequence families, DeepSequence, that can predict the effects of
mutations across a variety of deep mutational scanning experiments
significantly better than site independent or pairwise models that are based on
the same evolutionary data. The model, learned in an unsupervised manner solely
from sequence information, is grounded with biologically motivated priors,
reveals latent organization of sequence families, and can be used to
extrapolate to new parts of sequence space
",Physics; Statistics,Quantitative Biology
"Uniform Shapiro-Lopatinski conditions and boundary value problems on manifolds with bounded geometry   We study the regularity of the solutions of second order boundary value
problems on manifolds with boundary and bounded geometry. We first show that
the regularity property of a given boundary value problem $(P, C)$ is
equivalent to the uniform regularity of the natural family $(P_x, C_x)$ of
associated boundary value problems in local coordinates. We verify that this
property is satisfied for the Dirichlet boundary conditions and strongly
elliptic operators via a compactness argument. We then introduce a uniform
Shapiro-Lopatinski regularity condition, which is a modification of the
classical one, and we prove that it characterizes the boundary value problems
that satisfy the usual regularity property. We also show that the natural Robin
boundary conditions always satisfy the uniform Shapiro-Lopatinski regularity
condition, provided that our operator satisfies the strong Legendre condition.
This is achieved by proving that ""well-posedness implies regularity"" via a
modification of the classical ""Nirenberg trick"". When combining our regularity
results with the Poincaré inequality of (Ammann-Grosse-Nistor, preprint
2015), one obtains the usual well-posedness results for the classical boundary
value problems in the usual scale of Sobolev spaces, thus extending these
important, well-known theorems from smooth, bounded domains, to manifolds with
boundary and bounded geometry. As we show in several examples, these results do
not hold true anymore if one drops the bounded geometry assumption. We also
introduce a uniform Agmon condition and show that it is equivalent to the
coerciveness. Consequently, we prove a well-posedness result for parabolic
equations whose elliptic generator satisfies the uniform Agmon condition.
",Mathematics,Mathematics
"Critical exponent $ω$ in the Gross-Neveu-Yukawa model at $O(1/N)$   The critcal exponent $\omega$ is evaluated at $O(1/N)$ in $d$-dimensions in
the Gross-Neveu model using the large $N$ critical point formalism. It is shown
to be in agreement with the recently determined three loop $\beta$-functions of
the Gross-Neveu-Yukawa model in four dimensions. The same exponent is computed
for the chiral Gross-Neveu and non-abelian Nambu-Jona-Lasinio universality
classes.
",Physics,Mathematics
"A Loop-Based Methodology for Reducing Computational Redundancy in Workload Sets   The design of general purpose processors relies heavily on a workload
gathering step in which representative programs are collected from various
application domains. Processor performance, when running the workload set, is
profiled using simulators that model the targeted processor architecture.
However, simulating the entire workload set is prohibitively time-consuming,
which precludes considering a large number of programs. To reduce simulation
time, several techniques in the literature have exploited the internal program
repetitiveness to extract and execute only representative code segments.
Existing so- lutions are based on reducing cross-program computational
redundancy or on eliminating internal-program redundancy to decrease execution
time. In this work, we propose an orthogonal and complementary loop- centric
methodology that targets loop-dominant programs by exploiting internal-program
characteristics to reduce cross-program computational redundancy. The approach
employs a newly developed framework that extracts and analyzes core loops
within workloads. The collected characteristics model memory behavior,
computational complexity, and data structures of a program, and are used to
construct a signature vector for each program. From these vectors,
cross-workload similarity metrics are extracted, which are processed by a novel
heuristic to exclude similar programs and reduce redundancy within the set.
Finally, a reverse engineering approach that synthesizes executable
micro-benchmarks having the same instruction mix as the loops in the original
workload is introduced. A tool that automates the flow steps of the proposed
methodology is developed. Simulation results demonstrate that applying the
proposed methodology to a set of workloads reduces the set size by half, while
preserving the main characterizations of the initial workloads.
",Computer Science,Computer Science
"Continuous-Time Gaussian Process Motion Planning via Probabilistic Inference   We introduce a novel formulation of motion planning, for continuous-time
trajectories, as probabilistic inference. We first show how smooth
continuous-time trajectories can be represented by a small number of states
using sparse Gaussian process (GP) models. We next develop an efficient
gradient-based optimization algorithm that exploits this sparsity and GP
interpolation. We call this algorithm the Gaussian Process Motion Planner
(GPMP). We then detail how motion planning problems can be formulated as
probabilistic inference on a factor graph. This forms the basis for GPMP2, a
very efficient algorithm that combines GP representations of trajectories with
fast, structure-exploiting inference via numerical optimization. Finally, we
extend GPMP2 to an incremental algorithm, iGPMP2, that can efficiently replan
when conditions change. We benchmark our algorithms against several
sampling-based and trajectory optimization-based motion planning algorithms on
planning problems in multiple environments. Our evaluation reveals that GPMP2
is several times faster than previous algorithms while retaining robustness. We
also benchmark iGPMP2 on replanning problems, and show that it can find
successful solutions in a fraction of the time required by GPMP2 to replan from
scratch.
",Computer Science,Statistics
"A Formal Approach to Exploiting Multi-Stage Attacks based on File-System Vulnerabilities of Web Applications (Extended Version)   Web applications require access to the file-system for many different tasks.
When analyzing the security of a web application, secu- rity analysts should
thus consider the impact that file-system operations have on the security of
the whole application. Moreover, the analysis should take into consideration
how file-system vulnerabilities might in- teract with other vulnerabilities
leading an attacker to breach into the web application. In this paper, we first
propose a classification of file- system vulnerabilities, and then, based on
this classification, we present a formal approach that allows one to exploit
file-system vulnerabilities. We give a formal representation of web
applications, databases and file- systems, and show how to reason about
file-system vulnerabilities. We also show how to combine file-system
vulnerabilities and SQL-Injection vulnerabilities for the identification of
complex, multi-stage attacks. We have developed an automatic tool that
implements our approach and we show its efficiency by discussing several
real-world case studies, which are witness to the fact that our tool can
generate, and exploit, complex attacks that, to the best of our knowledge, no
other state-of-the-art-tool for the security of web applications can find.
",Computer Science,Computer Science
"Multi-Lane Perception Using Feature Fusion Based on GraphSLAM   An extensive, precise and robust recognition and modeling of the environment
is a key factor for next generations of Advanced Driver Assistance Systems and
development of autonomous vehicles. In this paper, a real-time approach for the
perception of multiple lanes on highways is proposed. Lane markings detected by
camera systems and observations of other traffic participants provide the input
data for the algorithm. The information is accumulated and fused using
GraphSLAM and the result constitutes the basis for a multilane clothoid model.
To allow incorporation of additional information sources, input data is
processed in a generic format. Evaluation of the method is performed by
comparing real data, collected with an experimental vehicle on highways, to a
ground truth map. The results show that ego and adjacent lanes are robustly
detected with high quality up to a distance of 120 m. In comparison to serial
lane detection, an increase in the detection range of the ego lane and a
continuous perception of neighboring lanes is achieved. The method can
potentially be utilized for the longitudinal and lateral control of
self-driving vehicles.
",Computer Science,Computer Science
"A taxonomy of learning dynamics in 2 x 2 games   Learning would be a convincing method to achieve coordination on an
equilibrium. But does learning converge, and to what? We answer this question
in generic 2-player, 2-strategy games, using Experience-Weighted Attraction
(EWA), which encompasses many extensively studied learning algorithms. We
exhaustively characterize the parameter space of EWA learning, for any payoff
matrix, and we understand the generic properties that imply convergent or
non-convergent behaviour in 2 x 2 games.
Irrational choice and lack of incentives imply convergence to a mixed
strategy in the centre of the strategy simplex, possibly far from the Nash
Equilibrium (NE). In the opposite limit, in which the players quickly modify
their strategies, the behaviour depends on the payoff matrix: (i) a strong
discrepancy between the pure strategies describes dominance-solvable games,
which show convergence to a unique fixed point close to the NE; (ii) a
preference towards profiles of strategies along the main diagonal describes
coordination games, with multiple stable fixed points corresponding to the NE;
(iii) a cycle of best responses defines discoordination games, which commonly
yield limit cycles or low-dimensional chaos.
While it is well known that mixed strategy equilibria may be unstable, our
approach is novel from several perspectives: we fully analyse EWA and provide
explicit thresholds that define the onset of instability; we find an emerging
taxonomy of the learning dynamics, without focusing on specific classes of
games ex-ante; we show that chaos can occur even in the simplest games; we make
a precise theoretical prediction that can be tested against data on
experimental learning of discoordination games.
",Physics,Computer Science; Statistics
"Nikol'ski\uı, Jackson and Ul'yanov type inequalities with Muckenhoupt weights   In the present work we prove a Nikol'ski inequality for trigonometric
polynomials and Ul'yanov type inequalities for functions in Lebesgue spaces
with Muckenhoupt weights. Realization result and Jackson inequalities are
obtained. Simultaneous approximation by polynomials is considered. Some uniform
norm inequalities are transferred to weighted Lebesgue space.
",Mathematics,Mathematics
"Global behaviour of radially symmetric solutions stable at infinity for gradient systems   This paper is concerned with radially symmetric solutions of systems of the
form \[ u_t = -\nabla V(u) + \Delta_x u \] where space variable $x$ and and
state-parameter $u$ are multidimensional, and the potential $V$ is coercive at
infinity. For such systems, under generic assumptions on the potential, the
asymptotic behaviour of solutions ""stable at infinity"", that is approaching a
spatially homogeneous equilibrium when $|x|$ approaches $+\infty$, is
investigated. It is proved that every such solutions approaches a stacked
family of radially symmetric bistable fronts travelling to infinity. This
behaviour is similar to the one of bistable solutions for gradient systems in
one unbounded spatial dimension, described in a companion paper. It is expected
(but unfortunately not proved at this stage) that behind these travelling
fronts the solution again behaves as in the one-dimensional case (that is, the
time derivative approaches zero and the solution approaches a pattern of
stationary solutions).
",Mathematics,Mathematics
"Noisy Networks for Exploration   We introduce NoisyNet, a deep reinforcement learning agent with parametric
noise added to its weights, and show that the induced stochasticity of the
agent's policy can be used to aid efficient exploration. The parameters of the
noise are learned with gradient descent along with the remaining network
weights. NoisyNet is straightforward to implement and adds little computational
overhead. We find that replacing the conventional exploration heuristics for
A3C, DQN and dueling agents (entropy reward and $\epsilon$-greedy respectively)
with NoisyNet yields substantially higher scores for a wide range of Atari
games, in some cases advancing the agent from sub to super-human performance.
",Computer Science; Statistics,Computer Science; Statistics
"VSE++: Improving Visual-Semantic Embeddings with Hard Negatives   We present a new technique for learning visual-semantic embeddings for
cross-modal retrieval. Inspired by hard negative mining, the use of hard
negatives in structured prediction, and ranking loss functions, we introduce a
simple change to common loss functions used for multi-modal embeddings. That,
combined with fine-tuning and use of augmented data, yields significant gains
in retrieval performance. We showcase our approach, VSE++, on MS-COCO and
Flickr30K datasets, using ablation studies and comparisons with existing
methods. On MS-COCO our approach outperforms state-of-the-art methods by 8.8%
in caption retrieval and 11.3% in image retrieval (at R@1).
",Computer Science,Computer Science
"Cepheids with the eyes of photometric space telescopes   Space photometric missions have been steadily accumulating observations of
Cepheids in recent years, leading to a flow of new discoveries. In this short
review we summarize the findings provided by the early missions such as WIRE,
MOST, and CoRoT, and the recent results of the Kepler and K2 missions. The
surprising and fascinating results from the high-precision, quasi-continuous
data include the detection of the amplitude increase of Polaris, and exquisite
details about V1154 Cyg within the original Kepler field of view. We also
briefly discuss the current opportunities with the K2 mission, and the
prospects of the TESS space telescope regarding Cepheids.
",Physics,Physics
"Recovering Sparse Nonnegative Signals via Non-convex Fraction Function Penalty   Many real world practical problems can be formulated as
$\ell_{0}$-minimization problems with nonnegativity constraints, which seek the
sparsest nonnegative signals to underdetermined linear systems. They have been
widely applied in signal and image processing, machine learning, pattern
recognition and computer vision. Unfortunately, this $\ell_{0}$-minimization
problem with nonnegativity constraint is computational and NP-hard because of
the discrete and discontinuous nature of the $\ell_{0}$-norm. In this paper, we
replace the $\ell_{0}$-norm with a non-convex fraction function, and study the
minimization problem of this non-convex fraction function in recovering the
sparse nonnegative signals from an underdetermined linear system. Firstly, we
discuss the equivalence between $(P_{0}^{\geq})$ and $(FP_{a}^{\geq})$, and the
equivalence between $(FP_{a}^{\geq})$ and $(FP_{a,\lambda}^{\geq})$. It is
proved that the optimal solution of the problem $(P_{0}^{\geq})$ could be
approximately obtained by solving the regularization problem
$(FP_{a,\lambda}^{\geq})$ if some specific conditions satisfied. Secondly, we
propose a nonnegative iterative thresholding algorithm to solve the
regularization problem $(FP_{a,\lambda}^{\geq})$ for all $a>0$. Finally, some
numerical experiments on sparse nonnegative siganl recovery problems show that
our method performs effective in finding sparse nonnegative signals compared
with the linear programming.
",Mathematics,Computer Science; Statistics
"A connection between MAX $κ$-CUT and the inhomogeneous Potts spin glass in the large degree limit   We study the asymptotic behavior of the Max $\kappa$-cut on a family of
sparse, inhomogeneous random graphs. In the large degree limit, the leading
term is a variational problem, involving the ground state of a constrained
inhomogeneous Potts spin glass. We derive a Parisi type formula for the free
energy of this model, with possible constraints on the proportions, and derive
the limiting ground state energy by a suitable zero temperature limit.
",Mathematics,Mathematics
"Delayed avalanches in Multi-Pixel Photon Counters   Hamamatsu Photonics introduced a new generation of their Multi-Pixel Photon
Counters in 2013 with significantly reduced after-pulsing rate. In this paper,
we investigate the causes of after-pulsing by testing pre-2013 and post-2013
devices using laser light ranging from 405 to 820nm. Doing so we investigate
the possibility that afterpulsing is also due to optical photons produced in
the avalanche rather than to impurities trapping charged carriers produced in
the avalanches and releasing them at a later time. For pre-2013 devices, we
observe avalanches delayed by ns to several 100~ns at 637, 777nm and 820 nm
demonstrating that holes created in the zero field region of the silicon bulk
can diffuse back to the high field region triggering delayed avalanches. On the
other hand post-2013 exhibit no delayed avalanches beyond 100~ns at 777nm. We
also confirm that post-2013 devices exhibit about 25 times lower after-pulsing.
Taken together, our measurements show that the absorption of photons from the
avalanche in the bulk of the silicon and the subsequent hole diffusion back to
the junction was a significant source of after-pulse for the pre-2013 devices.
Hamamatsu appears to have fixed this problem in 2013 following the preliminary
release of our results. We also show that even at short wavelength the timing
distribution exhibit tails in the sub-nanosecond range that may impair the MPPC
timing performances.
",Physics,Physics
"Machine Assisted Analysis of Vowel Length Contrasts in Wolof   Growing digital archives and improving algorithms for automatic analysis of
text and speech create new research opportunities for fundamental research in
phonetics. Such empirical approaches allow statistical evaluation of a much
larger set of hypothesis about phonetic variation and its conditioning factors
(among them geographical / dialectal variants). This paper illustrates this
vision and proposes to challenge automatic methods for the analysis of a not
easily observable phenomenon: vowel length contrast. We focus on Wolof, an
under-resourced language from Sub-Saharan Africa. In particular, we propose
multiple features to make a fine evaluation of the degree of length contrast
under different factors such as: read vs semi spontaneous speech ; standard vs
dialectal Wolof. Our measures made fully automatically on more than 20k vowel
tokens show that our proposed features can highlight different degrees of
contrast for each vowel considered. We notably show that contrast is weaker in
semi-spontaneous speech and in a non standard semi-spontaneous dialect.
",Computer Science,Computer Science
"Meta learning Framework for Automated Driving   The success of automated driving deployment is highly depending on the
ability to develop an efficient and safe driving policy. The problem is well
formulated under the framework of optimal control as a cost optimization
problem. Model based solutions using traditional planning are efficient, but
require the knowledge of the environment model. On the other hand, model free
solutions suffer sample inefficiency and require too many interactions with the
environment, which is infeasible in practice. Methods under the Reinforcement
Learning framework usually require the notion of a reward function, which is
not available in the real world. Imitation learning helps in improving sample
efficiency by introducing prior knowledge obtained from the demonstrated
behavior, on the risk of exact behavior cloning without generalizing to unseen
environments. In this paper we propose a Meta learning framework, based on data
set aggregation, to improve generalization of imitation learning algorithms.
Under the proposed framework, we propose MetaDAgger, a novel algorithm which
tackles the generalization issues in traditional imitation learning. We use The
Open Race Car Simulator (TORCS) to test our algorithm. Results on unseen test
tracks show significant improvement over traditional imitation learning
algorithms, improving the learning time and sample efficiency in the same time.
The results are also supported by visualization of the learnt features to prove
generalization of the captured details.
",Computer Science; Statistics,Computer Science; Statistics
"Global and local thermometry schemes in coupled quantum systems   We study the ultimate bounds on the estimation of temperature for an
interacting quantum system. We consider two coupled bosonic modes that are
assumed to be thermal and using quantum estimation theory establish the role
the Hamiltonian parameters play in thermometry. We show that in the case of a
conserved particle number the interaction between the modes leads to a decrease
in the overall sensitivity to temperature, while interestingly, if particle
exchange is allowed with the thermal bath the converse is true. We explain this
dichotomy by examining the energy spectra. Finally, we devise experimentally
implementable thermometry schemes that rely only on locally accessible
information from the total system, showing that almost Heisenberg limited
precision can still be achieved, and we address the (im)possibility for
multiparameter estimation in the system.
",Physics,Physics
"Analysis of Dropout in Online Learning   Deep learning is the state-of-the-art in fields such as visual object
recognition and speech recognition. This learning uses a large number of layers
and a huge number of units and connections. Therefore, overfitting is a serious
problem with it, and the dropout which is a kind of regularization tool is
used. However, in online learning, the effect of dropout is not well known.
This paper presents our investigation on the effect of dropout in online
learning. We analyzed the effect of dropout on convergence speed near the
singular point. Our results indicated that dropout is effective in online
learning. Dropout tends to avoid the singular point for convergence speed near
that point.
",Computer Science; Statistics,Computer Science
"A note on conditional covariance matrices for elliptical distributions   In this short note we provide an analytical formula for the conditional
covariance matrices of the elliptically distributed random vectors, when the
conditioning is based on the values of any linear combination of the marginal
random variables. We show that one could introduce the univariate invariant
depending solely on the conditioning set, which greatly simplifies the
calculations. As an application, we show that one could define uniquely defined
quantile-based sets on which conditional covariance matrices must be equal to
each other if only the vector is multivariate normal. The similar results are
obtained for conditional correlation matrices of the general elliptic case.
",Mathematics; Statistics,Mathematics; Statistics
"Controllability and optimal control of the transport equation with a localized vector field   We study controllability of a Partial Differential Equation of transport
type, that arises in crowd models. We are interested in controlling such system
with a control being a Lipschitz vector field on a fixed control set $\omega$.
We prove that, for each initial and final configuration, one can steer one to
another with such class of controls only if the uncontrolled dynamics allows to
cross the control set $\omega$. We also prove a minimal time result for such
systems. We show that the minimal time to steer one initial configuration to
another is related to the condition of having enough mass in $\omega$ to feed
the desired final configuration.
",Mathematics,Mathematics
"Irreducible characters with bounded root Artin conductor   In this work, we prove that the growth of the Artin conductor is at most,
exponential in the degree of the character.
",Mathematics,Mathematics
"Mining Application-aware Community Organization with Expanded Feature Subspaces from Concerned Attributes in Social Networks   Social networks are typical attributed networks with node attributes.
Different from traditional attribute community detection problem aiming at
obtaining the whole set of communities in the network, we study an
application-oriented problem of mining an application-aware community
organization with respect to specific concerned attributes. The concerned
attributes are designated based on the requirements of any application by a
user in advance. The application-aware community organization w.r.t. concerned
attributes consists of the communities with feature subspaces containing these
concerned attributes. Besides concerned attributes, feature subspace of each
required community may contain some other relevant attributes. All relevant
attributes of a feature subspace jointly describe and determine the community
embedded in such subspace. Thus the problem includes two subproblems, i.e., how
to expand the set of concerned attributes to complete feature subspaces and how
to mine the communities embedded in the expanded subspaces. Two subproblems are
jointly solved by optimizing a quality function called subspace fitness. An
algorithm called ACM is proposed. In order to locate the communities
potentially belonging to the application-aware community organization, cohesive
parts of a network backbone composed of nodes with similar concerned attributes
are detected and set as the community seeds. The set of concerned attributes is
set as the initial subspace for all community seeds. Then each community seed
and its attribute subspace are adjusted iteratively to optimize the subspace
fitness. Extensive experiments on synthetic datasets demonstrate the
effectiveness and efficiency of our method and applications on real-world
networks show its application values.
",Computer Science; Physics,Computer Science
"Higher Tetrahedral Algebras   We introduce and study the higher tetrahedral algebras, an exotic family of
finite-dimensional tame symmetric algebras over an algebraically closed field.
The Gabriel quiver of such an algebra is the triangulation quiver associated to
the coherent orientation of the tetrahedron. Surprisingly, these algebras
occurred in the classification of all algebras of generalised quaternion type,
but are not weighted surface algebras. We prove that a higher tetrahedral
algebra is periodic if and only if it is non-singular.
",Mathematics,Mathematics
"GraphGAN: Graph Representation Learning with Generative Adversarial Nets   The goal of graph representation learning is to embed each vertex in a graph
into a low-dimensional vector space. Existing graph representation learning
methods can be classified into two categories: generative models that learn the
underlying connectivity distribution in the graph, and discriminative models
that predict the probability of edge existence between a pair of vertices. In
this paper, we propose GraphGAN, an innovative graph representation learning
framework unifying above two classes of methods, in which the generative model
and discriminative model play a game-theoretical minimax game. Specifically,
for a given vertex, the generative model tries to fit its underlying true
connectivity distribution over all other vertices and produces ""fake"" samples
to fool the discriminative model, while the discriminative model tries to
detect whether the sampled vertex is from ground truth or generated by the
generative model. With the competition between these two models, both of them
can alternately and iteratively boost their performance. Moreover, when
considering the implementation of generative model, we propose a novel graph
softmax to overcome the limitations of traditional softmax function, which can
be proven satisfying desirable properties of normalization, graph structure
awareness, and computational efficiency. Through extensive experiments on
real-world datasets, we demonstrate that GraphGAN achieves substantial gains in
a variety of applications, including link prediction, node classification, and
recommendation, over state-of-the-art baselines.
",Computer Science; Statistics,Computer Science; Statistics
"Direct Evidence of Spontaneous Abrikosov Vortex State in Ferromagnetic Superconductor EuFe$_2$(As$_{1-x}$P$_x$)$_2$ with $x=0.21$   Using low-temperature Magnetic Force Microscopy (MFM) we provide direct
experimental evidence for spontaneous vortex phase (SVP) formation in
EuFe$_2$(As$_{0.79}$P$_{0.21}$)$_2$ single crystal with the superconducting
$T^{\rm 0}_{\rm SC}=23.6$~K and ferromagnetic $T_{\rm FM}\sim17.7$~K transition
temperatures. Spontaneous vortex-antivortex (V-AV) pairs are imaged in the
vicinity of $T_{\rm FM}$. Also, upon cooling cycle near $T_{\rm FM}$ we observe
the first-order transition from the short period domain structure, which
appears in the Meissner state, into the long period domain structure with
spontaneous vortices. It is the first experimental observation of this scenario
in the ferromagnetic superconductors. Low-temperature phase is characterized by
much larger domains in V-AV state and peculiar branched striped structures at
the surface, which are typical for uniaxial ferromagnets with perpendicular
magnetic anisotropy (PMA). The domain wall parameters at various temperatures
are estimated.
",Physics,Physics
"Reconstruction via the intrinsic geometric structures of interior transmission eigenfunctions   We are concerned with the inverse scattering problem of extracting the
geometric structures of an unknown/inaccessible inhomogeneous medium by using
the corresponding acoustic far-field measurement. Using the intrinsic geometric
properties of the so-called interior transmission eigenfunctions, we develop a
novel inverse scattering scheme. The proposed method can efficiently capture
the cusp singularities of the support of the inhomogeneous medium. If further a
priori information is available on the support of the medium, say, it is a
convex polyhedron, then one can actually recover its shape. Both theoretical
analysis and numerical experiments are provided. Our reconstruction method is
new to the literature and opens up a new direction in the study of inverse
scattering problems.
",Mathematics,Physics
"Conceptual Frameworks for Building Online Citizen Science Projects   In recent years, citizen science has grown in popularity due to a number of
reasons, including the emphasis on informal learning and creativity potential
associated with these initiatives. Citizen science projects address research
questions from various domains, ranging from Ecology to Astronomy. Due to the
advancement of communication technologies, which makes outreach and engagement
of wider communities easier, scientists are keen to turn their own research
into citizen science projects. However, the development, deployment and
management of these projects remains challenging. One of the most important
challenges is building the project itself. There is no single tool or
framework, which guides the step-by-step development of the project, since
every project has specific characteristics, such as geographical constraints or
volunteers' mode of participation. Therefore, in this article, we present a
series of conceptual frameworks for categorisation, decision and deployment,
which guide a citizen science project creator in every step of creating a new
project starting from the research question to project deployment. The
frameworks are designed with consideration to the properties of already
existing citizen science projects and could be easily extended to include other
dimensions, which are not currently perceived.
",Computer Science,Computer Science
"Scalable Bayesian shrinkage and uncertainty quantification in high-dimensional regression   Bayesian shrinkage methods have generated a lot of recent interest as tools
for high-dimensional regression and model selection. These methods naturally
facilitate tractable uncertainty quantification and incorporation of prior
information. A common feature of these models, including the Bayesian lasso,
global-local shrinkage priors, and spike-and-slab priors is that the
corresponding priors on the regression coefficients can be expressed as scale
mixture of normals. While the three-step Gibbs sampler used to sample from the
often intractable associated posterior density has been shown to be
geometrically ergodic for several of these models (Khare and Hobert, 2013; Pal
and Khare, 2014), it has been demonstrated recently that convergence of this
sampler can still be quite slow in modern high-dimensional settings despite
this apparent theoretical safeguard. We propose a new method to draw from the
same posterior via a tractable two-step blocked Gibbs sampler. We demonstrate
that our proposed two-step blocked sampler exhibits vastly superior convergence
behavior compared to the original three- step sampler in high-dimensional
regimes on both real and simulated data. We also provide a detailed theoretical
underpinning to the new method in the context of the Bayesian lasso. First, we
derive explicit upper bounds for the (geometric) rate of convergence.
Furthermore, we demonstrate theoretically that while the original Bayesian
lasso chain is not Hilbert-Schmidt, the proposed chain is trace class (and
hence Hilbert-Schmidt). The trace class property has useful theoretical and
practical implications. It implies that the corresponding Markov operator is
compact, and its eigenvalues are summable. It also facilitates a rigorous
comparison of the two-step blocked chain with ""sandwich"" algorithms which aim
to improve performance of the two-step chain by inserting an inexpensive extra
step.
",Statistics,Statistics
"Vertex algebras associated with hypertoric varieties   We construct a family of vertex algebras associated with a family of
symplectic singularity/resolution, called hypertoric varieties. While the
hypertoric varieties are constructed by a certain Hamiltonian reduction
associated with a torus action, our vertex algebras are constructed by
(semi-infinite) BRST reduction. The construction works algebro-geometrically
and we construct sheaves of $\hbar$-adic vertex algebras over hypertoric
varieties which localize the vertex algebras. We show when the vertex algebras
are vertex operator algebras by giving explicit conformal vectors. We also show
that the Zhu algebras of the vertex algebras, associative algebras associated
with non-negatively graded vertex algebras, gives a certain family of filtered
quantizations of the coordinate rings of the hypertoric varieties.
",Mathematics,Mathematics
"Joint Task Offloading and Resource Allocation for Multi-Server Mobile-Edge Computing Networks   Mobile-Edge Computing (MEC) is an emerging paradigm that provides a capillary
distribution of cloud computing capabilities to the edge of the wireless access
network, enabling rich services and applications in close proximity to the end
users. In this article, a MEC enabled multi-cell wireless network is considered
where each Base Station (BS) is equipped with a MEC server that can assist
mobile users in executing computation-intensive tasks via task offloading. The
problem of Joint Task Offloading and Resource Allocation (JTORA) is studied in
order to maximize the users' task offloading gains, which is measured by the
reduction in task completion time and energy consumption. The considered
problem is formulated as a Mixed Integer Non-linear Program (MINLP) that
involves jointly optimizing the task offloading decision, uplink transmission
power of mobile users, and computing resource allocation at the MEC servers.
Due to the NP-hardness of this problem, solving for optimal solution is
difficult and impractical for a large-scale network. To overcome this drawback,
our approach is to decompose the original problem into (i) a Resource
Allocation (RA) problem with fixed task offloading decision and (ii) a Task
Offloading (TO) problem that optimizes the optimal-value function corresponding
to the RA problem. We address the RA problem using convex and quasi-convex
optimization techniques, and propose a novel heuristic algorithm to the TO
problem that achieves a suboptimal solution in polynomial time. Numerical
simulation results show that our algorithm performs closely to the optimal
solution and that it significantly improves the users' offloading utility over
traditional approaches.
",Computer Science,Computer Science
"Threat analysis of IoT networks Using Artificial Neural Network Intrusion Detection System   The Internet of things (IoT) is still in its infancy and has attracted much
interest in many industrial sectors including medical fields, logistics
tracking, smart cities and automobiles. However as a paradigm, it is
susceptible to a range of significant intrusion threats. This paper presents a
threat analysis of the IoT and uses an Artificial Neural Network (ANN) to
combat these threats. A multi-level perceptron, a type of supervised ANN, is
trained using internet packet traces, then is assessed on its ability to thwart
Distributed Denial of Service (DDoS/DoS) attacks. This paper focuses on the
classification of normal and threat patterns on an IoT Network. The ANN
procedure is validated against a simulated IoT network. The experimental
results demonstrate 99.4% accuracy and can successfully detect various DDoS/DoS
attacks.
",Computer Science,Computer Science
"Topology and experimental distinguishability   In this work we introduce the idea that the primary application of topology
in experimental sciences is to keep track of what can be distinguished through
experimentation. This link provides understanding and justification as to why
topological spaces and continuous functions are pervasive tools in science. We
first define an experimental observation as a statement that can be verified
using an experimental procedure and show that observations are closed under
finite conjunction and countable disjunction. We then consider observations
that identify elements within a set and show how they induce a Hausdorff and
second-countable topology on that set, thus identifying an open set as one that
can be associated with an experimental observation. We then show that
experimental relationships are continuous functions, as they must preserve
experimental distinguishability, and that they are themselves experimentally
distinguishable by defining a Hausdorff and second-countable topology for this
collection.
",Mathematics,Physics
"Inter-Subject Analysis: Inferring Sparse Interactions with Dense Intra-Graphs   We develop a new modeling framework for Inter-Subject Analysis (ISA). The
goal of ISA is to explore the dependency structure between different subjects
with the intra-subject dependency as nuisance. It has important applications in
neuroscience to explore the functional connectivity between brain regions under
natural stimuli. Our framework is based on the Gaussian graphical models, under
which ISA can be converted to the problem of estimation and inference of the
inter-subject precision matrix. The main statistical challenge is that we do
not impose sparsity constraint on the whole precision matrix and we only assume
the inter-subject part is sparse. For estimation, we propose to estimate an
alternative parameter to get around the non-sparse issue and it can achieve
asymptotic consistency even if the intra-subject dependency is dense. For
inference, we propose an ""untangle and chord"" procedure to de-bias our
estimator. It is valid without the sparsity assumption on the inverse Hessian
of the log-likelihood function. This inferential method is general and can be
applied to many other statistical problems, thus it is of independent
theoretical interest. Numerical experiments on both simulated and brain imaging
data validate our methods and theory.
",Mathematics; Statistics,Statistics
"Spatial Models with the Integrated Nested Laplace Approximation within Markov Chain Monte Carlo   The Integrated Nested Laplace Approximation (INLA) is a convenient way to
obtain approximations to the posterior marginals for parameters in Bayesian
hierarchical models when the latent effects can be expressed as a Gaussian
Markov Random Field (GMRF). In addition, its implementation in the R-INLA
package for the R statistical software provides an easy way to fit models using
INLA in practice. R-INLA implements a number of widely used latent models,
including several spatial models. In addition, R-INLA can fit models in a
fraction of the time than other computer intensive methods (e.g. Markov Chain
Monte Carlo) take to fit the same model.
Although INLA provides a fast approximation to the marginals of the model
parameters, it is difficult to use it with models not implemented in R-INLA. It
is also difficult to make multivariate posterior inference on the parameters of
the model as INLA focuses on the posterior marginals and not the joint
posterior distribution.
In this paper we describe how to use INLA within the Metropolis-Hastings
algorithm to fit spatial models and estimate the joint posterior distribution
of a reduced number of parameters. We will illustrate the benefits of this new
method with two examples on spatial econometrics and disease mapping where
complex spatial models with several spatial structures need to be fitted.
",Statistics,Statistics
"On the Helium fingers in the intracluster medium   In this paper we investigate the convection phenomenon in the intracluster
medium (the weakly-collisional magnetized inhomogeneous plasma permeating
galaxy clusters) where the concentration gradient of the Helium ions is not
ignorable. To this end, we build upon the general machinery employed to study
the salt finger instability found in the oceans. The salt finger instability is
a form of double diffusive convection where the diffusions of two physical
quantities---heat and salt concentrations---occur with different diffusion
rates. The analogous instability in the intracluster medium may result owing to
the magnetic field mediated anisotropic diffusions of the heat and the Helium
ions (in the sea of the Hydrogen ions and the free electrons). These two
diffusions have inherently different diffusion rates. Hence the convection
caused by the onset of this instability is an example of double diffusive
convection in the astrophysical settings. A consequence of this instability is
the formation of the vertical filamentary structures having more concentration
of the Helium ions with respect to the immediate neighbourhoods of the
filaments. We term these structures as Helium fingers in analogy with the salt
fingers found in the case of the salt finger instability. Here we show that the
width of a Helium finger scales as one-fourth power of the radius of the inner
region of the intracluster medium in the supercritical regime. We also
determine the explicit mathematical expression of the criterion for the onset
of the heat-flux-driven buoyancy instability modified by the presence of
inhomogeneously distributed Helium ions.
",Physics,Physics
"Core2Vec: A core-preserving feature learning framework for networks   Recent advances in the field of network representation learning are mostly
attributed to the application of the skip-gram model in the context of graphs.
State-of-the-art analogues of skip-gram model in graphs define a notion of
neighbourhood and aim to find the vector representation for a node, which
maximizes the likelihood of preserving this neighborhood.
In this paper, we take a drastic departure from the existing notion of
neighbourhood of a node by utilizing the idea of coreness. More specifically,
we utilize the well-established idea that nodes with similar core numbers play
equivalent roles in the network and hence induce a novel and an organic notion
of neighbourhood. Based on this idea, we propose core2vec, a new algorithmic
framework for learning low dimensional continuous feature mapping for a node.
Consequently, the nodes having similar core numbers are relatively closer in
the vector space that we learn.
We further demonstrate the effectiveness of core2vec by comparing word
similarity scores obtained by our method where the node representations are
drawn from standard word association graphs against scores computed by other
state-of-the-art network representation techniques like node2vec, DeepWalk and
LINE. Our results always outperform these existing methods
",Computer Science,Computer Science; Statistics
"Learning with Changing Features   In this paper we study the setting where features are added or change
interpretation over time, which has applications in multiple domains such as
retail, manufacturing, finance. In particular, we propose an approach to
provably determine the time instant from which the new/changed features start
becoming relevant with respect to an output variable in an agnostic
(supervised) learning setting. We also suggest an efficient version of our
approach which has the same asymptotic performance. Moreover, our theory also
applies when we have more than one such change point. Independent post analysis
of a change point identified by our method for a large retailer revealed that
it corresponded in time with certain unflattering news stories about a brand
that resulted in the change in customer behavior. We also applied our method to
data from an advanced manufacturing plant identifying the time instant from
which downstream features became relevant. To the best of our knowledge this is
the first work that formally studies change point detection in a distribution
independent agnostic setting, where the change point is based on the changing
relationship between input and output.
",Computer Science; Statistics,Computer Science; Statistics
"Premise Selection for Theorem Proving by Deep Graph Embedding   We propose a deep learning-based approach to the problem of premise
selection: selecting mathematical statements relevant for proving a given
conjecture. We represent a higher-order logic formula as a graph that is
invariant to variable renaming but still fully preserves syntactic and semantic
information. We then embed the graph into a vector via a novel embedding method
that preserves the information of edge ordering. Our approach achieves
state-of-the-art results on the HolStep dataset, improving the classification
accuracy from 83% to 90.3%.
",Computer Science,Computer Science
"On the $p'$-subgraph of the Young graph   Let $p$ be a prime number. In this article we study the restriction to
$\mathfrak{S}_{n-1}$ of irreducible characters of degree coprime to $p$ of
$\mathfrak{S}_n$. In particular, we study the combinatorial properties of the
subgraph $\mathbb{Y}_{p'}$ of the Young graph $\mathbb{Y}$. This is an
extension to odd primes of the work done by Ayyer, Prasad and Spallone for
$p=2$.
",Mathematics,Mathematics
"Machine learning in protein engineering   Machine learning-guided protein engineering is a new paradigm that enables
the optimization of complex protein functions. Machine-learning methods use
data to predict protein function without requiring a detailed model of the
underlying physics or biological pathways. They accelerate protein engineering
by learning from information contained in all measured variants and using it to
select variants that are likely to be improved. In this review, we introduce
the steps required to collect protein data, train machine-learning models, and
use trained models to guide engineering. We make recommendations at each stage
and look to future opportunities for machine learning to enable the discovery
of new protein functions and uncover the relationship between protein sequence
and function.
",Quantitative Biology,Statistics
"Gamma-Band Correlations in Primary Visual Cortex   Neural field theory is used to quantitatively analyze the two-dimensional
spatiotemporal correlation properties of gamma-band (30 -- 70 Hz) oscillations
evoked by stimuli arriving at the primary visual cortex (V1), and modulated by
patchy connectivities that depend on orientation preference (OP). Correlation
functions are derived analytically under different stimulus and measurement
conditions. The predictions reproduce a range of published experimental
results, including the existence of two-point oscillatory temporal
cross-correlations with zero time-lag between neurons with similar OP, the
influence of spatial separation of neurons on the strength of the correlations,
and the effects of differing stimulus orientations.
",Quantitative Biology,Quantitative Biology
"Optimal learning via local entropies and sample compression   The aim of this paper is to provide several novel upper bounds on the excess
risk with a primal focus on classification problems. We suggest two approaches
and the obtained bounds are represented via the distribution dependent local
entropies of the classes or the sizes of specific sample com- pression schemes.
We show that in some cases, our guarantees are optimal up to constant factors
and outperform previously known results. As an application of our results, we
provide a new tight PAC bound for the hard-margin SVM, an extended analysis of
certain empirical risk minimizers under log-concave distributions, a new
variant of an online to batch conversion, and distribution dependent localized
bounds in the aggregation framework. We also develop techniques that allow to
replace empirical covering number or covering numbers with bracketing by the
coverings with respect to the distribution of the data. The proofs for the
sample compression schemes are based on the moment method combined with the
analysis of voting algorithms.
",Mathematics; Statistics,Computer Science; Statistics
"A Systematic Evaluation of Static API-Misuse Detectors   Application Programming Interfaces (APIs) often have usage constraints, such
as restrictions on call order or call conditions. API misuses, i.e., violations
of these constraints, may lead to software crashes, bugs, and vulnerabilities.
Though researchers developed many API-misuse detectors over the last two
decades, recent studies show that API misuses are still prevalent. Therefore,
we need to understand the capabilities and limitations of existing detectors in
order to advance the state of the art. In this paper, we present the first-ever
qualitative and quantitative evaluation that compares static API-misuse
detectors along the same dimensions, and with original author validation. To
accomplish this, we develop MUC, a classification of API misuses, and
MUBenchPipe, an automated benchmark for detector comparison, on top of our
misuse dataset, MUBench. Our results show that the capabilities of existing
detectors vary greatly and that existing detectors, though capable of detecting
misuses, suffer from extremely low precision and recall. A systematic
root-cause analysis reveals that, most importantly, detectors need to go beyond
the naive assumption that a deviation from the most-frequent usage corresponds
to a misuse and need to obtain additional usage examples to train their models.
We present possible directions towards more-powerful API-misuse detectors.
",Computer Science,Computer Science
"Superior lattice thermal conductance of single layer borophene   By way of the nonequilibrium Green's function simulations and first
principles calculations, we report that borophene, a single layer of boron
atoms that was fabricated recently, possesses an extraordinarily high lattice
thermal conductance in the ballistic transport regime, which even exceeds
graphene. In addition to the obvious reasons of light mass and strong bonding
of boron atoms, the superior thermal conductance is mainly rooted in its strong
structural anisotropy and unusual phonon transmission. For low-frequency
phonons, the phonon transmission within borophene is nearly isotropic, similar
to that of graphene. For high frequency phonons, however, the transmission is
one dimensional, that is, all the phonons travel in one direction, giving rise
to its ultrahigh thermal conductance. The present study suggests that borophene
is promising for applications in efficient heat dissipation and thermal
management, and also an ideal material for revealing fundamentals of
dimensionality effect on phonon transport in ballistic regime.
",Physics,Physics
"Timing Aware Dummy Metal Fill Methodology   In this paper, we analyzed parasitic coupling capacitance coming from dummy
metal fill and its impact on timing. Based on the modeling, we proposed two
approaches to minimize the timing impact from dummy metal fill. The first
approach applies more spacing between critical nets and metal fill, while the
second approach leverages the shielding effects of reference nets. Experimental
results show consistent improvement compared to traditional metal fill method.
",Computer Science,Computer Science
"Charge exchange in galaxy clusters   Though theoretically expected, the charge exchange emission from galaxy
clusters has not yet been confidently detected. Accumulating hints were
reported recently, including a rather marginal detection with the Hitomi data
of the Perseus cluster. As suggested in Gu et al. (2015), a detection of charge
exchange line emission from galaxy clusters would not only impact the
interpretation of the newly-discovered 3.5 keV line, but also open up a new
research topic on the interaction between hot and cold matter in clusters. We
aim to perform the most systematic search for the O VIII charge exchange line
in cluster spectra using the RGS on board XMM. We introduce a sample of 21
clusters observed with the RGS. The dominating thermal plasma emission is
modeled and subtracted with a two-temperature CIE component, and the residuals
are stacked for the line search. The systematic uncertainties in the fits are
quantified by refitting the spectra with a varying continuum and line
broadening. By the residual stacking, we do find a hint of a line-like feature
at 14.82 A, the characteristic wavelength expected for oxygen charge exchange.
This feature has a marginal significance of 2.8 sigma, and the average
equivalent width is 2.5E-4 keV. We further demonstrate that the putative
feature can be hardly affected by the systematic errors from continuum
modelling and instrumental effects, or the atomic uncertainties of the
neighbouring thermal lines. Assuming a realistic temperature and abundance
pattern, the physical model implied by the possible oxygen line agrees well
with the theoretical model proposed previously to explain the reported 3.5 keV
line. If the charge exchange source indeed exists, we would expect that the
oxygen abundance is potentially overestimated by 8-22% in previous X-ray
measurements which assumed pure thermal lines.
",Physics,Physics
"Bootstrapping Generalization Error Bounds for Time Series   We consider the problem of finding confidence intervals for the risk of
forecasting the future of a stationary, ergodic stochastic process, using a
model estimated from the past of the process. We show that a bootstrap
procedure provides valid confidence intervals for the risk, when the data
source is sufficiently mixing, and the loss function and the estimator are
suitably smooth. Autoregressive (AR(d)) models estimated by least squares obey
the necessary regularity conditions, even when mis-specified, and simulations
show that the finite- sample coverage of our bounds quickly converges to the
theoretical, asymptotic level. As an intermediate step, we derive sufficient
conditions for asymptotic independence between empirical distribution functions
formed by splitting a realization of a stochastic process, of independent
interest.
",Mathematics; Statistics,Mathematics; Statistics
"Predicting signatures of anisotropic resonance energy transfer in dye-functionalized nanoparticles   Resonance energy transfer (RET) is an inherently anisotropic process. Even
the simplest, well-known Förster theory, based on the transition
dipole-dipole coupling, implicitly incorporates the anisotropic character of
RET. In this theoretical work, we study possible signatures of the fundamental
anisotropic character of RET in hybrid nanomaterials composed of a
semiconductor nanoparticle (NP) decorated with molecular dyes. In particular,
by means of a realistic kinetic model, we show that the analysis of the dye
photoluminescence difference for orthogonal input polarizations reveals the
anisotropic character of the dye-NP RET which arises from the intrinsic
anisotropy of the NP lattice. In a prototypical core/shell wurtzite CdSe/ZnS NP
functionalized with cyanine dyes (Cy3B), this difference is predicted to be as
large as 75\% and it is strongly dependent in amplitude and sign on the dye-NP
distance. We account for all the possible RET processes within the system,
together with competing decay pathways in the separate segments. In addition,
we show that the anisotropic signature of RET is persistent up to a large
number of dyes per NP.
",Physics,Physics
"Possible evidence for spin-transfer torque induced by spin-triplet supercurrent   Cooper pairs in superconductors are normally spin singlet. Nevertheless,
recent studies suggest that spin-triplet Cooper pairs can be created at
carefully engineered superconductor-ferromagnet interfaces. If Cooper pairs are
spin-polarized they would transport not only charge but also a net spin
component, but without dissipation, and therefore minimize the heating effects
associated with spintronic devices. Although it is now established that triplet
supercurrents exist, their most interesting property - spin - is only inferred
indirectly from transport measurements. In conventional spintronics, it is well
known that spin currents generate spin-transfer torques that alter
magnetization dynamics and switch magnetic moments. The observation of similar
effects due to spin-triplet supercurrents would not only confirm the net spin
of triplet pairs but also pave the way for applications of superconducting
spintronics. Here, we present a possible evidence for spin-transfer torques
induced by triplet supercurrents in superconductor/ferromagnet/superconductor
(S/F/S) Josephson junctions. Below the superconducting transition temperature
T_c, the ferromagnetic resonance (FMR) field at X-band (~ 9.0 GHz) shifts
rapidly to a lower field with decreasing temperature due to the spin-transfer
torques induced by triplet supercurrents. In contrast, this phenomenon is
absent in ferromagnet/superconductor (F/S) bilayers and
superconductor/insulator/ferromagnet/superconductor (S/I/F/S) multilayers where
no supercurrents pass through the ferromagnetic layer. These experimental
observations are discussed with theoretical predictions for ferromagnetic
Josephson junctions with precessing magnetization.
",Physics,Physics
"The Gibbons-Hawking ansatz over a wedge   We discuss the Ricci-flat `model metrics' on $\mathbb{C}^2$ with cone
singularities along the conic $\{zw=1\}$ constructed by Donaldson using the
Gibbons-Hawking ansatz over wedges in $\mathbb{R}^3$. In particular we describe
their asymptotic behavior at infinity and compute their energies.
",Mathematics,Mathematics
"Explaining Transition Systems through Program Induction   Explaining and reasoning about processes which underlie observed black-box
phenomena enables the discovery of causal mechanisms, derivation of suitable
abstract representations and the formulation of more robust predictions. We
propose to learn high level functional programs in order to represent abstract
models which capture the invariant structure in the observed data. We introduce
the $\pi$-machine (program-induction machine) -- an architecture able to induce
interpretable LISP-like programs from observed data traces. We propose an
optimisation procedure for program learning based on backpropagation, gradient
descent and A* search. We apply the proposed method to three problems: system
identification of dynamical systems, explaining the behaviour of a DQN agent
and learning by demonstration in a human-robot interaction scenario. Our
experimental results show that the $\pi$-machine can efficiently induce
interpretable programs from individual data traces.
",Computer Science,Computer Science; Statistics
"The Genus-One Global Mirror Theorem for the Quintic Threefold   We prove the genus-one restriction of the all-genus
Landau-Ginzburg/Calabi-Yau conjecture of Chiodo and Ruan, stated in terms of
the geometric quantization of an explicit symplectomorphism determined by
genus-zero invariants. This provides the first evidence supporting the
higher-genus Landau-Ginzburg/Calabi-Yau correspondence for the quintic
threefold, and exhibits the first instance of the ""genus zero controls higher
genus"" principle, in the sense of Givental's quantization formalism, for
non-semisimple cohomological field theories.
",Mathematics,Mathematics
"On the ""Calligraphy"" of Books   Authorship attribution is a natural language processing task that has been
widely studied, often by considering small order statistics. In this paper, we
explore a complex network approach to assign the authorship of texts based on
their mesoscopic representation, in an attempt to capture the flow of the
narrative. Indeed, as reported in this work, such an approach allowed the
identification of the dominant narrative structure of the studied authors. This
has been achieved due to the ability of the mesoscopic approach to take into
account relationships between different, not necessarily adjacent, parts of the
text, which is able to capture the story flow. The potential of the proposed
approach has been illustrated through principal component analysis, a
comparison with the chance baseline method, and network visualization. Such
visualizations reveal individual characteristics of the authors, which can be
understood as a kind of calligraphy.
",Computer Science,Computer Science
"Stationary solutions for the ellipsoidal BGK model in a slab   We address the boundary value problem for the ellipsoidal BGK model of the
Boltzmann equation posed in a bounded interval. The existence of a unique mild
solution is established under the assumption that the inflow boundary data does
not concentrate too much around the zero velocity, and the gas is sufficiently
rarefied.
",Mathematics,Physics; Mathematics
"Exact description of coalescing eigenstates in open quantum systems in terms of microscopic Hamiltonian dynamics   At the exceptional point where two eigenstates coalesce in open quantum
systems, the usual diagonalization scheme breaks down and the Hamiltonian can
only be reduced to Jordan block form. Most of the studies on the exceptional
point appearing in the literature introduce a phenomenological effective
Hamiltonian that essentially reduces the problem to that of a finite
non-Hermitian matrix for which it is straightforward to obtain the Jordan form.
In this paper, we demonstrate how the Hamiltonian of an open quantum system
reduces to Jordan block form at an exceptional point in an exact manner that
treats the continuum without any approximation. Our method relies on the
Brillouin-Wigner-Feshbach projection method according to which we can obtain a
finite dimensional effective Hamiltonian that shares the discrete sector of the
spectrum with the original Hamiltonian. While owing to its eigenvalue
dependence this effective Hamiltonian cannot be used to write the Jordan block
directly, we show that by formally extending the problem to include eigenstates
with complex eigenvalues that reside outside the usual Hilbert space, we can
obtain the Jordan block form at the exceptional point without introducing any
approximation. We also introduce an extended Jordan form basis away from the
exceptional point, which provides an alternative way to obtain the Jordan block
at an exceptional point. The extended Jordan block connects continuously to the
Jordan block exactly at the exceptional point implying that the observable
quantities are continuous at the exceptional point.
",Physics,Physics
"Weighted network estimation by the use of topological graph metrics   Topological metrics of graphs provide a natural way to describe the prominent
features of various types of networks. Graph metrics describe the structure and
interplay of graph edges and have found applications in many scientific fields.
In this work, graph metrics are used in network estimation by developing
optimisation methods that incorporate prior knowledge of a network's topology.
The derivatives of graph metrics are used in gradient descent schemes for
weighted undirected network denoising, network completion, and network
decomposition. The successful performance of our methodology is shown in a
number of toy examples and real-world datasets. Most notably, our work
establishes a new link between graph theory, network science and optimisation.
",Computer Science,Computer Science; Statistics
"The Rees algebra of a two-Borel ideal is Koszul   Let $M$ and $N$ be two monomials of the same degree, and let $I$ be the
smallest Borel ideal containing $M$ and $N$. We show that the toric ring of $I$
is Koszul by constructing a quadratic Gröbner basis for the associated toric
ideal. Our proofs use the construction of graphs corresponding to fibers of the
toric map. As a consequence, we conclude that the Rees algebra is also Koszul.
",Mathematics,Mathematics
"Insulator to Metal Transition in WO$_3$ Induced by Electrolyte Gating   Tungsten oxide and its associated bronzes (compounds of tungsten oxide and an
alkali metal) are well known for their interesting optical and electrical
characteristics. We have modified the transport properties of thin WO$_3$ films
by electrolyte gating using both ionic liquids and polymer electrolytes. We are
able to tune the resistivity of the gated film by more than five orders of
magnitude, and a clear insulator-to-metal transition is observed. To clarify
the doping mechanism, we have performed a series of incisive operando
experiments, ruling out both a purely electronic effect (charge accumulation
near the interface) and oxygen-related mechanisms. We propose instead that
hydrogen intercalation is responsible for doping WO$_3$ into a highly
conductive ground state and provide evidence that it can be described as a
dense polaronic gas.
",Physics,Physics
"Measuring filament orientation: a new quantitative, local approach   The relative orientation between filamentary structures in molecular clouds
and the ambient magnetic field provides insight into filament formation and
stability. To calculate the relative orientation, a measurement of filament
orientation is first required. We propose a new method to calculate the
orientation of the one pixel wide filament skeleton that is output by filament
identification algorithms such as \textsc{filfinder}. We derive the local
filament orientation from the direction of the intensity gradient in the
skeleton image using the Sobel filter and a few simple post-processing steps.
We call this the `Sobel-gradient method'. The resulting filament orientation
map can be compared quantitatively on a local scale with the magnetic field
orientation map to then find the relative orientation of the filament with
respect to the magnetic field at each point along the filament. It can also be
used in constructing radial profiles for filament width fitting. The proposed
method facilitates automation in analysis of filament skeletons, which is
imperative in this era of `big data'.
",Physics,Physics
"Interface magnetism and electronic structure: ZnO(0001)/Co3O4(111)   We have studied the structural, electronic and magnetic properties of spinel
$\rm Co_3O_4$(111) surfaces and their interfaces with ZnO (0001) using density
functional theory (DFT) within the Generalized Gradient Approximation with
on-site Coulomb repulsion term (GGA+U). Two possible forms of spinel surface,
containing $\rm Co^{2+} $ and $\rm Co^{3+} $ ions and terminated with either
cobalt or oxygen ions were considered, as well as their interface with zinc
oxide. Our calculations demonstrate that $\rm Co^{3+} $ ions attain non-zero
magnetic moments at the surface and interface, in contrast to the bulk, where
they are not magnetic, leading to the ferromagnetic ordering. Since heavily
Co-doped ZnO samples can contain $\rm Co_3O_4 $ secondary phase, such a
magnetic ordering at the interface might explain the origin of the magnetism in
these diluted magnetic semiconductors (DMS).
",Physics,Physics
"Prediction Scores as a Window into Classifier Behavior   Most multi-class classifiers make their prediction for a test sample by
scoring the classes and selecting the one with the highest score. Analyzing
these prediction scores is useful to understand the classifier behavior and to
assess its reliability. We present an interactive visualization that
facilitates per-class analysis of these scores. Our system, called Classilist,
enables relating these scores to the classification correctness and to the
underlying samples and their features. We illustrate how such analysis reveals
varying behavior of different classifiers. Classilist is available for use
online, along with source code, video tutorials, and plugins for R, RapidMiner,
and KNIME at this https URL.
",Computer Science; Statistics,Computer Science; Statistics
"Free Information Flow Benefits Truth Seeking   How can we approach the truth in a society? It may depend on various factors.
In this paper, using a well-established truth seeking model, we show that the
persistent free information flow will bring us to the truth. Here the free
information flow is modeled as the environmental random noise that could alter
one's cognition. Without the random noise, the model predicts that the truth
can only be captured by the truth seekers who own active perceptive ability of
the truth and their believers, while the other individuals may stick to
falsehood. But under the influence of the random noise, we strictly prove that
even there is only one truth seeker in the group, all individuals will finally
approach the truth.
",Computer Science; Physics; Mathematics,Computer Science
"Ab initio effective Hamiltonians for cuprate superconductors   Ab initio low-energy effective Hamiltonians of two typical high-temperature
copper-oxide superconductors, whose mother compounds are La$_2$CuO$_4$ and
HgBa$_2$CuO$_4$, are derived by utilizing the multi-scale ab initio scheme for
correlated electrons (MACE). The effective Hamiltonians obtained in the present
study serve as platforms of future studies to accurately solve the low-energy
effective Hamiltonians beyond the density functional theory. It allows further
study on the superconducting mechanism from the first principles and
quantitative basis without adjustable parameters not only for the available
cuprates but also for future design of higher Tc in general. More concretely,
we derive effective Hamiltonians for three variations, 1)one-band Hamiltonian
for the antibonding orbital generated from strongly hybridized Cu
$3d_{x^2-y^2}$ and O $2p_\sigma$ orbitals 2)two-band Hamiltonian constructed
from the antibonding orbital and Cu $3d_{3z^2-r^2}$ orbital hybridized mainly
with the apex oxygen $p_z$ orbital 3)three-band Hamiltonian consisting mainly
of Cu $3d_{x^2-y^2}$ orbitals and two O $2p_\sigma$ orbitals. Differences
between the Hamiltonians for La$_2$CuO$_4$ and HgBa$_2$CuO$_4$, which have
relatively low and high critical temperatures, respectively, at optimally doped
compounds, are elucidated. The main differences are summarized as i) the oxygen
$2p_\sigma$ orbitals are farther(~3.7eV) below from the Cu $d_{x^2-y^2}$
orbital for the La compound than the Hg compound(~2.4eV) in the three-band
Hamiltonian. This causes a substantial difference in the character of the
$d_{x^2-y^2}-2p_\sigma$ antibonding band at the Fermi level and makes the
effective onsite Coulomb interaction U larger for the La compound than the Hg
compound for the two- and one-band Hamiltonians. ii)The ratio of the
second-neighbor to the nearest transfer t'/t is also substantially different
(~0.26) for the Hg and ~0.15 for the La compound in the one-band Hamiltonian.
",Physics,Physics
"Magnetic droplet nucleation with homochiral Neel domain wall   We investigate the effect of the Dzyaloshinskii Moriya interaction (DMI) on
magnetic domain nucleation in a ferromagnetic thin film with perpendicular
magnetic anisotropy. We propose an extended droplet model to determine the
nucleation field as a function of the in-plane field. The model can explain the
experimentally observed nucleation in a CoNi microstrip with the interfacial
DMI. The results are also reproduced by micromagnetic simulation based on the
string model. The electrical measurement method proposed in this study can be
widely used to quantitatively determine the DMI energy density.
",Physics,Physics
"One-loop binding corrections to the electron $g$ factor   We calculate the one-loop electron self-energy correction of order
$\alpha\,(Z\,\alpha)^5$ to the bound electron $g$ factor. Our result is in
agreement with the extrapolated numerical value and paves the way for the
calculation of the analogous, but as yet unknown two-loop correction.
",Physics,Physics
"Detection of low dimensionality and data denoising via set estimation techniques   This work is closely related to the theories of set estimation and manifold
estimation.
Our object of interest is a, possibly lower-dimensional, compact set $S
\subset {\mathbb R}^d$.
The general aim is to identify (via stochastic procedures) some qualitative
or quantitative features of $S$, of geometric or topological character. The
available information is just a random sample of points drawn on $S$.
The term ""to identify"" means here to achieve a correct answer almost surely
(a.s.) when the sample size tends to infinity. More specifically the paper aims
at giving some partial answers to the following questions: is $S$ full
dimensional? Is $S$ ""close to a lower dimensional set"" $\mathcal{M}$? If so,
can we estimate $\mathcal{M}$ or some functionals of $\mathcal{M}$ (in
particular, the Minkowski content of $\mathcal{M}$)? As an important auxiliary
tool in the answers of these questions, a denoising procedure is proposed in
order to partially remove the noise in the original data. The theoretical
results are complemented with some simulations and graphical illustrations.
",Mathematics; Statistics,Computer Science; Mathematics; Statistics
"Principal Eigenvalue of Mixed Problem for the Fractional Laplacian: Moving the Boundary Conditions   We analyze the behavior of the eigenvalues of the following non local mixed
problem $\left\{ \begin{array}{rcll} (-\Delta)^{s} u &=& \lambda_1(D) \ u
&\inn\Omega,\\ u&=&0&\inn D,\\ \mathcal{N}_{s}u&=&0&\inn N. \end{array}\right $
Our goal is to construct different sequences of problems by modifying the
configuration of the sets $D$ and $N$, and to provide sufficient and necessary
conditions on the size and the location of these sets in order to obtain
sequences of eigenvalues that in the limit recover the eigenvalues of the
Dirichlet or Neumann problem. We will see that the non locality plays a crucial
role here, since the sets $D$ and $N$ can have infinite measure, a phenomenon
that does not appear in the local case (see for example \cite{D,D2,CP}).
",Mathematics,Mathematics
"Efimov Effect in the Dirac Semi-metals   Efimov effect refers to quantum states with discrete scaling symmetry and a
universal scaling factor, and has attracted considerable interests from nuclear
to atomic physics communities. In a Dirac semi-metal, when an electron
interacts with a static impurity though a Coulomb interaction, the same scaling
of the kinetic and interaction energies also gives rise to such a Efimov
effect. However, even when the Fermi energy exactly lies at the Dirac point,
the vacuum polarization of electron-hole pair fluctuation can still screen the
Coulomb interaction, which leads to derivation from this scaling symmetry and
eventually breakdown of the Efimov effect. This distortion of the Efimov bound
state energy due to vacuum polarization is a relativistic electron analogy of
the Lamb shift for the hydrogen atom. Motivated by recent experimental
observations in two- and three-dimensional Dirac semi-metals, in this paper we
investigate this many-body correction to the Efimov effect, and answer the
question that under what condition a good number of Efimov-like bound states
can still be observed in these condensed matter experiments.
",Physics,Physics
"Interplay between relativistic energy corrections and resonant excitations in x-ray multiphoton ionization dynamics of Xe atoms   In this paper, we theoretically study x-ray multiphoton ionization dynamics
of heavy atoms taking into account relativistic and resonance effects. When an
atom is exposed to an intense x-ray pulse generated by an x-ray free-electron
laser (XFEL), it is ionized to a highly charged ion via a sequence of
single-photon ionization and accompanying relaxation processes, and its final
charge state is limited by the last ionic state that can be ionized by a
single-photon ionization. If x-ray multiphoton ionization involves deep
inner-shell electrons in heavy atoms, energy shifts by relativistic effects
play an important role in ionization dynamics, as pointed out in [Phys.\ Rev.\
Lett.\ \textbf{110}, 173005 (2013)]. On the other hand, if the x-ray beam has a
broad energy bandwidth, the high-intensity x-ray pulse can drive resonant
photo-excitations for a broad range of ionic states and ionize even beyond the
direct one-photon ionization limit, as first proposed in [Nature\ Photon.\
\textbf{6}, 858 (2012)]. To investigate both relativistic and resonance
effects, we extend the \textsc{xatom} toolkit to incorporate relativistic
energy corrections and resonant excitations in x-ray multiphoton ionization
dynamics calculations. Charge-state distributions are calculated for Xe atoms
interacting with intense XFEL pulses at a photon energy of 1.5~keV and 5.5~keV,
respectively. For both photon energies, we demonstrate that the role of
resonant excitations in ionization dynamics is altered due to significant
shifts of orbital energy levels by relativistic effects. Therefore it is
necessary to take into account both effects to accurately simulate multiphoton
multiple ionization dynamics at high x-ray intensity.
",Physics,Physics
"Deconvolutional Latent-Variable Model for Text Sequence Matching   A latent-variable model is introduced for text matching, inferring sentence
representations by jointly optimizing generative and discriminative objectives.
To alleviate typical optimization challenges in latent-variable models for
text, we employ deconvolutional networks as the sequence decoder (generator),
providing learned latent codes with more semantic information and better
generalization. Our model, trained in an unsupervised manner, yields stronger
empirical predictive performance than a decoder based on Long Short-Term Memory
(LSTM), with less parameters and considerably faster training. Further, we
apply it to text sequence-matching problems. The proposed model significantly
outperforms several strong sentence-encoding baselines, especially in the
semi-supervised setting.
",Computer Science; Statistics,Computer Science
"FNS: an event-driven spiking neural network framework for efficient simulations of large-scale brain models   Limitations in processing capabilities and memory of today's computers make
spiking neuron-based (human) whole-brain simulations inevitably characterized
by a compromise between bio-plausibility and computational cost. It translates
into brain models composed of a reduced number of neurons and a simplified
neuron's mathematical model. Taking advantage of the sparse character of
brain-like computation, eventdriven technique allows us to carry out efficient
simulation of large-scale Spiking Neural Networks (SNN). The recent Leaky
Integrate-and-Fire with Latency (LIFL) spiking neuron model is event-driven
compatible and exhibits some realistic neuronal features, opening new horizons
in whole-brain modelling. In this paper we present FNS, a LIFL-based exact
event-driven spiking neural network framework implemented in Java and oriented
to wholebrain simulations. FNS combines spiking/synaptic whole-brain modelling
with the event-driven approach, allowing us to define heterogeneous modules and
multi-scale connectivity with delayed connections and plastic synapses,
providing fast simulations at the same time. A novel parallelization strategy
is also implemented in order to further speed up simulations. This paper
presents mathematical models, software implementation and simulation routines
on which FNS is based. Finally, a reduced brain network model (1400 neurons and
45000 synapses) is synthesized on the basis of real brain structural data, and
the resulting model activity is compared with associated brain functional
(source-space MEG) data. The conducted test shows a good matching between the
activity of model and that of the emulated subject, in outstanding simulation
times (about 20s for simulating 4s of activity with a normal PC). Dedicated
sections of stimuli editing and output synthesis allow the neuroscientist to
introduce and extract brain-like signals, respectively...
",Quantitative Biology,Computer Science; Statistics
"The Łojasiewicz Exponent via The Valuative Hamburger-Noether Process   Let $k$ be an algebraically closed field of any characteristic. We apply the
Hamburger-Noether process of successive quadratic transformations to show the
equivalence of two definitions of the {\L}ojasiewicz exponent
$\mathfrak{L}(\mathfrak{a})$ of an ideal $\mathfrak{a}\subset k[[x,y]]$.
",Mathematics,Mathematics
"Satellite Image-based Localization via Learned Embeddings   We propose a vision-based method that localizes a ground vehicle using
publicly available satellite imagery as the only prior knowledge of the
environment. Our approach takes as input a sequence of ground-level images
acquired by the vehicle as it navigates, and outputs an estimate of the
vehicle's pose relative to a georeferenced satellite image. We overcome the
significant viewpoint and appearance variations between the images through a
neural multi-view model that learns location-discriminative embeddings in which
ground-level images are matched with their corresponding satellite view of the
scene. We use this learned function as an observation model in a filtering
framework to maintain a distribution over the vehicle's pose. We evaluate our
method on different benchmark datasets and demonstrate its ability localize
ground-level images in environments novel relative to training, despite the
challenges of significant viewpoint and appearance variations.
",Computer Science,Computer Science
"A convex penalty for switching control of partial differential equations   A convex penalty for promoting switching controls for partial differential
equations is introduced; such controls consist of an arbitrary number of
components of which at most one should be simultaneously active. Using a
Moreau-Yosida approximation, a family of approximating problems is obtained
that is amenable to solution by a semismooth Newton method. The efficiency of
this approach and the structure of the obtained controls are demonstrated by
numerical examples.
",Mathematics,Mathematics
"Predicting Tactical Solutions to Operational Planning Problems under Imperfect Information   This paper offers a methodological contribution at the intersection of
machine learning and operations research. Namely, we propose a methodology to
quickly predict tactical solutions to a given operational problem. In this
context, the tactical solution is less detailed than the operational one but it
has to be computed in very short time and under imperfect information. The
problem is of importance in various applications where tactical and operational
planning problems are interrelated and information about the operational
problem is revealed over time. This is for instance the case in certain
capacity planning and demand management systems.
We formulate the problem as a two-stage optimal prediction stochastic program
whose solution we predict with a supervised machine learning algorithm. The
training data set consists of a large number of deterministic (second stage)
problems generated by controlled probabilistic sampling. The labels are
computed based on solutions to the deterministic problems (solved independently
and offline) employing appropriate aggregation and subselection methods to
address uncertainty. Results on our motivating application in load planning for
rail transportation show that deep learning algorithms produce highly accurate
predictions in very short computing time (milliseconds or less). The prediction
accuracy is comparable to solutions computed by sample average approximation of
the stochastic program.
",Computer Science; Statistics,Computer Science; Statistics
"Synkhronos: a Multi-GPU Theano Extension for Data Parallelism   We present Synkhronos, an extension to Theano for multi-GPU computations
leveraging data parallelism. Our framework provides automated execution and
synchronization across devices, allowing users to continue to write serial
programs without risk of race conditions. The NVIDIA Collective Communication
Library is used for high-bandwidth inter-GPU communication. Further
enhancements to the Theano function interface include input slicing (with
aggregation) and input indexing, which perform common data-parallel computation
patterns efficiently. One example use case is synchronous SGD, which has
recently been shown to scale well for a growing set of deep learning problems.
When training ResNet-50, we achieve a near-linear speedup of 7.5x on an NVIDIA
DGX-1 using 8 GPUs, relative to Theano-only code running a single GPU in
isolation. Yet Synkhronos remains general to any data-parallel computation
programmable in Theano. By implementing parallelism at the level of individual
Theano functions, our framework uniquely addresses a niche between manual
multi-device programming and prescribed multi-GPU training routines.
",Computer Science,Computer Science
"Testing Microfluidic Fully Programmable Valve Arrays (FPVAs)   Fully Programmable Valve Array (FPVA) has emerged as a new architecture for
the next-generation flow-based microfluidic biochips. This 2D-array consists of
regularly-arranged valves, which can be dynamically configured by users to
realize microfluidic devices of different shapes and sizes as well as
interconnections. Additionally, the regularity of the underlying structure
renders FPVAs easier to integrate on a tiny chip. However, these arrays may
suffer from various manufacturing defects such as blockage and leakage in
control and flow channels. Unfortunately, no efficient method is yet known for
testing such a general-purpose architecture. In this paper, we present a novel
formulation using the concept of flow paths and cut-sets, and describe an
ILP-based hierarchical strategy for generating compact test sets that can
detect multiple faults in FPVAs. Simulation results demonstrate the efficacy of
the proposed method in detecting manufacturing faults with only a small number
of test vectors.
",Computer Science,Computer Science
"Coastal flood implications of 1.5 °C, 2.0 °C, and 2.5 °C temperature stabilization targets in the 21st and 22nd century   Sea-level rise (SLR) is magnifying the frequency and severity of coastal
flooding. The rate and amount of global mean sea-level (GMSL) rise is a
function of the trajectory of global mean surface temperature (GMST).
Therefore, temperature stabilization targets (e.g., 1.5 °C and 2.0 °C
of warming above pre-industrial levels, as from the Paris Agreement) have
important implications for coastal flood risk. Here, we assess differences in
the return periods of coastal floods at a global network of tide gauges between
scenarios that stabilize GMST warming at 1.5 °C, 2.0 °C, and 2.5
°C above pre-industrial levels. We employ probabilistic, localized SLR
projections and long-term hourly tide gauge records to construct estimates of
the return levels of current and future flood heights for the 21st and 22nd
centuries. By 2100, under 1.5 °C, 2.0 °C, and 2.5 °C GMST
stabilization, median GMSL is projected to rise 47 cm with a very likely range
of 28-82 cm (90% probability), 55 cm (very likely 30-94 cm), and 58 cm (very
likely 36-93 cm), respectively. As an independent comparison, a semi-empirical
sea level model calibrated to temperature and GMSL over the past two millennia
estimates median GMSL will rise within < 13% of these projections. By 2150,
relative to the 2.0 °C scenario, GMST stabilization of 1.5 °C
inundates roughly 5 million fewer inhabitants that currently occupy lands,
including 40,000 fewer individuals currently residing in Small Island
Developing States. Relative to a 2.0 °C scenario, the reduction in the
amplification of the frequency of the 100-yr flood arising from a 1.5 °C
GMST stabilization is greatest in the eastern United States and in Europe, with
flood frequency amplification being reduced by about half.
",Physics,Physics
"Lagrangian fibers of Gelfand-Cetlin systems   Motivated by the study of Nishinou-Nohara-Ueda on the Floer thoery of
Gelfand-Cetlin systems over complex partial flag manifolds, we provide a
complete description of the topology of Gelfand-Cetlin fibers. We prove that
all fibers are \emph{smooth} isotropic submanifolds and give a complete
description of the fiber to be Lagrangian in terms of combinatorics of
Gelfand-Cetlin polytope. Then we study (non-)displaceability of Lagrangian
fibers. After a few combinatorial and numercal tests for the displaceability,
using the bulk-deformation of Floer cohomology by Schubert cycles, we prove
that every full flag manifold $\mathcal{F}(n)$ ($n \geq 3$) with a monotone
Kirillov-Kostant-Souriau symplectic form carries a continuum of
non-displaceable Lagrangian tori which degenerates to a non-torus fiber in the
Hausdorff limit. In particular, the Lagrangian $S^3$-fiber in $\mathcal{F}(3)$
is non-displaceable the question of which was raised by Nohara-Ueda who
computed its Floer cohomology to be vanishing.
",Mathematics,Mathematics
"Determining Phonon Coherence Using Photon Sideband Detection   Generating and detection coherent high-frequency heat-carrying phonons has
been a great topic of interest in recent years. While there have been
successful attempts in generating and observing coherent phonons, rigorous
techniques to characterize and detect these phonon coherence in a crystalline
material have been lagging compared to what has been achieved for photons. One
main challenge is a lack of detailed understanding of how detection signals for
phonons can be related to coherence. The quantum theory of photoelectric
detection has greatly advanced the ability to characterize photon coherence in
the last century and a similar theory for phonon detection is necessary. Here,
we re-examine the optical sideband fluorescence technique that has been used
detect high frequency phonons in materials with optically active defects. We
apply the quantum theory of photodetection to the sideband technique and
propose signatures in sideband photon-counting statistics and second-order
correlation measurement of sideband signals that indicates the degree of phonon
coherence. Our theory can be implemented in recently performed experiments to
bridge the gap of determining phonon coherence to be on par with that of
photons.
",Physics,Physics
"Antireflection Coated Semiconductor Laser Amplifier   This paper presents a laser amplifier based on an antireflection coated laser
diode. This laser amplifier operates without active temperature stabilisation
at any wavelength within its gain profile without restrictions on the injection
current. Using a active feedback from an external detector to the laser current
the power stabilized to better than $10^{-4}$, even after additional optical
elements such as an optical fiber and/or a polarization cleaner. This power can
also be modulated and tuned arbitrarily. In the absence of the seeding light,
the laser amplifier does not lase, thus resulting in an extremely simple setup,
which requires neither an external Fabry Perot cavity for monitoring the mode
purity nor a temperature stabilization.
",Physics,Physics
"Quantum Field Theory, Quantum Geometry, and Quantum Algebras   We demonstrate how one can see quantization of geometry, and quantum
algebraic structure in supersymmetric gauge theory.
",Mathematics,Mathematics
"Duluth at SemEval-2017 Task 6: Language Models in Humor Detection   This paper describes the Duluth system that participated in SemEval-2017 Task
6 #HashtagWars: Learning a Sense of Humor. The system participated in Subtasks
A and B using N-gram language models, ranking highly in the task evaluation.
This paper discusses the results of our system in the development and
evaluation stages and from two post-evaluation runs.
",Computer Science,Computer Science
"Robust Navigation In GNSS Degraded Environment Using Graph Optimization   Robust navigation in urban environments has received a considerable amount of
both academic and commercial interest over recent years. This is primarily due
to large commercial organizations such as Google and Uber stepping into the
autonomous navigation market. Most of this research has shied away from Global
Navigation Satellite System (GNSS) based navigation. The aversion to utilizing
GNSS data is due to the degraded nature of the data in urban environment (e.g.,
multipath, poor satellite visibility). The degradation of the GNSS data in
urban environments makes it such that traditional (GNSS) positioning methods
(e.g., extended Kalman filter, particle filters) perform poorly. However,
recent advances in robust graph theoretic based sensor fusion methods,
primarily applied to Simultaneous Localization and Mapping (SLAM) based robotic
applications, can also be applied to GNSS data processing. This paper will
utilize one such method known as the factor graph in conjunction several robust
optimization techniques to evaluate their applicability to robust GNSS data
processing. The goals of this study are two-fold. First, for GNSS applications,
we will experimentally evaluate the effectiveness of robust optimization
techniques within a graph-theoretic estimation framework. Second, by releasing
the software developed and data sets used for this study, we will introduce a
new open-source front-end to the Georgia Tech Smoothing and Mapping (GTSAM)
library for the purpose of integrating GNSS pseudorange observations.
",Computer Science,Computer Science
"Free Cooling of a Granular Gas in Three Dimensions   Granular gases as dilute ensembles of particles in random motion are not only
at the basis of elementary structure-forming processes in the universe and
involved in many industrial and natural phenomena, but also excellent models to
study fundamental statistical dynamics. A vast number of theoretical and
numerical investigations have dealt with this apparently simple non-equilibrium
system. The essential difference to molecular gases is the energy dissipation
in particle collisions, a subtle distinction with immense impact on their
global dynamics. Its most striking manifestation is the so-called granular
cooling, the gradual loss of mechanical energy in absence of external
excitation.
We report an experimental study of homogeneous cooling of three-dimensional
(3D) granular gases in microgravity. Surprisingly, the asymptotic scaling
$E(t)\propto t^{-2}$ obtained by Haff's minimal model [J. Fluid Mech. 134, 401
(1983)] proves to be robust, despite the violation of several of its central
assumptions. The shape anisotropy of the grains influences the characteristic
time of energy loss quantitatively, but not qualitatively. We compare kinetic
energies in the individual degrees of freedom, and find a slight predominance
of the translational motions. In addition, we detect a certain preference of
the grains to align with their long axis in flight direction, a feature known
from active matter or animal flocks, and the onset of clustering.
",Physics,Physics
"Deep Learning in Customer Churn Prediction: Unsupervised Feature Learning on Abstract Company Independent Feature Vectors   As companies increase their efforts in retaining customers, being able to
predict accurately ahead of time, whether a customer will churn in the
foreseeable future is an extremely powerful tool for any marketing team. The
paper describes in depth the application of Deep Learning in the problem of
churn prediction. Using abstract feature vectors, that can generated on any
subscription based company's user event logs, the paper proves that through the
use of the intrinsic property of Deep Neural Networks (learning secondary
features in an unsupervised manner), the complete pipeline can be applied to
any subscription based company with extremely good churn predictive
performance. Furthermore the research documented in the paper was performed for
Framed Data (a company that sells churn prediction as a service for other
companies) in conjunction with the Data Science Institute at Lancaster
University, UK. This paper is the intellectual property of Framed Data.
",Computer Science; Statistics,Computer Science
"Scheduling with regular performance measures and optional job rejection on a single machine   We address single machine problems with optional jobs - rejection, studied
recently in Zhang et al. [21] and Cao et al. [2]. In these papers, the authors
focus on minimizing regular performance measures, i.e., functions that are
non-decreasing in the jobs completion time, subject to the constraint that the
total rejection cost cannot exceed a predefined upper bound. They also prove
that the considered problems are ordinary NP-hard and provide
pseudo-polynomial-time Dynamic Programming (DP) solutions. In this paper, we
focus on three of these problems: makespan with release-dates; total completion
times; and total weighted completion, and present enhanced DP solutions
demonstrating both theoretical and practical improvements. Moreover, we provide
extensive numerical studies verifying their efficiency.
",Computer Science,Computer Science
"Measurements of Three-Level Hierarchical Structure in the Outliers in the Spectrum of Deepnet Hessians   We consider deep classifying neural networks. We expose a structure in the
derivative of the logits with respect to the parameters of the model, which is
used to explain the existence of outliers in the spectrum of the Hessian.
Previous works decomposed the Hessian into two components, attributing the
outliers to one of them, the so-called Covariance of gradients. We show this
term is not a Covariance but a second moment matrix, i.e., it is influenced by
means of gradients. These means possess an additive two-way structure that is
the source of the outliers in the spectrum. This structure can be used to
approximate the principal subspace of the Hessian using certain ""averaging""
operations, avoiding the need for high-dimensional eigenanalysis. We
corroborate this claim across different datasets, architectures and sample
sizes.
",Computer Science; Statistics,Computer Science; Statistics
"Choreographic and Somatic Approaches for the Development of Expressive Robotic Systems   As robotic systems are moved out of factory work cells into human-facing
environments questions of choreography become central to their design,
placement, and application. With a human viewer or counterpart present, a
system will automatically be interpreted within context, style of movement, and
form factor by human beings as animate elements of their environment. The
interpretation by this human counterpart is critical to the success of the
system's integration: knobs on the system need to make sense to a human
counterpart; an artificial agent should have a way of notifying a human
counterpart of a change in system state, possibly through motion profiles; and
the motion of a human counterpart may have important contextual clues for task
completion. Thus, professional choreographers, dance practitioners, and
movement analysts are critical to research in robotics. They have design
methods for movement that align with human audience perception, can identify
simplified features of movement for human-robot interaction goals, and have
detailed knowledge of the capacity of human movement. This article provides
approaches employed by one research lab, specific impacts on technical and
artistic projects within, and principles that may guide future such work. The
background section reports on choreography, somatic perspectives,
improvisation, the Laban/Bartenieff Movement System, and robotics. From this
context methods including embodied exercises, writing prompts, and community
building activities have been developed to facilitate interdisciplinary
research. The results of this work is presented as an overview of a smattering
of projects in areas like high-level motion planning, software development for
rapid prototyping of movement, artistic output, and user studies that help
understand how people interpret movement. Finally, guiding principles for other
groups to adopt are posited.
",Computer Science,Computer Science
"Deep Illumination: Approximating Dynamic Global Illumination with Generative Adversarial Network   We present Deep Illumination, a novel machine learning technique for
approximating global illumination (GI) in real-time applications using a
Conditional Generative Adversarial Network. Our primary focus is on generating
indirect illumination and soft shadows with offline rendering quality at
interactive rates. Inspired from recent advancement in image-to-image
translation problems using deep generative convolutional networks, we introduce
a variant of this network that learns a mapping from Gbuffers (depth map,
normal map, and diffuse map) and direct illumination to any global illumination
solution. Our primary contribution is showing that a generative model can be
used to learn a density estimation from screen space buffers to an advanced
illumination model for a 3D environment. Once trained, our network can
approximate global illumination for scene configurations it has never
encountered before within the environment it was trained on. We evaluate Deep
Illumination through a comparison with both a state of the art real-time GI
technique (VXGI) and an offline rendering GI technique (path tracing). We show
that our method produces effective GI approximations and is also
computationally cheaper than existing GI techniques. Our technique has the
potential to replace existing precomputed and screen-space techniques for
producing global illumination effects in dynamic scenes with physically-based
rendering quality.
",Computer Science,Computer Science
"Reconstruction of a compact Riemannian manifold from the scattering data of internal sources   Given a smooth non-trapping compact manifold with strictly con- vex boundary,
we consider an inverse problem of reconstructing the manifold from the
scattering data initiated from internal sources. This data consist of the exit
directions of geodesics that are emaneted from interior points of the manifold.
We show that under certain generic assumption of the metric, one can
reconstruct an isometric copy of the manifold from such scattering data
measured on the boundary.
",Mathematics,Mathematics
"The Impact of Antenna Height Difference on the Performance of Downlink Cellular Networks   Capable of significantly reducing cell size and enhancing spatial reuse,
network densification is shown to be one of the most dominant approaches to
expand network capacity. Due to the scarcity of available spectrum resources,
nevertheless, the over-deployment of network infrastructures, e.g., cellular
base stations (BSs), would strengthen the inter-cell interference as well, thus
in turn deteriorating the system performance. On this account, we investigate
the performance of downlink cellular networks in terms of user coverage
probability (CP) and network spatial throughput (ST), aiming to shed light on
the limitation of network densification. Notably, it is shown that both CP and
ST would be degraded and even diminish to be zero when BS density is
sufficiently large, provided that practical antenna height difference (AHD)
between BSs and users is involved to characterize pathloss. Moreover, the
results also reveal that the increase of network ST is at the expense of the
degradation of CP. Therefore, to balance the tradeoff between user and network
performance, we further study the critical density, under which ST could be
maximized under the CP constraint. Through a special case study, it follows
that the critical density is inversely proportional to the square of AHD. The
results in this work could provide helpful guideline towards the application of
network densification in the next-generation wireless networks.
",Computer Science,Computer Science
"A comparative study of fairness-enhancing interventions in machine learning   Computers are increasingly used to make decisions that have significant
impact in people's lives. Often, these predictions can affect different
population subgroups disproportionately. As a result, the issue of fairness has
received much recent interest, and a number of fairness-enhanced classifiers
and predictors have appeared in the literature. This paper seeks to study the
following questions: how do these different techniques fundamentally compare to
one another, and what accounts for the differences? Specifically, we seek to
bring attention to many under-appreciated aspects of such fairness-enhancing
interventions. Concretely, we present the results of an open benchmark we have
developed that lets us compare a number of different algorithms under a variety
of fairness measures, and a large number of existing datasets. We find that
although different algorithms tend to prefer specific formulations of fairness
preservations, many of these measures strongly correlate with one another. In
addition, we find that fairness-preserving algorithms tend to be sensitive to
fluctuations in dataset composition (simulated in our benchmark by varying
training-test splits), indicating that fairness interventions might be more
brittle than previously thought.
",Statistics,Computer Science; Statistics
"Light curves of hydrogen-poor Superluminous Supernovae from the Palomar Transient Factory   We investigate the light-curve properties of a sample of 26 spectroscopically
confirmed hydrogen-poor superluminous supernovae (SLSNe-I) in the Palomar
Transient Factory (PTF) survey. These events are brighter than SNe Ib/c and SNe
Ic-BL, on average, by about 4 and 2~mag, respectively. The peak absolute
magnitudes of SLSNe-I in rest-frame $g$ band span $-22\lesssim M_g
\lesssim-20$~mag, and these peaks are not powered by radioactive $^{56}$Ni,
unless strong asymmetries are at play. The rise timescales are longer for SLSNe
than for normal SNe Ib/c, by roughly 10 days, for events with similar decay
times. Thus, SLSNe-I can be considered as a separate population based on
photometric properties. After peak, SLSNe-I decay with a wide range of slopes,
with no obvious gap between rapidly declining and slowly declining events. The
latter events show more irregularities (bumps) in the light curves at all
times. At late times, the SLSN-I light curves slow down and cluster around the
$^{56}$Co radioactive decay rate. Powering the late-time light curves with
radioactive decay would require between 1 and 10${\rm M}_\odot$ of Ni masses.
Alternatively, a simple magnetar model can reasonably fit the majority of
SLSNe-I light curves, with four exceptions, and can mimic the radioactive decay
of $^{56}$Co, up to $\sim400$ days from explosion. The resulting spin values do
not correlate with the host-galaxy metallicities. Finally, the analysis of our
sample cannot strengthen the case for using SLSNe-I for cosmology.
",Physics,Physics
"Instabilities of Internal Gravity Wave Beams   Internal gravity waves play a primary role in geophysical fluids: they
contribute significantly to mixing in the ocean and they redistribute energy
and momentum in the middle atmosphere. Until recently, most studies were
focused on plane wave solutions. However, these solutions are not a
satisfactory description of most geophysical manifestations of internal gravity
waves, and it is now recognized that internal wave beams with a confined
profile are ubiquitous in the geophysical context.
We will discuss the reason for the ubiquity of wave beams in stratified
fluids, related to the fact that they are solutions of the nonlinear governing
equations. We will focus more specifically on situations with a constant
buoyancy frequency. Moreover, in light of recent experimental and analytical
studies of internal gravity beams, it is timely to discuss the two main
mechanisms of instability for those beams. i) The Triadic Resonant Instability
generating two secondary wave beams. ii) The streaming instability
corresponding to the spontaneous generation of a mean flow.
",Physics,Physics
"Some remarks on Huisken's monotonicity formula for mean curvature flow   We discuss a monotone quantity related to Huisken's monotonicity formula and
some technical consequences for mean curvature flow.
",Mathematics,Mathematics
"Predicting Future Machine Failure from Machine State Using Logistic Regression   Accurately predicting machine failures in advance can decrease maintenance
cost and help allocate maintenance resources more efficiently. Logistic
regression was applied to predict machine state 24 hours in the future given
the current machine state.
",Statistics,Computer Science
"Exponentially Slow Heating in Short and Long-range Interacting Floquet Systems   We analyze the dynamics of periodically-driven (Floquet) Hamiltonians with
short- and long-range interactions, finding clear evidence for a thermalization
time, $\tau^*$, that increases exponentially with the drive frequency. We
observe this behavior, both in systems with short-ranged interactions, where
our results are consistent with rigorous bounds, and in systems with long-range
interactions, where such bounds do not exist at present. Using a combination of
heating and entanglement dynamics, we explicitly extract the effective energy
scale controlling the rate of thermalization. Finally, we demonstrate that for
times shorter than $\tau^*$, the dynamics of the system is well-approximated by
evolution under a time-independent Hamiltonian $D_{\mathrm{eff}}$, for both
short- and long-range interacting systems.
",Physics,Physics
"A recommender system to restore images with impulse noise   We build a collaborative filtering recommender system to restore images with
impulse noise for which the noisy pixels have been previously identified. We
define this recommender system in terms of a new color image representation
using three matrices that depend on the noise-free pixels of the image to
restore, and two parameters: $k$, the number of features; and $\lambda$, the
regularization factor. We perform experiments on a well known image database to
test our algorithm and we provide image quality statistics for the results
obtained. We discuss the roles of bias and variance in the performance of our
algorithm as determined by the values of $k$ and $\lambda$, and provide
guidance on how to choose the values of these parameters. Finally, we discuss
the possibility of using our collaborative filtering recommender system to
perform image inpainting and super-resolution.
",Computer Science; Statistics,Computer Science
"Online Factorization and Partition of Complex Networks From Random Walks   Finding the reduced-dimensional structure is critical to understanding
complex networks. Existing approaches such as spectral clustering are
applicable only when the full network is explicitly observed. In this paper, we
focus on the online factorization and partition of implicit large-scale
networks based on observations from an associated random walk. We formulate
this into a nonconvex stochastic factorization problem and propose an efficient
and scalable stochastic generalized Hebbian algorithm. The algorithm is able to
process dependent state-transition data dynamically generated by the underlying
network and learn a low-dimensional representation for each vertex. By applying
a diffusion approximation analysis, we show that the continuous-time limiting
process of the stochastic algorithm converges globally to the ""principal
components"" of the Markov chain and achieves a nearly optimal sample
complexity. Once given the learned low-dimensional representations, we further
apply clustering techniques to recover the network partition. We show that when
the associated Markov process is lumpable, one can recover the partition
exactly with high probability. We apply the proposed approach to model the
traffic flow of Manhattan as city-wide random walks. By using our algorithm to
analyze the taxi trip data, we discover a latent partition of the Manhattan
city that closely matches the traffic dynamics.
",Computer Science; Mathematics; Statistics,Computer Science; Statistics
"Social evolution of structural discrimination   Structural discrimination appears to be a persistent phenomenon in social
systems. We here outline the hypothesis that it can result from the
evolutionary dynamics of the social system itself. We study the evolutionary
dynamics of agents with neutral badges in a simple social game and find that
the badges are readily discriminated by the system although not being tied to
the payoff matrix of the game. The sole property of being distinguishable leads
to the subsequent discrimination, therefore providing a model for the emergence
and freezing of social prejudice.
",Physics,Quantitative Biology
"Solitary wave solutions and their interactions for fully nonlinear water waves with surface tension in the generalized Serre equations   Some effects of surface tension on fully-nonlinear, long, surface water waves
are studied by numerical means. The differences between various solitary waves
and their interactions in subcritical and supercritical surface tension regimes
are presented. Analytical expressions for new peaked travelling wave solutions
are presented in the case of critical surface tension. The numerical
experiments were performed using a high-accurate finite element method based on
smooth cubic splines and the four-stage, classical, explicit Runge-Kutta method
of order four.
",Physics; Mathematics,Physics
"Hidden Fermi Liquidity and Topological Criticality in the Finite Temperature Kitaev Model   The fate of exotic spin liquid states with fractionalized excitations at
finite temperature ($T$) is of great interest, since signatures of
fractionalization manifest in finite-temperature ($T$) dynamics in real
systems, above the tiny magnetic ordering scales. Here, we study a
Jordan-Wigner fermionized Kitaev spin liquid at finite $T$ employing combined
Exact diagonalization and Monte Carlo simulation methods. We uncover $(i)$
checkerboard or stripy-ordered flux crystals depending on density of flux, and
$(ii)$ establish, surprisingly, that: $(a)$ the finite-$T$ version of the $T=0$
transition from a gapless to gapped phases in the Kitaev model is a Mott
transition of the fermions, belonging to the two-dimensional Ising universality
class. These transitions correspond to a topological transition between a
string condensate and a dilute closed string state $(b)$ the Mott ""insulator""
phase is a precise realization of Laughlin's gossamer (here, p-wave)
superconductor (g-SC), and $(c)$ the Kitaev Toric Code phase (TC) is a {\it
fully} Gutzwiller-projected p-wave SC. These findings establish the finite-$T$
QSL phases in the $d = 2$ to be {\it hidden} Fermi liquid(s) of neutral
fermions.
",Physics,Physics
"The Accuracy of Confidence Intervals for Field Normalised Indicators   When comparing the average citation impact of research groups, universities
and countries, field normalisation reduces the influence of discipline and
time. Confidence intervals for these indicators can help with attempts to infer
whether differences between sets of publications are due to chance factors.
Although both bootstrapping and formulae have been proposed for these, their
accuracy is unknown. In response, this article uses simulated data to
systematically compare the accuracy of confidence limits in the simplest
possible case, a single field and year. The results suggest that the MNLCS
(Mean Normalised Log-transformed Citation Score) confidence interval formula is
conservative for large groups but almost always safe, whereas bootstrap MNLCS
confidence intervals tend to be accurate but can be unsafe for smaller world or
group sample sizes. In contrast, bootstrap MNCS (Mean Normalised Citation
Score) confidence intervals can be very unsafe, although their accuracy
increases with sample sizes.
",Computer Science,Computer Science; Statistics
"Rethinking Information Sharing for Actionable Threat Intelligence   In the past decade, the information security and threat landscape has grown
significantly making it difficult for a single defender to defend against all
attacks at the same time. This called for introduc- ing information sharing, a
paradigm in which threat indicators are shared in a community of trust to
facilitate defenses. Standards for representation, exchange, and consumption of
indicators are pro- posed in the literature, although various issues are
undermined. In this paper, we rethink information sharing for actionable
intelli- gence, by highlighting various issues that deserve further explo-
ration. We argue that information sharing can benefit from well- defined use
models, threat models, well-understood risk by mea- surement and robust
scoring, well-understood and preserved pri- vacy and quality of indicators and
robust mechanism to avoid free riding behavior of selfish agent. We call for
using the differential nature of data and community structures for optimizing
sharing.
",Computer Science,Computer Science
"Rydberg states of helium in electric and magnetic fields of arbitrary relative orientation   A spectroscopic study of Rydberg states of helium ($n$ = 30 and 45) in
magnetic, electric and combined magnetic and electric fields with arbitrary
relative orientations of the field vectors is presented. The emphasis is on two
special cases where (i) the diamagnetic term is negligible and both
paramagnetic Zeeman and Stark effects are linear ($n$ = 30, $B \leq$ 120 mT and
$F$ = 0 - 78 V/cm ), and (ii) the diamagnetic term is dominant and the Stark
effect is linear ($n$ = 45, $B$ = 277 mT and $F$ = 0 - 8 V/cm). Both cases
correspond to regimes where the interactions induced by the electric and
magnetic fields are much weaker than the Coulomb interaction, but much stronger
than the spin-orbit interaction. The experimental spectra are compared to
spectra calculated by determining the eigenvalues of the Hamiltonian matrix
describing helium Rydberg states in the external fields. The spectra and the
calculated energy-level diagrams in external fields reveal avoided crossings
between levels of different $m_l$ values and pronounced $m_l$-mixing effects at
all angles between the electric and magnetic field vectors other than 0. These
observations are discussed in the context of the development of a method to
generate dense samples of cold atoms and molecules in a magnetic trap following
Rydberg-Stark deceleration.
",Physics,Physics
"Topological and non inertial effects on the interbank light absorption   In this work, we investigate the combined influence of the nontrivial
topology introduced by a disclination and non inertial effects due to rotation,
in the energy levels and the wave functions of a noninteracting electron gas
confined to a two-dimensional pseudoharmonic quantum dot, under the influence
of an external uniform magnetic field. The exact solutions for energy
eigenvalues and wave functions are computed as functions of the applied
magnetic field strength, the disclination topological charge, magnetic quantum
number and the rotation speed of the sample. We investigate the modifications
on the light interband absorption coefficient and absorption threshold
frequency. We observe novel features in the system, including a range of
magnetic field without corresponding absorption phenomena, which is due to a
tripartite term of the Hamiltonian, involving magnetic field, the topological
charge of the defect and the rotation frequency.
",Physics,Physics
"Persistent Monitoring of Dynamically Changing Environments Using an Unmanned Vehicle   We consider the problem of planning a closed walk $\mathcal W$ for a UAV to
persistently monitor a finite number of stationary targets with equal
priorities and dynamically changing properties. A UAV must physically visit the
targets in order to monitor them and collect information therein. The frequency
of monitoring any given target is specified by a target revisit time, $i.e.$,
the maximum allowable time between any two successive visits to the target. The
problem considered in this paper is the following: Given $n$ targets and $k
\geq n$ allowed visits to them, find an optimal closed walk $\mathcal W^*(k)$
so that every target is visited at least once and the maximum revisit time over
all the targets, $\mathcal R(\mathcal W(k))$, is minimized. We prove the
following: If $k \geq n^2-n$, $\mathcal R(\mathcal W^*(k))$ (or simply,
$\mathcal R^*(k)$) takes only two values: $\mathcal R^*(n)$ when $k$ is an
integral multiple of $n$, and $\mathcal R^*(n+1)$ otherwise. This result
suggests significant computational savings - one only needs to determine
$\mathcal W^*(n)$ and $\mathcal W^*(n+1)$ to construct an optimal solution
$\mathcal W^*(k)$. We provide MILP formulations for computing $\mathcal W^*(n)$
and $\mathcal W^*(n+1)$. Furthermore, for {\it any} given $k$, we prove that
$\mathcal R^*(k) \geq \mathcal R^*(k+n)$.
",Computer Science,Computer Science
"Boltzmann Exploration Done Right   Boltzmann exploration is a classic strategy for sequential decision-making
under uncertainty, and is one of the most standard tools in Reinforcement
Learning (RL). Despite its widespread use, there is virtually no theoretical
understanding about the limitations or the actual benefits of this exploration
scheme. Does it drive exploration in a meaningful way? Is it prone to
misidentifying the optimal actions or spending too much time exploring the
suboptimal ones? What is the right tuning for the learning rate? In this paper,
we address several of these questions in the classic setup of stochastic
multi-armed bandits. One of our main results is showing that the Boltzmann
exploration strategy with any monotone learning-rate sequence will induce
suboptimal behavior. As a remedy, we offer a simple non-monotone schedule that
guarantees near-optimal performance, albeit only when given prior access to key
problem parameters that are typically not available in practical situations
(like the time horizon $T$ and the suboptimality gap $\Delta$). More
importantly, we propose a novel variant that uses different learning rates for
different arms, and achieves a distribution-dependent regret bound of order
$\frac{K\log^2 T}{\Delta}$ and a distribution-independent bound of order
$\sqrt{KT}\log K$ without requiring such prior knowledge. To demonstrate the
flexibility of our technique, we also propose a variant that guarantees the
same performance bounds even if the rewards are heavy-tailed.
",Computer Science; Statistics,Computer Science; Statistics
"Deep Learning for Classification Tasks on Geospatial Vector Polygons   In this paper, we evaluate the accuracy of deep learning approaches on
geospatial vector geometry classification tasks. The purpose of this evaluation
is to investigate the ability of deep learning models to learn from geometry
coordinates directly. Previous machine learning research applied to geospatial
polygon data did not use geometries directly, but derived properties thereof.
These are produced by way of extracting geometry properties such as Fourier
descriptors. Instead, our introduced deep neural net architectures are able to
learn on sequences of coordinates mapped directly from polygons. In three
classification tasks we show that the deep learning architectures are
competitive with common learning algorithms that require extracted features.
",Statistics,Computer Science; Statistics
"Fibers in the NGC1333 proto-cluster   Are the initial conditions for clustered star formation the same as for
non-clustered star formation? To investigate the initial gas properties in
young proto-clusters we carried out a comprehensive and high-sensitivity study
of the internal structure, density, temperature, and kinematics of the dense
gas content of the NGC1333 region in Perseus, one of the nearest and best
studied embedded clusters. The analysis of the gas velocities in the
Position-Position-Velocity space reveals an intricate underlying gas
organization both in space and velocity. We identified a total of 14
velocity-coherent, (tran-)sonic structures within NGC1333, with similar
physical and kinematic properties than those quiescent, star-forming (aka
fertile) fibers previously identified in low-mass star-forming clouds. These
fibers are arranged in a complex spatial network, build-up the observed total
column density, and contain the dense cores and protostars in this cloud. Our
results demonstrate that the presence of fibers is not restricted to low-mass
clouds but can be extended to regions of increasing mass and complexity. We
propose that the observational dichotomy between clustered and non-clustered
star-forming regions might be naturally explained by the distinct spatial
density of fertile fibers in these environments.
",Physics,Physics
"Monotonicity of non-pluripolar products and complex Monge-Ampère equations with prescribed singularity   We establish the monotonicity property for the mass of non-pluripolar
products on compact Kahler manifolds, and we initiate the study of complex
Monge-Ampere type equations with prescribed singularity type. Using the
variational method of Berman-Boucksom-Guedj-Zeriahi we prove existence and
uniqueness of solutions with small unbounded locus. We give applications to
Kahler-Einstein metrics with prescribed singularity, and we show that the
log-concavity property holds for non-pluripolar products with small unbounded
locus.
",Mathematics,Mathematics
"Imitating Driver Behavior with Generative Adversarial Networks   The ability to accurately predict and simulate human driving behavior is
critical for the development of intelligent transportation systems. Traditional
modeling methods have employed simple parametric models and behavioral cloning.
This paper adopts a method for overcoming the problem of cascading errors
inherent in prior approaches, resulting in realistic behavior that is robust to
trajectory perturbations. We extend Generative Adversarial Imitation Learning
to the training of recurrent policies, and we demonstrate that our model
outperforms rule-based controllers and maximum likelihood models in realistic
highway simulations. Our model both reproduces emergent behavior of human
drivers, such as lane change rate, while maintaining realistic control over
long time horizons.
",Computer Science,Computer Science; Statistics
"Time-reversed magnetically controlled perturbation (TRMCP) optical focusing inside scattering media   Manipulating and focusing light deep inside biological tissue and tissue-like
complex media has been desired for long yet considered challenging. One
feasible strategy is through optical wavefront engineering, where the optical
scattering-induced phase distortions are time reversed or pre-compensated so
that photons travel along different optical paths interfere constructively at
the targeted position within a scattering medium. To define the targeted
position, an internal guidestar is needed to guide or provide a feedback for
wavefront engineering. It could be injected or embedded probes such as
fluorescence or nonlinear microspheres, ultrasonic modulation, as well as
absorption perturbation. Here we propose to use a magnetically controlled
optical absorbing microsphere as the internal guidestar. Using a digital
optical phase conjugation system, we obtained sharp optical focusing within
scattering media through time-reversing the scattered light perturbed by the
magnetic microshpere. Since the object is magnetically controlled, dynamic
optical focusing is allowed with a relatively large field-of-view by scanning
the magnetic field externally. Moreover, the magnetic microsphere can be
packaged with an organic membrane, using biological or chemical means to serve
as a carrier. Therefore the technique may find particular applications for
enhanced targeted drug delivery, and imaging and photoablation of angiogenic
vessels in tumours.
",Physics,Physics
"Computability of semicomputable manifolds in computable topological spaces   We study computable topological spaces and semicomputable and computable sets
in these spaces. In particular, we investigate conditions under which
semicomputable sets are computable. We prove that a semicomputable compact
manifold $M$ is computable if its boundary $\partial M$ is computable. We also
show how this result combined with certain construction which compactifies a
semicomputable set leads to the conclusion that some noncompact semicomputable
manifolds in computable metric spaces are computable.
",Computer Science; Mathematics,Mathematics
"Twists of quantum Borel algebras   We classify Drinfeld twists for the quantum Borel subalgebra u_q(b) in the
Frobenius-Lusztig kernel u_q(g), where g is a simple Lie algebra over C and q
an odd root of unity. More specifically, we show that alternating forms on the
character group of the group of grouplikes for u_q(b) generate all twists for
u_q(b), under a certain algebraic group action. This implies a simple
classification of Hopf algebras whose categories of representations are tensor
equivalent to that of u_q(b). We also show that cocycle twists for the
corresponding De Concini-Kac algebra are in bijection with alternating forms on
the aforementioned character group.
",Mathematics,Mathematics
"Robust, Deep and Inductive Anomaly Detection   PCA is a classical statistical technique whose simplicity and maturity has
seen it find widespread use as an anomaly detection technique. However, it is
limited in this regard by being sensitive to gross perturbations of the input,
and by seeking a linear subspace that captures normal behaviour. The first
issue has been dealt with by robust PCA, a variant of PCA that explicitly
allows for some data points to be arbitrarily corrupted, however, this does not
resolve the second issue, and indeed introduces the new issue that one can no
longer inductively find anomalies on a test set. This paper addresses both
issues in a single model, the robust autoencoder. This method learns a
nonlinear subspace that captures the majority of data points, while allowing
for some data to have arbitrary corruption. The model is simple to train and
leverages recent advances in the optimisation of deep neural networks.
Experiments on a range of real-world datasets highlight the model's
effectiveness.
",Computer Science; Statistics,Computer Science; Statistics
"High resolution structural characterisation of laser-induced defect clusters inside diamond   Laser writing with ultrashort pulses provides a potential route for the
manufacture of three-dimensional wires, waveguides and defects within diamond.
We present a transmission electron microscopy (TEM) study of the intrinsic
structure of the laser modifications and reveal a complex distribution of
defects. Electron energy loss spectroscopy (EELS) indicates that the majority
of the irradiated region remains as $sp^3$ bonded diamond.
Electrically-conductive paths are attributed to the formation of multiple
nano-scale, $sp^2$-bonded graphitic wires and a network of strain-relieving
micro-cracks.
",Physics,Physics
"Deep Active Learning for Named Entity Recognition   Deep learning has yielded state-of-the-art performance on many natural
language processing tasks including named entity recognition (NER). However,
this typically requires large amounts of labeled data. In this work, we
demonstrate that the amount of labeled training data can be drastically reduced
when deep learning is combined with active learning. While active learning is
sample-efficient, it can be computationally expensive since it requires
iterative retraining. To speed this up, we introduce a lightweight architecture
for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and
word encoders and a long short term memory (LSTM) tag decoder. The model
achieves nearly state-of-the-art performance on standard datasets for the task
while being computationally much more efficient than best performing models. We
carry out incremental active learning, during the training process, and are
able to nearly match state-of-the-art performance with just 25\% of the
original training data.
",Computer Science,Computer Science
"An Expanded Local Variance Gamma model   The paper proposes an expanded version of the Local Variance Gamma model of
Carr and Nadtochiy by adding drift to the governing underlying process. Still
in this new model it is possible to derive an ordinary differential equation
for the option price which plays a role of Dupire's equation for the standard
local volatility model. It is shown how calibration of multiple smiles (the
whole local volatility surface) can be done in such a case. Further, assuming
the local variance to be a piecewise linear function of strike and piecewise
constant function of time this ODE is solved in closed form in terms of
Confluent hypergeometric functions. Calibration of the model to market smiles
does not require solving any optimization problem and, in contrast, can be done
term-by-term by solving a system of non-linear algebraic equations for each
maturity, which is fast.
",Quantitative Finance,Quantitative Finance
"Deep Encoder-Decoder Models for Unsupervised Learning of Controllable Speech Synthesis   Generating versatile and appropriate synthetic speech requires control over
the output expression separate from the spoken text. Important non-textual
speech variation is seldom annotated, in which case output control must be
learned in an unsupervised fashion. In this paper, we perform an in-depth study
of methods for unsupervised learning of control in statistical speech
synthesis. For example, we show that popular unsupervised training heuristics
can be interpreted as variational inference in certain autoencoder models. We
additionally connect these models to VQ-VAEs, another, recently-proposed class
of deep variational autoencoders, which we show can be derived from a very
similar mathematical argument. The implications of these new probabilistic
interpretations are discussed. We illustrate the utility of the various
approaches with an application to acoustic modelling for emotional speech
synthesis, where the unsupervised methods for learning expression control
(without access to emotional labels) are found to give results that in many
aspects match or surpass the previous best supervised approach.
",Computer Science; Statistics,Computer Science; Statistics
"A Hybrid Deep Learning Architecture for Privacy-Preserving Mobile Analytics   Deep Neural Networks are increasingly being used in a variety of machine
learning applications applied to user data on the cloud. However, this approach
introduces a number of privacy and efficiency challenges, as the cloud operator
can perform secondary inferences on the available data. Recently, advances in
edge processing have paved the way for more efficient, and private, data
processing at the source for simple tasks and lighter models, though they
remain a challenge for larger, and more complicated models. In this paper, we
present a hybrid approach for breaking down large, complex deep models for
cooperative, privacy-preserving analytics. We do this by breaking down the
popular deep architectures and fine-tune them in a suitable way. We then
evaluate the privacy benefits of this approach based on the information exposed
to the cloud service. We also assess the local inference cost of different
layers on a modern handset for mobile applications. Our evaluations show that
by using certain kind of fine-tuning and embedding techniques and at a small
processing cost, we can greatly reduce the level of information available to
unintended tasks applied to the data features on the cloud, and hence achieving
the desired tradeoff between privacy and performance.
",Computer Science,Computer Science
"An Earth-mass Planet in a 1-AU Orbit around an Ultracool Dwarf   We combine $Spitzer$ and ground-based KMTNet microlensing observations to
identify and precisely measure an Earth-mass ($1.43^{+0.45}_{-0.32} M_\oplus$)
planet OGLE-2016-BLG-1195Lb at $1.16^{+0.16}_{-0.13}$ AU orbiting a
$0.078^{+0.016}_{-0.012} M_\odot$ ultracool dwarf. This is the lowest-mass
microlensing planet to date. At $3.91^{+0.42}_{-0.46}$ kpc, it is the third
consecutive case among the $Spitzer$ ""Galactic distribution"" planets toward the
Galactic bulge that lies in the Galactic disk as opposed to the bulge itself,
hinting at a skewed distribution of planets. Together with previous
microlensing discoveries, the seven Earth-size planets orbiting the ultracool
dwarf TRAPPIST-1, and the detection of disks around young brown dwarfs,
OGLE-2016-BLG-1195Lb suggests that such planets might be common around
ultracool dwarfs. It therefore sheds light on the formation of both ultracool
dwarfs and planetary systems at the limit of low-mass protoplanetary disks.
",Physics,Physics
"Centralities of Nodes and Influences of Layers in Large Multiplex Networks   We formulate and propose an algorithm (MultiRank) for the ranking of nodes
and layers in large multiplex networks. MultiRank takes into account the full
multiplex network structure of the data and exploits the dual nature of the
network in terms of nodes and layers. The proposed centrality of the layers
(influences) and the centrality of the nodes are determined by a coupled set of
equations. The basic idea consists in assigning more centrality to nodes that
receive links from highly influential layers and from already central nodes.
The layers are more influential if highly central nodes are active in them. The
algorithm applies to directed/undirected as well as to weighted/unweighted
multiplex networks. We discuss the application of MultiRank to three major
examples of multiplex network datasets: the European Air Transportation
Multiplex Network, the Pierre Auger Multiplex Collaboration Network and the FAO
Multiplex Trade Network.
",Computer Science; Physics,Computer Science
"On certain weighted 7-colored partitions   Inspired by Andrews' 2-colored generalized Frobenius partitions, we consider
certain weighted 7-colored partition functions and establish some interesting
Ramanujan-type identities and congruences. Moreover, we provide combinatorial
interpretations of some congruences modulo 5 and 7. Finally, we study the
properties of weighted 7-colored partitions weighted by the parity of certain
partition statistics.
",Mathematics,Mathematics
"On the optimal investment-consumption and life insurance selection problem with an external stochastic factor   In this paper, we study a stochastic optimal control problem with stochastic
volatility. We prove the sufficient and necessary maximum principle for the
proposed problem. Then we apply the results to solve an investment, consumption
and life insurance problem with stochastic volatility, that is, we consider a
wage earner investing in one risk-free asset and one risky asset described by a
jump-diffusion process and has to decide concerning consumption and life
insurance purchase. We assume that the life insurance for the wage earner is
bought from a market composed of $M>1$ life insurance companies offering
pairwise distinct life insurance contracts. The goal is to maximize the
expected utilities derived from the consumption, the legacy in the case of a
premature death and the investor's terminal wealth.
",Quantitative Finance,Quantitative Finance
"An Optimization Based Control Framework for Balancing and Walking: Implementation on the iCub Robot   A whole-body torque control framework adapted for balancing and walking tasks
is presented in this paper. In the proposed approach, centroidal momentum terms
are excluded in favor of a hierarchy of high-priority position and orientation
tasks and a low-priority postural task. More specifically, the controller
stabilizes the position of the center of mass, the orientation of the pelvis
frame, as well as the position and orientation of the feet frames. The
low-priority postural task provides reference positions for each joint of the
robot. Joint torques and contact forces to stabilize tasks are obtained through
quadratic programming optimization. Besides the exclusion of centroidal
momentum terms, part of the novelty of the approach lies in the definition of
control laws in SE(3) which do not require the use of Euler parameterization.
Validation of the framework was achieved in a scenario where the robot kept
balance while walking in place. Experiments have been conducted with the iCub
robot, in simulation and in real-world experiments.
",Computer Science,Computer Science
"Well-balanced mesh-based and meshless schemes for the shallow-water equations   We formulate a general criterion for the exact preservation of the ""lake at
rest"" solution in general mesh-based and meshless numerical schemes for the
strong form of the shallow-water equations with bottom topography. The main
idea is a careful mimetic design for the spatial derivative operators in the
momentum flux equation that is paired with a compatible averaging rule for the
water column height arising in the bottom topography source term. We prove
consistency of the mimetic difference operators analytically and demonstrate
the well-balanced property numerically using finite difference and RBF-FD
schemes in the one- and two-dimensional cases.
",Physics; Mathematics,Mathematics
"Lattice thermal expansion and anisotropic displacements in urea, bromomalonic aldehyde, pentachloropyridine and naphthalene   Anisotropic displacement parameters (ADPs) are commonly used in
crystallography, chemistry and related fields to describe and quantify thermal
motion of atoms. Within the very recent years, these ADPs have become
predictable by lattice dynamics in combination with first-principles theory.
Here, we study four very different molecular crystals, namely urea,
bromomalonic aldehyde, pentachloropyridine, and naphthalene, by
first-principles theory to assess the quality of ADPs calculated in the
quasi-harmonic approximation. In addition, we predict both thermal expansion
and thermal motion within the quasi-harmonic approximation and compare the
predictions with experimental data. Very reliable ADPs are calculated within
the quasi-harmonic approximation for all four cases up to at least 200 K, and
they turn out to be in better agreement with experiment than the harmonic ones.
In one particular case, ADPs can even reliably be predicted up to room
temperature. Our results also hint at the importance of normal-mode
anharmonicity in the calculation of ADPs.
",Physics,Physics
"From atomistic model to the Peierls-Nabarro model with $γ$-surface for dislocations   The Peierls-Nabarro (PN) model for dislocations is a hybrid model that
incorporates the atomistic information of the dislocation core structure into
the continuum theory. In this paper, we study the convergence from a full
atomistic model to the PN model with $\gamma$-surface for the dislocation in a
bilayer system (e.g. bilayer graphene). We prove that the displacement field of
and the total energy of the dislocation solution of the PN model are
asymptotically close to those of the full atomistic model. Our work can be
considered as a generalization of the analysis of the convergence from
atomistic model to Cauchy-Born rule for crystals without defects in the
literature.
",Mathematics,Physics
"Community Detection in the Network of German Princes in 1225: a Case Study   Many social networks exhibit some underlying community structure. In
particular, in the context of historical research, clustering of different
groups into warring or friendly factions can lead to a better understanding of
how conflicts may arise, and whether they could be avoided or not. In this work
we study the crisis that started in 1225 when the Emperor of the Holy Roman
Empire, Frederick II and his son Henry VII got into a conflict which almost led
to the rupture and dissolution of the Empire. We use a spin-glass-based
community detection algorithm to see how good this method is in detecting this
rift and compare the results with an analysis performed by one of the authors
(Gramsch) using standard social balance theory applied to History.
",Computer Science; Physics,Computer Science; Physics
"Bilinear approach to the supersymmetric Gardner equation   We study a supersymmetric version of the Gardner equation (both focusing and
defocusing) using the superbilinear formalism. This equation is new and cannot
be obtained from supersymmetric modified Korteweg-de Vries equation with a
nonzero boundary condition. We construct supersymmetric solitons and then by
passing to the long-wave limit in the focusing case obtain rational nonsingular
solutions. We also discuss the supersymmetric version of the defocusing
equation and the dynamics of its solutions.
",Physics,Mathematics
"Magnetization jump in one dimensional $J-Q_{2}$ model with anisotropic exchange   We investigate the adiabatic magnetization process of the one-dimensional
$J-Q_{2}$ model with XXZ anisotropy $g$ in an external magnetic field $h$ by
using density matrix renormalization group (DMRG) method. According to the
characteristic of the magnetization curves, we draw a magnetization phase
diagram consisting of four phases. For a fixed nonzero pair coupling $Q$, i)
when $g<-1$, the ground state is always ferromagnetic in spite of $h$; ii) when
$g>-1$ but still small, the whole magnetization curve is continuous and smooth;
iii) if further increasing $g$, there is a macroscopic magnetization jump from
partially- to fully-polarized state; iv) for a sufficiently large $g$, the
magnetization jump is from non- to fully-polarized state. By examining the
energy per magnon and the correlation function, we find that the origin of the
magnetization jump is the condensation of magnons and the formation of magnetic
domains. We also demonstrate that while the experienced states are
Heisenberg-like without long-range order, all the \textit{jumped-over} states
have antiferromagnetic or Néel long-range orders, or their mixing.
",Physics,Physics
"Convergence Analysis of Gradient EM for Multi-component Gaussian Mixture   In this paper, we study convergence properties of the gradient
Expectation-Maximization algorithm \cite{lange1995gradient} for Gaussian
Mixture Models for general number of clusters and mixing coefficients. We
derive the convergence rate depending on the mixing coefficients, minimum and
maximum pairwise distances between the true centers and dimensionality and
number of components; and obtain a near-optimal local contraction radius. While
there have been some recent notable works that derive local convergence rates
for EM in the two equal mixture symmetric GMM, in the more general case, the
derivations need structurally different and non-trivial arguments. We use
recent tools from learning theory and empirical processes to achieve our
theoretical results.
",Computer Science; Mathematics; Statistics,Computer Science; Statistics
"Stochastic Evolution of Augmented Born--Infeld Equations   This paper compares the results of applying a recently developed method of
stochastic uncertainty quantification designed for fluid dynamics to the
Born-Infeld model of nonlinear electromagnetism. The similarities in the
results are striking. Namely, the introduction of Stratonovich cylindrical
noise into each of their Hamiltonian formulations introduces stochastic Lie
transport into their dynamics in the same form for both theories. Moreover, the
resulting stochastic partial differential equations (SPDE) retain their
unperturbed form, except for an additional term representing induced Lie
transport by the set of divergence-free vector fields associated with the
spatial correlations of the cylindrical noise. The explanation for this
remarkable similarity lies in the method of construction of the Hamiltonian for
the Stratonovich stochastic contribution to the motion in both cases; which is
done via pairing spatial correlation eigenvectors for cylindrical noise with
the momentum map for the deterministic motion. This momentum map is responsible
for the well-known analogy between hydrodynamics and electromagnetism. The
momentum map for the Maxwell and Born-Infeld theories of electromagnetism
treated here is the 1-form density known as the Poynting vector. Two Appendices
treat the Hamiltonian structures underlying these results.
",Physics; Mathematics,Physics
"Design and Analysis of a Secure Three Factor User Authentication Scheme Using Biometric and Smart Card   Password security can no longer provide enough security in the area of remote
user authentication. Considering this security drawback, researchers are trying
to find solution with multifactor remote user authentication system. Recently,
three factor remote user authentication using biometric and smart card has
drawn a considerable attention of the researchers. However, most of the current
proposed schemes have security flaws. They are vulnerable to attacks like user
impersonation attack, server masquerading attack, password guessing attack,
insider attack, denial of service attack, forgery attack, etc. Also, most of
them are unable to provide mutual authentication, session key agreement and
password, or smart card recovery system. Considering these drawbacks, we
propose a secure three factor user authentication scheme using biometric and
smart card. Through security analysis, we show that our proposed scheme can
overcome drawbacks of existing systems and ensure high security in remote user
authentication.
",Computer Science,Computer Science
"Search Engine Drives the Evolution of Social Networks   The search engine is tightly coupled with social networks and is primarily
designed for users to acquire interested information. Specifically, the search
engine assists the information dissemination for social networks, i.e.,
enabling users to access interested contents with keywords-searching and
promoting the process of contents-transferring from the source users directly
to potential interested users. Accompanying such processes, the social network
evolves as new links emerge between users with common interests. However, there
is no clear understanding of such a ""chicken-and-egg"" problem, namely, new
links encourage more social interactions, and vice versa. In this paper, we aim
to quantitatively characterize the social network evolution phenomenon driven
by a search engine. First, we propose a search network model for social network
evolution. Second, we adopt two performance metrics, namely, degree
distribution and network diameter. Theoretically, we prove that the degree
distribution follows an intensified power-law, and the network diameter
shrinks. Third, we quantitatively show that the search engine accelerates the
rumor propagation in social networks. Finally, based on four real-world data
sets (i.e., CDBLP, Facebook, Weibo Tweets, P2P), we verify our theoretical
findings. Furthermore, we find that the search engine dramatically increases
the speed of rumor propagation.
",Computer Science; Physics,Computer Science
"Detection of irregular QRS complexes using Hermite Transform and Support Vector Machine   Computer based recognition and detection of abnormalities in ECG signals is
proposed. For this purpose, the Support Vector Machines (SVM) are combined with
the advantages of Hermite transform representation. SVM represent a special
type of classification techniques commonly used in medical applications.
Automatic classification of ECG could make the work of cardiologic departments
faster and more efficient. It would also reduce the number of false diagnosis
and, as a result, save lives. The working principle of the SVM is based on
translating the data into a high dimensional feature space and separating it
using a linear classificator. In order to provide an optimal representation for
SVM application, the Hermite transform domain is used. This domain is proved to
be suitable because of the similarity of the QRS complex with Hermite basis
functions. The maximal signal information is obtained using a small set of
features that are used for detection of irregular QRS complexes. The aim of the
paper is to show that these features can be employed for automatic ECG signal
analysis.
",Computer Science,Computer Science
"Application of Superhalogens in the Design of Organic Superconductors   Bechgaard salts, (TMTSF)2X (TMTSF = tetramethyl tetraselenafulvalene and X =
complex anion), form the most popular series of organic superconductors. In
these salts, TMTSF molecules act as super-electron donor and X as acceptor. We
computationally examine the electronic structure and properties of X in
commonly used (TMTSF)2X (X = NO3, BF4, ClO4, PF6) superconductors and notice
that they belong to the class of superhalogens due to their higher vertical
detachment energy than halogen anions. This prompted us to choose other
superhalogens such as X = BO2, BH4, B2F7, AuF6 and study their (TMTSF)2X
complexes. Our findings suggest that these complexes behave more or less
similar to those of established (TMTSF)2X superconductors, particularly for X =
BO2 and B2F7. We, therefore, believe that the concept of superhalogen can be
successfully applied in the design of novel organic superconductors.
",Physics,Physics
"Homology of the family of hyperelliptic curves   Homology of braid groups and Artin groups can be related to the study of
spaces of curves. We completely calculate the integral homology of the family
of smooth curves of genus $g$ with one boundary component, that are double
coverings of the disk ramified over $n = 2g + 1$ points. The main part of such
homology is described by the homology of the braid group with coefficients in a
symplectic representation, namely the braid group $\mathrm{Br}_n$ acts on the
first homology group of a genus $g$ surface via Dehn twists. Our computations
shows that such groups have only $2$-torsion. We also investigate stabilization
properties and provide Poincaré series, both for unstable and stable
homology.
",Mathematics,Mathematics
"Dual gauge field theory of quantum liquid crystals in three dimensions   The dislocation-mediated quantum melting of solids into quantum liquid
crystals is extended from two to three spatial dimensions, using a
generalization of boson-vortex or Abelian-Higgs duality. Dislocations are now
Burgers-vector-valued strings that trace out worldsheets in spacetime while the
phonons of the solid dualize into two-form (Kalb-Ramond) gauge fields. We
propose an effective dual Higgs potential that allows for restoring
translational symmetry in either one, two or three directions, leading to the
quantum analogues of columnar, smectic or nematic liquid crystals. In these
phases, transverse phonons turn into gapped, propagating modes while
compressional stress remains massless. Rotational Goldstone modes emerge
whenever translational symmetry is restored. We also consider electrically
charged matter, and find amongst others that as a hard principle only two out
of the possible three rotational Goldstone modes are observable using
electromagnetic means.
",Physics,Physics
"On the evolution of galaxy spin in a cosmological hydrodynamic simulation of galaxy clusters   The traditional view of the morphology-spin connection is being challenged by
recent integral-field-unit observations, as the majority of early-type galaxies
are found to have a rotational component that is often as large as a dispersion
component. Mergers are often suspected to be critical in galaxy spin evolution,
yet the details of their roles are still unclear. We present the first results
on the spin evolution of galaxies in cluster environments through a
cosmological hydrodynamic simulation. Galaxies spin down globally with cosmic
evolution. Major (mass ratios > 1/4) and minor (1/4 $\geq$ mass ratios > 1/50)
mergers are important contributors to the spin down in particular in massive
galaxies. Minor mergers appear to have stronger cumulative effects than major
mergers. Surprisingly, the dominant driver of galaxy spin down seems to be
environmental effects rather than mergers. However, since multiple processes
act in combination, it is difficult to separate their individual roles. We
briefly discuss the caveats and future studies that are called for.
",Physics,Physics
"To Pool or Not To Pool? Revisiting an Old Pattern   We revisit the well-known object-pool design pattern in Java. In the last
decade, the pattern has attracted a lot of criticism regarding its validity
when used for light-weight objects that are only meant to hold memory rather
than any other resources (database connections, sockets etc.) and in fact,
common opinion holds that is an anti-pattern in such cases. Nevertheless, we
show through several experiments in different systems that the use of this
pattern for extremely short-lived and light-weight memory objects can in fact
significantly reduce the response time of high-performance multi-threaded
applications, especially in memory-constrained environments. In certain
multi-threaded applications where high performance is a requirement and/or
memory constraints exist, we recommend therefore that the object pool pattern
be given consideration and tested for possible run-time as well as memory
footprint improvements.
",Computer Science,Computer Science
"Measuring Sample Quality with Kernels   Approximate Markov chain Monte Carlo (MCMC) offers the promise of more rapid
sampling at the cost of more biased inference. Since standard MCMC diagnostics
fail to detect these biases, researchers have developed computable Stein
discrepancy measures that provably determine the convergence of a sample to its
target distribution. This approach was recently combined with the theory of
reproducing kernels to define a closed-form kernel Stein discrepancy (KSD)
computable by summing kernel evaluations across pairs of sample points. We
develop a theory of weak convergence for KSDs based on Stein's method,
demonstrate that commonly used KSDs fail to detect non-convergence even for
Gaussian targets, and show that kernels with slowly decaying tails provably
determine convergence for a large class of target distributions. The resulting
convergence-determining KSDs are suitable for comparing biased, exact, and
deterministic sample sequences and simpler to compute and parallelize than
alternative Stein discrepancies. We use our tools to compare biased samplers,
select sampler hyperparameters, and improve upon existing KSD approaches to
one-sample hypothesis testing and sample quality improvement.
",Computer Science; Statistics,Statistics
"Verifying the Medical Specialty from User Profile of Online Community for Health-Related Advices   The paper describes the verifying methods of medical specialty from user
profile of online community for health-related advices. To avoid critical
situations with the proliferation of unverified and inaccurate information in
medical online community, it is necessary to develop a comprehensive software
solution for verifying the user medical specialty of online community for
health-related advices. The algorithm for forming the information profile of a
medical online community user is designed. The scheme systems of formation of
indicators of user specialization in the profession based on a training sample
is presented. The method of forming the user information profile of online
community for healthrelated advices by computer-linguistic analysis of the
information content is suggested. The system of indicators based on a training
sample of users in medical online communities is formed. The matrix of medical
specialties indicators and method of determining weight coefficients these
indicators is investigated. The proposed method of verifying the medical
specialty from user profile is tested in online medical community.
",Computer Science,Computer Science
"Neutron Star Planets: Atmospheric processes and habitability   Of the roughly 3000 neutron stars known, only a handful have sub-stellar
companions. The most famous of these are the low-mass planets around the
millisecond pulsar B1257+12. New evidence indicates that observational biases
could still hide a wide variety of planetary systems around most neutron stars.
We consider the environment and physical processes relevant to neutron star
planets, in particular the effect of X-ray irradiation and the relativistic
pulsar wind on the planetary atmosphere. We discuss the survival time of planet
atmospheres and the planetary surface conditions around different classes of
neutron stars, and define a neutron star habitable zone. Depending on as-yet
poorly constrained aspects of the pulsar wind, both Super-Earths around
B1257+12 could lie within its habitable zone.
",Physics,Physics
"Avalanches and Plastic Flow in Crystal Plasticity: An Overview   Crystal plasticity is mediated through dislocations, which form knotted
configurations in a complex energy landscape. Once they disentangle and move,
they may also be impeded by permanent obstacles with finite energy barriers or
frustrating long-range interactions. The outcome of such complexity is the
emergence of dislocation avalanches as the basic mechanism of plastic flow in
solids at the nanoscale. While the deformation behavior of bulk materials
appears smooth, a predictive model should clearly be based upon the character
of these dislocation avalanches and their associated strain bursts. We provide
here a comprehensive overview of experimental observations, theoretical models
and computational approaches that have been developed to unravel the multiple
aspects of dislocation avalanche physics and the phenomena leading to strain
bursts in crystal plasticity.
",Physics,Physics
"A Fourier Disparity Layer representation for Light Fields   In this paper, we present a new Light Field representation for efficient
Light Field processing and rendering called Fourier Disparity Layers (FDL). The
proposed FDL representation samples the Light Field in the depth (or
equivalently the disparity) dimension by decomposing the scene as a discrete
sum of layers. The layers can be constructed from various types of Light Field
inputs including a set of sub-aperture images, a focal stack, or even a
combination of both. From our derivations in the Fourier domain, the layers are
simply obtained by a regularized least square regression performed
independently at each spatial frequency, which is efficiently parallelized in a
GPU implementation. Our model is also used to derive a gradient descent based
calibration step that estimates the input view positions and an optimal set of
disparity values required for the layer construction. Once the layers are
known, they can be simply shifted and filtered to produce different viewpoints
of the scene while controlling the focus and simulating a camera aperture of
arbitrary shape and size. Our implementation in the Fourier domain allows real
time Light Field rendering. Finally, direct applications such as view
interpolation or extrapolation and denoising are presented and evaluated.
",Computer Science,Computer Science
"Twisting and Mixing   We present a framework that connects three interesting classes of groups: the
twisted groups (also known as Suzuki-Ree groups), the mixed groups and the
exotic pseudo-reductive groups.
For a given characteristic p, we construct categories of twisted and mixed
schemes. Ordinary schemes are a full subcategory of the mixed schemes. Mixed
schemes arise from a twisted scheme by base change, although not every mixed
scheme arises this way. The group objects in these categories are called
twisted and mixed group schemes.
Our main theorems state: (1) The twisted Chevalley groups ${}^2\mathsf B_2$,
${}^2\mathsf G_2$ and ${}^2\mathsf F_4$ arise as rational points of twisted
group schemes. (2) The mixed groups in the sense of Tits arise as rational
points of mixed group schemes over mixed fields. (3) The exotic
pseudo-reductive groups of Conrad, Gabber and Prasad are Weil restrictions of
mixed group schemes.
",Mathematics,Mathematics
"On $C$-bases, partition pairs and filtrations for induced or restricted Specht modules   We obtain alternative explicit Specht filtrations for the induced and the
restricted Specht modules in the Hecke algebra of the symmetric group (defined
over the ring $A=\mathbb Z[q^{1/2},q^{-1/2}]$ where $q$ is an indeterminate)
using $C$-bases for these modules. Moreover, we provide a link between a
certain $C$-basis for the induced Specht module and the notion of pairs of
partitions.
",Mathematics,Mathematics
"Characterizing Feshbach resonances in ultracold scattering calculations   We describe procedures for converging on and characterizing zero-energy
Feshbach resonances that appear in scattering lengths as a function of an
external field. The elastic procedure is appropriate for purely elastic
scattering, where the scattering length is real and displays a true pole. The
regularized scattering length (RSL) procedure is appropriate when there is weak
background inelasticity, so that the scattering length is complex and displays
an oscillation rather than a pole, but the resonant scattering length $a_{\rm
res}$ is close to real. The fully complex procedure is appropriate when there
is substantial background inelasticity and the real and complex parts of
$a_{\rm res}$ are required. We demonstrate these procedures for scattering of
ultracold $^{85}$Rb in various initial states. All of them can converge on and
provide full characterization of resonances, from initial guesses many
thousands of widths away, using scattering calculations at only about 10 values
of the external field.
",Physics,Physics
"A globally stable attractor that is locally unstable everywhere   We construct two examples of invariant manifolds that despite being locally
unstable at every point in the transverse direction are globally stable. Using
numerical simulations we show that these invariant manifolds temporarily repel
nearby trajectories but act as global attractors. We formulate an explanation
for such global stability in terms of the `rate of rotation' of the stable and
unstable eigenvectors spanning the normal subspace associated with each point
of the invariant manifold. We discuss the role of this rate of rotation on the
transitions between the stable and unstable regimes.
",Physics,Mathematics
"Lie and Noether point Symmetries for a Class of Nonautonomous Dynamical Systems   We prove two general theorems which determine the Lie and the Noether point
symmetries for the equations of motion of a dynamical system which moves in a
general Riemannian space under the action of a time dependent potential
$W(t,x)=\omega(t)V(x)$. We apply the theorems to the case of a time dependent
central potential and the harmonic oscillator and determine all Lie and Noether
point symmetries. Finally we prove that these theorems also apply to the case
of a dynamical system with linear dumping and study two examples.
",Mathematics,Mathematics
"The trouble with tensor ring decompositions   The tensor train decomposition decomposes a tensor into a ""train"" of 3-way
tensors that are interconnected through the summation of auxiliary indices. The
decomposition is stable, has a well-defined notion of rank and enables the user
to perform various linear algebra operations on vectors and matrices of
exponential size in a computationally efficient manner. The tensor ring
decomposition replaces the train by a ring through the introduction of one
additional auxiliary variable. This article discusses a major issue with the
tensor ring decomposition: its inability to compute an exact minimal-rank
decomposition from a decomposition with sub-optimal ranks. Both the contraction
operation and Hadamard product are motivated from applications and it is shown
through simple examples how the tensor ring-rounding procedure fails to
retrieve minimal-rank decompositions with these operations. These observations,
together with the already known issue of not being able to find a best low-rank
tensor ring approximation to a given tensor indicate that the applicability of
tensor rings is severely limited.
",Computer Science,Computer Science
"Towards a fractal cohomology: Spectra of Polya--Hilbert operators, regularized determinants and Riemann zeros   Emil Artin defined a zeta function for algebraic curves over finite fields
and made a conjecture about them analogous to the famous Riemann hypothesis.
This and other conjectures about these zeta functions would come to be called
the Weil conjectures, which were proved by Weil for curves and later, by
Deligne for varieties over finite fields. Much work was done in the search for
a proof of these conjectures, including the development in algebraic geometry
of a Weil cohomology theory for these varieties, which uses the Frobenius
operator on a finite field. The zeta function is then expressed as a
determinant, allowing the properties of the function to relate to those of the
operator. The search for a suitable cohomology theory and associated operator
to prove the Riemann hypothesis is still on. In this paper, we study the
properties of the derivative operator $D = \frac{d}{dz}$ on a particular
weighted Bergman space of entire functions. The operator $D$ can be naturally
viewed as the `infinitesimal shift of the complex plane'. Furthermore, this
operator is meant to be the replacement for the Frobenius operator in the
general case and is used to construct an operator associated to any suitable
meromorphic function. We then show that the meromorphic function can be
recovered by using a regularized determinant involving the above operator. This
is illustrated in some important special cases: rational functions, zeta
functions of curves over finite fields, the Riemann zeta function, and
culminating in a quantized version of the Hadamard factorization theorem that
applies to any entire function of finite order. Our construction is motivated
in part by [23] on the infinitesimal shift of the real line, as well as by
earlier work of Deninger [10] on cohomology in number theory and a conjectural
`fractal cohomology theory' envisioned in [25] and [28].
",Mathematics,Mathematics
"Progressive Image Deraining Networks: A Better and Simpler Baseline   Along with the deraining performance improvement of deep networks, their
structures and learning become more and more complicated and diverse, making it
difficult to analyze the contribution of various network modules when
developing new deraining networks. To handle this issue, this paper provides a
better and simpler baseline deraining network by considering network
architecture, input and output, and loss functions. Specifically, by repeatedly
unfolding a shallow ResNet, progressive ResNet (PRN) is proposed to take
advantage of recursive computation. A recurrent layer is further introduced to
exploit the dependencies of deep features across stages, forming our
progressive recurrent network (PReNet). Furthermore, intra-stage recursive
computation of ResNet can be adopted in PRN and PReNet to notably reduce
network parameters with graceful degradation in deraining performance. For
network input and output, we take both stage-wise result and original rainy
image as input to each ResNet and finally output the prediction of {residual
image}. As for loss functions, single MSE or negative SSIM losses are
sufficient to train PRN and PReNet. Experiments show that PRN and PReNet
perform favorably on both synthetic and real rainy images. Considering its
simplicity, efficiency and effectiveness, our models are expected to serve as a
suitable baseline in future deraining research. The source codes are available
at this https URL.
",Computer Science,Computer Science; Statistics
"Magnetic Field Dependence of Spin Glass Free Energy Barriers   We measure the field dependence of spin glass free energy barriers in a thin
amorphous Ge:Mn film through the time dependence of the magnetization. After
the correlation length $\xi(t, T)$ has reached the film thickness $\mathcal
{L}=155$~\AA~so that the dynamics are activated, we change the initial magnetic
field by $\delta H$. In agreement with the scaling behavior exhibited in a
companion Letter [Janus collaboration: M. Baity-Jesi {\it et al.}, Phys. Rev.
Lett. {\bf 118}, 157202 (2017)], we find the activation energy is increased
when $\delta H < 0$. The change is proportional to $(\delta H)^2$ with the
addition of a small $(\delta H)^4$ term. The magnitude of the change of the
spin glass free energy barriers is in near quantitative agreement with the
prediction of a barrier model.
",Physics,Physics
"Quotients of Buildings as $W$-Groupoids   We introduce structures which model the quotients of buildings by
type-preserving group actions. These structures, which we call W-groupoids,
generalize Bruhat decompositions, chambers systems of type M, and Tits
amalgams. We define the fundamental group of a W-groupoid, and characterize
buildings as connected simply connected W-groupoids. We give a brief outline of
the covering theory of W-groupoids, which produces buildings as the universal
covers of W-groupoids. The local-to-global theorem of Tits concerning spherical
3-resides allows for the construction of W-groupoids by gluing together
quotients of generalized polygons. In this way, W-groupoids can be used to
construct exotic, hyperbolic, and wild buildings.
",Mathematics,Mathematics
"Phase unwinding, or invariant subspace decompositions of Hardy spaces   We consider orthogonal decompositions of invariant subspaces of Hardy spaces,
these relate to the Blaschke based phase unwinding decompositions. We prove
convergence in Lp. In particular we build an explicit multiscale wavelet basis.
We also obtain an explicit unwindinig decomposition for the singular inner
function, exp 2i\pi/x.
",Mathematics,Mathematics
"The multi-resonant Lugiato-Lefever model   We introduce a new model describing multiple resonances in Kerr optical
cavities. It perfectly agrees quantitatively with the Ikeda map and predicts
complex phenomena such as super cavity solitons and coexistence of multiple
nonlinear states.
",Physics,Physics
"Chance-Constrained AC Optimal Power Flow Integrating HVDC Lines and Controllability   The integration of large-scale renewable generation has major implications on
the operation of power systems, two of which we address in this paper. First,
system operators have to deal with higher degrees of uncertainty. Second, with
abundant potential of renewable generation in remote locations, they need to
incorporate the operation of High Voltage Direct Current lines (HVDC). This
paper introduces an optimization tool that addresses both challenges by
incorporating; the full AC power flow equations and chance constraints to
address the uncertainty of renewable infeed, HVDC modeling for point-to-point
lines, and optimizing generator and HVDC corrective control policies in
reaction to uncertainty. The main contributions are twofold. First, we
introduce a HVDC line model and the corresponding HVDC participation factors in
a chance-constrained AC-OPF framework. Second, we modify an existing algorithm
for solving the chance-constrained AC optimal power flow to allow for
optimization of the generation and HVDC participation factors. Using realistic
wind forecast data, and a 10 bus system with one HVDC line and two wind farms,
we demonstrate the performance of our algorithm and show the benefit of
controllability.
",Computer Science,Computer Science
"Can simple transmission chains foster collective intelligence in binary-choice tasks?   In many social systems, groups of individuals can find remarkably efficient
solutions to complex cognitive problems, sometimes even outperforming a single
expert. The success of the group, however, crucially depends on how the
judgments of the group members are aggregated to produce the collective answer.
A large variety of such aggregation methods have been described in the
literature, such as averaging the independent judgments, relying on the
majority or setting up a group discussion. In the present work, we introduce a
novel approach for aggregating judgments - the transmission chain - which has
not yet been consistently evaluated in the context of collective intelligence.
In a transmission chain, all group members have access to a unique collective
solution and can improve it sequentially. Over repeated improvements, the
collective solution that emerges reflects the judgments of every group members.
We address the question of whether such a transmission chain can foster
collective intelligence for binary-choice problems. In a series of numerical
simulations, we explore the impact of various factors on the performance of the
transmission chain, such as the group size, the model parameters, and the
structure of the population. The performance of this method is compared to
those of the majority rule and the confidence-weighted majority. Finally, we
rely on two existing datasets of individuals performing a series of binary
decisions to evaluate the expected performances of the three methods
empirically. We find that the parameter space where the transmission chain has
the best performance rarely appears in real datasets. We conclude that the
transmission chain is best suited for other types of problems, such as those
that have cumulative properties.
",Computer Science; Physics,Computer Science; Statistics
"Carleman estimates for the time-fractional advection-diffusion equations and applications   In this article, we prove Carleman estimates for the generalized
time-fractional advection-diffusion equations by considering the fractional
derivative as perturbation for the first order time-derivative. As a direct
application of the Carleman estimates, we show a conditional stability of a
lateral Cauchy problem for the time-fractional advection-diffusion equation,
and we also investigate the stability of an inverse source problem.
",Mathematics,Mathematics
"Denoising Adversarial Autoencoders   Unsupervised learning is of growing interest because it unlocks the potential
held in vast amounts of unlabelled data to learn useful representations for
inference. Autoencoders, a form of generative model, may be trained by learning
to reconstruct unlabelled input data from a latent representation space. More
robust representations may be produced by an autoencoder if it learns to
recover clean input samples from corrupted ones. Representations may be further
improved by introducing regularisation during training to shape the
distribution of the encoded data in latent space. We suggest denoising
adversarial autoencoders, which combine denoising and regularisation, shaping
the distribution of latent space using adversarial training. We introduce a
novel analysis that shows how denoising may be incorporated into the training
and sampling of adversarial autoencoders. Experiments are performed to assess
the contributions that denoising makes to the learning of representations for
classification and sample synthesis. Our results suggest that autoencoders
trained using a denoising criterion achieve higher classification performance,
and can synthesise samples that are more consistent with the input data than
those trained without a corruption process.
",Computer Science; Statistics,Statistics
"A deep learning-based method for prostate segmentation in T2-weighted magnetic resonance imaging   We propose a novel automatic method for accurate segmentation of the prostate
in T2-weighted magnetic resonance imaging (MRI). Our method is based on
convolutional neural networks (CNNs). Because of the large variability in the
shape, size, and appearance of the prostate and the scarcity of annotated
training data, we suggest training two separate CNNs. A global CNN will
determine a prostate bounding box, which is then resampled and sent to a local
CNN for accurate delineation of the prostate boundary. This way, the local CNN
can effectively learn to segment the fine details that distinguish the prostate
from the surrounding tissue using the small amount of available training data.
To fully exploit the training data, we synthesize additional data by deforming
the training images and segmentations using a learned shape model. We apply the
proposed method on the PROMISE12 challenge dataset and achieve state of the art
results. Our proposed method generates accurate, smooth, and artifact-free
segmentations. On the test images, we achieve an average Dice score of 90.6
with a small standard deviation of 2.2, which is superior to all previous
methods. Our two-step segmentation approach and data augmentation strategy may
be highly effective in segmentation of other organs from small amounts of
annotated medical images.
",Computer Science; Statistics,Statistics
"Scratch iridescence: Wave-optical rendering of diffractive surface structure   The surface of metal, glass and plastic objects is often characterized by
microscopic scratches caused by manufacturing and/or wear. A closer look onto
such scratches reveals iridescent colors with a complex dependency on viewing
and lighting conditions. The physics behind this phenomenon is well understood;
it is caused by diffraction of the incident light by surface features on the
order of the optical wavelength. Existing analytic models are able to reproduce
spatially unresolved microstructure such as the iridescent appearance of
compact disks and similar materials. Spatially resolved scratches, on the other
hand, have proven elusive due to the highly complex wave-optical light
transport simulations needed to account for their appearance. In this paper, we
propose a wave-optical shading model based on non-paraxial scalar diffraction
theory to render this class of effects. Our model expresses surface roughness
as a collection of line segments. To shade a point on the surface, the
individual diffraction patterns for contributing scratch segments are computed
analytically and superimposed coherently. This provides natural transitions
from localized glint-like iridescence to smooth BRDFs representing the
superposition of many reflections at large viewing distances. We demonstrate
that our model is capable of recreating the overall appearance as well as
characteristic detail effects observed on real-world examples.
",Computer Science,Physics
"AXNet: ApproXimate computing using an end-to-end trainable neural network   Neural network based approximate computing is a universal architecture
promising to gain tremendous energy-efficiency for many error resilient
applications. To guarantee the approximation quality, existing works deploy two
neural networks (NNs), e.g., an approximator and a predictor. The approximator
provides the approximate results, while the predictor predicts whether the
input data is safe to approximate with the given quality requirement. However,
it is non-trivial and time-consuming to make these two neural network
coordinate---they have different optimization objectives---by training them
separately. This paper proposes a novel neural network structure---AXNet---to
fuse two NNs to a holistic end-to-end trainable NN. Leveraging the philosophy
of multi-task learning, AXNet can tremendously improve the invocation
(proportion of safe-to-approximate samples) and reduce the approximation error.
The training effort also decrease significantly. Experiment results show 50.7%
more invocation and substantial cuts of training time when compared to existing
neural network based approximate computing framework.
",Statistics,Computer Science; Statistics
"Parabolic subgroup orbits on finite root systems   Oshima's Lemma describes the orbits of parabolic subgroups of irreducible
finite Weyl groups on crystallographic root systems. This note generalises that
result to all root systems of finite Coxeter groups, and provides a self
contained proof, independent of the representation theory of semisimple complex
Lie algebras.
",Mathematics,Mathematics
"Geometry and Arithmetic of Crystallographic Sphere Packings   We introduce the notion of a ""crystallographic sphere packing,"" defined to be
one whose limit set is that of a geometrically finite hyperbolic reflection
group in one higher dimension. We exhibit for the first time an infinite family
of conformally-inequivalent such with all radii being reciprocals of integers.
We then prove a result in the opposite direction: the ""superintegral"" ones
exist only in finitely many ""commensurability classes,"" all in dimensions below
30.
",Mathematics,Mathematics
"Impact of surface functionalisation on the quantum coherence of nitrogen vacancy centres in nanodiamond   Nanoscale quantum probes such as the nitrogen-vacancy centre in diamond have
demonstrated remarkable sensing capabilities over the past decade as control
over the fabrication and manipulation of these systems has evolved. However, as
the size of these nanoscale quantum probes is reduced, the surface termination
of the host material begins to play a prominent role as a source of magnetic
and electric field noise. In this work, we show that borane-reduced nanodiamond
surfaces can on average double the spin relaxation time of individual
nitrogen-vacancy centres in nanodiamonds when compared to the thermally
oxidised surfaces. Using a combination of infra-red and x-ray absorption
spectroscopy techniques, we correlate the changes in quantum relaxation rates
with the conversion of sp2 carbon to C-O and C-H bonds on the diamond surface.
These findings implicate double-bonded carbon species as a dominant source of
spin noise for near surface NV centres and show that through tailored
engineering of the surface, we can improve the quantum properties and magnetic
sensitivity of these nanoscale probes.
",Physics,Physics
"Complex Networks: from Classical to Quantum   Recent progress in applying complex network theory to problems faced in
quantum information and computation has resulted in a beneficial crossover
between two fields. Complex network methods have successfully been used to
characterize quantum walk and transport models, entangled communication
networks, graph-theoretic models of emergent space-time and in detecting
mesoscale structure in quantum systems. Information physics is setting the
stage for a theory of complex and networked systems with quantum
information-inspired methods appearing in complex network science, including
information-theoretic distance and correlation measures for network
characterization. Novel quantum induced effects have been predicted in random
graphs---where edges represent entangled links---and quantum computer
algorithms have recently been proposed to offer super-polynomial enhancement
for several network and graph theoretic problems. Here we review the results at
the cutting edge, pinpointing the similarities and reconciling the differences
found in the series of results at the intersection of these two fields.
",Computer Science; Physics,Computer Science; Physics
"CMS-HF Calorimeter Upgrade for Run II   CMS-HF Calorimeters have been undergoing a major upgrade for the last couple
of years to alleviate the problems encountered during Run I, especially in the
PMT and the readout systems. In this poster, the problems caused by the old
PMTs installed in the detectors and their solutions will be explained.
Initially, regular PMTs with thicker windows, causing large Cherenkov
radiation, were used. Instead of the light coming through the fibers from the
detector, stray muons passing through the PMT itself produce Cherenkov
radiation in the PMT window, resulting in erroneously large signals. Usually,
large signals are the result of very high-energy particles in the calorimeter
and are tagged as important. As a result, these so-called window events
generate false triggers. Four-anode PMTs with thinner windows were selected to
reduce these window events. Additional channels also help eliminate such
remaining events through algorithms comparing the output of different PMT
channels. During the EYETS 16/17 period in the LHC operations, the final
components of the modifications to the readout system, namely the two-channel
front-end electronics cards, are installed. Complete upgrade of the HF
Calorimeter, including the preparations for the Run II will be discussed in
this poster, with possible effects on the eventual data taking.
",Physics,Physics
"Results on the Hilbert coefficients and reduction numbers   Let $(R,\frak{m})$ be a $d$-dimensional Cohen-Macaulay local ring, $I$ an
$\frak{m}$-primary ideal and $J$ a minimal reduction of $I$. In this paper we
study the independence of reduction ideals and the behavior of the higher
Hilbert coefficients. In addition, we give some examples in this regards.
",Mathematics,Mathematics
"Resource Allocation for Wireless Networks: A Distributed Optimization Approach   We consider the multi-cell joint power control and scheduling problem in
cellular wireless networks as a weighted sum-rate maximization problem. This
formulation is very general and applies to a wide range of applications and QoS
requirements. The problem is inherently hard due to objective's non-convexity
and the knapsack-like constraints. Moreover, practical system requires a
distributed operation. We applied an existing algorithm proposed by Scutari et
al. in distributed optimization literature to our problem. The algorithm
performs local optimization followed by consensus update repeatedly. However,
it is not fully applicable to our problem, as it requires all decision
variables to be maintained at every base station (BS), which is impractical for
large-scale networks; also, it relies on the Lipschitz continuity of the
objective function's gradient, which does not hold here. We exploited the
nature of our objective function, and proposed a localized version of the
algorithm. Furthermore, we relaxed the requirements of Lipschitz continuity
with the proximal approximation. Convergence to local optimal solutions was
proved under some conditions. Future work includes proving the above results
from a stochastic approximation perspective, and investigating non-linear
consensus schemes to speed up the convergence.
",Mathematics,Computer Science
"Detecting Changes in Hidden Markov Models   We consider the problem of sequential detection of a change in the
statistical behavior of a hidden Markov model. By adopting a worst-case
analysis with respect to the time of change and by taking into account the data
that can be accessed by the change-imposing mechanism we offer alternative
formulations of the problem. For each formulation we derive the optimum
Shewhart test that maximizes the worst-case detection probability while
guaranteeing infrequent false alarms.
",Mathematics; Statistics,Computer Science; Statistics
"More declarative tabling in Prolog using multi-prompt delimited control   Several Prolog implementations include a facility for tabling, an alternative
resolution strategy which uses memoisation to avoid redundant duplication of
computations. Until relatively recently, tabling has required either low-level
support in the underlying Prolog engine, or extensive program transormation (de
Guzman et al., 2008). An alternative approach is to augment Prolog with low
level support for continuation capturing control operators, particularly
delimited continuations, which have been investigated in the field of
functional programming and found to be capable of supporting a wide variety of
computational effects within an otherwise declarative language.
This technical report describes an implementation of tabling in SWI Prolog
based on delimited control operators for Prolog recently introduced by
Schrijvers et al. (2013). In comparison with a previous implementation of
tabling for SWI Prolog using delimited control (Desouter et al., 2015), this
approach, based on the functional memoising parser combinators of Johnson
(1995), stays closer to the declarative core of Prolog, requires less code, and
is able to deliver solutions from systems of tabled predicates incrementally
(as opposed to finding all solutions before delivering any to the rest of the
program).
A collection of benchmarks shows that a small number of carefully targeted
optimisations yields performance within a factor of about 2 of the optimised
version of Desouter et al.'s system currently included in SWI Prolog.
",Computer Science,Computer Science
"The Compressed Model of Residual CNDS   Convolutional neural networks have achieved a great success in the recent
years. Although, the way to maximize the performance of the convolutional
neural networks still in the beginning. Furthermore, the optimization of the
size and the time that need to train the convolutional neural networks is very
far away from reaching the researcher's ambition. In this paper, we proposed a
new convolutional neural network that combined several techniques to boost the
optimization of the convolutional neural network in the aspects of speed and
size. As we used our previous model Residual-CNDS (ResCNDS), which solved the
problems of slower convergence, overfitting, and degradation, and compressed
it. The outcome model called Residual-Squeeze-CNDS (ResSquCNDS), which we
demonstrated on our sold technique to add residual learning and our model of
compressing the convolutional neural networks. Our model of compressing adapted
from the SQUEEZENET model, but our model is more generalizable, which can be
applied almost to any neural network model, and fully integrated into the
residual learning, which addresses the problem of the degradation very
successfully. Our proposed model trained on very large-scale MIT
Places365-Standard scene datasets, which backing our hypothesis that the new
compressed model inherited the best of the previous ResCNDS8 model, and almost
get the same accuracy in the validation Top-1 and Top-5 with 87.64% smaller in
size and 13.33% faster in the training time.
",Computer Science,Computer Science; Statistics
"Short-wavelength out-of-band EUV emission from Sn laser-produced plasma   We present the results of spectroscopic measurements in the extreme
ultraviolet (EUV) regime (7-17 nm) of molten tin microdroplets illuminated by a
high-intensity 3-J, 60-ns Nd:YAG laser pulse. The strong 13.5 nm emission from
this laser-produced plasma is of relevance for next-generation nanolithography
machines. Here, we focus on the shorter wavelength features between 7 and 12 nm
which have so far remained poorly investigated despite their diagnostic
relevance. Using flexible atomic code calculations and local thermodynamic
equilibrium arguments, we show that the line features in this region of the
spectrum can be explained by transitions from high-lying configurations within
the Sn$^{8+}$-Sn$^{15+}$ ions. The dominant transitions for all ions but
Sn$^{8+}$ are found to be electric-dipole transitions towards the $n$=4 ground
state from the core-excited configuration in which a 4$p$ electron is promoted
to the 5$s$ sub-shell. Our results resolve some long-standing spectroscopic
issues and provide reliable charge state identification for Sn laser-produced
plasma, which could be employed as a useful tool for diagnostic purposes.
",Physics,Physics
"Community structure: A comparative evaluation of community detection methods   Discovering community structure in complex networks is a mature field since a
tremendous number of community detection methods have been introduced in the
literature. Nevertheless, it is still very challenging for practioners to
determine which method would be suitable to get insights into the structural
information of the networks they study. Many recent efforts have been devoted
to investigating various quality scores of the community structure, but the
problem of distinguishing between different types of communities is still open.
In this paper, we propose a comparative, extensive and empirical study to
investigate what types of communities many state-of-the-art and well-known
community detection methods are producing. Specifically, we provide
comprehensive analyses on computation time, community size distribution, a
comparative evaluation of methods according to their optimisation schemes as
well as a comparison of their partioning strategy through validation metrics.
We process our analyses on a very large corpus of hundreds of networks from
five different network categories and propose ways to classify community
detection methods, helping a potential user to navigate the complex landscape
of community detection.
",Computer Science,Computer Science
"Finite homogeneous geometries   This paper reproduces the text of a part of the Author's DPhil thesis. It
gives a proof of the classification of non-trivial, finite homogeneous
geometries of sufficiently high dimension which does not depend on the
classification of the finite simple groups.
",Mathematics,Mathematics
"Semantic Evolutionary Concept Distances for Effective Information Retrieval in Query Expansion   In this work several semantic approaches to concept-based query expansion and
reranking schemes are studied and compared with different ontology-based
expansion methods in web document search and retrieval. In particular, we focus
on concept-based query expansion schemes, where, in order to effectively
increase the precision of web document retrieval and to decrease the users
browsing time, the main goal is to quickly provide users with the most suitable
query expansion. Two key tasks for query expansion in web document retrieval
are to find the expansion candidates, as the closest concepts in web document
domain, and to rank the expanded queries properly. The approach we propose aims
at improving the expansion phase for better web document retrieval and
precision. The basic idea is to measure the distance between candidate concepts
using the PMING distance, a collaborative semantic proximity measure, i.e. a
measure which can be computed by using statistical results from web search
engine. Experiments show that the proposed technique can provide users with
more satisfying expansion results and improve the quality of web document
retrieval.
",Computer Science; Mathematics,Computer Science
"Fixed points of n-valued maps, the fixed point property and the case of surfaces -- a braid approach   We study the fixed point theory of n-valued maps of a space X using the fixed
point theory of maps between X and its configuration spaces. We give some
general results to decide whether an n-valued map can be deformed to a fixed
point free n-valued map. In the case of surfaces, we provide an algebraic
criterion in terms of the braid groups of X to study this problem. If X is
either the k-dimensional ball or an even-dimensional real or complex projective
space, we show that the fixed point property holds for n-valued maps for all n
$\ge$ 1, and we prove the same result for even-dimensional spheres for all n
$\ge$ 2. If X is the 2-torus, we classify the homotopy classes of 2-valued maps
in terms of the braid groups of X. We do not currently have a complete
characterisation of the homotopy classes of split 2-valued maps of the 2-torus
that contain a fixed point free representative, but we give an infinite family
of such homotopy classes.
",Mathematics,Mathematics
"Carrier loss mechanisms in textured crystalline Si-based solar cells   A quite general device analysis method that allows the direct evaluation of
optical and recombination losses in crystalline silicon (c-Si)-based solar
cells has been developed. By applying this technique, the optical and physical
limiting factors of the state-of-the-art solar cells with ~20% efficiencies
have been revealed. In the established method, the carrier loss mechanisms are
characterized from the external quantum efficiency (EQE) analysis with very low
computational cost. In particular, the EQE analyses of textured c-Si solar
cells are implemented by employing the experimental reflectance spectra
obtained directly from the actual devices while using flat optical models
without any fitting parameters. We find that the developed method provides
almost perfect fitting to EQE spectra reported for various textured c-Si solar
cells, including c-Si heterojunction solar cells, a dopant-free c-Si solar cell
with a MoOx layer, and an n-type passivated emitter with rear locally diffused
(PERL) solar cell. The modeling of the recombination loss further allows the
extraction of the minority carrier diffusion length and surface recombination
velocity from the EQE analysis. Based on the EQE analysis results, the carrier
loss mechanisms in different types of c-Si solar cells are discussed.
",Physics,Physics
"Rank Two Non-Commutative Laurent Phenomenon and Pseudo-Positivity   We study polynomial generalizations of the Kontsevich automorphisms acting on
the skew-field of formal rational expressions in two non-commuting variables.
Our main result is the Laurentness and pseudo-positivity of iterations of these
automorphisms. The resulting expressions are described combinatorially using a
generalization of the combinatorics of compatible pairs in a maximal Dyck path
developed by Lee, Li, and Zelevinsky. By specializing to quasi-commuting
variables we obtain pseudo-positive expressions for rank 2 quantum generalized
cluster variables. In the case that all internal exchange coefficients are
zero, this quantum specialization provides a combinatorial construction of
counting polynomials for Grassmannians of submodules in exceptional
representations of valued quivers with two vertices.
",Mathematics,Mathematics
"Statistical inference in two-sample summary-data Mendelian randomization using robust adjusted profile score   Mendelian randomization (MR) is a method of exploiting genetic variation to
unbiasedly estimate a causal effect in presence of unmeasured confounding. MR
is being widely used in epidemiology and other related areas of population
science. In this paper, we study statistical inference in the increasingly
popular two-sample summary-data MR design. We show a linear model for the
observed associations approximately holds in a wide variety of settings when
all the genetic variants satisfy the exclusion restriction assumption, or in
genetic terms, when there is no pleiotropy. In this scenario, we derive a
maximum profile likelihood estimator with provable consistency and asymptotic
normality. However, through analyzing real datasets, we find strong evidence of
both systematic and idiosyncratic pleiotropy in MR, echoing the omnigenic model
of complex traits that is recently proposed in genetics. We model the
systematic pleiotropy by a random effects model, where no genetic variant
satisfies the exclusion restriction condition exactly. In this case we propose
a consistent and asymptotically normal estimator by adjusting the profile
score. We then tackle the idiosyncratic pleiotropy by robustifying the adjusted
profile score. We demonstrate the robustness and efficiency of the proposed
methods using several simulated and real datasets.
",Statistics,Statistics
"On gradient regularizers for MMD GANs   We propose a principled method for gradient-based regularization of the
critic of GAN-like models trained by adversarially optimizing the kernel of a
Maximum Mean Discrepancy (MMD). We show that controlling the gradient of the
critic is vital to having a sensible loss function, and devise a method to
enforce exact, analytical gradient constraints at no additional cost compared
to existing approximate techniques based on additive regularizers. The new loss
function is provably continuous, and experiments show that it stabilizes and
accelerates training, giving image generation models that outperform
state-of-the art methods on $160 \times 160$ CelebA and $64 \times 64$
unconditional ImageNet.
",Statistics,Computer Science; Statistics
"On Geodesic Completeness for Riemannian Metrics on Smooth Probability Densities   The geometric approach to optimal transport and information theory has
triggered the interpretation of probability densities as an
infinite-dimensional Riemannian manifold. The most studied Riemannian
structures are Otto's metric, yielding the $L^2$-Wasserstein distance of
optimal mass transport, and the Fisher--Rao metric, predominant in the theory
of information geometry. On the space of smooth probability densities, none of
these Riemannian metrics are geodesically complete---a property desirable for
example in imaging applications. That is, the existence interval for solutions
to the geodesic flow equations cannot be extended to the whole real line. Here
we study a class of Hamilton--Jacobi-like partial differential equations
arising as geodesic flow equations for higher-order Sobolev type metrics on the
space of smooth probability densities. We give order conditions for global
existence and uniqueness, thereby providing geodesic completeness. The system
we study is an interesting example of a flow equation with loss of derivatives,
which is well-posed in the smooth category, yet non-parabolic and fully
non-linear. On a more general note, the paper establishes a link between
geometric analysis on the space of probability densities and analysis of
Euler-Arnold equations in topological hydrodynamics.
",Mathematics,Physics; Mathematics
"Network analysis of Japanese global business using quasi-exhaustive micro-data for Japanese overseas subsidiaries   Network analysis techniques remain rarely used for understanding
international management strategies. Our paper highlights their value as
research tool in this field of social science using a large set of micro-data
(20,000) to investigate the presence of networks of subsidiaries overseas. The
research question is the following: to what extent did/do global Japanese
business networks mirror organizational models existing in Japan? In
particular, we would like to assess how much the links building such business
networks are shaped by the structure of big-size industrial conglomerates of
firms headquartered in Japan, also described as HK. The major part of the
academic community in the fields of management and industrial organization
considers that formal links can be identified among firms belonging to HK. Miwa
and Ramseyer (Miwa and Ramseyer 2002; Ramseyer 2006) challenge this claim and
argue that the evidence supporting the existence of HK is weak. So far,
quantitative empirical investigation has been conducted exclusively using data
for firms incorporated in Japan. Our study tests the Miwa-Ramseyer hypothesis
(MRH) at the global level using information on the network of Japanese
subsidiaries overseas. The results obtained lead us to reject the MRH for the
global dataset, as well as for subsets restricted to the two main
regions/countries of destination of Japanese foreign investment. The results
are robust to the weighting of the links, with different specifications, and
are observed in most industrial sectors. The global Japanese network became
increasingly complex during the late 20th century as a consequence of increase
in the number of Japanese subsidiaries overseas but the key features of the
structure remained rather stable. We draw implications of these findings for
academic research in international business and for professionals involved in
corporate strategy.
",Computer Science,Computer Science; Physics
"Non-orthogonal Multiple Access for High-reliable and Low-latency V2X Communications   In this paper, we consider a dense vehicular communication network where each
vehicle broadcasts its safety information to its neighborhood in each
transmission period. Such applications require low latency and high
reliability, and thus, we propose a non-orthogonal multiple access scheme to
reduce the latency and to improve the packet reception probability. In the
proposed scheme, the BS performs the semi-persistent scheduling to optimize the
time scheduling and allocate frequency resources in a non-orthogonal manner
while the vehicles autonomously perform distributed power control. We formulate
the centralized scheduling and resource allocation problem as equivalent to a
multi-dimensional stable roommate matching problem, in which the users and
time/frequency resources are considered as disjoint sets of players to be
matched with each other. We then develop a novel rotation matching algorithm,
which converges to a q-exchange stable matching after a limited number of
iterations. Simulation results show that the proposed scheme outperforms the
traditional orthogonal multiple access scheme in terms of the latency and
reliability.
",Computer Science,Computer Science
"A Flexible Procedure for Mixture Proportion Estimation in Positive--Unlabeled Learning   Positive--unlabeled (PU) learning considers two samples, a positive set P
with observations from only one class and an unlabeled set U with observations
from two classes. The goal is to classify observations in U. Class mixture
proportion estimation (MPE) in U is a key step in PU learning. Blanchard et al.
[2010] showed that MPE in PU learning is a generalization of the problem of
estimating the proportion of true null hypotheses in multiple testing problems.
Motivated by this idea, we propose reducing the problem to one dimension via
construction of a probabilistic classifier trained on the P and U data sets
followed by application of a one--dimensional mixture proportion method from
the multiple testing literature to the observation class probabilities. The
flexibility of this framework lies in the freedom to choose the classifier and
the one--dimensional MPE method. We prove consistency of two mixture proportion
estimators using bounds from empirical process theory, develop tuning parameter
free implementations, and demonstrate that they have competitive performance on
simulated waveform data and a protein signaling problem.
",Statistics,Statistics
"The role of complex analysis in modeling economic growth   Development and growth are complex and tumultuous processes. Modern economic
growth theories identify some key determinants of economic growth. However, the
relative importance of the determinants remains unknown, and additional
variables may help clarify the directions and dimensions of the interactions.
The novel stream of literature on economic complexity goes beyond aggregate
measures of productive inputs, and considers instead a more granular and
structural view of the productive possibilities of countries, i.e. their
capabilities. Different endowments of capabilities are crucial ingredients in
explaining differences in economic performances. In this paper we employ
economic fitness, a measure of productive capabilities obtained through complex
network techniques. Focusing on the combined roles of fitness and some more
traditional drivers of growth, we build a bridge between economic growth
theories and the economic complexity literature. Our findings, in agreement
with other recent empirical studies, show that fitness plays a crucial role in
fostering economic growth and, when it is included in the analysis, can be
either complementary to traditional drivers of growth or can completely
overshadow them.
",Quantitative Finance,Computer Science; Physics
"The Kellogg property and boundary regularity for p-harmonic functions with respect to the Mazurkiewicz boundary and other compactifications   In this paper boundary regularity for p-harmonic functions is studied with
respect to the Mazurkiewicz boundary and other compactifications. In
particular, the Kellogg property (which says that the set of irregular boundary
points has capacity zero) is obtained for a large class of compactifications,
but also two examples when it fails are given. This study is done for complete
metric spaces equipped with doubling measures supporting a p-Poincaré
inequality, but the results are new also in unweighted Euclidean spaces.
",Mathematics,Mathematics
"Collisions of Dark Matter Axion Stars with Astrophysical Sources   If QCD axions form a large fraction of the total mass of dark matter, then
axion stars could be very abundant in galaxies. As a result, collisions with
each other, and with other astrophysical bodies, can occur. We calculate the
rate and analyze the consequences of three classes of collisions, those
occurring between a dilute axion star and: another dilute axion star, an
ordinary star, or a neutron star. In all cases we attempt to quantify the most
important astrophysical uncertainties; we also pay particular attention to
scenarios in which collisions lead to collapse of otherwise stable axion stars,
and possible subsequent decay through number changing interactions. Collisions
between two axion stars can occur with a high total rate, but the low relative
velocity required for collapse to occur leads to a very low total rate of
collapses. On the other hand, collisions between an axion star and an ordinary
star have a large rate, $\Gamma_\odot \sim 3000$ collisions/year/galaxy, and
for sufficiently heavy axion stars, it is plausible that most or all such
collisions lead to collapse. We identify in this case a parameter space which
has a stable region and a region in which collision triggers collapse, which
depend on the axion number ($N$) in the axion star, and a ratio of mass to
radius cubed characterizing the ordinary star ($M_s/R_s^3$). Finally, we
revisit the calculation of collision rates between axion stars and neutron
stars, improving on previous estimates by taking cylindrical symmetry of the
neutron star distribution into account. Collapse and subsequent decay through
collision processes, if occurring with a significant rate, can affect dark
matter phenomenology and the axion star mass distribution.
",Physics,Physics
"Spectral algebra models of unstable v_n-periodic homotopy theory   We give a survey of a generalization of Quillen-Sullivan rational homotopy
theory which gives spectral algebra models of unstable v_n-periodic homotopy
types. In addition to describing and contextualizing our original approach, we
sketch two other recent approaches which are of a more conceptual nature, due
to Arone-Ching and Heuts. In the process, we also survey many relevant concepts
which arise in the study of spectral algebra over operads, including
topological André-Quillen cohomology, Koszul duality, and Goodwillie
calculus.
",Mathematics,Mathematics
"Imaging anomalous nematic order and strain in optimally doped BaFe$_2$(As,P)$_2$   We present the strain and temperature dependence of an anomalous nematic
phase in optimally doped BaFe$_2$(As,P)$_2$. Polarized ultrafast optical
measurements reveal broken 4-fold rotational symmetry in a temperature range
above $T_c$ in which bulk probes do not detect a phase transition. Using
ultrafast microscopy, we find that the magnitude and sign of this nematicity
vary on a ${50{-}100}~\mu$m length scale, and the temperature at which it
onsets ranges from 40 K near a domain boundary to 60 K deep within a domain.
Scanning Laue microdiffraction maps of local strain at room temperature
indicate that the nematic order appears most strongly in regions of weak,
isotropic strain. These results indicate that nematic order arises in a genuine
phase transition rather than by enhancement of local anisotropy by a strong
nematic susceptibility. We interpret our results in the context of a proposed
surface nematic phase.
",Physics,Physics
"Tomonaga-Luttinger spin liquid in the spin-1/2 inequilateral diamond-chain compound K$_3$Cu$_3$AlO$_2$(SO$_4$)$_4$   K$_3$Cu$_3$AlO$_2$(SO$_4$)$_4$ is a highly one-dimensional spin-1/2
inequilateral diamond-chain antiferromagnet. Spinon continuum and spin-singlet
dimer excitations are observed in the inelastic neutron scattering spectra,
which is in excellent agreement with a theoretical prediction: a dimer-monomer
composite structure, where the dimer is caused by strong antiferromagnetic
(AFM) coupling and the monomer forms an almost isolated quantum AFM chain
controlling low-energy excitations. Moreover, muon spin rotation/relaxation
spectroscopy shows no long-range ordering down to 90~mK, which is roughly three
orders of magnitude lower than the exchange interaction of the quantum AFM
chain. K$_3$Cu$_3$AlO$_2$(SO$_4$)$_4$ is, thus, regarded as a compound that
exhibits a Tomonaga-Luttinger spin liquid behavior at low temperatures close to
the ground state.
",Physics,Physics
"The generalized Fermat equation with exponents 2, 3, n   We study the Generalized Fermat Equation $x^2 + y^3 = z^p$, to be solved in
coprime integers, where $p \ge 7$ is prime. Using modularity and level lowering
techniques, the problem can be reduced to the determination of the sets of
rational points satisfying certain 2-adic and 3-adic conditions on a finite set
of twists of the modular curve $X(p)$.
We first develop new local criteria to decide if two elliptic curves with
certain types of potentially good reduction at 2 and 3 can have symplectically
or anti-symplectically isomorphic $p$-torsion modules. Using these criteria we
produce the minimal list of twists of $X(p)$ that have to be considered, based
on local information at 2 and 3; this list depends on $p \bmod 24$. Using
recent results on mod $p$ representations with image in the normalizer of a
split Cartan subgroup, the list can be further reduced in some cases.
Our second main result is the complete solution of the equation when $p =
11$, which previously was the smallest unresolved $p$. One relevant new
ingredient is the use of the `Selmer group Chabauty' method introduced by the
third author in a recent preprint, applied in an Elliptic Curve Chabauty
context, to determine relevant points on $X_0(11)$ defined over certain number
fields of degree 12. This result is conditional on GRH, which is needed to show
correctness of the computation of the class groups of five specific number
fields of degree 36.
We also give some partial results for the case $p = 13$.
",Mathematics,Mathematics
"Hubble PanCET: An isothermal day-side atmosphere for the bloated gas-giant HAT-P-32Ab   We present a thermal emission spectrum of the bloated hot Jupiter HAT-P-32Ab
from a single eclipse observation made in spatial scan mode with the Wide Field
Camera 3 (WFC3) aboard the Hubble Space Telescope (HST). The spectrum covers
the wavelength regime from 1.123 to 1.644 microns which is binned into 14
eclipse depths measured to an averaged precision of 104 parts-per million. The
spectrum is unaffected by a dilution from the close M-dwarf companion
HAT-P-32B, which was fully resolved. We complemented our spectrum with
literature results and performed a comparative forward and retrieval analysis
with the 1D radiative-convective ATMO model. Assuming solar abundance of the
planet atmosphere, we find that the measured spectrum can best be explained by
the spectrum of a blackbody isothermal atmosphere with Tp = 1995 +/- 17K, but
can equally-well be described by a spectrum with modest thermal inversion. The
retrieved spectrum suggests emission from VO at the WFC3 wavelengths and no
evidence of the 1.4 micron water feature. The emission models with temperature
profiles decreasing with height are rejected at a high confidence. An
isothermal or inverted spectrum can imply a clear atmosphere with an absorber,
a dusty cloud deck or a combination of both. We find that the planet can have
continuum of values for the albedo and recirculation, ranging from high albedo
and poor recirculation to low albedo and efficient recirculation. Optical
spectroscopy of the planet's day-side or thermal emission phase curves can
potentially resolve the current albedo with recirculation degeneracy.
",Physics,Physics
"Evaluation of Lightweight Block Ciphers in Hardware Implementation: A Comprehensive Survey   The conventional cryptography solutions are ill-suited to strict memory, size
and power limitations of resource-constrained devices, so lightweight
cryptography solutions have been specifically developed for this type of
applications. In this domain of cryptography, the term lightweight never refers
to inadequately low security, but rather to establishing the best balance to
maintain sufficient security. This paper presents the first comprehensive
survey evaluation of lightweight block ciphers in terms of their speed, cost,
performance, and balanced efficiency in hardware implementation, and
facilitates the comparison of studied ciphers in these respects. The cost of
lightweight block ciphers is evaluated with the metric of Gate Equivalent
(Fig.1), their speed with the metric of clock-cycle-per-block (Fig.2), their
performance with the metric of throughput (Fig.3) and their balanced efficiency
with the metric of Figure of Merit (Fig.4). The results of these evaluations
show that SIMON, SPECK, and Piccolo are the best lightweight block ciphers in
hardware implementation.(Abstract)
",Computer Science,Computer Science
"Convexification of Queueing Formulas by Mixed-Integer Second-Order Cone Programming: An Application to a Discrete Location Problem with Congestion   Mixed-Integer Second-Order Cone Programs (MISOCPs) form a nice class of
mixed-inter convex programs, which can be solved very efficiently due to the
recent advances in optimization solvers. Our paper bridges the gap between
modeling a class of optimization problems and using MISOCP solvers. It is shown
how various performance metrics of M/G/1 queues can be molded by different
MISOCPs. To motivate our method practically, it is first applied to a
challenging stochastic location problem with congestion, which is broadly used
to design socially optimal service networks. Four different MISOCPs are
developed and compared on sets of benchmark test problems. The new formulations
efficiently solve large-size test problems, which cannot be solved by the best
existing method. Then, the general applicability of our method is shown for
similar optimization problems that use queue-theoretic performance measures to
address customer satisfaction and service quality.
",Computer Science,Computer Science
"Large-Scale Low-Rank Matrix Learning with Nonconvex Regularizers   Low-rank modeling has many important applications in computer vision and
machine learning. While the matrix rank is often approximated by the convex
nuclear norm, the use of nonconvex low-rank regularizers has demonstrated
better empirical performance. However, the resulting optimization problem is
much more challenging. Recent state-of-the-art requires an expensive full SVD
in each iteration. In this paper, we show that for many commonly-used nonconvex
low-rank regularizers, a cutoff can be derived to automatically threshold the
singular values obtained from the proximal operator. This allows such operator
being efficiently approximated by power method. Based on it, we develop a
proximal gradient algorithm (and its accelerated variant) with inexact proximal
splitting and prove that a convergence rate of O(1/T) where T is the number of
iterations is guaranteed. Furthermore, we show the proposed algorithm can be
well parallelized, which achieves nearly linear speedup w.r.t the number of
threads. Extensive experiments are performed on matrix completion and robust
principal component analysis, which shows a significant speedup over the
state-of-the-art. Moreover, the matrix solution obtained is more accurate and
has a lower rank than that of the nuclear norm regularizer.
",Computer Science; Statistics,Computer Science; Statistics
"Determining stellar parameters of asteroseismic targets: going beyond the use of scaling relations   Asteroseismic parameters allow us to measure the basic stellar properties of
field giants observed far across the Galaxy. Most of such determinations are,
up to now, based on simple scaling relations involving the large frequency
separation, \Delta\nu, and the frequency of maximum power, \nu$_{max}$. In this
work, we implement \Delta\nu\ and the period spacing, {\Delta}P, computed along
detailed grids of stellar evolutionary tracks, into stellar isochrones and
hence in a Bayesian method of parameter estimation. Tests with synthetic data
reveal that masses and ages can be determined with typical precision of 5 and
19 per cent, respectively, provided precise seismic parameters are available.
Adding independent information on the stellar luminosity, these values can
decrease down to 3 and 10 per cent respectively. The application of these
methods to NGC 6819 giants produces a mean age in agreement with those derived
from isochrone fitting, and no evidence of systematic differences between RGB
and RC stars. The age dispersion of NGC 6819 stars, however, is larger than
expected, with at least part of the spread ascribable to stars that underwent
mass-transfer events.
",Physics,Physics
"Connectivity Learning in Multi-Branch Networks   While much of the work in the design of convolutional networks over the last
five years has revolved around the empirical investigation of the importance of
depth, filter sizes, and number of feature channels, recent studies have shown
that branching, i.e., splitting the computation along parallel but distinct
threads and then aggregating their outputs, represents a new promising
dimension for significant improvements in performance. To combat the complexity
of design choices in multi-branch architectures, prior work has adopted simple
strategies, such as a fixed branching factor, the same input being fed to all
parallel branches, and an additive combination of the outputs produced by all
branches at aggregation points.
In this work we remove these predefined choices and propose an algorithm to
learn the connections between branches in the network. Instead of being chosen
a priori by the human designer, the multi-branch connectivity is learned
simultaneously with the weights of the network by optimizing a single loss
function defined with respect to the end task. We demonstrate our approach on
the problem of multi-class image classification using three different datasets
where it yields consistently higher accuracy compared to the state-of-the-art
""ResNeXt"" multi-branch network given the same learning capacity.
",Computer Science,Computer Science; Statistics
"Thermodynamics of Higher Order Entropy Corrected Schwarzschild-Beltrami-de Sitter Black Hole   In this paper, we consider higher order correction of the entropy and study
the thermodynamical properties of recently proposed Schwarzschild-Beltrami-de
Sitter black hole, which is indeed an exact solution of Einstein equation with
a positive cosmological constant. By using the corrected entropy and Hawking
temperature we extract some thermodynamical quantities like Gibbs and Helmholtz
free energies and heat capacity. We also investigate the first and second laws
of thermodynamics. We find that presence of higher order corrections, which
come from thermal fluctuations, may remove some instabilities of the black
hole. Also unstable to stable phase transition is possible in presence of the
first and second order corrections.
",Physics,Physics
"Fracton topological phases from strongly coupled spin chains   We provide a new perspective on fracton topological phases, a class of
three-dimensional topologically ordered phases with unconventional
fractionalized excitations that are either completely immobile or only mobile
along particular lines or planes. We demonstrate that a wide range of these
fracton phases can be constructed by strongly coupling mutually intersecting
spin chains and explain via a concrete example how such a coupled-spin-chain
construction illuminates the generic properties of a fracton phase. In
particular, we describe a systematic translation from each coupled-spin-chain
construction into a parton construction where the partons correspond to the
excitations that are mobile along lines. Remarkably, our construction of
fracton phases is inherently based on spin models involving only two-spin
interactions and thus brings us closer to their experimental realization.
",Physics,Physics
"Sampling for Approximate Bipartite Network Projection   Bipartite networks manifest as a stream of edges that represent transactions,
e.g., purchases by retail customers. Many machine learning applications employ
neighborhood-based measures to characterize the similarity among the nodes,
such as the pairwise number of common neighbors (CN) and related metrics. While
the number of node pairs that share neighbors is potentially enormous, only a
relatively small proportion of them have many common neighbors. This motivates
finding a weighted sampling approach to preferentially sample these node pairs.
This paper presents a new sampling algorithm that provides a fixed size
unbiased estimate of the similarity matrix resulting from a bipartite graph
stream projection. The algorithm has two components. First, it maintains a
reservoir of sampled bipartite edges with sampling weights that favor selection
of high similarity nodes. Second, arriving edges generate a stream of
\textsl{similarity updates} based on their adjacency with the current sample.
These updates are aggregated in a second reservoir sample-based stream
aggregator to yield the final unbiased estimate. Experiments on real world
graphs show that a 10% sample at each stage yields estimates of high similarity
edges with weighted relative errors of about 1%.
",Computer Science; Mathematics,Computer Science; Statistics
"Joining Extractions of Regular Expressions   Regular expressions with capture variables, also known as ""regex formulas,""
extract relations of spans (interval positions) from text. These relations can
be further manipulated via Relational Algebra as studied in the context of
document spanners, Fagin et al.'s formal framework for information extraction.
We investigate the complexity of querying text by Conjunctive Queries (CQs) and
Unions of CQs (UCQs) on top of regex formulas. We show that the lower bounds
(NP-completeness and W[1]-hardness) from the relational world also hold in our
setting; in particular, hardness hits already single-character text! Yet, the
upper bounds from the relational world do not carry over. Unlike the relational
world, acyclic CQs, and even gamma-acyclic CQs, are hard to compute. The source
of hardness is that it may be intractable to instantiate the relation defined
by a regex formula, simply because it has an exponential number of tuples. Yet,
we are able to establish general upper bounds. In particular, UCQs can be
evaluated with polynomial delay, provided that every CQ has a bounded number of
atoms (while unions and projection can be arbitrary). Furthermore, UCQ
evaluation is solvable with FPT (Fixed-Parameter Tractable) delay when the
parameter is the size of the UCQ.
",Computer Science,Computer Science
"Tuning the effective spin-orbit coupling in molecular semiconductors   The control of spins and spin to charge conversion in organics requires
understanding the molecular spin-orbit coupling (SOC), and a means to tune its
strength. However, quantifying SOC strengths indirectly through spin relaxation
effects has proven diffi- cult due to competing relaxation mechanisms. Here we
present a systematic study of the g-tensor shift in molecular semiconductors
and link it directly to the SOC strength in a series of high mobility molecular
semiconductors with strong potential for future devices. The results
demonstrate a rich variability of the molecular g-shifts with the effective
SOC, depending on subtle aspects of molecular composition and structure. We
correlate the above g -shifts to spin-lattice relaxation times over four orders
of magnitude, from 200 {\mu}s to 0.15 {\mu}s, for isolated molecules in
solution and relate our findings for isolated molecules in solution to the spin
relaxation mechanisms that are likely to be relevant in solid state systems.
",Physics,Physics
"A short note on the computation of the generalised Jacobsthal function for paired progressions   Jacobsthal's function was recently generalised for the case of paired
progressions. It was proven that a specific bound of this function is
sufficient for the truth of Goldbach's conjecture and of the prime pairs
conjecture as well. We extended and adapted algorithms described for the
computation of the common Jacobsthal function, and computed respective function
values of the paired Jacobsthal function for primorial numbers for primes up to
73. All these values fulfil the conjectured specific bound. In addition to this
note, we provide a detailed review of the algorithmic approaches and the
complete computational results in ancillary files.
",Mathematics,Mathematics
"Judicious partitions of uniform hypergraphs   The vertices of any graph with $m$ edges may be partitioned into two parts so
that each part meets at least $\frac{2m}{3}$ edges. Bollobás and Thomason
conjectured that the vertices of any $r$-uniform hypergraph with $m$ edges may
likewise be partitioned into $r$ classes such that each part meets at least
$\frac{r}{2r-1}m$ edges. In this paper we prove the weaker statement that, for
each $r\ge 4$, a partition into $r$ classes may be found in which each class
meets at least $\frac{r}{3r-4}m$ edges, a substantial improvement on previous
bounds.
",Mathematics,Computer Science
"Time-domain THz spectroscopy reveals coupled protein-hydration dielectric response in solutions of native and fibrils of human lyso-zyme   Here we reveal details of the interaction between human lysozyme proteins,
both native and fibrils, and their water environment by intense terahertz time
domain spectroscopy. With the aid of a rigorous dielectric model, we determine
the amplitude and phase of the oscillating dipole induced by the THz field in
the volume containing the protein and its hydration water. At low
concentrations, the amplitude of this induced dipolar response decreases with
increasing concentration. Beyond a certain threshold, marking the onset of the
interactions between the extended hydration shells, the amplitude remains fixed
but the phase of the induced dipolar response, which is initially in phase with
the applied THz field, begins to change. The changes observed in the THz
response reveal protein-protein interactions me-diated by extended hydration
layers, which may control fibril formation and may have an important role in
chemical recognition phenomena.
",Physics,Physics
"Passive Classification of Source Printer using Text-line-level Geometric Distortion Signatures from Scanned Images of Printed Documents   In this digital era, one thing that still holds the convention is a printed
archive. Printed documents find their use in many critical domains such as
contract papers, legal tenders and proof of identity documents. As more
advanced printing, scanning and image editing techniques are becoming
available, forgeries on these legal tenders pose a serious threat. Ability to
easily and reliably identify source printer of a printed document can help a
lot in reducing this menace. During printing procedure, printer hardware
introduces certain distortions in printed characters' locations and shapes
which are invisible to naked eyes. These distortions are referred as geometric
distortions, their profile (or signature) is generally unique for each printer
and can be used for printer classification purpose. This paper proposes a set
of features for characterizing text-line-level geometric distortions, referred
as geometric distortion signatures and presents a novel system to use them for
identification of the origin of a printed document. Detailed experiments
performed on a set of thirteen printers demonstrate that the proposed system
achieves state of the art performance and gives much higher accuracy under
small training size constraint. For four training and six test pages of three
different fonts, the proposed method gives 99\% classification accuracy.
",Computer Science,Computer Science
"Observability of characteristic binary-induced structures in circumbinary disks   Context: A substantial fraction of protoplanetary disks forms around stellar
binaries. The binary system generates a time-dependent non-axisymmetric
gravitational potential, inducing strong tidal forces on the circumbinary disk.
This leads to a change in basic physical properties of the circumbinary disk,
which should in turn result in unique structures that are potentially
observable with the current generation of instruments.
Aims: The goal of this study is to identify these characteristic structures,
to constrain the physical conditions that cause them, and to evaluate the
feasibility to observe them in circumbinary disks.
Methods: To achieve this, at first two-dimensional hydrodynamic simulations
are performed. The resulting density distributions are post-processed with a 3D
radiative transfer code to generate re-emission and scattered light maps. Based
on these, we study the influence of various parameters, such as the mass of the
stellar components, the mass of the disk and the binary separation on
observable features in circumbinary disks.
Results: We find that the Atacama Large (sub-)Millimetre Array (ALMA) as well
as the European Extremely Large Telescope (E-ELT) are capable of tracing
asymmetries in the inner region of circumbinary disks which are affected most
by the binary-disk interaction. Observations at submillimetre/millimetre
wavelengths will allow the detection of the density waves at the inner rim of
the disk and the inner cavity. With the E-ELT one can partially resolve the
innermost parts of the disk in the infrared wavelength range, including the
disk's rim, accretion arms and potentially the expected circumstellar disks
around each of the binary components.
",Physics,Physics
"Unveiling Swarm Intelligence with Network Science$-$the Metaphor Explained   Self-organization is a natural phenomenon that emerges in systems with a
large number of interacting components. Self-organized systems show robustness,
scalability, and flexibility, which are essential properties when handling
real-world problems. Swarm intelligence seeks to design nature-inspired
algorithms with a high degree of self-organization. Yet, we do not know why
swarm-based algorithms work well and neither we can compare the different
approaches in the literature. The lack of a common framework capable of
characterizing these several swarm-based algorithms, transcending their
particularities, has led to a stream of publications inspired by different
aspects of nature without much regard as to whether they are similar to already
existing approaches. We address this gap by introducing a network-based
framework$-$the interaction network$-$to examine computational swarm-based
systems via the optics of social dynamics. We discuss the social dimension of
several swarm classes and provide a case study of the Particle Swarm
Optimization. The interaction network enables a better understanding of the
plethora of approaches currently available by looking at them from a general
perspective focusing on the structure of the social interactions.
",Computer Science,Computer Science
"Learning a Code: Machine Learning for Approximate Non-Linear Coded Computation   Machine learning algorithms are typically run on large scale, distributed
compute infrastructure that routinely face a number of unavailabilities such as
failures and temporary slowdowns. Adding redundant computations using
coding-theoretic tools called ""codes"" is an emerging technique to alleviate the
adverse effects of such unavailabilities. A code consists of an encoding
function that proactively introduces redundant computation and a decoding
function that reconstructs unavailable outputs using the available ones. Past
work focuses on using codes to provide resilience for linear computations and
specific iterative optimization algorithms. However, computations performed for
a variety of applications including inference on state-of-the-art machine
learning algorithms, such as neural networks, typically fall outside this
realm. In this paper, we propose taking a learning-based approach to designing
codes that can handle non-linear computations. We present carefully designed
neural network architectures and a training methodology for learning encoding
and decoding functions that produce approximate reconstructions of unavailable
computation results. We present extensive experimental results demonstrating
the effectiveness of the proposed approach: we show that the our learned codes
can accurately reconstruct $64 - 98\%$ of the unavailable predictions from
neural-network based image classifiers on the MNIST, Fashion-MNIST, and
CIFAR-10 datasets. To the best of our knowledge, this work proposes the first
learning-based approach for designing codes, and also presents the first
coding-theoretic solution that can provide resilience for any non-linear
(differentiable) computation. Our results show that learning can be an
effective technique for designing codes, and that learned codes are a highly
promising approach for bringing the benefits of coding to non-linear
computations.
",Statistics,Computer Science; Statistics
"Quantum quench dynamics   Quench dynamics is an active area of study encompassing condensed matter
physics and quantum information, with applications to cold-atomic gases and
pump-probe spectroscopy of materials. Recent theoretical progress in studying
quantum quenches is reviewed. Quenches in interacting one dimensional systems
as well as systems in higher spatial dimensions are covered. The appearance of
non-trivial steady states following a quench in exactly solvable models is
discussed, and the stability of these states to perturbations is described.
Proper conserving approximations needed to capture the onset of thermalization
at long times are outlined. The appearance of universal scaling for quenches
near critical points, and the role of the renormalization group in capturing
the transient regime, are reviewed. Finally the effect of quenches near
critical points on the dynamics of entanglement entropy and entanglement
statistics is discussed. The extraction of critical exponents from the
entanglement statistics is outlined.
",Physics,Physics
"On the Complexity of Robust Stable Marriage   Robust Stable Marriage (RSM) is a variant of the classical Stable Marriage
problem, where the robustness of a given stable matching is measured by the
number of modifications required for repairing it in case an unforeseen event
occurs. We focus on the complexity of finding an (a,b)-supermatch. An
(a,b)-supermatch is defined as a stable matching in which if any 'a'
(non-fixed) men/women break up it is possible to find another stable matching
by changing the partners of those 'a' men/women and also the partners of at
most 'b' other couples. In order to show deciding if there exists an
(a,b)-supermatch is NP-Complete, we first introduce a SAT formulation that is
NP-Complete by using Schaefer's Dichotomy Theorem. Then, we show the
equivalence between the SAT formulation and finding a (1,1)-supermatch on a
specific family of instances.
",Computer Science,Mathematics
"A geometric second-order-rectifiable stratification for closed subsets of Euclidean space   Defining the $m$-th stratum of a closed subset of an $n$ dimensional
Euclidean space to consist of those points, where it can be touched by a ball
from at least $n-m$ linearly independent directions, we establish that the
$m$-th stratum is second-order rectifiable of dimension $m$ and a Borel set.
This was known for convex sets, but is new even for sets of positive reach. The
result is based on a new criterion for second-order rectifiability.
",Mathematics,Mathematics
"Asteroid mass estimation using Markov-chain Monte Carlo   Estimates for asteroid masses are based on their gravitational perturbations
on the orbits of other objects such as Mars, spacecraft, or other asteroids
and/or their satellites. In the case of asteroid-asteroid perturbations, this
leads to an inverse problem in at least 13 dimensions where the aim is to
derive the mass of the perturbing asteroid(s) and six orbital elements for both
the perturbing asteroid(s) and the test asteroid(s) based on astrometric
observations. We have developed and implemented three different mass estimation
algorithms utilizing asteroid-asteroid perturbations: the very rough 'marching'
approximation, in which the asteroids' orbital elements are not fitted, thereby
reducing the problem to a one-dimensional estimation of the mass, an
implementation of the Nelder-Mead simplex method, and most significantly, a
Markov-chain Monte Carlo (MCMC) approach. We describe each of these algorithms
with particular focus on the MCMC algorithm, and present example results using
both synthetic and real data. Our results agree with the published mass
estimates, but suggest that the published uncertainties may be misleading as a
consequence of using linearized mass-estimation methods. Finally, we discuss
remaining challenges with the algorithms as well as future plans.
",Physics,Physics
"On decision regions of narrow deep neural networks   We show that for neural network functions that have width less or equal to
the input dimension all connected components of decision regions are unbounded.
The result holds for continuous and strictly monotonic activation functions as
well as for ReLU activation. This complements recent results on approximation
capabilities of [Hanin 2017 Approximating] and connectivity of decision regions
of [Nguyen 2018 Neural] for such narrow neural networks. Further, we give an
example that negatively answers the question posed in [Nguyen 2018 Neural]
whether one of their main results still holds for ReLU activation. Our results
are illustrated by means of numerical experiments.
",Statistics,Computer Science; Statistics
"Quantile Markov Decision Process   In this paper, we consider the problem of optimizing the quantiles of the
cumulative rewards of Markov Decision Processes (MDP), to which we refers as
Quantile Markov Decision Processes (QMDP). Traditionally, the goal of a Markov
Decision Process (MDP) is to maximize expected cumulative reward over a defined
horizon (possibly to be infinite). In many applications, however, a decision
maker may be interested in optimizing a specific quantile of the cumulative
reward instead of its expectation. Our framework of QMDP provides analytical
results characterizing the optimal QMDP solution and presents the algorithm for
solving the QMDP. We provide analytical results characterizing the optimal QMDP
solution and present the algorithms for solving the QMDP. We illustrate the
model with two experiments: a grid game and a HIV optimal treatment experiment.
",Computer Science,Computer Science
"Parameter Estimation for Thurstone Choice Models   We consider the estimation accuracy of individual strength parameters of a
Thurstone choice model when each input observation consists of a choice of one
item from a set of two or more items (so called top-1 lists). This model
accommodates the well-known choice models such as the Luce choice model for
comparison sets of two or more items and the Bradley-Terry model for pair
comparisons.
We provide a tight characterization of the mean squared error of the maximum
likelihood parameter estimator. We also provide similar characterizations for
parameter estimators defined by a rank-breaking method, which amounts to
deducing one or more pair comparisons from a comparison of two or more items,
assuming independence of these pair comparisons, and maximizing a likelihood
function derived under these assumptions. We also consider a related binary
classification problem where each individual parameter takes value from a set
of two possible values and the goal is to correctly classify all items within a
prescribed classification error.
",Mathematics; Statistics,Mathematics; Statistics
"Fooling the classifier: Ligand antagonism and adversarial examples   Machine learning algorithms are sensitive to so-called adversarial
perturbations. This is reminiscent of cellular decision-making where antagonist
ligands may prevent correct signaling, like during the early immune response.
We draw a formal analogy between neural networks used in machine learning and
the general class of adaptive proofreading networks. We then apply simple
adversarial strategies from machine learning to models of ligand
discrimination. We show how kinetic proofreading leads to ""boundary tilting""
and identify three types of perturbation (adversarial, non adversarial and
ambiguous). We then use a gradient-descent approach to compare different
adaptive proofreading models, and we reveal the existence of two qualitatively
different regimes characterized by the presence or absence of a critical point.
These regimes are reminiscent of the ""feature-to-prototype"" transition
identified in machine learning, corresponding to two strategies in ligand
antagonism (broad vs. specialized). Overall, our work connects evolved cellular
decision-making to classification in machine learning, showing that behaviours
close to the decision boundary can be understood through the same mechanisms.
",Statistics; Quantitative Biology,Computer Science; Statistics
"Completion of the integrable coupling systems   In this paper, we proposed an procedure to construct the completion of the
integrable system by adding a perturbation to the generalized matrix problem,
which can be used to continuous integrable couplings, discrete integrable
couplings and super integrable couplings. As example, we construct the
completion of the Kaup-Newell (KN) integrable coupling, the
Wadati-Konno-Ichikawa (WKI) integrable couplingsis, vector
Ablowitz-Kaup-Newell-Segur (vAKNS) integrable couplings, the Volterra
integrable couplings, Dirac type integrable couplings and NLS-mKdV type
integrable couplings.
",Physics,Mathematics
"Cosmological searches for a non-cold dark matter component   We explore an extended cosmological scenario where the dark matter is an
admixture of cold and additional non-cold species. The mass and temperature of
the non-cold dark matter particles are extracted from a number of cosmological
measurements. Among others, we consider tomographic weak lensing data and Milky
Way dwarf satellite galaxy counts. We also study the potential of these
scenarios in alleviating the existing tensions between local measurements and
Cosmic Microwave Background (CMB) estimates of the $S_8$ parameter, with
$S_8=\sigma_8\sqrt{\Omega_m}$, and of the Hubble constant $H_0$. In principle,
a sub-dominant, non-cold dark matter particle with a mass $m_X\sim$~keV, could
achieve the goals above. However, the preferred ranges for its temperature and
its mass are different when extracted from weak lensing observations and from
Milky Way dwarf satellite galaxy counts, since these two measurements require
suppressions of the matter power spectrum at different scales. Therefore,
solving simultaneously the CMB-weak lensing tensions and the small scale crisis
in the standard cold dark matter picture via only one non-cold dark matter
component seems to be challenging.
",Physics,Physics
"Binets: fundamental building blocks for phylogenetic networks   Phylogenetic networks are a generalization of evolutionary trees that are
used by biologists to represent the evolution of organisms which have undergone
reticulate evolution. Essentially, a phylogenetic network is a directed acyclic
graph having a unique root in which the leaves are labelled by a given set of
species. Recently, some approaches have been developed to construct
phylogenetic networks from collections of networks on 2- and 3-leaved networks,
which are known as binets and trinets, respectively. Here we study in more
depth properties of collections of binets, one of the simplest possible types
of networks into which a phylogenetic network can be decomposed. More
specifically, we show that if a collection of level-1 binets is compatible with
some binary network, then it is also compatible with a binary level-1 network.
Our proofs are based on useful structural results concerning lowest stable
ancestors in networks. In addition, we show that, although the binets do not
determine the topology of the network, they do determine the number of
reticulations in the network, which is one of its most important parameters. We
also consider algorithmic questions concerning binets. We show that deciding
whether an arbitrary set of binets is compatible with some network is at least
as hard as the well-known Graph Isomorphism problem. However, if we restrict to
level-1 binets, it is possible to decide in polynomial time whether there
exists a binary network that displays all the binets. We also show that to find
a network that displays a maximum number of the binets is NP-hard, but that
there exists a simple polynomial-time 1/3-approximation algorithm for this
problem. It is hoped that these results will eventually assist in the
development of new methods for constructing phylogenetic networks from
collections of smaller networks.
",Computer Science; Mathematics,Computer Science; Physics
"Bingham flow in porous media with obstacles of different size   By using the unfolding operators for periodic homogenization, we give a
general compactness result for a class of functions defined on bounded domains
presenting perforations of two different size. Then we apply this result to the
homogenization of the flow of a Bingham fluid in a porous medium with solid
obstacles of different size. Next we give the interpretation of the limit
problem in term of a non linear Darcy law.
",Mathematics,Mathematics
"MMD GAN: Towards Deeper Understanding of Moment Matching Network   Generative moment matching network (GMMN) is a deep generative model that
differs from Generative Adversarial Network (GAN) by replacing the
discriminator in GAN with a two-sample test based on kernel maximum mean
discrepancy (MMD). Although some theoretical guarantees of MMD have been
studied, the empirical performance of GMMN is still not as competitive as that
of GAN on challenging and large benchmark datasets. The computational
efficiency of GMMN is also less desirable in comparison with GAN, partially due
to its requirement for a rather large batch size during the training. In this
paper, we propose to improve both the model expressiveness of GMMN and its
computational efficiency by introducing adversarial kernel learning techniques,
as the replacement of a fixed Gaussian kernel in the original GMMN. The new
approach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN.
The new distance measure in MMD GAN is a meaningful loss that enjoys the
advantage of weak topology and can be optimized via gradient descent with
relatively small batch sizes. In our evaluation on multiple benchmark datasets,
including MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN
significantly outperforms GMMN, and is competitive with other representative
GAN works.
",Computer Science; Statistics,Computer Science; Statistics
"A Software Reuse Approach and Its Effect On Software Quality, An Empirical Study for The Software Industry   Software reusability has become much interesting because of increased quality
and reduce cost. A good process of software reuse leads to enhance the
reliability, productivity, quality and the reduction of time and cost. Current
reuse techniques focuses on the reuse of software artifact which grounded on
anticipated functionality whereas, the non-functional (quality) aspect are also
important. So, Software reusability used here to expand quality and
productivity of software. It improves overall quality of software in minimum
energy and time. Main objective of this study was to present a reuse approach
that discovered that how software reuse improves the quality in Software
Industry. The V&V technique used for this purpose which is part of software
quality management process, it checks the quality and correctness during the
software life cycle. A survey study conducted as QUESTIONAIR to find the impact
of reuse approach on quality attributes which are requirement specification and
design specification. Other quality enhancement techniques like ad hoc, CBSE,
MBSE, Product line, COTS reuse checked on existing software industry. Results
analyzed with the help of MATLAB tool as it provides effective data management,
wide range of options, better output organization, to check weather quality
enhancement technique is affected due to reusability and how quality will
improve.
",Computer Science,Computer Science
"The maximal order of iterated multiplicative functions   Following Wigert, a great number of authors including Ramanujan, Gronwall,
Erdős, Ivić, Heppner, J. Knopfmacher, Nicolas, Schwarz, Wirsing,
Freiman, Shiu et al. determined the maximal order of several multiplicative
functions, generalizing Wigert's result \[\max_{n\leq x} \log d(n)= (\log
2+o(1))\frac{\log x}{\log \log x}.\]
On the contrary, for many multiplicative functions, the maximal order of
iterations of the functions remains wide open. The case of the iterated divisor
function was only recently solved, answering a question of Ramanujan (1915).
Here, we determine the maximal order of $\log f(f(n))$ for a class of
multiplicative functions $f$ which are related to the divisor function. As a
corollary, we apply this to the function counting representations as sums of
two squares of non-negative integers, also known as $r_2(n)/4$, and obtain an
asymptotic formula: \[\max_{n\leq x} \log f(f(n))= (c+o(1))\frac{\sqrt{\log
x}}{\log \log x},\] with some explicitly given positive constant $c$.
",Mathematics,Mathematics
"A connection between the good property of an Artinian Gorenstein local ring and that of its quotient modulo socle   Following Roos, we say that a local ring $R$ is good if all finitely
generated $R$-modules have rational Poincaré series over $R$, sharing a
common denominator. Rings with the Backelin-Roos property and generalised Golod
rings are good due to results of Levin and Avramov respectively. Let $R$ be an
Artinian Gorenstein local ring. The ring $R$ is shown to have the Backelin-Roos
property if $R/ soc(R)$ is a Golod ring. Furthermore the ring $R$ is
generalised Golod if and only if $R/ soc(R)$ is so.
We explore when connected sums of Artinian Gorenstein local rings are good.
We provide a uniform argument to show that stretched, almost stretched
Gorenstein rings are good and show further that the Auslander-Reiten conjecture
holds true for such rings. We prove that Gorenstein rings of multiplicity at
most eleven are good. We recover a result of Rossi-Şega on the good
property of compressed Gorenstein local rings in a stronger form by a shorter
argument.
",Mathematics,Mathematics
"The duration of load effect in lumber as stochastic degradation   This paper proposes a gamma process for modelling the damage that accumulates
over time in the lumber used in structural engineering applications when stress
is applied. The model separates the stochastic processes representing features
internal to the piece of lumber on the one hand, from those representing
external forces due to applied dead and live loads. The model applies those
external forces through a time-varying population level function designed for
time-varying loads. The application of this type of model, which is standard in
reliability analysis, is novel in this context, which has been dominated by
accumulated damage models (ADMs) over more than half a century. The proposed
model is compared with one of the traditional ADMs. Our statistical results
based on a Bayesian analysis of experimental data highlight the limitations of
using accelerated testing data to assess long-term reliability, as seen in the
wide posterior intervals. This suggests the need for more comprehensive testing
in future applications, or to encode appropriate expert knowledge in the priors
used for Bayesian analysis.
",Statistics,Physics
"Modeling of hysteresis loop and its applications in ferroelectric materials   In order to understand the physical hysteresis loops clearly, we constructed
a novel model, which is combined with the electric field, the temperature, and
the stress as one synthetically parameter. This model revealed the shape of
hysteresis loop was determined by few variables in ferroelectric materials: the
saturation of polarization, the coercive field, the electric susceptibility and
the equivalent field. Comparison with experimental results revealed the model
can retrace polarization versus electric field and temperature. As a
applications of this model, the calculate formula of energy storage efficiency,
the electrocaloric effect, and the P(E,T) function have also been included in
this article.
",Physics,Physics
"Persistence barcodes and Laplace eigenfunctions on surfaces   We obtain restrictions on the persistence barcodes of Laplace-Beltrami
eigenfunctions and their linear combinations on compact surfaces with
Riemannian metrics. Some applications to uniform approximation by linear
combinations of Laplace eigenfunctions are also discussed.
",Mathematics,Mathematics
"Single-beam dielectric-microsphere trapping with optical heterodyne detection   A technique to levitate and measure the three-dimensional position of
micrometer-sized dielectric spheres with heterodyne detection is presented. The
two radial degrees of freedom are measured by interfering light transmitted
through the microsphere with a reference wavefront, while the axial degree of
freedom is measured from the phase of the light reflected from the surface of
the microsphere. This method pairs the simplicity and accessibility of single
beam optical traps to a measurement of displacement that is intrinsically
calibrated by the wavelength of the trapping light and has exceptional immunity
to stray light. A theoretical shot noise limit of
$1.3\times10^{-13}\,\text{m}/\sqrt{\text{Hz}}$ for the radial degrees of
freedom, and $3.0\times10^{-15} \, \text{m}/\sqrt{\text{Hz}}$ for the axial
degree of freedom can be obtained in the system described. The measured
acceleration noise in the radial direction is $7.5\times10^{-5} \,
(\text{m/s}^2)/\sqrt{\text{Hz}}$.
",Physics,Physics
"Memories of a Theoretical Physicist   While I was dealing with a brain injury and finding it difficult to work, two
friends (Derek Westen, a friend of the KITP, and Steve Shenker, with whom I was
recently collaborating), suggested that a new direction might be good. Steve in
particular regarded me as a good writer and suggested that I try that. I
quickly took to Steve's suggestion. Having only two bodies of knowledge, myself
and physics, I decided to write an autobiography about my development as a
theoretical physicist.
This is not written for any particular audience, but just to give myself a
goal. It will probably have too much physics for a nontechnical reader, and too
little for a physicist, but perhaps there with be different things for each.
Parts may be tedious. But it is somewhat unique, I think, a blow-by-blow
history of where I started and where I got to.
Probably the target audience is theoretical physicists, especially young
ones, who may enjoy comparing my struggles with their own. Some disclaimers:
This is based on my own memories, jogged by the arXiv and Inspire. There will
surely be errors and omissions. And note the title: this is about my memories,
which will be different for other people. Also, it would not be possible for me
to mention all the authors whose work might intersect mine, so this should not
be treated as a reference work.
",Physics,Computer Science
"A Time-Spectral Method for Initial-Value Problems Using a Novel Spatial Subdomain Scheme   We analyse a new subdomain scheme for a time-spectral method for solving
initial boundary value problems. Whilst spectral methods are commonplace for
spatially dependent systems, finite difference schemes are typically applied
for the temporal domain. The Generalized Weighted Residual Method (GWRM) is a
fully spectral method in that it spectrally decomposes all specified domains,
including the temporal domain, with multivariate Chebyshev polynomials. The
Common Boundary-Condition method (CBC) is a spatial subdomain scheme that
solves the physical equations independently from the global connection of
subdomains. It is here evaluated against two finite difference methods. For the
linearised Burger equation the CBC-GWRM is $\sim30\%$ faster and $\sim50\%$
more memory efficient than the semi implicit Crank-Nicolson method at a maximum
error $\sim10^{-5}$. For a forced wave equation the CBC-GWRM manages to average
efficiently over the small time-scale in the entire temporal domain. The
CBC-GWRM is also applied to the linearised ideal magnetohydrodynamic (MHD)
equations for a screw pinch equilibrium. The growth rate of the most unstable
mode was efficiently computed with an error $<0.1\%$.
",Physics,Mathematics
"Ultra-Fast Relaxation, Decoherence and Localization of Photoexcited States in $π$-Conjugated Polymers: A TEBD Study   The exciton relaxation dynamics of photoexcited electronic states in
poly($p$-phenylenevinylene) (PPV) are theoretically investigated within a
coarse-grained model, in which both the exciton and nuclear degrees of freedom
are treated quantum mechanically. The Frenkel-Holstein Hamiltonian is used to
describe the strong exciton-phonon coupling present in the system, while
external damping of the internal nuclear degrees of freedom are accounted for
by a Lindblad master equation. Numerically, the dynamics are computed using the
time evolving block decimation (TEBD) and quantum jump trajectory techniques.
The values of the model parameters physically relevant to polymer systems
naturally lead to a separation of time scales, with the ultra-fast dynamics
corresponding to energy transfer from the exciton to the internal phonon modes
(i.e., the C-C bond oscillations), while the longer time dynamics correspond to
damping of these phonon modes by the external dissipation. Associated with
these time scales, we investigate the following processes that are indicative
of the system relaxing onto the emissive chromophores of the polymer: 1)
Exciton-polaron formation occurs on an ultra-fast time scale, with the
associated exciton-phonon correlations present within half a vibrational time
period of the C-C bond oscillations. 2) Exciton decoherence is driven by the
decay in the vibrational overlaps associated with exciton-polaron formation,
occurring on the same time scale. 3) Exciton density localization is driven by
the external dissipation, arising from `wavefunction collapse' occurring as a
result of the system-environment interactions. Finally, we show how
fluorescence anisotropy measurements can be used to investigate the exciton
decoherence process during the relaxation dynamics.
",Physics,Physics
"The ALMA View of the OMC1 Explosion in Orion   Most massive stars form in dense clusters where gravitational interactions
with other stars may be common. The two nearest forming massive stars, the BN
object and Source I, located behind the Orion Nebula, were ejected with
velocities of $\sim$29 and $\sim$13 km s$^{-1}$ about 500 years ago by such
interactions. This event generated an explosion in the gas. New ALMA
observations show in unprecedented detail, a roughly spherically symmetric
distribution of over a hundred $^{12}$CO J=2$-$1 streamers with velocities
extending from V$_{LSR}$ =$-$150 to +145 km s$^{-1}$. The streamer radial
velocities increase (or decrease) linearly with projected distance from the
explosion center, forming a `Hubble Flow' confined to within 50 arcseconds of
the explosion center. They point toward the high proper-motion, shock-excited
H$_2$ and [Fe ii ] `fingertips' and lower-velocity CO in the H$_2$ wakes
comprising Orion's `fingers'. In some directions, the H$_2$ `fingers' extend
more than a factor of two farther from the ejection center than the CO
streamers. Such deviations from spherical symmetry may be caused by ejecta
running into dense gas or the dynamics of the N-body interaction that ejected
the stars and produced the explosion. This $\sim$10$^{48}$ erg event may have
been powered by the release of gravitational potential energy associated with
the formation of a compact binary or a protostellar merger. Orion may be the
prototype for a new class of stellar explosion responsible for luminous
infrared transients in nearby galaxies.
",Physics,Physics
"Nematic superconductivity in Cu$_{x}$Bi$_{2}$Se$_{3}$: The surface Andreev bound states   We study theoretically the topological surface states (TSSs) and the possible
surface Andreev bound states (SABSs) of Cu$_{x}$Bi$_{2}$Se$_{3}$ which is known
to be a topological insulator at $x=0$. The superconductivity (SC) pairing of
this compound is assumed to have the broken spin-rotation symmetry, similar to
that of the A-phase of $^{3}$He as suggested by recent nuclear-magnetic
resonance experiments. For both spheroidal and corrugated cylindrical Fermi
surfaces with the hexagonal warping terms, we show that the bulk SC gap is
rather anisotropic; the minimum of the gap is negligibly small as comparing to
the maximum of the gap. This would make the fully-gapped pairing effectively
nodal. For a clean system, our results indicate the bulk of this compound to be
a topological superconductor with the SABSs appearing inside the bulk SC gap.
The zero-energy SABSs which are Majorana fermions, together with the TSSs not
gapped by the pairing, produce a zero-energy peak in the surface density of
states (SDOS). The SABSs are expected to be stable against short-range
nonmagnetic impurities, and the local SDOS is calculated around a nonmagnetic
impurity. The relevance of our results to experiments is discussed.
",Physics,Physics
"Buy your coffee with bitcoin: Real-world deployment of a bitcoin point of sale terminal   In this paper we discuss existing approaches for Bitcoin payments, as
suitable for a small business for small-value transactions. We develop an
evaluation framework utilizing security, usability, deployability criteria,,
examine several existing systems, tools. Following a requirements engineering
approach, we designed, implemented a new Point of Sale (PoS) system that
satisfies an optimal set of criteria within our evaluation framework. Our open
source system, Aunja PoS, has been deployed in a real world cafe since October
2014.
",Computer Science,Computer Science
"Provability Logics of Hierarchies   The branch of provability logic investigates the provability-based behavior
of the mathematical theories. In a more precise way, it studies the relation
between a mathematical theory $T$ and a modal logic $L$ via the provability
interpretation which interprets the modality as the provability predicate of
$T$. In this paper we will extend this relation to investigate the
provability-based behavior of a hierarchy of theories. More precisely, using
the modal language with infinitely many modalities,
$\{\Box_n\}_{n=0}^{\infty}$, we will define the hierarchical counterparts of
some of the classical modal theories such as $\mathbf{K4}$, $\mathbf{KD4}$,
$\mathbf{GL}$ and $\mathbf{S4}$. Then we will define their canonical
provability interpretations and their corresponding soundness-completeness
theorems.
",Mathematics,Computer Science
"Two-domain and three-domain limit cycles in a typical aeroelastic system with freeplay in pitch   Freeplay is a significant source of nonlinearity in aeroelastic systems and
is strictly regulated by airworthiness authorities. It splits the phase plane
of such systems into three piecewise linear subdomains. Depending on the
location of the freeplay, limit cycle oscillations can result that span either
two or three of these subdomains. The purpose of this work is to demonstrate
the existence of two-domain cycles both theoretically and experimentally. A
simple aeroelastic system with pitch, plunge and control deflection degrees of
freedom is investigated in the presence of freeplay in pitch. It is shown that
two-domain and three-domain cycles can result from a grazing bifurcation and
propagate in the decreasing airspeed direction. Close to the bifurcation, the
two limit cycle branches interact with each other and aperiodic oscillations
ensue. Equivalent linearization is used to derive the conditions of existence
of each type of limit cycle and to predict their amplitudes and frequencies.
Comparisons with measurements from wind tunnel experiments demonstrate that the
theory describes these phenomena with accuracy.
",Physics,Physics
"Improved self-energy correction method for accurate and efficient band structure calculation   The LDA-1/2 method for self-energy correction is a powerful tool for
calculating accurate band structures of semiconductors, while keeping the
computational load as low as standard LDA. Nevertheless, controversies remain
regarding the arbitrariness of choice between (1/2)e and (1/4)e charge
stripping from the atoms in group IV semiconductors, the incorrect direct band
gap predicted for Ge, and inaccurate band structures for III-V semiconductors.
Here we propose an improved method named shell-LDA-1/2 (shLDA-1/2 for short),
which is based on a shell-like trimming function for the self-energy potential.
With the new approach, we obtained accurate band structures for group IV, and
for III-V and II-VI compound semiconductors. In particular, we reproduced the
complete band structure of Ge in good agreement with experimental data.
Moreover, we have defined clear rules for choosing when (1/2)e or (1/4)e charge
ought to be stripped in covalent semiconductors, and for identifying materials
for which shLDA-1/2 is expected to fail.
",Physics,Physics
"Abstracting Event-Driven Systems with Lifestate Rules   We present lifestate rules--an approach for abstracting event-driven object
protocols. Developing applications against event-driven software frameworks is
notoriously difficult. One reason why is that to create functioning
applications, developers must know about and understand the complex protocols
that abstract the internal behavior of the framework. Such protocols intertwine
the proper registering of callbacks to receive control from the framework with
appropriate application programming interface (API) calls to delegate back to
it. Lifestate rules unify lifecycle and typestate constraints in one common
specification language. Our primary contribution is a model of event-driven
systems from which lifestate rules can be derived. We then apply specification
mining techniques to learn lifestate specifications for Android framework
types. In the end, our implementation is able to find several rules that
characterize actual behavior of the Android framework.
",Computer Science,Computer Science
"Generating Representative Executions [Extended Abstract]   Analyzing the behaviour of a concurrent program is made difficult by the
number of possible executions. This problem can be alleviated by applying the
theory of Mazurkiewicz traces to focus only on the canonical representatives of
the equivalence classes of the possible executions of the program. This paper
presents a generic framework that allows to specify the possible behaviours of
the execution environment, and generate all Foata-normal executions of a
program, for that environment, by discarding abnormal executions during the
generation phase. The key ingredient of Mazurkiewicz trace theory, the
dependency relation, is used in the framework in two roles: first, as part of
the specification of which executions are allowed at all, and then as part of
the normality checking algorithm, which is used to discard the abnormal
executions. The framework is instantiated to the relaxed memory models of the
SPARC hierarchy.
",Computer Science,Computer Science
"The Massive CO White Dwarf in the Symbiotic Recurrent Nova RS Ophiuchi   If accreting white dwarfs (WD) in binary systems are to produce type Ia
supernovae (SNIa), they must grow to nearly the Chandrasekhar mass and ignite
carbon burning. Proving conclusively that a WD has grown substantially since
its birth is a challenging task. Slow accretion of hydrogen inevitably leads to
the erosion, rather than the growth of WDs. Rapid hydrogen accretion does lead
to growth of a helium layer, due to both decreased degeneracy and the
inhibition of mixing of the accreted hydrogen with the underlying WD. However,
until recently, simulations of helium-accreting WDs all claimed to show the
explosive ejection of a helium envelope once it exceeded $\sim 10^{-1}\, \rm
M_{\odot}$. Because CO WDs cannot be born with masses in excess of $\sim 1.1\,
\rm M_{\odot}$, any such object, in excess of $\sim 1.2\, \rm M_{\odot}$, must
have grown substantially. We demonstrate that the WD in the symbiotic nova RS
Oph is in the mass range 1.2-1.4\,M$_{\odot}$. We compare UV spectra of RS Oph
with those of novae with ONe WDs, and with novae erupting on CO WDs. The RS Oph
WD is clearly made of CO, demonstrating that it has grown substantially since
birth. It is a prime candidate to eventually produce an SNIa.
",Physics,Physics
"Perturbing Eisenstein polynomials over local fields   Let $K$ be a local field whose residue field has characteristic $p$ and let
$L/K$ be a finite separable totally ramified extension. Let $\pi_L$ be a
uniformizer for $L$ and let $f(X)$ be the minimum polynomial for $\pi_L$ over
$K$. Suppose $\tilde{\pi}_L$ is another uniformizer for $L$ such that
$\tilde{\pi}_L\equiv\pi_L+r\pi_L^{\ell+1} \pmod{\pi_L^{\ell+2}}$ for some
$\ell\ge1$ and $r\in O_K$. Let $\tilde{f}(X)$ be the minimum polynomial for
$\tilde{\pi}_L$ over $K$. In this paper we give congruences for the
coefficients of $\tilde{f}(X)$ in terms of $r$ and the coefficients of $f(X)$.
These congruences improve and extend work of Krasner.
",Mathematics,Mathematics
"Automata-Guided Hierarchical Reinforcement Learning for Skill Composition   Skills learned through (deep) reinforcement learning often generalizes poorly
across domains and re-training is necessary when presented with a new task. We
present a framework that combines techniques in \textit{formal methods} with
\textit{reinforcement learning} (RL). The methods we provide allows for
convenient specification of tasks with logical expressions, learns hierarchical
policies (meta-controller and low-level controllers) with well-defined
intrinsic rewards, and construct new skills from existing ones with little to
no additional exploration. We evaluate the proposed methods in a simple grid
world simulation as well as a more complicated kitchen environment in AI2Thor
",Computer Science,Computer Science
"Glitch Classification and Clustering for LIGO with Deep Transfer Learning   The detection of gravitational waves with LIGO and Virgo requires a detailed
understanding of the response of these instruments in the presence of
environmental and instrumental noise. Of particular interest is the study of
anomalous non-Gaussian noise transients known as glitches, since their high
occurrence rate in LIGO/Virgo data can obscure or even mimic true gravitational
wave signals. Therefore, successfully identifying and excising glitches is of
utmost importance to detect and characterize gravitational waves. In this
article, we present the first application of Deep Learning combined with
Transfer Learning for glitch classification, using real data from LIGO's first
discovery campaign labeled by Gravity Spy, showing that knowledge from
pre-trained models for real-world object recognition can be transferred for
classifying spectrograms of glitches. We demonstrate that this method enables
the optimal use of very deep convolutional neural networks for glitch
classification given small unbalanced training datasets, significantly reduces
the training time, and achieves state-of-the-art accuracy above 98.8%. Once
trained via transfer learning, we show that the networks can be truncated and
used as feature extractors for unsupervised clustering to automatically group
together new classes of glitches and anomalies. This novel capability is of
critical importance to identify and remove new types of glitches which will
occur as the LIGO/Virgo detectors gradually attain design sensitivity.
",Computer Science; Physics; Statistics,Computer Science; Statistics
"Orbit classification in the Hill problem: I. The classical case   The case of the classical Hill problem is numerically investigated by
performing a thorough and systematic classification of the initial conditions
of the orbits. More precisely, the initial conditions of the orbits are
classified into four categories: (i) non-escaping regular orbits; (ii) trapped
chaotic orbits; (iii) escaping orbits; and (iv) collision orbits. In order to
obtain a more general and complete view of the orbital structure of the
dynamical system our exploration takes place in both planar (2D) and the
spatial (3D) version of the Hill problem. For the 2D system we numerically
integrate large sets of initial conditions in several types of planes, while
for the system with three degrees of freedom, three-dimensional distributions
of initial conditions of orbits are examined. For distinguishing between
ordered and chaotic bounded motion the Smaller ALingment Index (SALI) method is
used. We managed to locate the several bounded basins, as well as the basins of
escape and collision and also to relate them with the corresponding escape and
collision time of the orbits. Our numerical calculations indicate that the
overall orbital dynamics of the Hamiltonian system is a complicated but highly
interested problem. We hope our contribution to be useful for a further
understanding of the orbital properties of the classical Hill problem.
",Physics,Physics
"Efficient Test-based Variable Selection for High-dimensional Linear Models   Variable selection plays a fundamental role in high-dimensional data
analysis. Various methods have been developed for variable selection in recent
years. Well-known examples are forward stepwise regression (FSR) and least
angle regression (LARS), among others. These methods typically add variables
into the model one by one. For such selection procedures, it is crucial to find
a stopping criterion that controls model complexity. One of the most commonly
used techniques to this end is cross-validation (CV) which, in spite of its
popularity, has two major drawbacks: expensive computational cost and lack of
statistical interpretation. To overcome these drawbacks, we introduce a
flexible and efficient test-based variable selection approach that can be
incorporated into any sequential selection procedure. The test, which is on the
overall signal in the remaining inactive variables, is based on the maximal
absolute partial correlation between the inactive variables and the response
given active variables. We develop the asymptotic null distribution of the
proposed test statistic as the dimension tends to infinity uniformly in the
sample size. We also show that the test is consistent. With this test, at each
step of the selection, a new variable is included if and only if the $p$-value
is below some pre-defined level. Numerical studies show that the proposed
method delivers very competitive performance in terms of variable selection
accuracy and computational complexity compared to CV.
",Statistics,Statistics
"3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks   The success of various applications including robotics, digital content
creation, and visualization demand a structured and abstract representation of
the 3D world from limited sensor data. Inspired by the nature of human
perception of 3D shapes as a collection of simple parts, we explore such an
abstract shape representation based on primitives. Given a single depth image
of an object, we present 3D-PRNN, a generative recurrent neural network that
synthesizes multiple plausible shapes composed of a set of primitives. Our
generative model encodes symmetry characteristics of common man-made objects,
preserves long-range structural coherence, and describes objects of varying
complexity with a compact representation. We also propose a method based on
Gaussian Fields to generate a large scale dataset of primitive-based shape
representations to train our network. We evaluate our approach on a wide range
of examples and show that it outperforms nearest-neighbor based shape retrieval
methods and is on-par with voxel-based generative models while using a
significantly reduced parameter space.
",Computer Science; Statistics,Computer Science
"SAGA and Restricted Strong Convexity   SAGA is a fast incremental gradient method on the finite sum problem and its
effectiveness has been tested on a vast of applications. In this paper, we
analyze SAGA on a class of non-strongly convex and non-convex statistical
problem such as Lasso, group Lasso, Logistic regression with $\ell_1$
regularization, linear regression with SCAD regularization and Correct Lasso.
We prove that SAGA enjoys the linear convergence rate up to the statistical
estimation accuracy, under the assumption of restricted strong convexity (RSC).
It significantly extends the applicability of SAGA in convex and non-convex
optimization.
",Statistics,Mathematics; Statistics
"Generalized Yangians and their Poisson counterparts   By a generalized Yangian we mean a Yangian-like algebra of one of two
classes. One of these classes consists of the so-called braided Yangians,
introduced in our previous paper. The braided Yangians are in a sense similar
to the reflection equation algebra. The generalized Yangians of second class,
called the Yangians of RTT type, are defined by the same formulae as the usual
Yangians are but with other quantum $R$-matrices. If such an $R$-matrix is the
simplest trigonometrical $R$-matrix, the corresponding Yangian of RTT type is
the so-called q-Yangian. We claim that each generalized Yangian is a
deformation of the commutative algebra ${\rm Sym}(gl(m)[t^{-1}])$ provided that
the corresponding $R$-matrix is a deformation of the flip. Also, we exhibit the
corresponding Poisson brackets.
",Mathematics,Mathematics
"Direct Mapping Hidden Excited State Interaction Patterns from ab initio Dynamics and Its Implications on Force Field Development   The excited states of polyatomic systems are rather complex, and often
exhibit meta-stable dynamical behaviors. Static analysis of reaction pathway
often fails to sufficiently characterize excited state motions due to their
highly non-equilibrium nature. Here, we proposed a time series guided
clustering algorithm to generate most relevant meta-stable patterns directly
from ab initio dynamic trajectories. Based on the knowledge of these
meta-stable patterns, we suggested an interpolation scheme with only a concrete
and finite set of known patterns to accurately predict the ground and excited
state properties of the entire dynamics trajectories. As illustrated with the
example of sinapic acids, the estimation error for both ground and excited
state is very close, which indicates one could predict the ground and excited
state molecular properties with similar accuracy. These results may provide us
some insights to construct an excited state force field with compatible energy
terms as traditional ones.
",Physics; Statistics,Physics
"Anisotropic triangulations via discrete Riemannian Voronoi diagrams   The construction of anisotropic triangulations is desirable for various
applications, such as the numerical solving of partial differential equations
and the representation of surfaces in graphics. To solve this notoriously
difficult problem in a practical way, we introduce the discrete Riemannian
Voronoi diagram, a discrete structure that approximates the Riemannian Voronoi
diagram. This structure has been implemented and was shown to lead to good
triangulations in $\mathbb{R}^2$ and on surfaces embedded in $\mathbb{R}^3$ as
detailed in our experimental companion paper.
In this paper, we study theoretical aspects of our structure. Given a finite
set of points $\cal P$ in a domain $\Omega$ equipped with a Riemannian metric,
we compare the discrete Riemannian Voronoi diagram of $\cal P$ to its
Riemannian Voronoi diagram. Both diagrams have dual structures called the
discrete Riemannian Delaunay and the Riemannian Delaunay complex. We provide
conditions that guarantee that these dual structures are identical. It then
follows from previous results that the discrete Riemannian Delaunay complex can
be embedded in $\Omega$ under sufficient conditions, leading to an anisotropic
triangulation with curved simplices. Furthermore, we show that, under similar
conditions, the simplices of this triangulation can be straightened.
",Computer Science,Mathematics
"An Efficient Deep Learning Technique for the Navier-Stokes Equations: Application to Unsteady Wake Flow Dynamics   We present an efficient deep learning technique for the model reduction of
the Navier-Stokes equations for unsteady flow problems. The proposed technique
relies on the Convolutional Neural Network (CNN) and the stochastic gradient
descent method. Of particular interest is to predict the unsteady fluid forces
for different bluff body shapes at low Reynolds number. The discrete
convolution process with a nonlinear rectification is employed to approximate
the mapping between the bluff-body shape and the fluid forces. The deep neural
network is fed by the Euclidean distance function as the input and the target
data generated by the full-order Navier-Stokes computations for primitive bluff
body shapes. The convolutional networks are iteratively trained using the
stochastic gradient descent method with the momentum term to predict the fluid
force coefficients of different geometries and the results are compared with
the full-order computations. We attempt to provide a physical analogy of the
stochastic gradient method with the momentum term with the simplified form of
the incompressible Navier-Stokes momentum equation. We also construct a direct
relationship between the CNN-based deep learning and the Mori-Zwanzig formalism
for the model reduction of a fluid dynamical system. A systematic convergence
and sensitivity study is performed to identify the effective dimensions of the
deep-learned CNN process such as the convolution kernel size, the number of
kernels and the convolution layers. Within the error threshold, the prediction
based on our deep convolutional network has a speed-up nearly four orders of
magnitude compared to the full-order results and consumes an insignificant
fraction of computational resources. The proposed CNN-based approximation
procedure has a profound impact on the parametric design of bluff bodies and
the feedback control of separated flows.
",Physics,Computer Science; Statistics
"Go game formal revealing by Ising model   Go gaming is a struggle for territory control between rival, black and white,
stones on a board. We model the Go dynamics in a game by means of the Ising
model whose interaction coefficients reflect essential rules and tactics
employed in Go to build long-term strategies. At any step of the game, the
energy functional of the model provides the control degree (strength) of a
player over the board. A close fit between predictions of the model with actual
games is obtained.
",Computer Science,Computer Science
"Three-Dimensional Electronic Structure of type-II Weyl Semimetal WTe$_2$   By combining bulk sensitive soft-X-ray angular-resolved photoemission
spectroscopy and accurate first-principles calculations we explored the bulk
electronic properties of WTe$_2$, a candidate type-II Weyl semimetal featuring
a large non-saturating magnetoresistance. Despite the layered geometry
suggesting a two-dimensional electronic structure, we find a three-dimensional
electronic dispersion. We report an evident band dispersion in the reciprocal
direction perpendicular to the layers, implying that electrons can also travel
coherently when crossing from one layer to the other. The measured Fermi
surface is characterized by two well-separated electron and hole pockets at
either side of the $\Gamma$ point, differently from previous more surface
sensitive ARPES experiments that additionally found a significant quasiparticle
weight at the zone center. Moreover, we observe a significant sensitivity of
the bulk electronic structure of WTe$_2$ around the Fermi level to electronic
correlations and renormalizations due to self-energy effects, previously
neglected in first-principles descriptions.
",Physics,Physics
"Cognitive networks: brains, internet, and civilizations   In this short essay, we discuss some basic features of cognitive activity at
several different space-time scales: from neural networks in the brain to
civilizations. One motivation for such comparative study is its heuristic
value. Attempts to better understand the functioning of ""wetware"" involved in
cognitive activities of central nervous system by comparing it with a computing
device have a long tradition. We suggest that comparison with Internet might be
more adequate. We briefly touch upon such subjects as encoding, compression,
and Saussurean trichotomy langue/langage/parole in various environments.
",Computer Science,Computer Science; Physics
"HARP: Hierarchical Representation Learning for Networks   We present HARP, a novel method for learning low dimensional embeddings of a
graph's nodes which preserves higher-order structural features. Our proposed
method achieves this by compressing the input graph prior to embedding it,
effectively avoiding troublesome embedding configurations (i.e. local minima)
which can pose problems to non-convex optimization. HARP works by finding a
smaller graph which approximates the global structure of its input. This
simplified graph is used to learn a set of initial representations, which serve
as good initializations for learning representations in the original, detailed
graph. We inductively extend this idea, by decomposing a graph in a series of
levels, and then embed the hierarchy of graphs from the coarsest one to the
original graph. HARP is a general meta-strategy to improve all of the
state-of-the-art neural algorithms for embedding graphs, including DeepWalk,
LINE, and Node2vec. Indeed, we demonstrate that applying HARP's hierarchical
paradigm yields improved implementations for all three of these methods, as
evaluated on both classification tasks on real-world graphs such as DBLP,
BlogCatalog, CiteSeer, and Arxiv, where we achieve a performance gain over the
original implementations by up to 14% Macro F1.
",Computer Science,Computer Science; Statistics
"Almost sure scattering for the energy-critical NLS with radial data below $H^1(\mathbb{R}^4)$   We prove almost sure global existence and scattering for the energy-critical
nonlinear Schrödinger equation with randomized spherically symmetric initial
data in $H^s(\mathbb{R}^4)$ with $\frac56<s<1$. We were inspired to consider
this problem by the recent work of Dodson--Lührmann--Mendelson, which treated
the analogous problem for the energy-critical wave equation.
",Mathematics,Mathematics
"Passivity Based Whole-body Control for Quadrupedal Locomotion on Challenging Terrain   We present a passivity-based Whole-Body Control approach for quadruped robots
that achieves dynamic locomotion while compliantly balancing the robot's trunk.
We formulate the motion tracking as a Quadratic Program that takes into account
the full robot rigid body dynamics, the actuation limit, the joint limits and
the contact interaction. We analyze the controller robustness against
inaccurate friction coefficient estimates and unstable footholds, as well as
its capability to redistribute the load as a consequence of enforcing actuation
limits. Additionally, we present some practical implementation details gained
from the experience with the real platform. Extensive experimental trials on
the 90 Kg Hydraulically actuated Quadruped robot validate the capabilities of
this controller under various terrain conditions and gaits. The proposed
approach is expedient for accurate execution of high dynamic motions with
respect to the current state of the art.
",Computer Science,Computer Science
"Temporal Difference Learning with Neural Networks - Study of the Leakage Propagation Problem   Temporal-Difference learning (TD) [Sutton, 1988] with function approximation
can converge to solutions that are worse than those obtained by Monte-Carlo
regression, even in the simple case of on-policy evaluation. To increase our
understanding of the problem, we investigate the issue of approximation errors
in areas of sharp discontinuities of the value function being further
propagated by bootstrap updates. We show empirical evidence of this leakage
propagation, and show analytically that it must occur, in a simple Markov
chain, when function approximation errors are present. For reversible policies,
the result can be interpreted as the tension between two terms of the loss
function that TD minimises, as recently described by [Ollivier, 2018]. We show
that the upper bounds from [Tsitsiklis and Van Roy, 1997] hold, but they do not
imply that leakage propagation occurs and under what conditions. Finally, we
test whether the problem could be mitigated with a better state representation,
and whether it can be learned in an unsupervised manner, without rewards or
privileged information.
",Statistics,Statistics
"Adaptive Matching for Expert Systems with Uncertain Task Types   A matching in a two-sided market often incurs an externality: a matched
resource may become unavailable to the other side of the market, at least for a
while. This is especially an issue in online platforms involving human experts
as the expert resources are often scarce. The efficient utilization of experts
in these platforms is made challenging by the fact that the information
available about the parties involved is usually limited.
To address this challenge, we develop a model of a task-expert matching
system where a task is matched to an expert using not only the prior
information about the task but also the feedback obtained from the past
matches. In our model the tasks arrive online while the experts are fixed and
constrained by a finite service capacity. For this model, we characterize the
maximum task resolution throughput a platform can achieve. We show that the
natural greedy approaches where each expert is assigned a task most suitable to
her skill is suboptimal, as it does not internalize the above externality. We
develop a throughput optimal backpressure algorithm which does so by accounting
for the `congestion' among different task types. Finally, we validate our model
and confirm our theoretical findings with data-driven simulations via logs of
Math.StackExchange, a StackOverflow forum dedicated to mathematics.
",Computer Science; Statistics,Computer Science
"Sea of Lights: Practical Device-to-Device Security Bootstrapping in the Dark   Practical solutions to bootstrap security in today's information and
communication systems critically depend on centralized services for
authentication as well as key and trust management. This is particularly true
for mobile users. Identity providers such as Google or Facebook have active
user bases of two billion each, and the subscriber number of mobile operators
exceeds five billion unique users as of early 2018. If these centralized
services go completely `dark' due to natural or man made disasters, large scale
blackouts, or country-wide censorship, the users are left without practical
solutions to bootstrap security on their mobile devices. Existing distributed
solutions, for instance, the so-called web-of-trust are not sufficiently
lightweight. Furthermore, they support neither cross-application on mobile
devices nor strong protection of key material using hardware security modules.
We propose Sea of Lights(SoL), a practical lightweight scheme for bootstrapping
device-to-device security wirelessly, thus, enabling secure distributed
self-organized networks. It is tailored to operate `in the dark' and provides
strong protection of key material as well as an intuitive means to build a
lightweight web-of-trust. SoL is particularly well suited for local or urban
operation in scenarios such as the coordination of emergency response, where it
helps containing/limiting the spreading of misinformation. As a proof of
concept, we implement SoL in the Android platform and hence test its
feasibility on real mobile devices. We further evaluate its key performance
aspects using simulation.
",Computer Science,Computer Science
"Radial Surface Density Profiles of Gas and Dust in the Debris Disk around 49 Ceti   We present ~0.4 resolution images of CO(3-2) and associated continuum
emission from the gas-bearing debris disk around the nearby A star 49 Ceti,
observed with the Atacama Large Millimeter/Submillimeter Array (ALMA). We
analyze the ALMA visibilities in tandem with the broad-band spectral energy
distribution to measure the radial surface density profiles of dust and gas
emission from the system. The dust surface density decreases with radius
between ~100 and 310 au, with a marginally significant enhancement of surface
density at a radius of ~110 au. The SED requires an inner disk of small grains
in addition to the outer disk of larger grains resolved by ALMA. The gas disk
exhibits a surface density profile that increases with radius, contrary to most
previous spatially resolved observations of circumstellar gas disks. While ~80%
of the CO flux is well described by an axisymmetric power-law disk in Keplerian
rotation about the central star, residuals at ~20% of the peak flux exhibit a
departure from axisymmetry suggestive of spiral arms or a warp in the gas disk.
The radial extent of the gas disk (~220 au) is smaller than that of the dust
disk (~300 au), consistent with recent observations of other gas-bearing debris
disks. While there are so far only three broad debris disks with well
characterized radial dust profiles at millimeter wavelengths, 49 Ceti's disk
shows a markedly different structure from two radially resolved gas-poor debris
disks, implying that the physical processes generating and sculpting the gas
and dust are fundamentally different.
",Physics,Physics
"Extraordinary linear dynamic range in laser-defined functionalized graphene photodetectors   Graphene-based photodetectors have demonstrated mechanical flexibility, large
operating bandwidth, and broadband spectral response. However, their linear
dynamic range (LDR) is limited by graphene's intrinsichot-carrier dynamics,
which causes deviation from a linear photoresponse at low incident powers. At
the same time, multiplication of hot carriers causes the photoactive region to
be smeared over distances of a few micro-meters, limiting the use of graphene
in high-resolution applications. We present a novel method for engineer-ing
photoactive junctions in FeCl3-intercalated graphene using laser irradiation.
Photocurrent measured at these planar junctions shows an extraordinary linear
response with an LDR value at least 4500 times larger than that of other
graphene devices (44 dB) while maintaining high stability against environmental
contamination without the need for encapsulation. The observed photoresponse is
purely photovoltaic, demonstrating complete quenching of hot-carrier effects.
These results pave the way toward the design of ultrathin photode-tectors with
unprecedented LDR for high-definition imaging and sensing.
",Physics,Physics
"Identifiability of phylogenetic parameters from k-mer data under the coalescent   Distances between sequences based on their $k$-mer frequency counts can be
used to reconstruct phylogenies without first computing a sequence alignment.
Past work has shown that effective use of k-mer methods depends on 1)
model-based corrections to distances based on $k$-mers and 2) breaking long
sequences into blocks to obtain repeated trials from the sequence-generating
process. Good performance of such methods is based on having many high-quality
blocks with many homologous sites, which can be problematic to guarantee a
priori.
Nature provides natural blocks of sequences into homologous regions---namely,
the genes. However, directly using past work in this setting is problematic
because of possible discordance between different gene trees and the underlying
species tree. Using the multispecies coalescent model as a basis, we derive
model-based moment formulas that involve the divergence times and the
coalescent parameters. From this setting, we prove identifiability results for
the tree and branch length parameters under the Jukes-Cantor model of sequence
mutations.
",Mathematics,Quantitative Biology
"Active set algorithms for estimating shape-constrained density ratios   We review and modify the active set algorithm by Duembgen et al. (2011) for
nonparametric maximum-likelihood estimation of a log-concave density. This
particular estimation problem is embedded into a more general framework
including also the estimation of a log-convex tail inflation function as
proposed by McCullagh and Polson (2012).
",Statistics,Mathematics; Statistics
"Binary systems with an RR Lyrae component - progress in 2016   In this contribution, we summarize the progress made in the investigation of
binary candidates with an RR Lyrae component in 2016. We also discuss the
actual status of the RRLyrBinCan database.
",Physics,Computer Science
"Estimating Local Interactions Among Many Agents Who Observe Their Neighbors   In various economic environments, people observe those with whom they
strategically interact. We can model such information-sharing relations as an
information network, and the strategic interactions as a game on the network.
When any two agents in the network are connected either directly or indirectly,
empirical modeling using an equilibrium approach is cumbersome, since the
testable implications from an equilibrium generally involve all the players of
the game, whereas a researcher's data set may contain only a fraction of these
players in practice. This paper develops a tractable empirical model of linear
interactions where each agent, after observing part of his neighbors' types,
not knowing the full information network, uses best responses that are linear
in his and other players' types that he observes, based on simple beliefs about
other players' strategies. We provide conditions on information networks and
beliefs such that best responses take an explicit form with multiple intuitive
features. Furthermore, the best responses reveal how local payoff
interdependence among agents is translated into local stochastic dependence of
their actions, allowing the econometrician to perform asymptotic inference
without having to observe all the players in the game or having to know
precisely the sampling process.
",Statistics,Computer Science; Statistics
"A Controlled Set-Up Experiment to Establish Personalized Baselines for Real-Life Emotion Recognition   We design, conduct and present the results of a highly personalized baseline
emotion recognition experiment, which aims to set reliable ground-truth
estimates for the subject's emotional state for real-life prediction under
similar conditions using a small number of physiological sensors. We also
propose an adaptive stimuli-selection mechanism that would use the user's
feedback as guide for future stimuli selection in the controlled-setup
experiment and generate optimal ground-truth personalized sessions
systematically. Initial results are very promising (85% accuracy) and variable
importance analysis shows that only a few features, which are easy-to-implement
in portable devices, would suffice to predict the subject's emotional state.
",Computer Science; Statistics,Computer Science; Statistics
"Integrability of dispersionless Hirota type equations in 4D and the symplectic Monge-Ampere property   We prove that integrability of a dispersionless Hirota type equation implies
the symplectic Monge-Ampere property in any dimension $\geq 4$. In 4D this
yields a complete classification of integrable dispersionless PDEs of Hirota
type through a list of heavenly type equations arising in self-dual gravity. As
a by-product of our approach we derive an involutive system of relations
characterising symplectic Monge-Ampere equations in any dimension.
Moreover, we demonstrate that in 4D the requirement of integrability is
equivalent to self-duality of the conformal structure defined by the
characteristic variety of the equation on every solution, which is in turn
equivalent to the existence of a dispersionless Lax pair. We also give a
criterion of linerisability of a Hirota type equation via flatness of the
corresponding conformal structure, and study symmetry properties of integrable
equations.
",Physics; Mathematics,Mathematics
"Optical quality assurance of GEM foils   An analysis software was developed for the high aspect ratio optical scanning
system in the Detec- tor Laboratory of the University of Helsinki and the
Helsinki Institute of Physics. The system is used e.g. in the quality assurance
of the GEM-TPC detectors being developed for the beam diagnostics system of the
SuperFRS at future FAIR facility. The software was tested by analyzing five
CERN standard GEM foils scanned with the optical scanning system. The
measurement uncertainty of the diameter of the GEM holes and the pitch of the
hole pattern was found to be 0.5 {\mu}m and 0.3 {\mu}m, respectively. The
software design and the performance are discussed. The correlation between the
GEM hole size distribution and the corresponding gain variation was studied by
comparing them against a detailed gain mapping of a foil and a set of six lower
precision control measurements. It can be seen that a qualitative estimation of
the behavior of the local variation in gain across the GEM foil can be made
based on the measured sizes of the outer and inner holes.
",Physics,Physics
"GP-SUM. Gaussian Processes Filtering of non-Gaussian Beliefs   This work studies the problem of stochastic dynamic filtering and state
propagation with complex beliefs. The main contribution is GP-SUM, a filtering
algorithm tailored to dynamic systems and observation models expressed as
Gaussian Processes (GP), and to states represented as a weighted sum of
Gaussians. The key attribute of GP-SUM is that it does not rely on
linearizations of the dynamic or observation models, or on unimodal Gaussian
approximations of the belief, hence enables tracking complex state
distributions. The algorithm can be seen as a combination of a sampling-based
filter with a probabilistic Bayes filter. On the one hand, GP-SUM operates by
sampling the state distribution and propagating each sample through the dynamic
system and observation models. On the other hand, it achieves effective
sampling and accurate probabilistic propagation by relying on the GP form of
the system, and the sum-of-Gaussian form of the belief. We show that GP-SUM
outperforms several GP-Bayes and Particle Filters on a standard benchmark. We
also demonstrate its use in a pushing task, predicting with experimental
accuracy the naturally occurring non-Gaussian distributions.
",Computer Science; Statistics,Computer Science; Statistics
"Reflexive Regular Equivalence for Bipartite Data   Bipartite data is common in data engineering and brings unique challenges,
particularly when it comes to clustering tasks that impose on strong structural
assumptions. This work presents an unsupervised method for assessing similarity
in bipartite data. Similar to some co-clustering methods, the method is based
on regular equivalence in graphs. The algorithm uses spectral properties of a
bipartite adjacency matrix to estimate similarity in both dimensions. The
method is reflexive in that similarity in one dimension is used to inform
similarity in the other. Reflexive regular equivalence can also use the
structure of transitivities -- in a network sense -- the contribution of which
is controlled by the algorithm's only free-parameter, $\alpha$. The method is
completely unsupervised and can be used to validate assumptions of
co-similarity, which are required but often untested, in co-clustering
analyses. Three variants of the method with different normalizations are tested
on synthetic data. The method is found to be robust to noise and well-suited to
asymmetric co-similar structure, making it particularly informative for cluster
analysis and recommendation in bipartite data of unknown structure. In
experiments, the convergence and speed of the algorithm are found to be stable
for different levels of noise. Real-world data from a network of malaria genes
are analyzed, where the similarity produced by the reflexive method is shown to
out-perform other measures' ability to correctly classify genes.
",Computer Science; Statistics,Statistics
"Learning Overcomplete HMMs   We study the problem of learning overcomplete HMMs---those that have many
hidden states but a small output alphabet. Despite having significant practical
importance, such HMMs are poorly understood with no known positive or negative
results for efficient learning. In this paper, we present several new
results---both positive and negative---which help define the boundaries between
the tractable and intractable settings. Specifically, we show positive results
for a large subclass of HMMs whose transition matrices are sparse,
well-conditioned, and have small probability mass on short cycles. On the other
hand, we show that learning is impossible given only a polynomial number of
samples for HMMs with a small output alphabet and whose transition matrices are
random regular graphs with large degree. We also discuss these results in the
context of learning HMMs which can capture long-term dependencies.
",Computer Science; Statistics,Computer Science; Statistics
"Bounds on the Approximation Power of Feedforward Neural Networks   The approximation power of general feedforward neural networks with piecewise
linear activation functions is investigated. First, lower bounds on the size of
a network are established in terms of the approximation error and network depth
and width. These bounds improve upon state-of-the-art bounds for certain
classes of functions, such as strongly convex functions. Second, an upper bound
is established on the difference of two neural networks with identical weights
but different activation functions.
",Statistics,Computer Science; Mathematics
"Incentivized Advertising: Treatment Effect and Adverse Selection   Incentivized advertising is a new ad format that is gaining popularity in
digital mobile advertising. In incentivized advertising, the publisher rewards
users for watching an ad. An endemic issue here is adverse selection, where
reward-seeking users select into incentivized ad placements to obtain rewards.
Adverse selection reduces the publisher's ad profit as well as poses a
difficulty to causal inference of the effectiveness of incentivized
advertising. To this end, we develop a treatment effect model that allows and
controls for unobserved adverse selection, and estimate the model using data
from a mobile gaming app that offers both incentivized and non-incentivized
ads. We find that rewarding users to watch an ad has an overall positive effect
on the ad conversion rate. A user is 27% more likely to convert when being
rewarded to watch an ad. However there is a negative offsetting effect that
reduces the effectiveness of incentivized ads. Some users are averse to delayed
rewards, they prefer to collect their rewards immediately after watching the
incentivized ads, instead of pursuing the content of the ads further. For the
subset of users who are averse to delayed rewards, the treatment effect is only
13%, while it can be as high as 47% for other users.
",Statistics,Computer Science
"On the complexity of generalized chromatic polynomials   J. Makowsky and B. Zilber (2004) showed that many variations of graph
colorings, called CP-colorings in the sequel, give rise to graph polynomials.
This is true in particular for harmonious colorings, convex colorings,
mcc_t-colorings, and rainbow colorings, and many more. N. Linial (1986) showed
that the chromatic polynomial $\chi(G;X)$ is #P-hard to evaluate for all but
three values X=0,1,2, where evaluation is in P. This dichotomy includes
evaluation at real or complex values, and has the further property that the set
of points for which evaluation is in P is finite. We investigate how the
complexity of evaluating univariate graph polynomials that arise from
CP-colorings varies for different evaluation points. We show that for some
CP-colorings (harmonious, convex) the complexity of evaluation follows a
similar pattern to the chromatic polynomial. However, in other cases (proper
edge colorings, mcc_t-colorings, H-free colorings) we could only obtain a
dichotomy for evaluations at non-negative integer points. We also discuss some
CP-colorings where we only have very partial results.
",Computer Science; Mathematics,Mathematics
"The Algorithmic Inflection of Russian and Generation of Grammatically Correct Text   We present a deterministic algorithm for Russian inflection. This algorithm
is implemented in a publicly available web-service www.passare.ru which
provides functions for inflection of single words, word matching and synthesis
of grammatically correct Russian text. The inflectional functions have been
tested against the annotated corpus of Russian language OpenCorpora.
",Computer Science,Computer Science
"Multidimensional upwind hydrodynamics on unstructured meshes using Graphics Processing Units I. Two-dimensional uniform meshes   We present a new method for numerical hydrodynamics which uses a
multidimensional generalisation of the Roe solver and operates on an
unstructured triangular mesh. The main advantage over traditional methods based
on Riemann solvers, which commonly use one-dimensional flux estimates as
building blocks for a multidimensional integration, is its inherently
multidimensional nature, and as a consequence its ability to recognise
multidimensional stationary states that are not hydrostatic. A second novelty
is the focus on Graphics Processing Units (GPUs). By tailoring the algorithms
specifically to GPUs we are able to get speedups of 100-250 compared to a
desktop machine. We compare the multidimensional upwind scheme to a
traditional, dimensionally split implementation of the Roe solver on several
test problems, and we find that the new method significantly outperforms the
Roe solver in almost all cases. This comes with increased computational costs
per time step, which makes the new method approximately a factor of 2 slower
than a dimensionally split scheme acting on a structured grid.
",Physics,Computer Science
"Universal abstract elementary classes and locally multipresentable categories   We exhibit an equivalence between the model-theoretic framework of universal
classes and the category-theoretic framework of locally multipresentable
categories. We similarly give an equivalence between abstract elementary
classes (AECs) admitting intersections and locally polypresentable categories.
We use these results to shed light on Shelah's presentation theorem for AECs.
",Mathematics,Mathematics
"Shannon's entropy and its Generalizations towards Statistics, Reliability and Information Science during 1948-2018   Starting from the pioneering works of Shannon and Weiner in 1948, a plethora
of works have been reported on entropy in different directions. Entropy-related
review work in the direction of statistics, reliability and information
science, to the best of our knowledge, has not been reported so far. Here we
have tried to collect all possible works in this direction during the period
1948-2018 so that people interested in entropy, specially the new researchers,
get benefited.
",Statistics,Statistics
"Simple labeled graph $C^*$-algebras are associated to disagreeable labeled spaces   By a labeled graph $C^*$-algebra we mean a $C^*$-algebra associated to a
labeled space $(E,\mathcal L,\mathcal E)$ consisting of a labeled graph
$(E,\mathcal L)$ and the smallest normal accommodating set $\mathcal E$ of
vertex subsets. Every graph $C^*$-algebra $C^*(E)$ is a labeled graph
$C^*$-algebra and it is well known that $C^*(E)$ is simple if and only if the
graph $E$ is cofinal and satisfies Condition (L). Bates and Pask extend these
conditions of graphs $E$ to labeled spaces, and show that if a set-finite and
receiver set-finite labeled space $(E,\mathcal L, \mathcal E)$ is cofinal and
disagreeable, then its $C^*$-algebra $C^*(E,\mathcal L, \mathcal E)$ is simple.
In this paper, we show that the converse is also true.
",Mathematics,Mathematics
"Quantification of the memory effect of steady-state currents from interaction-induced transport in quantum systems   Dynamics of a system in general depends on its initial state and how the
system is driven, but in many-body systems the memory is usually averaged out
during evolution. Here, interacting quantum systems without external
relaxations are shown to retain long-time memory effects in steady states. To
identify memory effects, we first show quasi-steady state currents form in
finite, isolated Bose and Fermi Hubbard models driven by interaction imbalance
and they become steady-state currents in the thermodynamic limit. By comparing
the steady state currents from different initial states or ramping rates of the
imbalance, long-time memory effects can be quantified. While the memory effects
of initial states are more ubiquitous, the memory effects of switching
protocols are mostly visible in interaction-induced transport in lattices. Our
simulations suggest the systems enter a regime governed by a generalized Fick's
law and memory effects lead to initial-state dependent diffusion coefficients.
We also identify conditions for enhancing memory effects and discuss possible
experimental implications.
",Physics,Physics
"SETI in vivo: testing the we-are-them hypothesis   After it was proposed that life on Earth might descend from seeding by an
earlier civilization, some authors noted that this alternative offers a
testable aspect: the seeds could be supplied with a signature that might be
found in extant organisms. In particular, it was suggested that the optimal
location for such an artifact is the genetic code, as the least evolving part
of cells. However, as the mainstream view goes, this scenario is too
speculative and cannot be meaningfully tested because encoding/decoding a
signature within the genetic code is ill-defined, so any retrieval attempt is
doomed to guesswork. Here we refresh the seeded-Earth hypothesis and discuss
the motivation for inserting a signature. We then show that ""biological SETI""
involves even weaker assumptions than traditional SETI and admits a
well-defined methodological framework. After assessing the possibility in terms
of molecular and evolutionary biology, we formalize the approach and, adopting
the guideline of SETI that encoding/decoding should follow from first
principles and be convention-free, develop a retrieval strategy. Applied to the
canonical code, it reveals a nontrivial precision structure of interlocked
systematic attributes. To assess this result in view of the initial assumption,
we perform statistical, comparison, interdependence, and semiotic analyses.
Statistical analysis reveals no causal connection to evolutionary models of the
code, interdependence analysis precludes overinterpretation, and comparison
analysis shows that known code variations lack any precision-logic structures,
in agreement with these variations being post-seeding deviations from the
canonical code. Finally, semiotic analysis shows that not only the found
attributes are consistent with the initial assumption, but that they make
perfect sense from SETI perspective, as they maintain some of the most
universal codes of culture.
",Physics,Quantitative Biology
"Transaction Support over Redis: An Overview   This document outlines the approach to supporting cross-node transactions
over a Redis cluster.
",Computer Science,Computer Science
"EV-FlowNet: Self-Supervised Optical Flow Estimation for Event-based Cameras   Event-based cameras have shown great promise in a variety of situations where
frame based cameras suffer, such as high speed motions and high dynamic range
scenes. However, developing algorithms for event measurements requires a new
class of hand crafted algorithms. Deep learning has shown great success in
providing model free solutions to many problems in the vision community, but
existing networks have been developed with frame based images in mind, and
there does not exist the wealth of labeled data for events as there does for
images for supervised training. To these points, we present EV-FlowNet, a novel
self-supervised deep learning pipeline for optical flow estimation for event
based cameras. In particular, we introduce an image based representation of a
given event stream, which is fed into a self-supervised neural network as the
sole input. The corresponding grayscale images captured from the same camera at
the same time as the events are then used as a supervisory signal to provide a
loss function at training time, given the estimated flow from the network. We
show that the resulting network is able to accurately predict optical flow from
events only in a variety of different scenes, with performance competitive to
image based networks. This method not only allows for accurate estimation of
dense optical flow, but also provides a framework for the transfer of other
self-supervised methods to the event-based domain.
",Computer Science,Computer Science
"Fermion condensation and super pivotal categories   We study fermionic topological phases using the technique of fermion
condensation. We give a prescription for performing fermion condensation in
bosonic topological phases which contain a fermion. Our approach to fermion
condensation can roughly be understood as coupling the parent bosonic
topological phase to a phase of physical fermions, and condensing pairs of
physical and emergent fermions. There are two distinct types of objects in
fermionic theories, which we call ""m-type"" and ""q-type"" particles. The
endomorphism algebras of q-type particles are complex Clifford algebras, and
they have no analogues in bosonic theories. We construct a fermionic
generalization of the tube category, which allows us to compute the
quasiparticle excitations in fermionic topological phases. We then prove a
series of results relating data in condensed theories to data in their parent
theories; for example, if $\mathcal{C}$ is a modular tensor category containing
a fermion, then the tube category of the condensed theory satisfies
$\textbf{Tube}(\mathcal{C}/\psi) \cong \mathcal{C} \times (\mathcal{C}/\psi)$.
We also study how modular transformations, fusion rules, and coherence
relations are modified in the fermionic setting, prove a fermionic version of
the Verlinde dimension formula, construct a commuting projector lattice
Hamiltonian for fermionic theories, and write down a fermionic version of the
Turaev-Viro-Barrett-Westbury state sum. A large portion of this work is devoted
to three detailed examples of performing fermion condensation to produce
fermionic topological phases: we condense fermions in the Ising theory, the
$SO(3)_6$ theory, and the $\frac{1}{2}\text{E}_6$ theory, and compute the
quasiparticle excitation spectrum in each of these examples.
",Physics; Mathematics,Physics
"Computer Self-efficacy and Its Relationship with Web Portal Usage: Evidence from the University of the East   The University of the East Web Portal is an academic, web based system that
provides educational electronic materials and e-learning services. To fully
optimize its usage, it is imperative to determine the factors that relate to
its usage. Thus, this study, to determine the computer self-efficacy of the
faculty members of the University of the East and its relationship with their
web portal usage, was conceived. Using a validated questionnaire, the profile
of the respondents, their computer self-efficacy, and web portal usage were
gathered. Data showed that the respondents were relatively young (M = 40 years
old), majority had masters degree (f = 85, 72%), most had been using the web
portal for four semesters (f = 60, 51%), and the large part were intermediate
web portal users (f = 69, 59%). They were highly skilled in using the computer
(M = 4.29) and skilled in using the Internet (M = 4.28). E-learning services (M
= 3.29) and online library resources (M = 3.12) were only used occasionally.
Pearson correlation revealed that age was positively correlated with online
library resources (r = 0.267, p < 0.05) and a negative relationship existed
between perceived skill level in using the portal and online library resources
usage (r = -0.206, p < 0.05). A 2x2 chi square revealed that the highest
educational attainment had a significant relationship with online library
resources (chi square = 5.489, df = 1, p < 0.05). Basic computer (r = 0.196, p
< 0.05) and Internet skills (r = 0.303, p < 0.05) were significantly and
positively related with e-learning services usage but not with online library
resources usage. Other individual factors such as attitudes towards the web
portal and anxiety towards using the web portal can be investigated.
",Computer Science,Computer Science
"Itineraries for Inverse Limits of Tent Maps: a Backward View   Previously published admissibility conditions for an element of
$\{0,1\}^{\mathbb{Z}}$ to be the itinerary of a point of the inverse limit of a
tent map are expressed in terms of forward orbits. We give necessary and
sufficient conditions in terms of backward orbits, which is more natural for
inverse limits. These backward admissibility conditions are not symmetric
versions of the forward ones: in particular, the maximum backward itinerary
which can be realised by a tent map mode locks on intervals of kneading
sequences.
",Mathematics,Mathematics
"Charge compensation at the interface between the polar NaCl(111) surface and a NaCl aqueous solution   Periodic supercell models of electric double layers formed at the interface
between a charged surface and an electrolyte are subject to serious finite size
errors and require certain adjustments in the treatment of the long-range
electrostatic interactions. In a previous publication (C. Zhang, M. Sprik,
Phys. Rev. B 94, 245309 (2016)) we have shown how this can be achieved using
finite field methods. The test system was the familiar simple point charge
model of a NaCl aqueous solution confined between two oppositely charged walls.
Here this method is extended to the interface between the (111) polar surface
of a NaCl crystal and a high concentration NaCl aqueous solution. The crystal
is kept completely rigid and the compensating charge screening the polarization
can only be provided by the electrolyte. We verify that the excess electrolyte
ionic charge at the interface conforms to the Tasker 1/2 rule for compensating
charge in the theory of polar rocksalt (111) surfaces. The interface can be
viewed as an electric double layer with a net charge. We define a generalized
Helmholtz capacitance $C_\text{H}$ which can be computed by varying the applied
electric field. We find $C_\text{H} = 8.23 \, \mu \mathrm{Fcm}^{-2}$, which
should be compared to the $4.23 \, \mu \mathrm{Fcm}^{-2}$ for the (100)
non-polar surface of the same NaCl crystal. This is rationalized by the
observation that compensating ions shed their first solvation shell adsorbing
as contact ions pairs on the polar surface.
",Physics,Physics
"Geometric Surface-Based Tracking Control of a Quadrotor UAV under Actuator Constraints   This paper presents contributions on nonlinear tracking control systems for a
quadrotor unmanned micro aerial vehicle. New controllers are proposed based on
nonlinear surfaces composed by tracking errors that evolve directly on the
nonlinear configuration manifold thus inherently including in the control
design the nonlinear characteristics of the SE(3) configuration space. In
particular geometric surface-based controllers are developed, and through
rigorous stability proofs they are shown to have desirable closed loop
properties that are almost global. A region of attraction, independent of the
position error, is produced and its effects are analyzed. A strategy allowing
the quadrotor to achieve precise attitude tracking while simultaneously
following a desired position command and complying to actuator constraints in a
computationally inexpensive manner is derived. This important contribution
differentiates this work from existing Geometric Nonlinear Control System
solutions (GNCSs) since the commanded thrusts can be realized by the majority
of quadrotors produced by the industry. The new features of the proposed GNCSs
are illustrated by numerical simulations of aggressive maneuvers and a
comparison with a GNCSs from the bibliography.
",Computer Science,Computer Science
"On the computability of graph Turing machines   We consider graph Turing machines, a model of parallel computation on a
graph, in which each vertex is only capable of performing one of a finite
number of operations. This model of computation is a natural generalization of
several well-studied notions of computation, including ordinary Turing
machines, cellular automata, and parallel graph dynamical systems. We analyze
the power of computations that can take place in this model, both in terms of
the degrees of computability of the functions that can be computed, and the
time and space resources needed to carry out these computations. We further
show that properties of the underlying graph have significant consequences for
the power of computation thereby obtained. In particular, we show that every
arithmetically definable set can be computed by a graph Turing machine in
constant time, and that every computably enumerable Turing degree can be
computed in constant time and linear space by a graph Turing machine whose
underlying graph has finite degree.
",Computer Science; Mathematics,Computer Science
"Taggle: Scalable Visualization of Tabular Data through Aggregation   Visualization of tabular data---for both presentation and exploration
purposes---is a well-researched area. Although effective visual presentations
of complex tables are supported by various plotting libraries, creating such
tables is a tedious process and requires scripting skills. In contrast,
interactive table visualizations that are designed for exploration purposes
either operate at the level of individual rows, where large parts of the table
are accessible only via scrolling, or provide a high-level overview that often
lacks context-preserving drill-down capabilities. In this work we present
Taggle, a novel visualization technique for exploring and presenting large and
complex tables that are composed of individual columns of categorical or
numerical data and homogeneous matrices. The key contribution of Taggle is the
hierarchical aggregation of data subsets, for which the user can also choose
suitable visual representations.The aggregation strategy is complemented by the
ability to sort hierarchically such that groups of items can be flexibly
defined by combining categorical stratifications and by rich data selection and
filtering capabilities. We demonstrate the usefulness of Taggle for interactive
analysis and presentation of complex genomics data for the purpose of drug
discovery.
",Computer Science,Computer Science; Statistics
"Spatial interactions and oscillatory tragedies of the commons   A tragedy of the commons (TOC) occurs when individuals acting in their own
self-interest deplete commonly-held resources, leading to a worse outcome than
had they cooperated. Over time, the depletion of resources can change
incentives for subsequent actions. Here, we investigate long-term feedback
between game and environment across a continuum of incentives in an
individual-based framework. We identify payoff-dependent transition rules that
lead to oscillatory TOC-s in stochastic simulations and the mean field limit.
Further extending the stochastic model, we find that spatially explicit
interactions can lead to emergent, localized dynamics, including the
propagation of cooperative wave fronts and cluster formation of both social
context and resources. These dynamics suggest new mechanisms underlying how
TOCs arise and how they might be averted.
",Quantitative Biology,Physics
"Do Developers Update Their Library Dependencies? An Empirical Study on the Impact of Security Advisories on Library Migration   Third-party library reuse has become common practice in contemporary software
development, as it includes several benefits for developers. Library
dependencies are constantly evolving, with newly added features and patches
that fix bugs in older versions. To take full advantage of third-party reuse,
developers should always keep up to date with the latest versions of their
library dependencies. In this paper, we investigate the extent of which
developers update their library dependencies. Specifically, we conducted an
empirical study on library migration that covers over 4,600 GitHub software
projects and 2,700 library dependencies. Results show that although many of
these systems rely heavily on dependencies, 81.5% of the studied systems still
keep their outdated dependencies. In the case of updating a vulnerable
dependency, the study reveals that affected developers are not likely to
respond to a security advisory. Surveying these developers, we find that 69% of
the interviewees claim that they were unaware of their vulnerable dependencies.
Furthermore, developers are not likely to prioritize library updates, citing it
as extra effort and added responsibility. This study concludes that even though
third-party reuse is commonplace, the practice of updating a dependency is not
as common for many developers.
",Computer Science,Computer Science
"Fermionic Matrix Product States and One-Dimensional Short-Range Entangled Phases with Anti-Unitary Symmetries   We extend the formalism of Matrix Product States (MPS) to describe
one-dimensional gapped systems of fermions with both unitary and anti-unitary
symmetries. Additionally, systems with orientation-reversing spatial symmetries
are considered. The short-ranged entangled phases of such systems are
classified by three invariants, which characterize the projective action of the
symmetry on edge states. We give interpretations of these invariants as
properties of states on the closed chain. The relationship between fermionic
MPS systems at an RG fixed point and equivariant algebras is exploited to
derive a group law for the stacking of fermionic phases. The result generalizes
known classifications to symmetry groups that are non-trivial extensions of
fermion parity and time-reversal.
",Physics,Physics
"Analysing Temporal Evolution of Interlingual Wikipedia Article Pairs   Wikipedia articles representing an entity or a topic in different language
editions evolve independently within the scope of the language-specific user
communities. This can lead to different points of views reflected in the
articles, as well as complementary and inconsistent information. An analysis of
how the information is propagated across the Wikipedia language editions can
provide important insights in the article evolution along the temporal and
cultural dimensions and support quality control. To facilitate such analysis,
we present MultiWiki - a novel web-based user interface that provides an
overview of the similarities and differences across the article pairs
originating from different language editions on a timeline. MultiWiki enables
users to observe the changes in the interlingual article similarity over time
and to perform a detailed visual comparison of the article snapshots at a
particular time point.
",Computer Science,Computer Science
"Deep Generative Models with Learnable Knowledge Constraints   The broad set of deep generative models (DGMs) has achieved remarkable
advances. However, it is often difficult to incorporate rich structured domain
knowledge with the end-to-end DGMs. Posterior regularization (PR) offers a
principled framework to impose structured constraints on probabilistic models,
but has limited applicability to the diverse DGMs that can lack a Bayesian
formulation or even explicit density evaluation. PR also requires constraints
to be fully specified a priori, which is impractical or suboptimal for complex
knowledge with learnable uncertain parts. In this paper, we establish
mathematical correspondence between PR and reinforcement learning (RL), and,
based on the connection, expand PR to learn constraints as the extrinsic reward
in RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is
flexible to adapt arbitrary constraints with the model jointly. Experiments on
human image generation and templated sentence generation show models with
learned knowledge constraints by our algorithm greatly improve over base
generative models.
",Statistics,Computer Science; Statistics
"On the complexity of the projective splitting and Spingarn's methods for the sum of two maximal monotone operators   In this work we study the pointwise and ergodic iteration-complexity of a
family of projective splitting methods proposed by Eckstein and Svaiter, for
finding a zero of the sum of two maximal monotone operators. As a consequence
of the complexity analysis of the projective splitting methods, we obtain
complexity bounds for the two-operator case of Spingarn's partial inverse
method. We also present inexact variants of two specific instances of this
family of algorithms, and derive corresponding convergence rate results.
",Mathematics,Mathematics
"Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks   Efforts to reduce the numerical precision of computations in deep learning
training have yielded systems that aggressively quantize weights and
activations, yet employ wide high-precision accumulators for partial sums in
inner-product operations to preserve the quality of convergence. The absence of
any framework to analyze the precision requirements of partial sum
accumulations results in conservative design choices. This imposes an
upper-bound on the reduction of complexity of multiply-accumulate units. We
present a statistical approach to analyze the impact of reduced accumulation
precision on deep learning training. Observing that a bad choice for
accumulation precision results in loss of information that manifests itself as
a reduction in variance in an ensemble of partial sums, we derive a set of
equations that relate this variance to the length of accumulation and the
minimum number of bits needed for accumulation. We apply our analysis to three
benchmark networks: CIFAR-10 ResNet 32, ImageNet ResNet 18 and ImageNet
AlexNet. In each case, with accumulation precision set in accordance with our
proposed equations, the networks successfully converge to the single precision
floating-point baseline. We also show that reducing accumulation precision
further degrades the quality of the trained network, proving that our equations
produce tight bounds. Overall this analysis enables precise tailoring of
computation hardware to the application, yielding area- and power-optimal
systems.
",Computer Science; Statistics,Computer Science; Statistics
"Estimation of the asymptotic variance of univariate and multivariate random fields and statistical inference   Correlated random fields are a common way to model dependence struc- tures in
high-dimensional data, especially for data collected in imaging. One important
parameter characterizing the degree of dependence is the asymp- totic variance
which adds up all autocovariances in the temporal and spatial domain.
Especially, it arises in the standardization of test statistics based on
partial sums of random fields and thus the construction of tests requires its
estimation. In this paper we propose consistent estimators for this parameter
for strictly stationary {\phi}-mixing random fields with arbitrary dimension of
the domain and taking values in a Euclidean space of arbitrary dimension, thus
allowing for multivariate random fields. We establish consistency, provide cen-
tral limit theorems and show that distributional approximations of related test
statistics based on sample autocovariances of random fields can be obtained by
the subsampling approach. As in applications the spatial-temporal correlations
are often quite local, such that a large number of autocovariances vanish or
are negligible, we also investigate a thresholding approach where sample
autocovariances of small magnitude are omitted. Extensive simulation studies
show that the proposed estimators work well in practice and, when used to
standardize image test statistics, can provide highly accurate image testing
procedures.
",Mathematics; Statistics,Mathematics; Statistics
"Family-specific scaling laws in bacterial genomes   Among several quantitative invariants found in evolutionary genomics, one of
the most striking is the scaling of the overall abundance of proteins, or
protein domains, sharing a specific functional annotation across genomes of
given size. The size of these functional categories change, on average, as
power-laws in the total number of protein-coding genes. Here, we show that such
regularities are not restricted to the overall behavior of high-level
functional categories, but also exist systematically at the level of single
evolutionary families of protein domains. Specifically, the number of proteins
within each family follows family-specific scaling laws with genome size.
Functionally similar sets of families tend to follow similar scaling laws, but
this is not always the case. To understand this systematically, we provide a
comprehensive classification of families based on their scaling properties.
Additionally, we develop a quantitative score for the heterogeneity of the
scaling of families belonging to a given category or predefined group. Under
the common reasonable assumption that selection is driven solely or mainly by
biological function, these findings point to fine-tuned and interdependent
functional roles of specific protein domains, beyond our current functional
annotations. This analysis provides a deeper view on the links between
evolutionary expansion of protein families and the functional constraints
shaping the gene repertoire of bacterial genomes.
",Physics,Quantitative Biology
"Learning Navigation Behaviors End to End   A longstanding goal of behavior-based robotics is to solve high-level
navigation tasks using end to end navigation behaviors that directly map
sensors to actions. Navigation behaviors, such as reaching a goal or following
a path without collisions, can be learned from exploration and interaction with
the environment, but are constrained by the type and quality of a robot's
sensors, dynamics, and actuators. Traditional motion planning handles varied
robot geometry and dynamics, but typically assumes high-quality observations.
Modern vision-based navigation typically considers imperfect or partial
observations, but simplifies the robot action space. With both approaches, the
transition from simulation to reality can be difficult. Here, we learn two end
to end navigation behaviors that avoid moving obstacles: point to point and
path following. These policies receive noisy lidar observations and output
robot linear and angular velocities. We train these policies in small, static
environments with Shaped-DDPG, an adaptation of the Deep Deterministic Policy
Gradient (DDPG) reinforcement learning method which optimizes reward and
network architecture. Over 500 meters of on-robot experiments show , these
policies generalize to new environments and moving obstacles, are robust to
sensor, actuator, and localization noise, and can serve as robust building
blocks for larger navigation tasks. The path following and point and point
policies are 83% and 56% more successful than the baseline, respectively.
",Computer Science,Computer Science
"Classification of Casimirs in 2D hydrodynamics   We describe a complete list of Casimirs for 2D Euler hydrodynamics on a
surface without boundary: we define generalized enstrophies which, along with
circulations, form a complete set of invariants for coadjoint orbits of
area-preserving diffeomorphisms on a surface. We also outline a possible
extension of main notions to the boundary case and formulate several open
questions in that setting.
",Physics; Mathematics,Mathematics
"On a backward problem for multidimensional Ginzburg-Landau equation with random data   In this paper, we consider a backward in time problem for Ginzburg-Landau
equation in multidimensional domain associated with some random data. The
problem is ill-posed in the sense of Hadamard. To regularize the instable
solution, we develop a new regularized method combined with statistical
approach to solve this problem. We prove a upper bound, on the rate of
convergence of the mean integrated squared error in $L^2 $ norm and $H^1$ norm.
",Mathematics,Mathematics
"II-FCN for skin lesion analysis towards melanoma detection   Dermoscopy image detection stays a tough task due to the weak distinguishable
property of the object.Although the deep convolution neural network
signifigantly boosted the performance on prevelance computer vision tasks in
recent years,there remains a room to explore more robust and precise models to
the problem of low contrast image segmentation.Towards the challenge of Lesion
Segmentation in ISBI 2017,we built a symmetrical identity inception fully
convolution network which is based on only 10 reversible inception blocks,every
block composed of four convolution branches with combination of different layer
depth and kernel size to extract sundry semantic features.Then we proposed an
approximate loss function for jaccard index metrics to train our model.To
overcome the drawbacks of traditional convolution,we adopted the dilation
convolution and conditional random field method to rectify our segmentation.We
also introduced multiple ways to prevent the problem of overfitting.The
experimental results shows that our model achived jaccard index of 0.82 and
kept learning from epoch to epoch.
",Computer Science,Computer Science
"SCRank: Spammer and Celebrity Ranking in Directed Social Networks   Many online social networks allow directed edges: Alice can unilaterally add
an ""edge"" to Bob, typically indicating interest in Bob or Bob's content,
without Bob's permission or reciprocation. In directed social networks we
observe the rise of two distinctive classes of users: celebrities who accrue
unreciprocated incoming links, and follow spammers, who generate unreciprocated
outgoing links. Identifying users in these two classes is important for abuse
detection, user and content ranking, privacy choices, and other social network
features.
In this paper we develop SCRank, an iterative algorithm to identify such
users. We analyze SCRank both theoretically and experimentally. The
spammer-celebrity definition is not amenable to analysis using standard power
iteration, so we develop a novel potential function argument to show
convergence to an approximate equilibrium point for a class of algorithms
including SCRank. We then use experimental evaluation on a real global-scale
social network and on synthetically generated graphs to observe that the
algorithm converges quickly and consistently. Using synthetic data with
built-in ground truth, we also experimentally show that the algorithm provides
a good approximation to planted celebrities and spammers.
",Computer Science,Computer Science
"On topological obstructions to global stabilization of an inverted pendulum   We consider a classical problem of control of an inverted pendulum by means
of a horizontal motion of its pivot point. We suppose that the control law can
be non-autonomous and non-periodic w.r.t. the position of the pendulum. It is
shown that global stabilization of the vertical upward position of the pendulum
cannot be obtained for any Lipschitz control law, provided some natural
assumptions. Moreover, we show that there always exists a solution separated
from the vertical position and along which the pendulum never becomes
horizontal. Hence, we also prove that global stabilization cannot be obtained
in the system where the pendulum can impact the horizontal plane (for any
mechanical model of impact). Similar results are presented for several
analogous systems: a pendulum on a cart, a spherical pendulum, and a pendulum
with an additional torque control.
",Physics; Mathematics,Mathematics
"Regulating Highly Automated Robot Ecologies: Insights from Three User Studies   Highly automated robot ecologies (HARE), or societies of independent
autonomous robots or agents, are rapidly becoming an important part of much of
the world's critical infrastructure. As with human societies, regulation,
wherein a governing body designs rules and processes for the society, plays an
important role in ensuring that HARE meet societal objectives. However, to
date, a careful study of interactions between a regulator and HARE is lacking.
In this paper, we report on three user studies which give insights into how to
design systems that allow people, acting as the regulatory authority, to
effectively interact with HARE. As in the study of political systems in which
governments regulate human societies, our studies analyze how interactions
between HARE and regulators are impacted by regulatory power and individual
(robot or agent) autonomy. Our results show that regulator power, decision
support, and adaptive autonomy can each diminish the social welfare of HARE,
and hint at how these seemingly desirable mechanisms can be designed so that
they become part of successful HARE.
",Computer Science,Computer Science
"State-of-the-art Speech Recognition With Sequence-to-Sequence Models   Attention-based encoder-decoder architectures such as Listen, Attend, and
Spell (LAS), subsume the acoustic, pronunciation and language model components
of a traditional automatic speech recognition (ASR) system into a single neural
network. In previous work, we have shown that such architectures are comparable
to state-of-theart ASR systems on dictation tasks, but it was not clear if such
architectures would be practical for more challenging tasks such as voice
search. In this work, we explore a variety of structural and optimization
improvements to our LAS model which significantly improve performance. On the
structural side, we show that word piece models can be used instead of
graphemes. We also introduce a multi-head attention architecture, which offers
improvements over the commonly-used single-head attention. On the optimization
side, we explore synchronous training, scheduled sampling, label smoothing, and
minimum word error rate optimization, which are all shown to improve accuracy.
We present results with a unidirectional LSTM encoder for streaming
recognition. On a 12, 500 hour voice search task, we find that the proposed
changes improve the WER from 9.2% to 5.6%, while the best conventional system
achieves 6.7%; on a dictation task our model achieves a WER of 4.1% compared to
5% for the conventional system.
",Computer Science; Statistics,Computer Science
"Microscopic Conductivity of Lattice Fermions at Equilibrium - Part II: Interacting Particles   We apply Lieb-Robinson bounds for multi-commutators we recently derived to
study the (possibly non-linear) response of interacting fermions at thermal
equilibrium to perturbations of the external electromagnetic field. This
analysis leads to an extension of the results for quasi-free fermions of
\cite{OhmI,OhmII} to fermion systems on the lattice with short-range
interactions. More precisely, we investigate entropy production and charge
transport properties of non-autonomous $C^{\ast }$-dynamical systems associated
with interacting lattice fermions within bounded static potentials and in
presence of an electric field that is time- and space-dependent. We verify the
1st law of thermodynamics for the heat production of the system under
consideration. In linear response theory, the latter is related with Ohm and
Joule's laws. These laws are proven here to hold at the microscopic scale,
uniformly with respect to the size of the (microscopic) region where the
electric field is applied. An important outcome is the extension of the notion
of conductivity measures to interacting fermions.
",Mathematics,Physics
"Many cubic surfaces contain rational points   Building on recent work of Bhargava--Elkies--Schnidman and Kriz--Li, we
produce infinitely many smooth cubic surfaces defined over the field of
rational numbers that contain rational points.
",Mathematics,Mathematics
"Network of sensitive magnetometers for urban studies   The magnetic signature of an urban environment is investigated using a
geographically distributed network of fluxgate magnetometers deployed in and
around Berkeley, California. The system hardware and software are described and
results from initial operation of the network are reported. The sensors sample
the vector magnetic field with a 4 kHz resolution and are sensitive to
fluctuations below 0.1 $\textrm{nT}/\sqrt{\textrm{Hz}}$. Data from separate
stations are synchronized to around $\pm100$ $\mu{s}$ using GPS and computer
system clocks. Data from all sensors are automatically uploaded to a central
server. Anomalous events, such as lightning strikes, have been observed. A
wavelet analysis is used to study observations over a wide range of temporal
scales up to daily variations that show strong differences between weekend and
weekdays. The Bay Area Rapid Transit (BART) is identified as the most dominant
signal from these observations and a superposed epoch analysis is used to study
and extract the BART signal. Initial results of the correlation between sensors
are also presented.
",Physics,Physics
"Cross-label Suppression: A Discriminative and Fast Dictionary Learning with Group Regularization   This paper addresses image classification through learning a compact and
discriminative dictionary efficiently. Given a structured dictionary with each
atom (columns in the dictionary matrix) related to some label, we propose
cross-label suppression constraint to enlarge the difference among
representations for different classes. Meanwhile, we introduce group
regularization to enforce representations to preserve label properties of
original samples, meaning the representations for the same class are encouraged
to be similar. Upon the cross-label suppression, we don't resort to
frequently-used $\ell_0$-norm or $\ell_1$-norm for coding, and obtain
computational efficiency without losing the discriminative power for
categorization. Moreover, two simple classification schemes are also developed
to take full advantage of the learnt dictionary. Extensive experiments on six
data sets including face recognition, object categorization, scene
classification, texture recognition and sport action categorization are
conducted, and the results show that the proposed approach can outperform lots
of recently presented dictionary algorithms on both recognition accuracy and
computational efficiency.
",Computer Science; Statistics,Computer Science
"Radially distributed values and normal families   Let $L_0$ and $L_1$ be two distinct rays emanating from the origin and let
${\mathcal F}$ be the family of all functions holomorphic in the unit disk
${\mathbb D}$ for which all zeros lie on $L_0$ while all $1$-points lie on
$L_1$. It is shown that ${\mathcal F}$ is normal in ${\mathbb
D}\backslash\{0\}$. The case where $L_0$ is the positive real axis and $L_1$ is
the negative real axis is studied in more detail.
",Mathematics,Mathematics
"Universal Constraints on the Location of Extrema of Eigenfunctions of Non-Local Schrödinger Operators   We derive a lower bound on the location of global extrema of eigenfunctions
for a large class of non-local Schrödinger operators in convex domains under
Dirichlet exterior conditions, featuring the symbol of the kinetic term, the
strength of the potential, and the corresponding eigenvalue, and involving a
new universal constant. We show a number of probabilistic and spectral
geometric implications, and derive a Faber-Krahn type inequality for non-local
operators. Our study also extends to potentials with compact support, and we
establish bounds on the location of extrema relative to the boundary edge of
the support or level sets around minima of the potential.
",Mathematics,Mathematics
"A Continuous Beam Steering Slotted Waveguide Antenna Using Rotating Dielectric Slabs   The design, simulation and measurement of a beam steerable slotted waveguide
antenna operating in X band are presented. The proposed beam steerable antenna
consists of a standard rectangular waveguide (RWG) section with longitudinal
slots in the broad wall. The beam steering in this configuration is achieved by
rotating two dielectric slabs inside the waveguide and consequently changing
the phase of the slots excitations. In order to confirm the usefulness of this
concept, a non-resonant 20-slot waveguide array antenna with an element spacing
of d = 0.58{\lambda}0 has been designed, built and measured. A 14 deg beam
scanning from near broadside ({\theta} = 4 deg) toward end-fire ({\theta} = 18
deg) direction is observed. The gain varies from 18.33 dB to 19.11 dB which
corresponds to the radiation efficiencies between 95% and 79%. The side-lobe
level is -14 dB at the design frequency of 9.35 GHz. The simulated co-polarized
realized gain closely matches the fabricated prototype patterns.
",Physics,Physics
"Learning Wasserstein Embeddings   The Wasserstein distance received a lot of attention recently in the
community of machine learning, especially for its principled way of comparing
distributions. It has found numerous applications in several hard problems,
such as domain adaptation, dimensionality reduction or generative models.
However, its use is still limited by a heavy computational cost. Our goal is to
alleviate this problem by providing an approximation mechanism that allows to
break its inherent complexity. It relies on the search of an embedding where
the Euclidean distance mimics the Wasserstein distance. We show that such an
embedding can be found with a siamese architecture associated with a decoder
network that allows to move from the embedding space back to the original input
space. Once this embedding has been found, computing optimization problems in
the Wasserstein space (e.g. barycenters, principal directions or even
archetypes) can be conducted extremely fast. Numerical experiments supporting
this idea are conducted on image datasets, and show the wide potential benefits
of our method.
",Computer Science; Statistics,Computer Science; Statistics
"Contraction and uniform convergence of isotonic regression   We consider the problem of isotonic regression, where the underlying signal
$x$ is assumed to satisfy a monotonicity constraint, that is, $x$ lies in the
cone $\{ x\in\mathbb{R}^n : x_1 \leq \dots \leq x_n\}$. We study the isotonic
projection operator (projection to this cone), and find a necessary and
sufficient condition characterizing all norms with respect to which this
projection is contractive. This enables a simple and non-asymptotic analysis of
the convergence properties of isotonic regression, yielding uniform confidence
bands that adapt to the local Lipschitz properties of the signal.
",Mathematics; Statistics,Mathematics; Statistics
"Moment conditions in strong laws of large numbers for multiple sums and random measures   The validity of the strong law of large numbers for multiple sums $S_n$ of
independent identically distributed random variables $Z_k$, $k\leq n$, with
$r$-dimensional indices is equivalent to the integrability of
$|Z|(\log^+|Z|)^{r-1}$, where $Z$ is the typical summand. We consider the
strong law of large numbers for more general normalisations, without assuming
that the summands $Z_k$ are identically distributed, and prove a multiple sum
generalisation of the Brunk--Prohorov strong law of large numbers. In the case
of identical finite moments of irder $2q$ with integer $q\geq1$, we show that
the strong law of large numbers holds with the normalisation $\|n_1\cdots
n_r\|^{1/2}(\log n_1\cdots\log n_r)^{1/(2q)+\varepsilon}$ for any
$\varepsilon>0$. The obtained results are also formulated in the setting of
ergodic theorems for random measures, in particular those generated by marked
point processes.
",Mathematics,Mathematics
"Practical Bayesian Optimization for Transportation Simulators   We provide a method to solve optimization problem when objective function is
a complex stochastic simulator of an urban transportation system. To reach this
goal, a Bayesian optimization framework is introduced. We show how the choice
of prior and inference algorithm effect the outcome of our optimization
procedure. We develop dimensionality reduction techniques that allow for our
optimization techniques to be applicable for real-life problems. We develop a
distributed, Gaussian Process Bayesian regression and active learning models
that allow parallel execution of our algorithms and enable usage of high
performance computing. We present a fully Bayesian approach that is more sample
efficient and reduces computational budget. Our framework is supported by
theoretical analysis and an empirical study. We demonstrate our framework on
the problem of calibrating a multi-modal transportation network of city of
Bloomington, Illinois. Finally, we discuss directions for further research.
",Statistics,Computer Science; Statistics
"From acquaintance to best friend forever: robust and fine-grained inference of social tie strengths   Social networks often provide only a binary perspective on social ties: two
individuals are either connected or not. While sometimes external information
can be used to infer the strength of social ties, access to such information
may be restricted or impractical. Sintos and Tsaparas (KDD 2014) first
suggested to infer the strength of social ties from the topology of the network
alone, by leveraging the Strong Triadic Closure (STC) property. The STC
property states that if person A has strong social ties with persons B and C, B
and C must be connected to each other as well (whether with a weak or strong
tie). Sintos and Tsaparas exploited this to formulate the inference of the
strength of social ties as NP-hard optimization problem, and proposed two
approximation algorithms. We refine and improve upon this landmark paper, by
developing a sequence of linear relaxations of this problem that can be solved
exactly in polynomial time. Usefully, these relaxations infer more fine-grained
levels of tie strength (beyond strong and weak), which also allows to avoid
making arbitrary strong/weak strength assignments when the network topology
provides inconclusive evidence. One of the relaxations simultaneously infers
the presence of a limited number of STC violations. An extensive theoretical
analysis leads to two efficient algorithmic approaches. Finally, our
experimental results elucidate the strengths of the proposed approach, and
sheds new light on the validity of the STC property in practice.
",Computer Science,Computer Science; Statistics
"Learning the Kernel for Classification and Regression   We investigate a series of learning kernel problems with polynomial
combinations of base kernels, which will help us solve regression and
classification problems. We also perform some numerical experiments of
polynomial kernels with regression and classification tasks on different
datasets.
",Computer Science,Computer Science; Statistics
"Optimal boundary gradient estimates for Lamé systems with partially infinite coefficients   In this paper, we derive the pointwise upper bounds and lower bounds on the
gradients of solutions to the Lamé systems with partially infinite
coefficients as the surface of discontinuity of the coefficients of the system
is located very close to the boundary. When the distance tends to zero, the
optimal blow-up rates of the gradients are established for inclusions with
arbitrary shapes and in all dimensions.
",Mathematics,Mathematics
"A new, large-scale map of interstellar reddening derived from HI emission   We present a new map of interstellar reddening, covering the 39\% of the sky
with low {\rm HI} column densities ($N_{\rm HI} < 4\times10^{20}\,\rm cm^{-2}$
or $E(B-V)\approx 45\rm\, mmag$) at $16\overset{'}{.}1$ resolution, based on
all-sky observations of Galactic HI emission by the HI4PI Survey. In this low
column density regime, we derive a characteristic value of $N_{\rm HI}/E(B-V) =
8.8\times10^{21}\, \rm\, cm^{2}\, mag^{-1}$ for gas with $|v_{\rm LSR}| <
90\,\rm km\, s^{-1}$ and find no significant reddening associated with gas at
higher velocities. We compare our HI-based reddening map with the Schlegel,
Finkbeiner, and Davis (1998, SFD) reddening map and find them consistent to
within a scatter of $\simeq 5\,\rm mmag$. Further, the differences between our
map and the SFD map are in excellent agreement with the low resolution
($4\overset{\circ}{.}5$) corrections to the SFD map derived by Peek and Graves
(2010) based on observed reddening toward passive galaxies. We therefore argue
that our HI-based map provides the most accurate interstellar reddening
estimates in the low column density regime to date. Our reddening map is made
publicly available (this http URL).
",Physics,Physics
"Theoretical study of HfF$^+$ cation to search for the T,P-odd interactions   The combined all-electron and two-step approach is applied to calculate the
molecular parameters which are required to interpret the ongoing experiment to
search for the effects of manifestation of the T,P-odd fundamental interactions
in the HfF$^+$ cation by Cornell/Ye group [Science 342, 1220 (2013); J. Mol.
Spectrosc. 300, 12 (2014)]. The effective electric field that is required to
interpret the experiment in terms of the electron electric dipole moment is
found to be 22.5 GV/cm. In Ref. [Phys. Rev. D 89, 056006 (2014)] it was shown
that another source of T,P-odd interaction, the scalar-pseudoscalar
nucleus-electron interaction with the dimensionless strength constant $k_{T,P}$
can dominate over the direct contribution from the electron EDM within the
standard model and some of its extensions. Therefore, for the comprehensive and
correct interpretation of the HfF$^+$ experiment one should also know the
molecular parameter $W_{T,P}$ the value of which is reported here to be 20.1
kHz.
",Physics,Physics
"Good Similar Patches for Image Denoising   Patch-based denoising algorithms like BM3D have achieved outstanding
performance. An important idea for the success of these methods is to exploit
the recurrence of similar patches in an input image to estimate the underlying
image structures. However, in these algorithms, the similar patches used for
denoising are obtained via Nearest Neighbour Search (NNS) and are sometimes not
optimal. First, due to the existence of noise, NNS can select similar patches
with similar noise patterns to the reference patch. Second, the unreliable
noisy pixels in digital images can bring a bias to the patch searching process
and result in a loss of color fidelity in the final denoising result. We
observe that given a set of good similar patches, their distribution is not
necessarily centered at the noisy reference patch and can be approximated by a
Gaussian component. Based on this observation, we present a patch searching
method that clusters similar patch candidates into patch groups using Gaussian
Mixture Model-based clustering, and selects the patch group that contains the
reference patch as the final patches for denoising. We also use an unreliable
pixel estimation algorithm to pre-process the input noisy images to further
improve the patch searching. Our experiments show that our approach can better
capture the underlying patch structures and can consistently enable the
state-of-the-art patch-based denoising algorithms, such as BM3D, LPCA and PLOW,
to better denoise images by providing them with patches found by our approach
while without modifying these algorithms.
",Computer Science,Computer Science
"Integrating Lipschitzian Dynamical Systems using Piecewise Algorithmic Differentiation   In this article we analyze a generalized trapezoidal rule for initial value
problems with piecewise smooth right hand side \(F:\R^n\to\R^n\). When applied
to such a problem the classical trapezoidal rule suffers from a loss of
accuracy if the solution trajectory intersects a nondifferentiability of \(F\).
The advantage of the proposed generalized trapezoidal rule is threefold:
Firstly we can achieve a higher convergence order than with the classical
method. Moreover, the method is energy preserving for piecewise linear
Hamiltonian systems. Finally, in analogy to the classical case we derive a
third order interpolation polynomial for the numerical trajectory. In the
smooth case the generalized rule reduces to the classical one. Hence, it is a
proper extension of the classical theory. An error estimator is given and
numerical results are presented.
",Mathematics,Mathematics; Statistics
"Variational Autoencoders for Learning Latent Representations of Speech Emotion: A Preliminary Study   Learning the latent representation of data in unsupervised fashion is a very
interesting process that provides relevant features for enhancing the
performance of a classifier. For speech emotion recognition tasks, generating
effective features is crucial. Currently, handcrafted features are mostly used
for speech emotion recognition, however, features learned automatically using
deep learning have shown strong success in many problems, especially in image
processing. In particular, deep generative models such as Variational
Autoencoders (VAEs) have gained enormous success for generating features for
natural images. Inspired by this, we propose VAEs for deriving the latent
representation of speech signals and use this representation to classify
emotions. To the best of our knowledge, we are the first to propose VAEs for
speech emotion classification. Evaluations on the IEMOCAP dataset demonstrate
that features learned by VAEs can produce state-of-the-art results for speech
emotion classification.
",Computer Science; Statistics,Computer Science
"Probing magnetism in the vortex phase of PuCoGa$_5$ by X-ray magnetic circular dichroism   We have measured X-ray magnetic circular dichroism (XMCD) spectra at the Pu
$M_{4,5}$ absorption edges from a newly-prepared high-quality single crystal of
the heavy fermion superconductor $^{242}$PuCoGa$_{5}$, exhibiting a critical
temperature $T_{c} = 18.7~{\rm K}$. The experiment probes the vortex phase
below $T_{c}$ and shows that an external magnetic field induces a Pu 5$f$
magnetic moment at 2 K equal to the temperature-independent moment measured in
the normal phase up to 300 K by a SQUID device. This observation is in
agreement with theoretical models claiming that the Pu atoms in PuCoGa$_{5}$
have a nonmagnetic singlet ground state resulting from the hybridization of the
conduction electrons with the intermediate-valence 5$f$ electronic shell.
Unexpectedly, XMCD spectra show that the orbital component of the $5f$ magnetic
moment increases significantly between 30 and 2 K; the antiparallel spin
component increases as well, leaving the total moment practically constant. We
suggest that this indicates a low-temperature breakdown of the complete
Kondo-like screening of the local 5$f$ moment.
",Physics,Physics
"Scatteract: Automated extraction of data from scatter plots   Charts are an excellent way to convey patterns and trends in data, but they
do not facilitate further modeling of the data or close inspection of
individual data points. We present a fully automated system for extracting the
numerical values of data points from images of scatter plots. We use deep
learning techniques to identify the key components of the chart, and optical
character recognition together with robust regression to map from pixels to the
coordinate system of the chart. We focus on scatter plots with linear scales,
which already have several interesting challenges. Previous work has done fully
automatic extraction for other types of charts, but to our knowledge this is
the first approach that is fully automatic for scatter plots. Our method
performs well, achieving successful data extraction on 89% of the plots in our
test set.
",Computer Science; Statistics,Computer Science; Statistics
"A combinatorial model for the path fibration   We introduce the abstract notion of a necklical set in order to describe a
functorial combinatorial model of the path fibration over the geometric
realization of a path connected simplicial set. In particular, to any path
connected simplicial set $X$ we associate a necklical set
$\widehat{\mathbf{\Omega}}X$ such that its geometric realization
$|\widehat{\mathbf{\Omega}}X|$, a space built out of gluing cubical cells, is
homotopy equivalent to the based loop space on $|X|$ and the differential
graded module of chains $C_*(\widehat{\mathbf{\Omega}}X)$ is a differential
graded associative algebra generalizing Adams' cobar construction.
",Mathematics,Mathematics
"Quantized Minimum Error Entropy Criterion   Comparing with traditional learning criteria, such as mean square error
(MSE), the minimum error entropy (MEE) criterion is superior in nonlinear and
non-Gaussian signal processing and machine learning. The argument of the
logarithm in Renyis entropy estimator, called information potential (IP), is a
popular MEE cost in information theoretic learning (ITL). The computational
complexity of IP is however quadratic in terms of sample number due to double
summation. This creates computational bottlenecks especially for large-scale
datasets. To address this problem, in this work we propose an efficient
quantization approach to reduce the computational burden of IP, which decreases
the complexity from O(N*N) to O (MN) with M << N. The new learning criterion is
called the quantized MEE (QMEE). Some basic properties of QMEE are presented.
Illustrative examples are provided to verify the excellent performance of QMEE.
",Computer Science; Statistics,Computer Science; Statistics
"Learning Deep Networks from Noisy Labels with Dropout Regularization   Large datasets often have unreliable labels-such as those obtained from
Amazon's Mechanical Turk or social media platforms-and classifiers trained on
mislabeled datasets often exhibit poor performance. We present a simple,
effective technique for accounting for label noise when training deep neural
networks. We augment a standard deep network with a softmax layer that models
the label noise statistics. Then, we train the deep network and noise model
jointly via end-to-end stochastic gradient descent on the (perhaps mislabeled)
dataset. The augmented model is overdetermined, so in order to encourage the
learning of a non-trivial noise model, we apply dropout regularization to the
weights of the noise model during training. Numerical experiments on noisy
versions of the CIFAR-10 and MNIST datasets show that the proposed dropout
technique outperforms state-of-the-art methods.
",Computer Science; Statistics,Computer Science; Statistics
"A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI   Two popular classes of methods for approximate inference are Markov chain
Monte Carlo (MCMC) and variational inference. MCMC tends to be accurate if run
for a long enough time, while variational inference tends to give better
approximations at shorter time horizons. However, the amount of time needed for
MCMC to exceed the performance of variational methods can be quite high,
motivating more fine-grained tradeoffs. This paper derives a distribution over
variational parameters, designed to minimize a bound on the divergence between
the resulting marginal distribution and the target, and gives an example of how
to sample from this distribution in a way that interpolates between the
behavior of existing methods based on Langevin dynamics and stochastic gradient
variational inference (SGVI).
",Computer Science; Statistics,Statistics
"Local asymptotic equivalence of pure quantum states ensembles and quantum Gaussian white noise   Quantum technology is increasingly relying on specialised statistical
inference methods for analysing quantum measurement data. This motivates the
development of ""quantum statistics"", a field that is shaping up at the overlap
of quantum physics and ""classical"" statistics. One of the less investigated
topics to date is that of statistical inference for infinite dimensional
quantum systems, which can be seen as quantum counterpart of non-parametric
statistics. In this paper we analyse the asymptotic theory of quantum
statistical models consisting of ensembles of quantum systems which are
identically prepared in a pure state. In the limit of large ensembles we
establish the local asymptotic equivalence (LAE) of this i.i.d. model to a
quantum Gaussian white noise model. We use the LAE result in order to establish
minimax rates for the estimation of pure states belonging to Hermite-Sobolev
classes of wave functions. Moreover, for quadratic functional estimation of the
same states we note an elbow effect in the rates, whereas for testing a pure
state a sharp parametric rate is attained over the nonparametric
Hermite-Sobolev class.
",Mathematics; Statistics,Physics; Mathematics; Statistics
"Dark matter spin determination with directional direct detection experiments   If the dark matter particle has spin 0, only two types of WIMP-nucleon
interaction can arise from the non-relativistic reduction of renormalisable
single-mediator models for dark matter-quark interactions. Based on this
crucial observation, we show that about 100 signal events at next generation
directional detection experiments can be enough to enable a $2\sigma$ rejection
of the spin 0 dark matter hypothesis in favour of alternative hypotheses where
the dark matter particle has spin 1/2 or 1. In this context directional
sensitivity is crucial, since anisotropy patterns in the sphere of nuclear
recoil directions depend on the spin of the dark matter particle. For
comparison, about 100 signal events are expected in a CF$_4$ detector operating
at a pressure of 30 torr with an exposure of approximately 26,000
cubic-meter-detector days for WIMPs of 100 GeV mass and a WIMP-Fluorine
scattering cross-section of 0.25 pb. Comparable exposures are within reach of
an array of cubic meter time projection chamber detectors.
",Physics,Physics
"Class-Splitting Generative Adversarial Networks   Generative Adversarial Networks (GANs) produce systematically better quality
samples when class label information is provided., i.e. in the conditional GAN
setup. This is still observed for the recently proposed Wasserstein GAN
formulation which stabilized adversarial training and allows considering high
capacity network architectures such as ResNet. In this work we show how to
boost conditional GAN by augmenting available class labels. The new classes
come from clustering in the representation space learned by the same GAN model.
The proposed strategy is also feasible when no class information is available,
i.e. in the unsupervised setup. Our generated samples reach state-of-the-art
Inception scores for CIFAR-10 and STL-10 datasets in both supervised and
unsupervised setup.
",Statistics,Computer Science; Statistics
"Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam   Uncertainty computation in deep learning is essential to design robust and
reliable systems. Variational inference (VI) is a promising approach for such
computation, but requires more effort to implement and execute compared to
maximum-likelihood methods. In this paper, we propose new natural-gradient
algorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms
can be implemented within the Adam optimizer by perturbing the network weights
during gradient evaluations, and uncertainty estimates can be cheaply obtained
by using the vector that adapts the learning rate. This requires lower memory,
computation, and implementation effort than existing VI methods, while
obtaining uncertainty estimates of comparable quality. Our empirical results
confirm this and further suggest that the weight-perturbation in our algorithm
could be useful for exploration in reinforcement learning and stochastic
optimization.
",Statistics,Computer Science; Statistics
"Long ties accelerate noisy threshold-based contagions   Changes to network structure can substantially affect when and how widely new
ideas, products, and conventions are adopted. In models of biological
contagion, interventions that randomly rewire edges (making them ""longer"")
accelerate spread. However, there are other models relevant to social
contagion, such as those motivated by myopic best-response in games with
strategic complements, in which individual's behavior is described by a
threshold number of adopting neighbors above which adoption occurs (i.e.,
complex contagions). Recent work has argued that highly clustered, rather than
random, networks facilitate spread of these complex contagions. Here we show
that minor modifications of prior analyses, which make them more realistic,
reverse this result. The modification is that we allow very rarely below
threshold adoption, i.e., very rarely adoption occurs, where there is only one
adopting neighbor. To model the trade-off between long and short edges we
consider networks that are the union of cycle-power-$k$ graphs and random
graphs on $n$ nodes. We study how the time to global spread changes as we
replace the cycle edges with (random) long ties. Allowing adoptions below
threshold to occur with order $1/\sqrt{n}$ probability is enough to ensure that
random rewiring accelerates spread. Simulations illustrate the robustness of
these results to other commonly-posited models for noisy best-response
behavior. We then examine empirical social networks, where we find that
hypothetical interventions that (a) randomly rewire existing edges or (b) add
random edges reduce time to spread compared with the original network or
addition of ""short"", triad-closing edges, respectively. This substantially
revises conclusions about how interventions change the spread of behavior,
suggesting that those wanting to increase spread should induce formation of
long ties, rather than triad-closing ties.
",Computer Science,Computer Science; Physics
"Fluid-Structure Interaction with the Entropic Lattice Boltzmann Method   We propose a novel fluid-structure interaction (FSI) scheme using the
entropic multi-relaxation time lattice Boltzmann (KBC) model for the fluid
domain in combination with a nonlinear finite element solver for the structural
part. We show validity of the proposed scheme for various challenging set-ups
by comparison to literature data. Beyond validation, we extend the KBC model to
multiphase flows and couple it with FEM solver. Robustness and viability of the
entropic multi-relaxation time model for complex FSI applications is shown by
simulations of droplet impact on elastic superhydrophobic surfaces.
",Physics,Physics
"Analysis and Control of a Non-Standard Hyperbolic PDE Traffic Flow Model   The paper provides results for a non-standard, hyperbolic, 1-D, nonlinear
traffic flow model on a bounded domain. The model consists of two first-order
PDEs with a dynamic boundary condition that involves the time derivative of the
velocity. The proposed model has features that are important from a
traffic-theoretic point of view: is completely anisotropic and information
travels forward exactly at the same speed as traffic. It is shown that, for all
physically meaningful initial conditions, the model admits a globally defined,
unique, classical solution that remains positive and bounded for all times.
Moreover, it is shown that global stabilization can be achieved for arbitrary
equilibria by means of an explicit boundary feedback law. The stabilizing
feedback law depends only on the inlet velocity and consequently, the
measurement requirements for the implementation of the proposed boundary
feedback law are minimal. The efficiency of the proposed boundary feedback law
is demonstrated by means of a numerical example.
",Computer Science; Mathematics,Computer Science
"Equilibrium distributions and discrete Schur-constant models   This paper introduces Schur-constant equilibrium distribution models of
dimension n for arithmetic non-negative random variables. Such a model is
defined through the (several orders) equilibrium distributions of a univariate
survival function. First, the bivariate case is considered and analyzed in
depth, stressing the main characteristics of the Poisson case. The analysis is
then extended to the multivariate case. Several properties are derived,
including the implicit correlation and the distribution of the sum.
",Statistics,Mathematics; Statistics
"Distributive Minimization Comprehensions and the Polynomial Hierarchy   A categorical point of view about minimization in subrecursive classes is
presented by extending the concept of Symmetric Monoidal Comprehension to that
of Distributive Minimization Comprehension. This is achieved by endowing the
former with coproducts and a finality condition for coalgebras over the
endofunctor sending X to ${1}\oplus{X}$ to perform a safe minimization
operator. By relying on the characterization given by Bellantoni, a tiered
structure is presented from which one can obtain the levels of the Polytime
Hierarchy as those classes of partial functions obtained after a certain number
of minimizations.
",Computer Science; Mathematics,Mathematics
"Phase transitions of the dimerized Kane-Mele model with/without the strong interaction   The dimerized Kane-Mele model with/without the strong interaction is studied
using analytical methods. The boundary of the topological phase transition of
the model without strong interaction is obtained. Our results show that the
occurrence of the transition only depends on dimerized parameter . From the
one-particle spectrum, we obtain the completed phase diagram including the
quantum spin Hall (QSH) state and the topologically trivial insulator. Then,
using different mean-field methods, we investigate the Mott transition and the
magnetic transition of the strongly correlated dimerized Kane-Mele model. In
the region between the two transitions, the topological Mott insulator (TMI)
with characters of Mott insulators and topological phases may be the most
interesting phase. In this work, effects of the hopping anisotropy and Hubbard
interaction U on boundaries of the two transitions are observed in detail. The
completed phase diagram of the dimerized Kane-Mele-Hubbard model is also
obtained in this work. Quantum fluctuations have extremely important influences
on a quantum system. However, investigations are under the framework of the
mean field treatment in this work and the effects of fluctuations in this model
will be discussed in the future.
",Physics,Physics
"Explaining Parochialism: A Causal Account for Political Polarization in Changing Economic Environments   Political and social polarization are a significant cause of conflict and
poor governance in many societies, thus understanding their causes is of
considerable importance. Here we demonstrate that shifts in socialization
strategy similar to political polarization and/or identity politics could be a
constructive response to periods of apparent economic decline. We start from
the observation that economies, like ecologies are seldom at equilibrium.
Rather, they often suffer both negative and positive shocks. We show that even
where in an expanding economy, interacting with diverse out-groups can afford
benefits through innovation and exploration, if that economy contracts, a
strategy of seeking homogeneous groups can be important to maintaining
individual solvency. This is true even where the expected value of out group
interaction exceeds that of in group interactions. Our account unifies what
were previously seen as conflicting explanations: identity threat versus
economic anxiety. Our model indicates that in periods of extreme deprivation,
cooperation with diversity again becomes the best (in fact, only viable)
strategy. However, our model also shows that while polarization may increase
gradually in response to shifts in the economy, gradual decrease of
polarization may not be an available strategy; thus returning to previous
levels of cooperation may require structural change.
",Quantitative Biology; Quantitative Finance,Quantitative Finance
"Multiple Hypothesis Tracking Algorithm for Multi-Target Multi-Camera Tracking with Disjoint Views   In this study, a multiple hypothesis tracking (MHT) algorithm for
multi-target multi-camera tracking (MCT) with disjoint views is proposed. Our
method forms track-hypothesis trees, and each branch of them represents a
multi-camera track of a target that may move within a camera as well as move
across cameras. Furthermore, multi-target tracking within a camera is performed
simultaneously with the tree formation by manipulating a status of each track
hypothesis. Each status represents three different stages of a multi-camera
track: tracking, searching, and end-of-track. The tracking status means targets
are tracked by a single camera tracker. In the searching status, the
disappeared targets are examined if they reappear in other cameras. The
end-of-track status does the target exited the camera network due to its
lengthy invisibility. These three status assists MHT to form the
track-hypothesis trees for multi-camera tracking. Furthermore, they present a
gating technique for eliminating of unlikely observation-to-track association.
In the experiments, they evaluate the proposed method using two datasets,
DukeMTMC and NLPR-MCT, which demonstrates that the proposed method outperforms
the state-of-the-art method in terms of improvement of the accuracy. In
addition, they show that the proposed method can operate in real-time and
online.
",Computer Science,Computer Science
"Adversarially Regularized Graph Autoencoder for Graph Embedding   Graph embedding is an effective method to represent graph data in a low
dimensional space for graph analytics. Most existing embedding algorithms
typically focus on preserving the topological structure or minimizing the
reconstruction errors of graph data, but they have mostly ignored the data
distribution of the latent codes from the graphs, which often results in
inferior embedding in real-world graph data. In this paper, we propose a novel
adversarial graph embedding framework for graph data. The framework encodes the
topological structure and node content in a graph to a compact representation,
on which a decoder is trained to reconstruct the graph structure. Furthermore,
the latent representation is enforced to match a prior distribution via an
adversarial training scheme. To learn a robust embedding, two variants of
adversarial approaches, adversarially regularized graph autoencoder (ARGA) and
adversarially regularized variational graph autoencoder (ARVGA), are developed.
Experimental studies on real-world graphs validate our design and demonstrate
that our algorithms outperform baselines by a wide margin in link prediction,
graph clustering, and graph visualization tasks.
",Statistics,Computer Science
"T-duality in rational homotopy theory via $L_\infty$-algebras   We combine Sullivan models from rational homotopy theory with Stasheff's
$L_\infty$-algebras to describe a duality in string theory. Namely, what in
string theory is known as topological T-duality between $K^0$-cocycles in type
IIA string theory and $K^1$-cocycles in type IIB string theory, or as Hori's
formula, can be recognized as a Fourier-Mukai transform between twisted
cohomologies when looked through the lenses of rational homotopy theory. We
show this as an example of topological T-duality in rational homotopy theory,
which in turn can be completely formulated in terms of morphisms of
$L_\infty$-algebras.
",Mathematics,Mathematics
"PPMF: A Patient-based Predictive Modeling Framework for Early ICU Mortality Prediction   To date, developing a good model for early intensive care unit (ICU)
mortality prediction is still challenging. This paper presents a patient based
predictive modeling framework (PPMF) to improve the performance of ICU
mortality prediction using data collected during the first 48 hours of ICU
admission. PPMF consists of three main components verifying three related
research hypotheses. The first component captures dynamic changes of patients
status in the ICU using their time series data (e.g., vital signs and
laboratory tests). The second component is a local approximation algorithm that
classifies patients based on their similarities. The third component is a
Gradient Decent wrapper that updates feature weights according to the
classification feedback. Experiments using data from MIMICIII show that PPMF
significantly outperforms: (1) the severity score systems, namely SASP III,
APACHE IV, and MPM0III, (2) the aggregation based classifiers that utilize
summarized time series, and (3) baseline feature selection methods.
",Computer Science,Statistics
"From Attention to Participation: Reviewing and Modelling Engagement with Computers   Over the last decades, the Internet and mobile technology have consolidated
the digital as a public sphere of life. Designers are asked to create engaging
digital experiences. However, in some cases engagement is seen as a
psychological state, while in others it emphasizes a participative vein. In
this paper, I review and discuss both and propose a new definition to clarify
the concept engagement with computers. Thus, engagement is a quality of an
active connection between a user and a computing product, either a website or a
mobile phone app. Studying it requires understanding a set of aspects like the
user's affect, motivation and attention, as well as the product's design,
content and composition. Finally, I propose explaining these concepts aligned
with engagement and integrate them into a preliminary model to measure the
manifestations.
",Computer Science,Computer Science
"Mixed one-bit compressive sensing with applications to overexposure correction for CT reconstruction   When a measurement falls outside the quantization or measurable range, it
becomes saturated and cannot be used in classical reconstruction methods. For
example, in C-arm angiography systems, which provide projection radiography,
fluoroscopy, digital subtraction angiography, and are widely used for medical
diagnoses and interventions, the limited dynamic range of C-arm flat detectors
leads to overexposure in some projections during an acquisition, such as
imaging relatively thin body parts (e.g., the knee). Aiming at overexposure
correction for computed tomography (CT) reconstruction, we in this paper
propose a mixed one-bit compressive sensing (M1bit-CS) to acquire information
from both regular and saturated measurements. This method is inspired by the
recent progress on one-bit compressive sensing, which deals with only sign
observations. Its successful applications imply that information carried by
saturated measurements is useful to improve recovery quality. For the proposed
M1bit-CS model, alternating direction methods of multipliers is developed and
an iterative saturation detection scheme is established. Then we evaluate
M1bit-CS on one-dimensional signal recovery tasks. In some experiments, the
performance of the proposed algorithms on mixed measurements is almost the same
as recovery on unsaturated ones with the same amount of measurements. Finally,
we apply the proposed method to overexposure correction for CT reconstruction
on a phantom and a simulated clinical image. The results are promising, as the
typical streaking artifacts and capping artifacts introduced by saturated
projection data are effectively reduced, yielding significant error reduction
compared with existing algorithms based on extrapolation.
",Computer Science; Mathematics,Computer Science; Physics
"Probabilistic Forwarding of Coded Packets on Networks   We consider a scenario of broadcasting information over a network of nodes
connected by noiseless communication links. A source node in the network has
$k$ data packets to broadcast, and it suffices that a large fraction of the
network nodes receives the broadcast. The source encodes the $k$ data packets
into $n \ge k$ coded packets using a maximum distance separable (MDS) code, and
transmits them to its one-hop neighbours. Every other node in the network
follows a probabilistic forwarding protocol, in which it forwards a previously
unreceived packet to all its neighbours with a certain probability $p$. A
""near-broadcast"" is when the expected fraction of nodes that receive at least
$k$ of the $n$ coded packets is close to $1$. The forwarding probability $p$ is
chosen so as to minimize the expected total number of transmissions needed for
a near-broadcast. In this paper, we analyze the probabilistic forwarding of
coded packets on two specific network topologies: binary trees and square
grids. For trees, our analysis shows that for fixed $k$, the expected total
number of transmissions increases with $n$. On the other hand, on grids, we use
ideas from percolation theory to show that a judicious choice of $n$ will
significantly reduce the expected total number of transmissions needed for a
near-broadcast.
",Computer Science,Computer Science
"Detecting Qualia in Natural and Artificial Agents   The Hard Problem of consciousness has been dismissed as an illusion. By
showing that computers are capable of experiencing, we show that they are at
least rudimentarily conscious with potential to eventually reach
superconsciousness. The main contribution of the paper is a test for confirming
certain subjective experiences in a tested agent. We follow with analysis of
benefits and problems with conscious machines and implications of such
capability on future of computing, machine rights and artificial intelligence
safety.
",Computer Science,Computer Science
"Effect of stellar flares on the upper atmospheres of HD 189733b and HD 209458b   Stellar flares are a frequent occurrence on young low-mass stars around which
many detected exoplanets orbit. Flares are energetic, impulsive events, and
their impact on exoplanetary atmospheres needs to be taken into account when
interpreting transit observations. We have developed a model to describe the
upper atmosphere of Extrasolar Giant Planets (EGPs) orbiting flaring stars. The
model simulates thermal escape from the upper atmospheres of close-in EGPs.
Ionisation by solar radiation and electron impact is included and photochemical
and diffusive transport processes are simulated. This model is used to study
the effect of stellar flares from the solar-like G star HD209458 and the young
K star HD189733 on their respective planets. A hypothetical HD209458b-like
planet orbiting the active M star AU Mic is also simulated. We find that the
neutral upper atmosphere of EGPs is not significantly affected by typical
flares. Therefore, stellar flares alone would not cause large enough changes in
planetary mass loss to explain the variations in HD189733b transit depth seen
in previous studies, although we show that it may be possible that an extreme
stellar proton event could result in the required mass loss. Our simulations do
however reveal an enhancement in electron number density in the ionosphere of
these planets, the peak of which is located in the layer where stellar X-rays
are absorbed. Electron densities are found to reach 2.2 to 3.5 times pre-flare
levels and enhanced electron densities last from about 3 to 10 hours after the
onset of the flare. The strength of the flare and the width of its spectral
energy distribution affect the range of altitudes that see enhancements in
ionisation. A large broadband continuum component in the XUV portion of the
flaring spectrum in very young flare stars, such as AU Mic, results in a broad
range of altitudes affected in planets orbiting this star.
",Physics,Physics
"Mathematical Knowledge and the Role of an Observer: Ontological and epistemological aspects   As David Berlinski writes (1997), the existence and nature of mathematics is
a more compelling and far deeper problem than any of the problems raised by
mathematics itself. Here we analyze the essence of mathematics making the main
emphasis on mathematics as an advanced system of knowledge. This knowledge
consists of structures and represents structures, existence of which depends on
observers in a nonstandard way. Structural nature of mathematics explains its
reasonable effectiveness.
",Mathematics,Computer Science; Physics
"DeepProbe: Information Directed Sequence Understanding and Chatbot Design via Recurrent Neural Networks   Information extraction and user intention identification are central topics
in modern query understanding and recommendation systems. In this paper, we
propose DeepProbe, a generic information-directed interaction framework which
is built around an attention-based sequence to sequence (seq2seq) recurrent
neural network. DeepProbe can rephrase, evaluate, and even actively ask
questions, leveraging the generative ability and likelihood estimation made
possible by seq2seq models. DeepProbe makes decisions based on a derived
uncertainty (entropy) measure conditioned on user inputs, possibly with
multiple rounds of interactions. Three applications, namely a rewritter, a
relevance scorer and a chatbot for ad recommendation, were built around
DeepProbe, with the first two serving as precursory building blocks for the
third. We first use the seq2seq model in DeepProbe to rewrite a user query into
one of standard query form, which is submitted to an ordinary recommendation
system. Secondly, we evaluate DeepProbe's seq2seq model-based relevance
scoring. Finally, we build a chatbot prototype capable of making active user
interactions, which can ask questions that maximize information gain, allowing
for a more efficient user intention idenfication process. We evaluate first two
applications by 1) comparing with baselines by BLEU and AUC, and 2) human judge
evaluation. Both demonstrate significant improvements compared with current
state-of-the-art systems, proving their values as useful tools on their own,
and at the same time laying a good foundation for the ongoing chatbot
application.
",Computer Science; Statistics,Computer Science
"Quasi Maximum-Likelihood Estimation of Dynamic Panel Data Models   This paper establishes the almost sure convergence and asymptotic normality
of levels and differenced quasi maximum-likelihood (QML) estimators of dynamic
panel data models. The QML estimators are robust with respect to initial
conditions, conditional and time-series heteroskedasticity, and
misspecification of the log-likelihood. The paper also provides an ECME
algorithm for calculating levels QML estimates. Finally, it uses Monte Carlo
experiments to compare the finite sample performance of levels and differenced
QML estimators, the differenced GMM estimator, and the system GMM estimator. In
these experiments the QML estimators usually have smaller --- typically
substantially smaller --- bias and root mean squared errors than the panel data
GMM estimators.
",Mathematics; Statistics,Mathematics; Statistics
"Coarse Grained Parallel Selection   We analyze the running time of the Saukas-Song algorithm for selection on a
coarse grained multicomputer without expressing the running time in terms of
communication rounds. This shows that while in the best case the Saukas-Song
algorithm runs in asymptotically optimal time, in general it does not. We
propose other algorithms for coarse grained selection that have optimal
expected running time.
",Computer Science,Computer Science; Statistics
"The Cross-section of a Spherical Double Cone   We show that the poset of $SL(n)$-orbit closures in the product of two
partial flag varieties is a lattice if the action of $SL(n)$ is spherical.
",Mathematics,Mathematics
"Full Quantification of Left Ventricle via Deep Multitask Learning Network Respecting Intra- and Inter-Task Relatedness   Cardiac left ventricle (LV) quantification is among the most clinically
important tasks for identification and diagnosis of cardiac diseases, yet still
a challenge due to the high variability of cardiac structure and the complexity
of temporal dynamics. Full quantification, i.e., to simultaneously quantify all
LV indices including two areas (cavity and myocardium), six regional wall
thicknesses (RWT), three LV dimensions, and one cardiac phase, is even more
challenging since the uncertain relatedness intra and inter each type of
indices may hinder the learning procedure from better convergence and
generalization. In this paper, we propose a newly-designed multitask learning
network (FullLVNet), which is constituted by a deep convolution neural network
(CNN) for expressive feature embedding of cardiac structure; two followed
parallel recurrent neural network (RNN) modules for temporal dynamic modeling;
and four linear models for the final estimation. During the final estimation,
both intra- and inter-task relatedness are modeled to enforce improvement of
generalization: 1) respecting intra-task relatedness, group lasso is applied to
each of the regression tasks for sparse and common feature selection and
consistent prediction; 2) respecting inter-task relatedness, three phase-guided
constraints are proposed to penalize violation of the temporal behavior of the
obtained LV indices. Experiments on MR sequences of 145 subjects show that
FullLVNet achieves high accurate prediction with our intra- and inter-task
relatedness, leading to MAE of 190mm$^2$, 1.41mm, 2.68mm for average areas,
RWT, dimensions and error rate of 10.4\% for the phase classification. This
endows our method a great potential in comprehensive clinical assessment of
global, regional and dynamic cardiac function.
",Computer Science,Computer Science; Statistics
"Fabrication of antenna-coupled KID array for Cosmic Microwave Background detection   Kinetic Inductance Detectors (KIDs) have become an attractive alternative to
traditional bolometers in the sub-mm and mm observing community due to their
innate frequency multiplexing capabilities and simple lithographic processes.
These advantages make KIDs a viable option for the $O(500,000)$ detectors
needed for the upcoming Cosmic Microwave Background - Stage 4 (CMB-S4)
experiment. We have fabricated antenna-coupled MKID array in the 150GHz band
optimized for CMB detection. Our design uses a twin slot antenna coupled to
inverted microstrip made from a superconducting Nb/Al bilayer and SiN$_x$,
which is then coupled to an Al KID grown on high resistivity Si. We present the
fabrication process and measurements of SiN$_x$ microstrip resonators.
",Physics,Physics
"Core of communities in bipartite networks   We use the information present in a bipartite network to detect cores of
communities of each set of the bipartite system. Cores of communities are found
by investigating statistically validated projected networks obtained using
information present in the bipartite network. Cores of communities are highly
informative and robust with respect to the presence of errors or missing
entries in the bipartite network. We assess the statistical robustness of cores
by investigating an artificial benchmark network, the co-authorship network,
and the actor-movie network. The accuracy and precision of the partition
obtained with respect to the reference partition are measured in terms of the
adjusted Rand index and of the adjusted Wallace index respectively. The
detection of cores is highly precise although the accuracy of the methodology
can be limited in some cases.
",Computer Science; Physics,Computer Science; Mathematics
"Four-dimensional Lens Space Index from Two-dimensional Chiral Algebra   We study the supersymmetric partition function on $S^1 \times L(r, 1)$, or
the lens space index of four-dimensional $\mathcal{N}=2$ superconformal field
theories and their connection to two-dimensional chiral algebras. We primarily
focus on free theories as well as Argyres-Douglas theories of type $(A_1, A_k)$
and $(A_1, D_k)$. We observe that in specific limits, the lens space index is
reproduced in terms of the (refined) character of an appropriately twisted
module of the associated two-dimensional chiral algebra or a generalized vertex
operator algebra. The particular twisted module is determined by the choice of
discrete holonomies for the flavor symmetry in four-dimensions.
",Mathematics,Mathematics
"Convexity in scientific collaboration networks   Convexity in a network (graph) has been recently defined as a property of
each of its subgraphs to include all shortest paths between the nodes of that
subgraph. It can be measured on the scale [0, 1] with 1 being assigned to fully
convex networks. The largest convex component of a graph that emerges after the
removal of the least number of edges is called a convex skeleton. It is
basically a tree of cliques, which has been shown to have many interesting
features. In this article the notions of convexity and convex skeletons in the
context of scientific collaboration networks are discussed. More specifically,
we analyze the co-authorship networks of Slovenian researchers in computer
science, physics, sociology, mathematics, and economics and extract convex
skeletons from them. We then compare these convex skeletons with the residual
graphs (remainders) in terms of collaboration frequency distributions by
various parameters such as the publication year and type, co-authors' birth
year, status, gender, discipline, etc. We also show the top-ranked scientists
by four basic centrality measures as calculated on the original networks and
their skeletons and conclude that convex skeletons may help detect influential
scholars that are hardly identifiable in the original collaboration network. As
their inherent feature, convex skeletons retain the properties of collaboration
networks. These include high-level structural properties but also the fact that
the same authors are highlighted by centrality measures. Moreover, the most
important ties and thus the most important collaborations are retained in the
skeletons.
",Computer Science,Computer Science; Physics
"A Batch-Incremental Video Background Estimation Model using Weighted Low-Rank Approximation of Matrices   Principal component pursuit (PCP) is a state-of-the-art approach for
background estimation problems. Due to their higher computational cost, PCP
algorithms, such as robust principal component analysis (RPCA) and its
variants, are not feasible in processing high definition videos. To avoid the
curse of dimensionality in those algorithms, several methods have been proposed
to solve the background estimation problem in an incremental manner. We propose
a batch-incremental background estimation model using a special weighted
low-rank approximation of matrices. Through experiments with real and synthetic
video sequences, we demonstrate that our method is superior to the
state-of-the-art background estimation algorithms such as GRASTA, ReProCS,
incPCP, and GFL.
",Computer Science; Mathematics,Computer Science
"A cyclic system with delay and its characteristic equation   A nonlinear cyclic system with delay and the overall negative feedback is
considered. The characteristic equation of the linearized system is studied in
detail. Sufficient conditions for the oscillation of all solutions and for the
existence of monotone solutions are derived in terms of roots of the
characteristic equation.
",Mathematics,Mathematics
"Small Resolution Proofs for QBF using Dependency Treewidth   In spite of the close connection between the evaluation of quantified Boolean
formulas (QBF) and propositional satisfiability (SAT), tools and techniques
which exploit structural properties of SAT instances are known to fail for QBF.
This is especially true for the structural parameter treewidth, which has
allowed the design of successful algorithms for SAT but cannot be
straightforwardly applied to QBF since it does not take into account the
interdependencies between quantified variables.
In this work we introduce and develop dependency treewidth, a new structural
parameter based on treewidth which allows the efficient solution of QBF
instances. Dependency treewidth pushes the frontiers of tractability for QBF by
overcoming the limitations of previously introduced variants of treewidth for
QBF. We augment our results by developing algorithms for computing the
decompositions that are required to use the parameter.
",Computer Science,Computer Science
"21 cm Angular Power Spectrum from Minihalos as a Probe of Primordial Spectral Runnings   Measurements of 21 cm line fluctuations from minihalos have been discussed as
a powerful probe of a wide range of cosmological models. However, previous
studies have taken into account only the pixel variance, where contributions
from different scales are integrated. In order to sort out information from
different scales, we formulate the angular power spectrum of 21 cm line
fluctuations from minihalos at different redshifts, which can enhance the
constraining power enormously. By adopting this formalism, we investigate
expected constraints on parameters characterizing the primordial power
spectrum, particularly focusing on the spectral index $n_s$ and its runnings
$\alpha_s$ and $\beta_s$. We show that future observations of 21 cm line
fluctuations from minihalos, in combination with cosmic microwave background,
can potentially probe these runnings as $\alpha_s \sim {\cal O}(10^{-3})$ and
$\beta_s \sim {\cal O}(10^{-4})$. Its implications to the test of inflationary
models are also discussed.
",Physics,Physics
"Regular characters of classical groups over complete discrete valuation rings   Let $\mathfrak{o}$ be a complete discrete valuation ring with finide residue
field $\mathsf{k}$ of odd characteristic, and let $\mathbf{G}$ be a symplectic
or special orthogonal group scheme over $\mathfrak{o}$. For any
$\ell\in\mathbb{N}$ let $G^\ell$ denote the $\ell$-th principal congruence
subgroup of $\mathbf{G}(\mathfrak{o})$. An irreducible character of the group
$\mathbf{G}(\mathfrak{o})$ is said to be regular if it is trivial on a subgroup
$G^{\ell+1}$ for some $\ell$, and if its restriction to
$G^\ell/G^{\ell+1}\simeq \mathrm{Lie}(\mathbf{G})(\mathsf{k})$ consists of
characters of minimal $\mathbf{G}(\mathsf{k}^{\rm alg})$ stabilizer dimension.
In the present paper we consider the regular characters of such classical
groups over $\mathfrak{o}$, and construct and enumerate all regular characters
of $\mathbf{G}(\mathfrak{o})$, when the characteristic of $\mathsf{k}$ is
greater than two. As a result, we compute the regular part of their
representation zeta function.
",Mathematics,Mathematics
"Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms?   We study how the behavior of deep policy gradient algorithms reflects the
conceptual framework motivating their development. We propose a fine-grained
analysis of state-of-the-art methods based on key aspects of this framework:
gradient estimation, value prediction, optimization landscapes, and trust
region enforcement. We find that from this perspective, the behavior of deep
policy gradient algorithms often deviates from what their motivating framework
would predict. Our analysis suggests first steps towards solidifying the
foundations of these algorithms, and in particular indicates that we may need
to move beyond the current benchmark-centric evaluation methodology.
",Computer Science,Computer Science; Statistics
"Pore lifetimes in cell electroporation: Complex dark pores?   We review some of the basic concepts and the possible pore structures
associated with electroporation (EP) for times after electrical pulsing. We
purposefully give only a short description of pore creation and subsequent
evolution of pore populations, as these are adequately discussed in both
reviews and original research reports. In contrast, post-pulse pore concepts
have changed dramatically. For perspective we note that pores are not directly
observed. Instead understanding of pores is based on inference from experiments
and, increasingly, molecular dynamics (MD) simulations. In the past decade
concepts for post-pulse pores have changed significantly: The idea of pure
lipidic transient pores (TPs) that exist for milliseconds or longer post-pulse
has become inconsistent with MD results, which support TP lifetimes of only
$\sim$100 ns. A typical large TP number during cell EP pulsing is of order
$10^6$. In twenty MD-based TP lifetimes (2 us total), the TP number plummets to
$\sim$0.001. In short, TPs vanish 2 us after a pulse ends, and cannot account
for post-pulse behavior such as large and relatively non-specific ionic and
molecular transport. Instead, an early conjecture of complex pores (CPs) with
both lipidic and other molecule should be taken seriously. Indeed, in the past
decade several experiments provide partial support for complex pores (CPs).
Presently, CPs are ""dark"", in the sense that while some CP functions are known,
little is known about their structure(s). There may be a wide range of
lifetimes and permeabilities, not yet revealed by experiments. Like cosmology's
dark matter, these unseen pores present us with an outstanding problem.
",Physics,Physics
"Single-Queue Decoding for Neural Machine Translation   Neural machine translation models rely on the beam search algorithm for
decoding. In practice, we found that the quality of hypotheses in the search
space is negatively affected owing to the fixed beam size. To mitigate this
problem, we store all hypotheses in a single priority queue and use a universal
score function for hypothesis selection. The proposed algorithm is more
flexible as the discarded hypotheses can be revisited in a later step. We
further design a penalty function to punish the hypotheses that tend to produce
a final translation that is much longer or shorter than expected. Despite its
simplicity, we show that the proposed decoding algorithm is able to select
hypotheses with better qualities and improve the translation performance.
",Computer Science,Computer Science
"On reproduction of On the regularization of Wasserstein GANs   This report has several purposes. First, our report is written to investigate
the reproducibility of the submitted paper On the regularization of Wasserstein
GANs (2018). Second, among the experiments performed in the submitted paper,
five aspects were emphasized and reproduced: learning speed, stability,
robustness against hyperparameter, estimating the Wasserstein distance, and
various sampling method. Finally, we identify which parts of the contribution
can be reproduced, and at what cost in terms of resources. All source code for
reproduction is open to the public.
",Computer Science; Statistics,Computer Science; Statistics
"Optimization of Executable Formal Interpreters developed in Higher-order Theorem Proving Systems   In recent publications, we presented a novel formal symbolic process virtual
machine (FSPVM) framework that combined higher-order theorem proving and
symbolic execution for verifying the reliability and security of smart
contracts developed in the Ethereum blockchain system without suffering the
standard issues surrounding reusability, consistency, and automation. A
specific FSPVM, denoted as FSPVM-E, was developed in Coq based on a general,
extensible, and reusable formal memory (GERM) framework, an extensible and
universal formal intermediate programming language, denoted as Lolisa, which is
a large subset of the Solidity programming language that uses generalized
algebraic datatypes, and a corresponding formally verified interpreter for
Lolisa, denoted as FEther, which serves as a crucial component of FSPVM-E.
However, our past work has demonstrated that the execution efficiency of the
standard development of FEther is extremely low. As a result, FSPVM-E fails to
achieve its expected verification effect. The present work addresses this issue
by first identifying three root causes of the low execution efficiency of
formal interpreters. We then build abstract models of these causes, and present
respective optimization schemes for rectifying the identified conditions.
Finally, we apply these optimization schemes to FEther, and demonstrate that
its execution efficiency has been improved significantly.
",Computer Science,Computer Science
"Hydra: An Accelerator for Real-Time Edge-Aware Permeability Filtering in 65nm CMOS   Many modern video processing pipelines rely on edge-aware (EA) filtering
methods. However, recent high-quality methods are challenging to run in
real-time on embedded hardware due to their computational load. To this end, we
propose an area-efficient and real-time capable hardware implementation of a
high quality EA method. In particular, we focus on the recently proposed
permeability filter (PF) that delivers promising quality and performance in the
domains of HDR tone mapping, disparity and optical flow estimation. We present
an efficient hardware accelerator that implements a tiled variant of the PF
with low on-chip memory requirements and a significantly reduced external
memory bandwidth (6.4x w.r.t. the non-tiled PF). The design has been taped out
in 65 nm CMOS technology, is able to filter 720p grayscale video at 24.8 Hz and
achieves a high compute density of 6.7 GFLOPS/mm2 (12x higher than embedded
GPUs when scaled to the same technology node). The low area and bandwidth
requirements make the accelerator highly suitable for integration into SoCs
where silicon area budget is constrained and external memory is typically a
heavily contended resource.
",Computer Science,Computer Science
"Suppression of Hall number due to charge density wave order in high-$T_c$ cuprates   Understanding the pseudogap phase in hole-doped high temperature cuprate
superconductors remains a central challenge in condensed matter physics. From a
host of recent experiments there is now compelling evidence of translational
symmetry breaking charge density wave (CDW) order in a wide range of doping
inside this phase. Two distinct types of incommensurate charge order --
bidirectional at zero or low magnetic fields and unidirectional at high
magnetic fields close to the upper critical field $H_{c2}$ -- have been
reported so far in approximately the same doping range between $p\simeq 0.08$
and $p\simeq 0.16$. In concurrent developments, recent high field Hall
experiments have also revealed two indirect but striking signatures of Fermi
surface reconstruction in the pseudogap phase, namely, a sign change of the
Hall coefficient to negative values at low temperatures at intermediate range
of hole doping and a rapid suppression of the positive Hall number without
change in sign near optimal doping $p \sim 0.19$. We show that the assumption
of a unidirectional incommensurate CDW (with or without a coexisting weak
bidirectional order) at high magnetic fields near optimal doping and a
coexistence of both types of orders of approximately equal magnitude at high
magnetic fields at intermediate range of doping may help explain the striking
behavior of low temperature Hall effect in the entire pseudogap phase.
",Physics,Physics
"Field dependent neutron diffraction study in Ni50Mn38Sb12 Heusler alloy   In this paper, we present temperature and field dependent neutron diffraction
(ND) study to unravel the structural and the magnetic properties in
Ni50Mn38Sb12 Heusler system. This alloy shows martensitic transition from high
temperature austenite cubic phase to low temperature martensite orthorhombic
phase on cooling. At 3 K, the lattice parameters and magnetic moments are found
to be almost insensitive to field. Just below the martensitic transition
temperature, the martensite phase fraction is found to be 85%. Upon applying
the field, the austenite phase becomes dominant, and the field induced reverse
martensitic transition is clearly observed in the ND data. Therefore, the
present study gives an estimate of the strength of the martensite phase or the
sharpness of the martensitic transition. Variation of individual moments and
the change in the phase fraction obtained from the analysis of the ND data
vividly show the change in the magneto-structural state of the material across
the transition.
",Physics,Physics
"Nontrivial Turmites are Turing-universal   A Turmit is a Turing machine that works over a two-dimensional grid, that is,
an agent that moves, reads and writes symbols over the cells of the grid. Its
state is an arrow and, depending on the symbol that it reads, it turns to the
left or to the right, switching the symbol at the same time. Several symbols
are admitted, and the rule is specified by the turning sense that the machine
has over each symbol. Turmites are a generalization of Langtons ant, and they
present very complex and diverse behaviors. We prove that any Turmite, except
for those whose rule does not depend on the symbol, can simulate any Turing
Machine. We also prove the P-completeness of prediction their future behavior
by explicitly giving a log-space reduction from the Topological Circuit Value
Problem. A similar result was already established for Langtons ant; here we use
a similar technique but prove a stronger notion of simulation, and for a more
general family.
",Computer Science; Physics,Computer Science; Mathematics
"Ro-vibrational states of H$_2^+$. Variational calculations   The nonrelativistic variational calculation of a complete set of
ro-vibrational states in the H$_2^+$ molecular ion supported by the ground
$1s\sigma$ adiabatic potential is presented. It includes both bound states and
resonances located above the $n=1$ threshold. In the latter case we also
evaluate a predissociation width of a state wherever it is significant.
Relativistic and radiative corrections are discussed and effective adiabatic
potentials of these corrections are included as supplementary files.
",Physics,Physics
"RDMAvisor: Toward Deploying Scalable and Simple RDMA as a Service in Datacenters   RDMA is increasingly adopted by cloud computing platforms to provide low CPU
overhead, low latency, high throughput network services. On the other hand,
however, it is still challenging for developers to realize fast deployment of
RDMA-aware applications in the datacenter, since the performance is highly
related to many lowlevel details of RDMA operations. To address this problem,
we present a simple and scalable RDMA as Service (RaaS) to mitigate the impact
of RDMA operational details. RaaS provides careful message buffer management to
improve CPU/memory utilization and improve the scalability of RDMA operations.
These optimized designs lead to simple and flexible programming model for
common and knowledgeable users. We have implemented a prototype of RaaS, named
RDMAvisor, and evaluated its performance on a cluster with a large number of
connections. Our experiment results demonstrate that RDMAvisor achieves high
throughput for thousand of connections and maintains low CPU and memory
overhead through adaptive RDMA transport selection.
",Computer Science,Computer Science
"Life in the ""Matrix"": Human Mobility Patterns in the Cyber Space   With the wide adoption of the multi-community setting in many popular social
media platforms, the increasing user engagements across multiple online
communities warrant research attention. In this paper, we introduce a novel
analogy between the movements in the cyber space and the physical space. This
analogy implies a new way of studying human online activities by modelling the
activities across online communities in a similar fashion as the movements
among locations. First, we quantitatively validate the analogy by comparing
several important properties of human online activities and physical movements.
Our experiments reveal striking similarities between the cyber space and the
physical space. Next, inspired by the established methodology on human mobility
in the physical space, we propose a framework to study human ""mobility"" across
online platforms. We discover three interesting patterns of user engagements in
online communities. Furthermore, our experiments indicate that people with
different mobility patterns also exhibit divergent preferences to online
communities. This work not only attempts to achieve a better understanding of
human online activities, but also intends to open a promising research
direction with rich implications and applications.
",Computer Science,Computer Science
"Annealed Generative Adversarial Networks   We introduce a novel framework for adversarial training where the target
distribution is annealed between the uniform distribution and the data
distribution. We posited a conjecture that learning under continuous annealing
in the nonparametric regime is stable irrespective of the divergence measures
in the objective function and proposed an algorithm, dubbed {\ss}-GAN, in
corollary. In this framework, the fact that the initial support of the
generative network is the whole ambient space combined with annealing are key
to balancing the minimax game. In our experiments on synthetic data, MNIST, and
CelebA, {\ss}-GAN with a fixed annealing schedule was stable and did not suffer
from mode collapse.
",Computer Science; Statistics,Computer Science; Statistics
"Polishness of some topologies related to word or tree automata   We prove that the Büchi topology and the automatic topology are Polish. We
also show that this cannot be fully extended to the case of a space of infinite
labelled binary trees; in particular the Büchi and the Muller topologies are
not Polish in this case.
",Computer Science; Mathematics,Computer Science; Mathematics
"Knowledge Reuse for Customization: Metamodels in an Open Design Community for 3d Printing   Theories of knowledge reuse posit two distinct processes: reuse for
replication and reuse for innovation. We identify another distinct process,
reuse for customization. Reuse for customization is a process in which
designers manipulate the parameters of metamodels to produce models that
fulfill their personal needs. We test hypotheses about reuse for customization
in Thingiverse, a community of designers that shares files for
three-dimensional printing. 3D metamodels are reused more often than the 3D
models they generate. The reuse of metamodels is amplified when the metamodels
are created by designers with greater community experience. Metamodels make the
community's design knowledge available for reuse for customization-or further
extension of the metamodels, a kind of reuse for innovation.
",Computer Science,Computer Science
"Coupled spin-charge dynamics in helical Fermi liquids beyond the random phase approximation   We consider a helical system of fermions with a generic spin (or pseudospin)
orbit coupling. Using the equation of motion approach for the single-particle
distribution functions, and a mean-field decoupling of the higher order
distribution functions, we find a closed form for the charge and spin density
fluctuations in terms of the charge and spin density linear response functions.
Approximating the nonlocal exchange term with a Hubbard-like local-field
factor, we obtain coupled spin and charge density response matrix beyond the
random phase approximation, whose poles give the dispersion of four collective
spin-charge modes. We apply our generic technique to the well-explored
two-dimensional system with Rashba spin-orbit coupling and illustrate how it
gives results for the collective modes, Drude weight, and spin-Hall
conductivity which are in very good agreement with the results obtained from
other more sophisticated approaches.
",Physics,Physics
"Unsupervised robust nonparametric learning of hidden community properties   We consider learning of fundamental properties of communities in large noisy
networks, in the prototypical situation where the nodes or users are split into
two classes according to a binary property, e.g., according to their opinions
or preferences on a topic. For learning these properties, we propose a
nonparametric, unsupervised, and scalable graph scan procedure that is, in
addition, robust against a class of powerful adversaries. In our setup, one of
the communities can fall under the influence of a knowledgeable adversarial
leader, who knows the full network structure, has unlimited computational
resources and can completely foresee our planned actions on the network. We
prove strong consistency of our results in this setup with minimal assumptions.
In particular, the learning procedure estimates the baseline activity of normal
users asymptotically correctly with probability 1; the only assumption being
the existence of a single implicit community of asymptotically negligible
logarithmic size. We provide experiments on real and synthetic data to
illustrate the performance of our method, including examples with adversaries.
",Computer Science; Statistics,Computer Science; Statistics
"Low Rank Matrix Recovery with Simultaneous Presence of Outliers and Sparse Corruption   We study a data model in which the data matrix D can be expressed as D = L +
S + C, where L is a low rank matrix, S an element-wise sparse matrix and C a
matrix whose non-zero columns are outlying data points. To date, robust PCA
algorithms have solely considered models with either S or C, but not both. As
such, existing algorithms cannot account for simultaneous element-wise and
column-wise corruptions. In this paper, a new robust PCA algorithm that is
robust to simultaneous types of corruption is proposed. Our approach hinges on
the sparse approximation of a sparsely corrupted column so that the sparse
expansion of a column with respect to the other data points is used to
distinguish a sparsely corrupted inlier column from an outlying data point. We
also develop a randomized design which provides a scalable implementation of
the proposed approach. The core idea of sparse approximation is analyzed
analytically where we show that the underlying ell_1-norm minimization can
obtain the representation of an inlier in presence of sparse corruptions.
",Computer Science; Statistics,Computer Science
"Recovering piecewise constant refractive indices by a single far-field pattern   We are concerned with the inverse scattering problem of recovering an
inhomogeneous medium by the associated acoustic wave measurement. We prove that
under certain assumptions, a single far-field pattern determines the values of
a perturbation to the refractive index on the corners of its support. These
assumptions are satisfied for example in the low acoustic frequency regime. As
a consequence if the perturbation is piecewise constant with either a
polyhedral nest geometry or a known polyhedral cell geometry, such as a pixel
or voxel array, we establish the injectivity of the perturbation to far-field
map given a fixed incident wave. This is the first unique determinancy result
of its type in the literature, and all of the existing results essentially make
use of infinitely many measurements.
",Mathematics,Physics
"Remote Sensing Image Scene Classification: Benchmark and State of the Art   Remote sensing image scene classification plays an important role in a wide
range of applications and hence has been receiving remarkable attention. During
the past years, significant efforts have been made to develop various datasets
or present a variety of approaches for scene classification from remote sensing
images. However, a systematic review of the literature concerning datasets and
methods for scene classification is still lacking. In addition, almost all
existing datasets have a number of limitations, including the small scale of
scene classes and the image numbers, the lack of image variations and
diversity, and the saturation of accuracy. These limitations severely limit the
development of new approaches especially deep learning-based methods. This
paper first provides a comprehensive review of the recent progress. Then, we
propose a large-scale dataset, termed ""NWPU-RESISC45"", which is a publicly
available benchmark for REmote Sensing Image Scene Classification (RESISC),
created by Northwestern Polytechnical University (NWPU). This dataset contains
31,500 images, covering 45 scene classes with 700 images in each class. The
proposed NWPU-RESISC45 (i) is large-scale on the scene classes and the total
image number, (ii) holds big variations in translation, spatial resolution,
viewpoint, object pose, illumination, background, and occlusion, and (iii) has
high within-class diversity and between-class similarity. The creation of this
dataset will enable the community to develop and evaluate various data-driven
algorithms. Finally, several representative methods are evaluated using the
proposed dataset and the results are reported as a useful baseline for future
research.
",Computer Science,Computer Science
"Nearly-Linear Time Spectral Graph Reduction for Scalable Graph Partitioning and Data Visualization   This paper proposes a scalable algorithmic framework for spectral reduction
of large undirected graphs. The proposed method allows computing much smaller
graphs while preserving the key spectral (structural) properties of the
original graph. Our framework is built upon the following two key components: a
spectrum-preserving node aggregation (reduction) scheme, as well as a spectral
graph sparsification framework with iterative edge weight scaling. We show that
the resulting spectrally-reduced graphs can robustly preserve the first few
nontrivial eigenvalues and eigenvectors of the original graph Laplacian. In
addition, the spectral graph reduction method has been leveraged to develop
much faster algorithms for multilevel spectral graph partitioning as well as
t-distributed Stochastic Neighbor Embedding (t-SNE) of large data sets. We
conducted extensive experiments using a variety of large graphs and data sets,
and obtained very promising results. For instance, we are able to reduce the
""coPapersCiteseer"" graph with 0.43 million nodes and 16 million edges to a much
smaller graph with only 13K (32X fewer) nodes and 17K (950X fewer) edges in
about 16 seconds; the spectrally-reduced graphs also allow us to achieve up to
1100X speedup for spectral graph partitioning and up to 60X speedup for t-SNE
visualization of large data sets.
",Computer Science,Computer Science
"Advanced Bayesian Multilevel Modeling with the R Package brms   The brms package allows R users to easily specify a wide range of Bayesian
single-level and multilevel models, which are fitted with the probabilistic
programming language Stan behind the scenes. Several response distributions are
supported, of which all parameters (e.g., location, scale, and shape) can be
predicted at the same time thus allowing for distributional regression.
Non-linear relationships may be specified using non-linear predictor terms or
semi-parametric approaches such as splines or Gaussian processes. To make all
of these modeling options possible in a multilevel framework, brms provides an
intuitive and powerful formula syntax, which extends the well known formula
syntax of lme4. The purpose of the present paper is to introduce this syntax in
detail and to demonstrate its usefulness with four examples, each showing other
relevant aspects of the syntax.
",Statistics,Computer Science; Statistics
"Semi-Supervised Learning via New Deep Network Inversion   We exploit a recently derived inversion scheme for arbitrary deep neural
networks to develop a new semi-supervised learning framework that applies to a
wide range of systems and problems. The approach outperforms current
state-of-the-art methods on MNIST reaching $99.14\%$ of test set accuracy while
using $5$ labeled examples per class. Experiments with one-dimensional signals
highlight the generality of the method. Importantly, our approach is simple,
efficient, and requires no change in the deep network architecture.
",Computer Science; Statistics,Computer Science; Statistics
"Coherence for braided and symmetric pseudomonoids   Presentations for unbraided, braided and symmetric pseudomonoids are defined.
Biequivalences characterising the semistrict bicategories generated by these
presentations are proven. It is shown that these biequivalences categorify
results in the theory of monoids and commutative monoids, and generalise
standard coherence theorems for braided and symmetric monoidal categories.
",Computer Science; Mathematics,Mathematics
"Dynamics beyond dynamic jam; unfolding the Painlevé paradox singularity   This paper analyses in detail the dynamics in a neighbourhood of a
Génot-Brogliato point, colloquially termed the G-spot, which physically
represents so-called dynamic jam in rigid body mechanics with unilateral
contact and Coulomb friction. Such singular points arise in planar rigid body
problems with slipping point contacts at the intersection between the
conditions for onset of lift-off and for the Painlevé paradox. The G-spot can
be approached in finite time by an open set of initial conditions in a general
class of problems. The key question addressed is what happens next. In
principle trajectories could, at least instantaneously, lift off, continue in
slip, or undergo a so-called impact without collision. Such impacts are
non-local in momentum space and depend on properties evaluated away from the
G-spot. The results are illustrated on a particular physical example, namely
the a frictional impact oscillator first studied by Leine et al.
The answer is obtained via an analysis that involves a consistent contact
regularisation with a stiffness proportional to $1/\varepsilon^2$. Taking a
singular limit as $\varepsilon \to 0$, one finds an inner and an outer
asymptotic zone in the neighbourhood of the G-spot. Two distinct cases are
found according to whether the contact force becomes infinite or remains finite
as the G-spot is approached. In the former case it is argued that there can be
no such canards and so an impact without collision must occur. In the latter
case, the canard trajectory acts as a dividing surface between trajectories
that momentarily lift off and those that do not before taking the impact. The
orientation of the initial condition set leading to each eventuality is shown
to change each time a certain positive parameter $\beta$ passes through an
integer.
",Physics,Physics
"Sharp total variation results for maximal functions   In this article, we prove some total variation inequalities for maximal
functions. Our results deal with two possible generalizations of the results
contained in Aldaz and Pérez Lázaro's work, one of whose considers a
variable truncation of the maximal function, and the other one interpolates the
centered and the uncentered maximal functions. In both contexts, we find sharp
constants for the desired inequalities, which can be viewed as progress towards
the conjecture that the best constant for the variation inequality in the
centered context is one. We also provide counterexamples showing that our
methods do not apply outside the stated parameter ranges.
",Mathematics,Mathematics
"On the composition of an arbitrary collection of $SU(2)$ spins: An Enumerative Combinatoric Approach   The whole enterprise of spin compositions can be recast as simple enumerative
combinatoric problems. We show here that enumerative combinatorics
(EC)\citep{book:Stanley-2011} is a natural setting for spin composition, and
easily leads to very general analytic formulae -- many of which hitherto not
present in the literature. Based on it, we propose three general methods for
computing spin multiplicities; namely, 1) the multi-restricted composition, 2)
the generalized binomial and 3) the generating function methods. Symmetric and
anti-symmetric compositions of $SU(2)$ spins are also discussed, using
generating functions. Of particular importance is the observation that while
the common Clebsch-Gordan decomposition (CGD) -- which considers the spins as
distinguishable -- is related to integer compositions, the symmetric and
anti-symmetric compositions (where one considers the spins as
indistinguishable) are obtained considering integer partitions. The integers in
question here are none other but the occupation numbers of the
Holstein-Primakoff bosons.
\par The pervasiveness of $q-$analogues in our approach is a testament to the
fundamental role they play in spin compositions. In the appendix, some new
results in the power series representation of Gaussian polynomials (or
$q-$binomial coefficients) -- relevant to symmetric and antisymmetric
compositions -- are presented.
",Computer Science; Mathematics,Mathematics
"Probabilistic learning of nonlinear dynamical systems using sequential Monte Carlo   Probabilistic modeling provides the capability to represent and manipulate
uncertainty in data, models, predictions and decisions. We are concerned with
the problem of learning probabilistic models of dynamical systems from measured
data. Specifically, we consider learning of probabilistic nonlinear state-space
models. There is no closed-form solution available for this problem, implying
that we are forced to use approximations. In this tutorial we will provide a
self-contained introduction to one of the state-of-the-art methods---the
particle Metropolis--Hastings algorithm---which has proven to offer a practical
approximation. This is a Monte Carlo based method, where the particle filter is
used to guide a Markov chain Monte Carlo method through the parameter space.
One of the key merits of the particle Metropolis--Hastings algorithm is that it
is guaranteed to converge to the ""true solution"" under mild assumptions,
despite being based on a particle filter with only a finite number of
particles. We will also provide a motivating numerical example illustrating the
method using a modeling language tailored for sequential Monte Carlo methods.
The intention of modeling languages of this kind is to open up the power of
sophisticated Monte Carlo methods---including particle
Metropolis--Hastings---to a large group of users without requiring them to know
all the underlying mathematical details.
",Computer Science; Statistics,Computer Science; Statistics
"Mobility tensor of a sphere moving on a super-hydrophobic wall: application to particle separation   The paper addresses the hydrodynamic behavior of a sphere close to a
micro-patterned superhydrophobic surface described in terms of alternated
no-slip and perfect-slip stripes. Physically, the perfect-slip stripes model
the parallel grooves where a large gas cushion forms between fluid and solid
wall, giving rise to slippage at the gas-liquid interface. The potential of the
boundary element method (BEM) in dealing with mixed no-slip/perfect-slip
boundary conditions is exploited to systematically calculate the mobility
tensor for different particle-to-wall relative positions and for different
particle radii. The particle hydrodynamics is characterized by a non trivial
mobility field which presents a distinct near wall behavior where the wall
patterning directly affects the particle motion. In the far field, the effects
of the wall pattern can be accurately represented via an effective description
in terms of a homogeneous wall with a suitably defined apparent slippage. The
trajectory of the sphere under the action of an external force is also
described in some detail. A resonant regime is found when the frequency of the
transversal component of the force matches a characteristic crossing frequency
imposed by the wall pattern. It is found that, under resonance, the particle
undergoes a mean transversal drift. Since the resonance condition depends on
the particle radius the effect can in principle be used to conceive devices for
particle sorting based on superhydrophobic surfaces.
",Physics,Physics
"Variational characterization of H^p   In this paper we obtain the variational characterization of Hardy space $H^p$
for $p\in(\frac n{n+1},1]$ and get estimates for the oscillation operator and
the $\lambda$-jump operator associated with approximate identities acting on
$H^p$ for $p\in(\frac n{n+1},1]$. Moreover, we give counterexamples to show
that the oscillation and $\lambda$-jump associated with some approximate
identity can not be used to characterize $H^p$ for $p\in(\frac n{n+1},1]$.
",Mathematics,Mathematics
"Learning Task-Oriented Grasping for Tool Manipulation from Simulated Self-Supervision   Tool manipulation is vital for facilitating robots to complete challenging
task goals. It requires reasoning about the desired effect of the task and thus
properly grasping and manipulating the tool to achieve the task. Task-agnostic
grasping optimizes for grasp robustness while ignoring crucial task-specific
constraints. In this paper, we propose the Task-Oriented Grasping Network
(TOG-Net) to jointly optimize both task-oriented grasping of a tool and the
manipulation policy for that tool. The training process of the model is based
on large-scale simulated self-supervision with procedurally generated tool
objects. We perform both simulated and real-world experiments on two tool-based
manipulation tasks: sweeping and hammering. Our model achieves overall 71.1%
task success rate for sweeping and 80.0% task success rate for hammering.
Supplementary material is available at: bit.ly/task-oriented-grasp
",Computer Science; Statistics,Computer Science
"A central limit theorem for the realised covariation of a bivariate Brownian semistationary process   This article presents a weak law of large numbers and a central limit theorem
for the scaled realised covariation of a bivariate Brownian semistationary
process. The novelty of our results lies in the fact that we derive the
suitable asymptotic theory both in a multivariate setting and outside the
classical semimartingale framework. The proofs rely heavily on recent
developments in Malliavin calculus.
",Mathematics; Statistics,Mathematics
"On Multilevel Coding Schemes Based on Non-Binary LDPC Codes   We address the problem of constructing of coding schemes for the channels
with high-order modulations. It is known, that non-binary LDPC codes are
especially good for such channels and significantly outperform their binary
counterparts. Unfortunately, their decoding complexity is still large. In order
to reduce the decoding complexity we consider multilevel coding schemes based
on non-binary LDPC codes (NB-LDPC-MLC schemes) over smaller fields. The use of
such schemes gives us a reasonable gain in complexity. At the same time the
performance of NB-LDPC-MLC schemes is practically the same as the performance
of LDPC codes over the field matching the modulation order. In particular by
means of simulations we showed that the performance of NB-LDPC-MLC schemes over
GF(16) is the same as the performance of non-binary LDPC codes over GF(64) and
GF(256) in AWGN channel with QAM64 and QAM256 accordingly. We also perform a
comparison with binary LDPC codes.
",Computer Science,Computer Science
"Wild theories with o-minimal open core   Let $T$ be a consistent o-minimal theory extending the theory of densely
ordered groups and let $T'$ be a consistent theory. Then there is a complete
theory $T^*$ extending $T$ such that $T$ is an open core of $T^*$, but every
model of $T^*$ interprets a model of $T'$. If $T'$ is NIP, $T^*$ can be chosen
to be NIP as well. From this we deduce the existence of an NIP expansion of the
real field that has no distal expansion.
",Mathematics,Mathematics
"The careless use of language in quantum information   An imperative aspect of modern science is that scientific institutions act
for the benefit of a common scientific enterprise, rather than for the personal
gain of individuals within them. This implies that science should not
perpetuate existing or historical unequal social orders. Some scientific
terminology, though, gives a very different impression. I will give two
examples of terminology invented recently for the field of quantum information
which use language associated with subordination, slavery, and racial
segregation: 'ancilla qubit' and 'quantum supremacy'.
",Physics,Computer Science; Mathematics
"Monte-Carlo Tree Search by Best Arm Identification   Recent advances in bandit tools and techniques for sequential learning are
steadily enabling new applications and are promising the resolution of a range
of challenging related problems. We study the game tree search problem, where
the goal is to quickly identify the optimal move in a given game tree by
sequentially sampling its stochastic payoffs. We develop new algorithms for
trees of arbitrary depth, that operate by summarizing all deeper levels of the
tree into confidence intervals at depth one, and applying a best arm
identification procedure at the root. We prove new sample complexity guarantees
with a refined dependence on the problem instance. We show experimentally that
our algorithms outperform existing elimination-based algorithms and match
previous special-purpose methods for depth-two trees.
",Computer Science; Statistics,Computer Science; Statistics
"Panchromatic Hubble Andromeda Treasury XVIII. The High-mass Truncation of the Star Cluster Mass Function   We measure the mass function for a sample of 840 young star clusters with
ages between 10-300 Myr observed by the Panchromatic Hubble Andromeda Treasury
(PHAT) survey in M31. The data show clear evidence of a high-mass truncation:
only 15 clusters more massive than $10^4$ $M_{\odot}$ are observed, compared to
$\sim$100 expected for a canonical $M^{-2}$ pure power-law mass function with
the same total number of clusters above the catalog completeness limit.
Adopting a Schechter function parameterization, we fit a characteristic
truncation mass of $M_c = 8.5^{+2.8}_{-1.8} \times 10^3$ $M_{\odot}$. While
previous studies have measured cluster mass function truncations, the
characteristic truncation mass we measure is the lowest ever reported.
Combining this M31 measurement with previous results, we find that the cluster
mass function truncation correlates strongly with the characteristic star
formation rate surface density of the host galaxy, where $M_c \propto$ $\langle
\Sigma_{\mathrm{SFR}} \rangle^{\sim1.1}$. We also find evidence that suggests
the observed $M_c$-$\Sigma_{\mathrm{SFR}}$ relation also applies to globular
clusters, linking the two populations via a common formation pathway. If so,
globular cluster mass functions could be useful tools for constraining the star
formation properties of their progenitor host galaxies in the early Universe.
",Physics,Physics
"Weighted blowup correspondence of orbifold Gromov--Witten invariants and applications   Let $\sf X$ be a symplectic orbifold groupoid with $\sf S$ being a symplectic
sub-orbifold groupoid, and $\sf X_{\mathfrak a}$ be the weight-$\mathfrak a$
blowup of $\sf X$ along $\sf S$ with $\sf Z$ being the corresponding
exceptional divisor. We show that there is a weighted blowup correspondence
between some certain absolute orbifold Gromov--Witten invariants of $\sf X$
relative to $\sf S$ and some certain relative orbifold Gromov--Witten
invariants of the pair $(\sf X_{\mathfrak a}|Z)$. As an application, we prove
that the symplectic uniruledness of symplectic orbifold groupoids is a weighted
blowup invariant.
",Mathematics,Mathematics
"Information Pursuit: A Bayesian Framework for Sequential Scene Parsing   Despite enormous progress in object detection and classification, the problem
of incorporating expected contextual relationships among object instances into
modern recognition systems remains a key challenge. In this work we propose
Information Pursuit, a Bayesian framework for scene parsing that combines prior
models for the geometry of the scene and the spatial arrangement of objects
instances with a data model for the output of high-level image classifiers
trained to answer specific questions about the scene. In the proposed
framework, the scene interpretation is progressively refined as evidence
accumulates from the answers to a sequence of questions. At each step, we
choose the question to maximize the mutual information between the new answer
and the full interpretation given the current evidence obtained from previous
inquiries. We also propose a method for learning the parameters of the model
from synthesized, annotated scenes obtained by top-down sampling from an
easy-to-learn generative scene model. Finally, we introduce a database of
annotated indoor scenes of dining room tables, which we use to evaluate the
proposed approach.
",Computer Science; Statistics,Computer Science
"Characterization of a Deuterium-Deuterium Plasma Fusion Neutron Generator   We characterize the neutron output of a deuterium-deuterium plasma fusion
neutron generator, model 35-DD-W-S, manufactured by NSD/Gradel-Fusion. The
measured energy spectrum is found to be dominated by neutron peaks at 2.2 MeV
and 2.7 MeV. A detailed GEANT4 simulation accurately reproduces the measured
energy spectrum and confirms our understanding of the fusion process in this
generator. Additionally, a contribution of 14.1 MeV neutrons from
deuterium-tritium fusion is found at a level of~$3.5\%$, from tritium produced
in previous deuterium-deuterium reactions. We have measured both the absolute
neutron flux as well as its relative variation on the operational parameters of
the generator. We find the flux to be proportional to voltage $V^{3.32 \pm
0.14}$ and current $I^{0.97 \pm 0.01}$. Further, we have measured the angular
dependence of the neutron emission with respect to the polar angle. We conclude
that it is well described by isotropic production of neutrons within the
cathode field cage.
",Physics,Physics
"Prevalence of DNSSEC for hospital websites in Illinois   The domain name system translates human friendly web addresses to a computer
readable internet protocol address. This basic infrastructure is insecure and
can be manipulated. Deployment of technology to secure the DNS system has been
slow, reaching about 20% of all web sites based in the USA. Little is known
about the efforts hospitals and health systems make to secure the domain name
system for their websites. To investigate the prevalence of implementing Domain
Name System Security Extensions (DNSSEC), we analyzed the websites of the 210
public hospitals in the state of Illinois, USA. Only one Illinois hospital
website was found to have implemented DNSSEC by December, 2017.
",Computer Science,Computer Science
"A Deep Neural Network Surrogate for High-Dimensional Random Partial Differential Equations   Developing efficient numerical algorithms for the solution of high
dimensional random Partial Differential Equations (PDEs) has been a challenging
task due to the well-known curse of dimensionality. We present a new solution
framework for these problems based on a deep learning approach. Specifically,
the random PDE is approximated by a feed-forward fully-connected deep residual
network, with either strong or weak enforcement of initial and boundary
constraints. The framework is mesh-free, and can handle irregular computational
domains. Parameters of the approximating deep neural network are determined
iteratively using variants of the Stochastic Gradient Descent (SGD) algorithm.
The satisfactory accuracy of the proposed frameworks is numerically
demonstrated on diffusion and heat conduction problems, in comparison with the
converged Monte Carlo-based finite element results.
",Computer Science; Statistics,Computer Science; Statistics
"Stellar Abundances for Galactic Archaeology Database IV - Compilation of Stars in Dwarf Galaxies   We have constructed the database of stars in the local group using the
extended version of the SAGA (Stellar Abundances for Galactic Archaeology)
database that contains stars in 24 dwarf spheroidal galaxies and ultra faint
dwarfs. The new version of the database includes more than 4500 stars in the
Milky Way, by removing the previous metallicity criterion of [Fe/H] <= -2.5,
and more than 6000 stars in the local group galaxies. We examined a validity of
using a combined data set for elemental abundances. We also checked a
consistency between the derived distances to individual stars and those to
galaxies in the literature values. Using the updated database, the
characteristics of stars in dwarf galaxies are discussed. Our statistical
analyses of alpha-element abundances show that the change of the slope of the
[alpha/Fe] relative to [Fe/H] (so-called ""knee"") occurs at [Fe/H] = -1.0+-0.1
for the Milky Way. The knee positions for selected galaxies are derived by
applying the same method. Star formation history of individual galaxies are
explored using the slope of the cumulative metallicity distribution function.
Radial gradients along the four directions are inspected in six galaxies where
we find no direction dependence of metallicity gradients along the major and
minor axes. The compilation of all the available data shows a lack of CEMP-s
population in dwarf galaxies, while there may be some CEMP-no stars at [Fe/H]
<~ -3 even in the very small sample. The inspection of the relationship between
Eu and Ba abundances confirms an anomalously Ba-rich population in Fornax,
which indicates a pre-enrichment of interstellar gas with r-process elements.
We do not find any evidence of anti-correlations in O-Na and Mg-Al abundances,
which characterises the abundance trends in the Galactic globular clusters.
",Physics,Physics
"Existence of infinite Viterbi path for pairwise Markov models   For hidden Markov models one of the most popular estimates of the hidden
chain is the Viterbi path -- the path maximising the posterior probability. We
consider a more general setting, called the pairwise Markov model, where the
joint process consisting of finite-state hidden regime and observation process
is assumed to be a Markov chain. We prove that under some conditions it is
possible to extend the Viterbi path to infinity for almost every observation
sequence which in turn enables to define an infinite Viterbi decoding of the
observation process, called the Viterbi process. This is done by constructing a
block of observations, called a barrier, which ensures that the Viterbi path
goes trough a given state whenever this block occurs in the observation
sequence.
",Mathematics; Statistics,Mathematics; Statistics
"Simulating optical coherence tomography for observing nerve activity: a finite difference time domain bi-dimensional model   We present a finite difference time domain (FDTD) model for computation of A
line scans in time domain optical coherence tomography (OCT). By simulating
only the end of the two arms of the interferometer and computing the
interference signal in post processing, it is possible to reduce the
computational time required by the simulations and, thus, to simulate much
bigger environments. Moreover, it is possible to simulate successive A lines
and thus obtaining a cross section of the sample considered. In this paper we
present the model applied to two different samples: a glass rod filled with
water-sucrose solution at different concentrations and a peripheral nerve. This
work demonstrates the feasibility of using OCT for non-invasive, direct optical
monitoring of peripheral nerve activity, which is a long-sought goal of
neuroscience.
",Physics,Physics
"Training Neural Networks as Learning Data-adaptive Kernels: Provable Representation and Approximation Benefits   Consider the problem: given data pair $(\mathbf{x}, \mathbf{y})$ drawn from a
population with $f_*(x) = \mathbf{E}[\mathbf{y} | \mathbf{x} = x]$, specify a
neural network and run gradient flow on the weights over time until reaching
any stationarity. How does $f_t$, the function computed by the neural network
at time $t$, relate to $f_*$, in terms of approximation and representation?
What are the provable benefits of the adaptive representation by neural
networks compared to the pre-specified fixed basis representation in the
classical nonparametric literature? We answer the above questions via a dynamic
reproducing kernel Hilbert space (RKHS) approach indexed by the training
process of neural networks. We show that when reaching any local stationarity,
gradient flow learns an adaptive RKHS representation, and performs the global
least squares projection onto the adaptive RKHS, simultaneously. In addition,
we prove that as the RKHS is data-adaptive and task-specific, the residual for
$f_*$ lies in a subspace that is smaller than the orthogonal complement of the
RKHS, formalizing the representation and approximation benefits of neural
networks.
",Computer Science; Mathematics; Statistics,Computer Science; Statistics
"Polymorphism and the obstinate circularity of second order logic: a victims' tale   The investigations on higher-order type theories and on the related notion of
parametric polymorphism constitute the technical counterpart of the old
foundational problem of the circularity (or impredicativity) of second and
higher order logic. However, the epistemological significance of such
investigations, and of their often non trivial results, has not received much
attention in the contemporary foundational debate. The results recalled in this
paper suggest that the question of the circularity of second order logic cannot
be reduced to the simple assessment of a vicious circle. Through a comparison
between the faulty consistency arguments given by Frege and Martin-Löf,
respectively for the logical system of the Grundgesetze (shown inconsistent by
Russell's paradox) and for the intuitionistic type theory with a type of all
types (shown inconsistent by Girard's paradox), and the normalization argument
for second order type theory (or System F), we indicate a bunch of subtle
mathematical problems and logical concepts hidden behind the hazardous idea of
impredicative quantification, constituting a vast (and largely unexplored)
domain for foundational research.
",Computer Science; Mathematics,Mathematics
"Document Retrieval for Large Scale Content Analysis using Contextualized Dictionaries   This paper presents a procedure to retrieve subsets of relevant documents
from large text collections for Content Analysis, e.g. in social sciences.
Document retrieval for this purpose needs to take account of the fact that
analysts often cannot describe their research objective with a small set of key
terms, especially when dealing with theoretical or rather abstract research
interests. Instead, it is much easier to define a set of paradigmatic documents
which reflect topics of interest as well as targeted manner of speech. Thus, in
contrast to classic information retrieval tasks we employ manually compiled
collections of reference documents to compose large queries of several hundred
key terms, called dictionaries. We extract dictionaries via Topic Models and
also use co-occurrence data from reference collections. Evaluations show that
the procedure improves retrieval results for this purpose compared to
alternative methods of key term extraction as well as neglecting co-occurrence
data.
",Computer Science,Computer Science
"Deorbitalization strategies for meta-GGA exchange-correlation functionals   We explore the simplification of widely used meta-generalized-gradient
approximation (mGGA) exchange-correlation functionals to the Laplacian level of
refinement by use of approximate kinetic energy density functionals (KEDFs).
Such deorbitalization is motivated by the prospect of reducing computational
cost while recovering a strictly Kohn-Sham local potential framework (rather
than the usual generalized Kohn-Sham treatment of mGGAs). A KEDF that has been
rather successful in solid simulations proves to be inadequate for
deorbitalization but we produce other forms which, with parametrization to
Kohn-Sham results (not experimental data) on a small training set, yield rather
good results on standard molecular test sets when used to deorbitalize the
meta-GGA made very simple, TPSS, and SCAN functionals. We also study the
difference between high-fidelity and best-performing deorbitalizations and
discuss possible implications for use in ab initio molecular dynamics
simulations of complicated condensed phase systems.
",Physics,Physics
"Fréchet Analysis Of Variance For Random Objects   Fréchet mean and variance provide a way of obtaining mean and variance for
general metric space valued random variables and can be used for statistical
analysis of data objects that lie in abstract spaces devoid of algebraic
structure and operations. Examples of such spaces include covariance matrices,
graph Laplacians of networks and univariate probability distribution functions.
We derive a central limit theorem for Fréchet variance under mild regularity
conditions, utilizing empirical process theory, and also provide a consistent
estimator of the asymptotic variance. These results lead to a test to compare k
populations based on Fréchet variance for general metric space valued data
objects, with emphasis on comparing means and variances. We examine the finite
sample performance of this inference procedure through simulation studies for
several special cases that include probability distributions and graph
Laplacians, which leads to tests to compare populations of networks. The
proposed methodology has good finite sample performance in simulations for
different kinds of random objects. We illustrate the proposed methods with data
on mortality profiles of various countries and resting state Functional
Magnetic Resonance Imaging data.
",Mathematics; Statistics,Mathematics; Statistics
"Principal Boundary on Riemannian Manifolds   We revisit the classification problem and focus on nonlinear methods for
classification on manifolds. For multivariate datasets lying on an embedded
nonlinear Riemannian manifold within the higher-dimensional space, our aim is
to acquire a classification boundary between the classes with labels. Motivated
by the principal flow [Panaretos, Pham and Yao, 2014], a curve that moves along
a path of the maximum variation of the data, we introduce the principal
boundary. From the classification perspective, the principal boundary is
defined as an optimal curve that moves in between the principal flows traced
out from two classes of the data, and at any point on the boundary, it
maximizes the margin between the two classes. We estimate the boundary in
quality with its direction supervised by the two principal flows. We show that
the principal boundary yields the usual decision boundary found by the support
vector machine, in the sense that locally, the two boundaries coincide. By
means of examples, we illustrate how to find, use and interpret the principal
boundary.
",Computer Science; Statistics,Computer Science; Statistics
"Spectral determination of semi-regular polygons   Let us say that an $n$-sided polygon is semi-regular if it is
circumscriptible and its angles are all equal but possibly one, which is then
larger than the rest. Regular polygons, in particular, are semi-regular. We
prove that semi-regular polygons are spectrally determined in the class of
convex piecewise smooth domains. Specifically, we show that if $\Omega$ is a
convex piecewise smooth planar domain, possibly with straight corners, whose
Dirichlet or Neumann spectrum coincides with that of an $n$-sided semi-regular
polygon $P_n$, then $\Omega$ is congruent to $P_n$.
",Mathematics,Mathematics
"Lifelong Multi-Agent Path Finding for Online Pickup and Delivery Tasks   The multi-agent path-finding (MAPF) problem has recently received a lot of
attention. However, it does not capture important characteristics of many
real-world domains, such as automated warehouses, where agents are constantly
engaged with new tasks. In this paper, we therefore study a lifelong version of
the MAPF problem, called the multi-agent pickup and delivery (MAPD) problem. In
the MAPD problem, agents have to attend to a stream of delivery tasks in an
online setting. One agent has to be assigned to each delivery task. This agent
has to first move to a given pickup location and then to a given delivery
location while avoiding collisions with other agents. We present two decoupled
MAPD algorithms, Token Passing (TP) and Token Passing with Task Swaps (TPTS).
Theoretically, we show that they solve all well-formed MAPD instances, a
realistic subclass of MAPD instances. Experimentally, we compare them against a
centralized strawman MAPD algorithm without this guarantee in a simulated
warehouse system. TP can easily be extended to a fully distributed MAPD
algorithm and is the best choice when real-time computation is of primary
concern since it remains efficient for MAPD instances with hundreds of agents
and tasks. TPTS requires limited communication among agents and balances well
between TP and the centralized MAPD algorithm.
",Computer Science,Computer Science
"A Data-Driven Approach for Predicting Vegetation-Related Outages in Power Distribution Systems   This paper presents a novel data-driven approach for predicting the number of
vegetation-related outages that occur in power distribution systems on a
monthly basis. In order to develop an approach that is able to successfully
fulfill this objective, there are two main challenges that ought to be
addressed. The first challenge is to define the extent of the target area. An
unsupervised machine learning approach is proposed to overcome this difficulty.
The second challenge is to correctly identify the main causes of
vegetation-related outages and to thoroughly investigate their nature. In this
paper, these outages are categorized into two main groups: growth-related and
weather-related outages, and two types of models, namely time series and
non-linear machine learning regression models are proposed to conduct the
prediction tasks, respectively. Moreover, various features that can explain the
variability in vegetation-related outages are engineered and employed. Actual
outage data, obtained from a major utility in the U.S., in addition to
different types of weather and geographical data are utilized to build the
proposed approach. Finally, a comprehensive case study is carried out to
demonstrate how the proposed approach can be used to successfully predict the
number of vegetation-related outages and to help decision-makers to detect
vulnerable zones in their systems.
",Statistics,Computer Science; Statistics
"On the wave propagation analysis and supratransmission prediction of a metastable modular metastructure for adaptive non-reciprocal energy transmission   In this research, we investigate the nonlinear energy transmission phenomenon
in a reconfigurable and adaptable metastable modular metastructure. Numerical
studies on a 1D metastable chain uncover that when the driving frequency is
within the stopband of the periodic structure, there exists a threshold input
amplitude, beyond which sudden increase in the energy transmission can be
observed. This onset of transmission is due to nonlinear instability and is
known as supratransmission. We show that due to spatial asymmetry of
strategically configured constituents, such transmission thresholds could shift
considerably when the structure is excited from different ends and therefore
enabling the non-reciprocal energy transmission. We discover that the critical
threshold amplitude can be predicted analytically using a localized
nonlinear-linear model combining harmonic balancing and transfer matrix
analyses. Additionally, influences of important parameters on the change of
threshold amplitude are investigated to provide insight on synthesizing systems
with desired non-reciprocal characteristics. These investigations elucidate the
rich and intricate dynamics achievable by nonlinearity, asymmetry, and
metastability, and provide new insights and opportunities to accomplish
adaptable non-reciprocal wave energy transmission.
",Physics,Physics
"The dependence of protostar formation on the geometry and strength of the initial magnetic field   We report results from twelve simulations of the collapse of a molecular
cloud core to form one or more protostars, comprising three field strengths
(mass-to-flux ratios, {\mu}, of 5, 10, and 20) and four field geometries (with
values of the angle between the field and rotation axes, {\theta}, of 0°,
20°, 45°, and 90°), using a smoothed particle
magnetohydrodynamics method. We find that the values of both parameters have a
strong effect on the resultant protostellar system and outflows. This ranges
from the formation of binary systems when {\mu} = 20 to strikingly differing
outflow structures for differing values of {\theta}, in particular highly
suppressed outflows when {\theta} = 90°. Misaligned magnetic fields can
also produce warped pseudo-discs where the outer regions align perpendicular to
the magnetic field but the innermost region re-orientates to be perpendicular
to the rotation axis. We follow the collapse to sizes comparable to those of
first cores and find that none of the outflow speeds exceed 8 km s$^{-1}$.
These results may place constraints on both observed protostellar outflows, and
also on which molecular cloud cores may eventually form either single stars and
binaries: a sufficiently weak magnetic field may allow for disc fragmentation,
whilst conversely the greater angular momentum transport of a strong field may
inhibit disc fragmentation.
",Physics,Physics
"Secure Search on the Cloud via Coresets and Sketches   \emph{Secure Search} is the problem of retrieving from a database table (or
any unsorted array) the records matching specified attributes, as in SQL SELECT
queries, but where the database and the query are encrypted. Secure search has
been the leading example for practical applications of Fully Homomorphic
Encryption (FHE) starting in Gentry's seminal work; however, to the best of our
knowledge all state-of-the-art secure search algorithms to date are realized by
a polynomial of degree $\Omega(m)$ for $m$ the number of records, which is
typically too slow in practice even for moderate size $m$.
In this work we present the first algorithm for secure search that is
realized by a polynomial of degree polynomial in $\log m$. We implemented our
algorithm in an open source library based on HELib implementation for the
Brakerski-Gentry-Vaikuntanthan's FHE scheme, and ran experiments on Amazon's
EC2 cloud. Our experiments show that we can retrieve the first match in a
database of millions of entries in less than an hour using a single machine;
the time reduced almost linearly with the number of machines.
Our result utilizes a new paradigm of employing coresets and sketches, which
are modern data summarization techniques common in computational geometry and
machine learning, for efficiency enhancement for homomorphic encryption. As a
central tool we design a novel sketch that returns the first positive entry in
a (not necessarily sparse) array; this sketch may be of independent interest.
",Computer Science,Computer Science
"Large-Margin Classification in Hyperbolic Space   Representing data in hyperbolic space can effectively capture latent
hierarchical relationships. With the goal of enabling accurate classification
of points in hyperbolic space while respecting their hyperbolic geometry, we
introduce hyperbolic SVM, a hyperbolic formulation of support vector machine
classifiers, and elucidate through new theoretical work its connection to the
Euclidean counterpart. We demonstrate the performance improvement of hyperbolic
SVM for multi-class prediction tasks on real-world complex networks as well as
simulated datasets. Our work allows analytic pipelines that take the inherent
hyperbolic geometry of the data into account in an end-to-end fashion without
resorting to ill-fitting tools developed for Euclidean space.
",Statistics,Statistics
"Doing Things Twice (Or Differently): Strategies to Identify Studies for Targeted Validation   The ""reproducibility crisis"" has been a highly visible source of scientific
controversy and dispute. Here, I propose and review several avenues for
identifying and prioritizing research studies for the purpose of targeted
validation. Of the various proposals discussed, I identify scientific data
science as being a strategy that merits greater attention among those
interested in reproducibility. I argue that the tremendous potential of
scientific data science for uncovering high-value research studies is a
significant and rarely discussed benefit of the transition to a fully
open-access publishing model.
",Computer Science; Physics,Computer Science
"Empirical study on social groups in pedestrian evacuation dynamics   Pedestrian crowds often include social groups, i.e. pedestrians that walk
together because of social relationships. They show characteristic
configurations and influence the dynamics of the entire crowd. In order to
investigate the impact of social groups on evacuations we performed an
empirical study with pupils. Several evacuation runs with groups of different
sizes and different interactions were performed. New group parameters are
introduced which allow to describe the dynamics of the groups and the
configuration of the group members quantitatively. The analysis shows a
possible decrease of evacuation times for large groups due to self-ordering
effects. Social groups can be approximated as ellipses that orientate along
their direction of motion. Furthermore, explicitly cooperative behaviour among
group members leads to a stronger aggregation of group members and an
intermittent way of evacuation.
",Physics,Physics
"Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning   We present Deep Voice 3, a fully-convolutional attention-based neural
text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural
speech synthesis systems in naturalness while training ten times faster. We
scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more
than eight hundred hours of audio from over two thousand speakers. In addition,
we identify common error modes of attention-based speech synthesis networks,
demonstrate how to mitigate them, and compare several different waveform
synthesis methods. We also describe how to scale inference to ten million
queries per day on one single-GPU server.
",Computer Science,Computer Science
"On the Impossibility of Supersized Machines   In recent years, a number of prominent computer scientists, along with
academics in fields such as philosophy and physics, have lent credence to the
notion that machines may one day become as large as humans. Many have further
argued that machines could even come to exceed human size by a significant
margin. However, there are at least seven distinct arguments that preclude this
outcome. We show that it is not only implausible that machines will ever exceed
human size, but in fact impossible.
",Computer Science; Physics,Computer Science
"End-to-End Multi-Task Denoising for joint SDR and PESQ Optimization   Supervised learning based on a deep neural network recently has achieved
substantial improvement on speech enhancement. Denoising networks learn mapping
from noisy speech to clean one directly, or to a spectra mask which is the
ratio between clean and noisy spectrum. In either case, the network is
optimized by minimizing mean square error (MSE) between predefined labels and
network output of spectra or time-domain signal. However, existing schemes have
either of two critical issues: spectra and metric mismatches. The spectra
mismatch is a well known issue that any spectra modification after short-time
Fourier transform (STFT), in general, cannot be fully recovered after inverse
STFT. The metric mismatch is that a conventional MSE metric is sub-optimal to
maximize our target metrics, signal-to-distortion ratio (SDR) and perceptual
evaluation of speech quality (PESQ). This paper presents a new end-to-end
denoising framework with the goal of joint SDR and PESQ optimization. First,
the network optimization is performed on the time-domain signals after ISTFT to
avoid spectra mismatch. Second, two loss functions which have improved
correlations with SDR and PESQ metrics are proposed to minimize metric
mismatch. The experimental result showed that the proposed denoising scheme
significantly improved both SDR and PESQ performance over the existing methods.
",Computer Science; Statistics,Computer Science
"Learning Local Receptive Fields and their Weight Sharing Scheme on Graphs   We propose a simple and generic layer formulation that extends the properties
of convolutional layers to any domain that can be described by a graph. Namely,
we use the support of its adjacency matrix to design learnable weight sharing
filters able to exploit the underlying structure of signals in the same fashion
as for images. The proposed formulation makes it possible to learn the weights
of the filter as well as a scheme that controls how they are shared across the
graph. We perform validation experiments with image datasets and show that
these filters offer performances comparable with convolutional ones.
",Computer Science,Computer Science; Statistics
"A Certified-Complete Bimanual Manipulation Planner   Planning motions for two robot arms to move an object collaboratively is a
difficult problem, mainly because of the closed-chain constraint, which arises
whenever two robot hands simultaneously grasp a single rigid object. In this
paper, we propose a manipulation planning algorithm to bring an object from an
initial stable placement (position and orientation of the object on the support
surface) towards a goal stable placement. The key specificity of our algorithm
is that it is certified-complete: for a given object and a given environment,
we provide a certificate that the algorithm will find a solution to any
bimanual manipulation query in that environment whenever one exists. Moreover,
the certificate is constructive: at run-time, it can be used to quickly find a
solution to a given query. The algorithm is tested in software and hardware on
a number of large pieces of furniture.
",Computer Science,Computer Science
"Orbital-dependent correlations in PuCoGa$_5$   We investigate the normal state of the superconducting compound PuCoGa$_5$
using the combination of density functional theory (DFT) and dynamical mean
field theory (DMFT), with the continuous time quantum Monte Carlo (CTQMC) and
the vertex-corrected one-crossing approximation (OCA) as the impurity solvers.
Our DFT+DMFT(CTQMC) calculations suggest a strong tendency of Pu-5$f$ orbitals
to differentiate at low temperatures. The renormalized 5$f_{5/2}$ states
exhibit a Fermi-liquid behavior whereas one electron in the 5$f_{7/2}$ states
is at the edge of a Mott localization. We find that the orbital differentiation
is manifested as the removing of 5$f_{7/2}$ spectral weight from the Fermi
level relative to DFT. We corroborate these conclusions with DFT+DMFT(OCA)
calculations which demonstrate that 5$f_{5/2}$ electrons have a much larger
Kondo scale than the 5$f_{7/2}$.
",Physics,Physics
"Non-parametric estimation of Jensen-Shannon Divergence in Generative Adversarial Network training   Generative Adversarial Networks (GANs) have become a widely popular framework
for generative modelling of high-dimensional datasets. However their training
is well-known to be difficult. This work presents a rigorous statistical
analysis of GANs providing straight-forward explanations for common training
pathologies such as vanishing gradients. Furthermore, it proposes a new
training objective, Kernel GANs, and demonstrates its practical effectiveness
on large-scale real-world data sets. A key element in the analysis is the
distinction between training with respect to the (unknown) data distribution,
and its empirical counterpart. To overcome issues in GAN training, we pursue
the idea of smoothing the Jensen-Shannon Divergence (JSD) by incorporating
noise in the input distributions of the discriminator. As we show, this
effectively leads to an empirical version of the JSD in which the true and the
generator densities are replaced by kernel density estimates, which leads to
Kernel GANs.
",Statistics,Statistics
"Optimal Service Elasticity in Large-Scale Distributed Systems   A fundamental challenge in large-scale cloud networks and data centers is to
achieve highly efficient server utilization and limit energy consumption, while
providing excellent user-perceived performance in the presence of uncertain and
time-varying demand patterns. Auto-scaling provides a popular paradigm for
automatically adjusting service capacity in response to demand while meeting
performance targets, and queue-driven auto-scaling techniques have been widely
investigated in the literature. In typical data center architectures and cloud
environments however, no centralized queue is maintained, and load balancing
algorithms immediately distribute incoming tasks among parallel queues. In
these distributed settings with vast numbers of servers, centralized
queue-driven auto-scaling techniques involve a substantial communication
overhead and major implementation burden, or may not even be viable at all.
Motivated by the above issues, we propose a joint auto-scaling and load
balancing scheme which does not require any global queue length information or
explicit knowledge of system parameters, and yet provides provably near-optimal
service elasticity. We establish the fluid-level dynamics for the proposed
scheme in a regime where the total traffic volume and nominal service capacity
grow large in proportion. The fluid-limit results show that the proposed scheme
achieves asymptotic optimality in terms of user-perceived delay performance as
well as energy consumption. Specifically, we prove that both the waiting time
of tasks and the relative energy portion consumed by idle servers vanish in the
limit. At the same time, the proposed scheme operates in a distributed fashion
and involves only constant communication overhead per task, thus ensuring
scalability in massive data center operations.
",Computer Science; Mathematics,Computer Science
"Linear-time approximation schemes for planar minimum three-edge connected and three-vertex connected spanning subgraphs   We present the first polynomial-time approximation schemes, i.e., (1 +
{\epsilon})-approximation algorithm for any constant {\epsilon} > 0, for the
minimum three-edge connected spanning subgraph problem and the minimum
three-vertex connected spanning subgraph problem in undirected planar graphs.
Both the approximation schemes run in linear time.
",Computer Science,Computer Science; Mathematics
"Bi-National Delay Pattern Analysis For Commercial and Passenger Vehicles at Niagara Frontier Border   Border crossing delays between New York State and Southern Ontario cause
problems like enormous economic loss and massive environmental pollutions. In
this area, there are three border-crossing ports: Peace Bridge (PB), Rainbow
Bridge (RB) and Lewiston-Queenston Bridge (LQ) at Niagara Frontier border. The
goals of this paper are to figure out whether the distributions of bi-national
wait times for commercial and passenger vehicles are evenly distributed among
the three ports and uncover the hidden significant influential factors that
result in the possible insufficient utilization. The historical border wait
time data from 7:00 to 21:00 between 08/22/2016 and 06/20/2017 are archived, as
well as the corresponding temporal and weather data. For each vehicle type
towards each direction, a Decision Tree is built to identify the various border
delay patterns over the three bridges. We find that for the passenger vehicles
to the USA, the convenient connections between the Canada freeways with USA
I-190 by LQ and PB may cause these two bridges more congested than RB,
especially when it is a holiday in Canada. For the passenger vehicles in the
other bound, RB is much more congested than LQ and PB in some cases, and the
visitors to Niagara Falls in the USA in summer may be a reason. For the
commercial trucks to the USA, the various delay patterns show PB is always more
congested than LQ. Hour interval and weekend are the most significant factors
appearing in all the four Decision Trees. These Decision Trees can help the
authorities to make specific routing suggestions when the corresponding
conditions are satisfied.
",Computer Science,Computer Science
"Construction of curve pairs and their applications   In this study, we introduce a new approach to curve pairs by using integral
curves. We consider the direction curve and donor curve to study curve couples
such as involute-evolute curves, Mannheim partner curves and Bertrand partner
curves. We obtain new methods to construct partner curves of a unit speed curve
and give some applications related to helices, slant helices and plane curves.
",Mathematics,Mathematics
"Methodological Approach for the Design of a Complex Inclusive Human-Machine System   Modern industrial automatic machines and robotic cells are equipped with
highly complex human-machine interfaces (HMIs) that often prevent human
operators from an effective use of the automatic systems. In particular, this
applies to vulnerable users, such as those with low experience or education
level, the elderly and the disabled. To tackle this issue, it becomes necessary
to design user-oriented HMIs, which adapt to the capabilities and skills of
users, thus compensating their limitations and taking full advantage of their
knowledge. In this paper, we propose a methodological approach to the design of
complex adaptive human-machine systems that might be inclusive of all users, in
particular the vulnerable ones. The proposed approach takes into account both
the technical requirements and the requirements for ethical, legal and social
implications (ELSI) for the design of automatic systems. The technical
requirements derive from a thorough analysis of three use cases taken from the
European project INCLUSIVE. To achieve the ELSI requirements, the MEESTAR
approach is combined with the specific legal issues for occupational systems
and requirements of the target users.
",Computer Science,Computer Science
"Asymptotics and Optimal Bandwidth Selection for Nonparametric Estimation of Density Level Sets   Bandwidth selection is crucial in the kernel estimation of density level
sets. Risk based on the symmetric difference between the estimated and true
level sets is usually used to measure their proximity. In this paper we provide
an asymptotic $L^p$ approximation to this risk, where $p$ is characterized by
the weight function in the risk. In particular the excess risk corresponds to
an $L^2$ type of risk, and is adopted in an optimal bandwidth selection rule
for nonparametric level set estimation of $d$-dimensional density functions
($d\geq 1$).
",Mathematics; Statistics,Mathematics; Statistics
"On the compressibility of the transition-metal carbides and nitrides alloys Zr_xNb_{1-x}C and Zr_xNb_{1-x}N   The 4d-transition-metals carbides (ZrC, NbC) and nitrides (ZrN, NbN) in the
rocksalt structure, as well as their ternary alloys, have been recently studied
by means of a first-principles full potential linearized augmented plane waves
method within the local density approximation. These materials are important
because of their interesting mechanical and physical properties, which make
them suitable for many technological applications. Here, by using a simple
theoretical model, we estimate the bulk moduli of their ternary alloys
Zr$_x$Nb$_{1-x}$C and Zr$_x$Nb$_{1-x}$N in terms of the bulk moduli of the end
members alone. The results are comparable to those deduced from the
first-principles calculations.
",Physics,Physics
"Modeling and Control of Humanoid Robots in Dynamic Environments: iCub Balancing on a Seesaw   Forthcoming applications concerning humanoid robots may involve physical
interaction between the robot and a dynamic environment. In such scenario,
classical balancing and walking controllers that neglect the environment
dynamics may not be sufficient for achieving a stable robot behavior. This
paper presents a modeling and control framework for balancing humanoid robots
in contact with a dynamic environment. We first model the robot and environment
dynamics, together with the contact constraints. Then, a control strategy for
stabilizing the full system is proposed. Theoretical results are verified in
simulation with robot iCub balancing on a seesaw.
",Computer Science,Computer Science
"Banach-Alaoglu theorem for Hilbert $H^*$-module   We provided an analogue Banach-Alaoglu theorem for Hilbert $H^*$-module. We
construct a $\Lambda$-weak$^*$ topology on a Hilbert $H^*$-module over a proper
$H^*$-algebra $\Lambda$, such that the unit ball is compact with respect to
$\Lambda$-weak$^*$ topology.
",Mathematics,Mathematics
"Life and work of Egbert Brieskorn (1936 - 2013)   Egbert Brieskorn died on July 11, 2013, a few days after his 77th birthday.
He was an impressive personality who has left a lasting impression on all who
knew him, whether inside or outside of mathematics. Brieskorn was a great
mathematician, but his interests, his knowledge, and activities ranged far
beyond mathematics. In this contribution, which is strongly influenced by many
years of personal connectedness of the authors with Brieskorn, we try to give a
deeper insight into the life and work of Brieskorn. We illuminate both his
personal commitment to peace and the environment as well as his long-term study
of the life and work of Felix Hausdorff and the publication of Hausdorff's
collected works. However, the main focus of the article is on the presentation
of his remarkable and influential mathematical work.
",Mathematics,Mathematics
"Identification of Treatment Effects under Conditional Partial Independence   Conditional independence of treatment assignment from potential outcomes is a
commonly used but nonrefutable assumption. We derive identified sets for
various treatment effect parameters under nonparametric deviations from this
conditional independence assumption. These deviations are defined via a
conditional treatment assignment probability, which makes it straightforward to
interpret. Our results can be used to assess the robustness of empirical
conclusions obtained under the baseline conditional independence assumption.
",Statistics,Mathematics; Statistics
"Canonical correlation coefficients of high-dimensional Gaussian vectors: finite rank case   Consider a Gaussian vector $\mathbf{z}=(\mathbf{x}',\mathbf{y}')'$,
consisting of two sub-vectors $\mathbf{x}$ and $\mathbf{y}$ with dimensions $p$
and $q$ respectively, where both $p$ and $q$ are proportional to the sample
size $n$. Denote by $\Sigma_{\mathbf{u}\mathbf{v}}$ the population
cross-covariance matrix of random vectors $\mathbf{u}$ and $\mathbf{v}$, and
denote by $S_{\mathbf{u}\mathbf{v}}$ the sample counterpart. The canonical
correlation coefficients between $\mathbf{x}$ and $\mathbf{y}$ are known as the
square roots of the nonzero eigenvalues of the canonical correlation matrix
$\Sigma_{\mathbf{x}\mathbf{x}}^{-1}\Sigma_{\mathbf{x}\mathbf{y}}\Sigma_{\mathbf{y}\mathbf{y}}^{-1}\Sigma_{\mathbf{y}\mathbf{x}}$.
In this paper, we focus on the case that $\Sigma_{\mathbf{x}\mathbf{y}}$ is of
finite rank $k$, i.e. there are $k$ nonzero canonical correlation coefficients,
whose squares are denoted by $r_1\geq\cdots\geq r_k>0$. We study the sample
counterparts of $r_i,i=1,\ldots,k$, i.e. the largest $k$ eigenvalues of the
sample canonical correlation matrix
$§_{\mathbf{x}\mathbf{x}}^{-1}§_{\mathbf{x}\mathbf{y}}§_{\mathbf{y}\mathbf{y}}^{-1}§_{\mathbf{y}\mathbf{x}}$,
denoted by $\lambda_1\geq\cdots\geq \lambda_k$. We show that there exists a
threshold $r_c\in(0,1)$, such that for each $i\in\{1,\ldots,k\}$, when $r_i\leq
r_c$, $\lambda_i$ converges almost surely to the right edge of the limiting
spectral distribution of the sample canonical correlation matrix, denoted by
$d_{+}$. When $r_i>r_c$, $\lambda_i$ possesses an almost sure limit in
$(d_{+},1]$. We also obtain the limiting distribution of $\lambda_i$'s under
appropriate normalization. Specifically, $\lambda_i$ possesses Gaussian type
fluctuation if $r_i>r_c$, and follows Tracy-Widom distribution if $r_i<r_c$.
Some applications of our results are also discussed.
",Mathematics; Statistics,Mathematics; Statistics
"A Computational Approach to Extinction Events in Chemical Reaction Networks with Discrete State Spaces   Recent work of M.D. Johnston et al. has produced sufficient conditions on the
structure of a chemical reaction network which guarantee that the corresponding
discrete state space system exhibits an extinction event. The conditions
consist of a series of systems of equalities and inequalities on the edges of a
modified reaction network called a domination-expanded reaction network. In
this paper, we present a computational implementation of these conditions
written in Python and apply the program on examples drawn from the biochemical
literature, including a model of polyamine metabolism in mammals and a model of
the pentose phosphate pathway in Trypanosoma brucei. We also run the program on
458 models from the European Bioinformatics Institute's BioModels Database and
report our results.
",Mathematics,Computer Science; Physics
"A General Scheme Implicit Force Control for a Flexible-Link Manipulator   In this paper we propose an implicit force control scheme for a one-link
flexible manipulator that interact with a compliant environment. The controller
was based in the mathematical model of the manipulator, considering the
dynamics of the beam flexible and the gravitational force. With this method,
the controller parameters are obtained from the structural parameters of the
beam (link) of the manipulator. This controller ensure the stability based in
the Lyapunov Theory. The controller proposed has two closed loops: the inner
loop is a tracking control with gravitational force and vibration frequencies
compensation and the outer loop is a implicit force control. To evaluate the
performance of the controller, we have considered to three different
manipulators (the length, the diameter were modified) and three environments
with compliance modified. The results obtained from simulations verify the
asymptotic tracking and regulated in position and force respectively and the
vibrations suppression of the beam in a finite time.
",Computer Science; Physics,Physics
"Thickening and sickening the SYK model   We discuss higher dimensional generalizations of the 0+1-dimensional
Sachdev-Ye-Kitaev (SYK) model that has recently become the focus of intensive
interdisciplinary studies by, both, the condensed matter and field-theoretical
communities. Unlike the previous constructions where multiple SYK copies would
be coupled to each other and/or hybridized with itinerant fermions via
spatially short-ranged random hopping processes, we study algebraically varying
long-range (spatially and/or temporally) correlated random couplings in the
general d+1 dimensions. Such pertinent topics as translationally-invariant
strong-coupling solutions, emergent reparametrization symmetry, effective
action for fluctuations, chaotic behavior, and diffusive transport (or a lack
thereof) are all addressed. We find that the most appealing properties of the
original SYK model that suggest the existence of its 1+1-dimensional
holographic gravity dual do not survive the aforementioned generalizations,
thus lending no additional support to the hypothetical broad (including
'non-AdS/non-CFT') holographic correspondence.
",Physics,Physics
"Basic concepts and tools for the Toki Pona minimal and constructed language: description of the language and main issues; analysis of the vocabulary; text synthesis and syntax highlighting; Wordnet synsets   A minimal constructed language (conlang) is useful for experiments and
comfortable for making tools. The Toki Pona (TP) conlang is minimal both in the
vocabulary (with only 14 letters and 124 lemmas) and in the (about) 10 syntax
rules. The language is useful for being a used and somewhat established minimal
conlang with at least hundreds of fluent speakers. This article exposes current
concepts and resources for TP, and makes available Python (and Vim) scripted
routines for the analysis of the language, synthesis of texts, syntax
highlighting schemes, and the achievement of a preliminary TP Wordnet. Focus is
on the analysis of the basic vocabulary, as corpus analyses were found. The
synthesis is based on sentence templates, relates to context by keeping track
of used words, and renders larger texts by using a fixed number of phonemes
(e.g. for poems) and number of sentences, words and letters (e.g. for
paragraphs). Syntax highlighting reflects morphosyntactic classes given in the
official dictionary and different solutions are described and implemented in
the well-established Vim text editor. The tentative TP Wordnet is made
available in three patterns of relations between synsets and word lemmas. In
summary, this text holds potentially novel conceptualizations about, and tools
and results in analyzing, synthesizing and syntax highlighting the TP language.
",Computer Science,Computer Science
"BSDEs and SDEs with time-advanced and -delayed coefficients   This paper introduces a class of backward stochastic differential equations
(BSDEs), whose coefficients not only depend on the value of its solutions of
the present but also the past and the future. For a sufficiently small time
delay or a sufficiently small Lipschitz constant, the existence and uniqueness
of such BSDEs is obtained. As an adjoint process, a class of stochastic
differential equations (SDEs) is introduced, whose coefficients also depend on
the present, the past and the future of its solutions. The existence and
uniqueness of such SDEs is proved for a sufficiently small time advance or a
sufficiently small Lipschitz constant. A duality between such BSDEs and SDEs is
established.
",Mathematics,Mathematics
"Radiation Hardness Test of Eljen EJ-500 Optical Cement   We present a comprehensive account of the proton radiation hardness of Eljen
Technology's EJ-500 optical cement used in the construction of experiment
detectors. The cement was embedded into five plastic scintillator tiles which
were each exposed to one of five different levels of radiation by a 50 MeV
proton beam produced at the 88-Inch Cyclotron at Lawrence Berkeley National
Laboratory. A cosmic ray telescope setup was used to measure signal amplitudes
before and after irradiation. Another post-radiation measurement was taken four
months after the experiment to investigate whether the radiation damage to the
cement recovers after a short amount of time. We verified that the radiation
damage to the tiles increased with increasing dose but showed significant
improvement after the four months time interval.
",Physics,Physics
"Data Distillation for Controlling Specificity in Dialogue Generation   People speak at different levels of specificity in different situations.
Depending on their knowledge, interlocutors, mood, etc.} A conversational agent
should have this ability and know when to be specific and when to be general.
We propose an approach that gives a neural network--based conversational agent
this ability. Our approach involves alternating between \emph{data
distillation} and model training : removing training examples that are closest
to the responses most commonly produced by the model trained from the last
round and then retrain the model on the remaining dataset. Dialogue generation
models trained with different degrees of data distillation manifest different
levels of specificity.
We then train a reinforcement learning system for selecting among this pool
of generation models, to choose the best level of specificity for a given
input. Compared to the original generative model trained without distillation,
the proposed system is capable of generating more interesting and
higher-quality responses, in addition to appropriately adjusting specificity
depending on the context.
Our research constitutes a specific case of a broader approach involving
training multiple subsystems from a single dataset distinguished by differences
in a specific property one wishes to model. We show that from such a set of
subsystems, one can use reinforcement learning to build a system that tailors
its output to different input contexts at test time.
",Computer Science,Computer Science
"A Lifelong Learning Approach to Brain MR Segmentation Across Scanners and Protocols   Convolutional neural networks (CNNs) have shown promising results on several
segmentation tasks in magnetic resonance (MR) images. However, the accuracy of
CNNs may degrade severely when segmenting images acquired with different
scanners and/or protocols as compared to the training data, thus limiting their
practical utility. We address this shortcoming in a lifelong multi-domain
learning setting by treating images acquired with different scanners or
protocols as samples from different, but related domains. Our solution is a
single CNN with shared convolutional filters and domain-specific batch
normalization layers, which can be tuned to new domains with only a few
($\approx$ 4) labelled images. Importantly, this is achieved while retaining
performance on the older domains whose training data may no longer be
available. We evaluate the method for brain structure segmentation in MR
images. Results demonstrate that the proposed method largely closes the gap to
the benchmark, which is training a dedicated CNN for each scanner.
",Statistics,Computer Science; Statistics
"Learning Disentangled Representations with Semi-Supervised Deep Generative Models   Variational autoencoders (VAEs) learn representations of data by jointly
training a probabilistic encoder and decoder network. Typically these models
encode all features of the data into a single variable. Here we are interested
in learning disentangled representations that encode distinct aspects of the
data into separate variables. We propose to learn such representations using
model architectures that generalise from standard VAEs, employing a general
graphical model structure in the encoder and decoder. This allows us to train
partially-specified models that make relatively strong assumptions about a
subset of interpretable variables and rely on the flexibility of neural
networks to learn representations for the remaining variables. We further
define a general objective for semi-supervised learning in this model class,
which can be approximated using an importance sampling procedure. We evaluate
our framework's ability to learn disentangled representations, both by
qualitative exploration of its generative capacity, and quantitative evaluation
of its discriminative ability on a variety of models and datasets.
",Computer Science; Statistics,Computer Science; Statistics
"SceneCut: Joint Geometric and Object Segmentation for Indoor Scenes   This paper presents SceneCut, a novel approach to jointly discover previously
unseen objects and non-object surfaces using a single RGB-D image. SceneCut's
joint reasoning over scene semantics and geometry allows a robot to detect and
segment object instances in complex scenes where modern deep learning-based
methods either fail to separate object instances, or fail to detect objects
that were not seen during training. SceneCut automatically decomposes a scene
into meaningful regions which either represent objects or scene surfaces. The
decomposition is qualified by an unified energy function over objectness and
geometric fitting. We show how this energy function can be optimized
efficiently by utilizing hierarchical segmentation trees. Moreover, we leverage
a pre-trained convolutional oriented boundary network to predict accurate
boundaries from images, which are used to construct high-quality region
hierarchies. We evaluate SceneCut on several different indoor environments, and
the results show that SceneCut significantly outperforms all the existing
methods.
",Computer Science,Computer Science
"ADE surfaces and their moduli   We define a class of surfaces and surface pairs corresponding to the ADE root
lattices and construct compactifications of their moduli spaces, generalizing
Losev-Manin spaces of curves.
",Mathematics,Mathematics
"Classification of Local Field Potentials using Gaussian Sequence Model   A problem of classification of local field potentials (LFPs), recorded from
the prefrontal cortex of a macaque monkey, is considered. An adult macaque
monkey is trained to perform a memory-based saccade. The objective is to decode
the eye movement goals from the LFP collected during a memory period. The LFP
classification problem is modeled as that of classification of smooth functions
embedded in Gaussian noise. It is then argued that using minimax function
estimators as features would lead to consistent LFP classifiers. The theory of
Gaussian sequence models allows us to represent minimax estimators as finite
dimensional objects. The LFP classifier resulting from this mathematical
endeavor is a spectrum based technique, where Fourier series coefficients of
the LFP data, followed by appropriate shrinkage and thresholding, are used as
features in a linear discriminant classifier. The classifier is then applied to
the LFP data to achieve high decoding accuracy. The function classification
approach taken in the paper also provides a systematic justification for using
Fourier series, with shrinkage and thresholding, as features for the problem,
as opposed to using the power spectrum. It also suggests that phase information
is crucial to the decision making.
",Statistics,Statistics
"Discovering Visual Concept Structure with Sparse and Incomplete Tags   Discovering automatically the semantic structure of tagged visual data (e.g.
web videos and images) is important for visual data analysis and
interpretation, enabling the machine intelligence for effectively processing
the fast-growing amount of multi-media data. However, this is non-trivial due
to the need for jointly learning underlying correlations between heterogeneous
visual and tag data. The task is made more challenging by inherently sparse and
incomplete tags. In this work, we develop a method for modelling the inherent
visual data concept structures based on a novel Hierarchical-Multi-Label Random
Forest model capable of correlating structured visual and tag information so as
to more accurately interpret the visual semantics, e.g. disclosing meaningful
visual groups with similar high-level concepts, and recovering missing tags for
individual visual data samples. Specifically, our model exploits hierarchically
structured tags of different semantic abstractness and multiple tag statistical
correlations in addition to modelling visual and tag interactions. As a result,
our model is able to discover more accurate semantic correlation between
textual tags and visual features, and finally providing favourable visual
semantics interpretation even with highly sparse and incomplete tags. We
demonstrate the advantages of our proposed approach in two fundamental
applications, visual data clustering and missing tag completion, on
benchmarking video (i.e. TRECVID MED 2011) and image (i.e. NUS-WIDE) datasets.
",Computer Science,Computer Science
"Maximum a posteriori estimation through simulated annealing for binary asteroid orbit determination   This paper considers a new method for the binary asteroid orbit determination
problem. The method is based on the Bayesian approach with a global
optimisation algorithm. The orbital parameters to be determined are modelled
through an a posteriori distribution made of a priori and likelihood terms. The
first term constrains the parameters space and it allows the introduction of
available knowledge about the orbit. The second term is based on given
observations and it allows us to use and compare different observational error
models. Once the a posteriori model is built, the estimator of the orbital
parameters is computed using a global optimisation procedure: the simulated
annealing algorithm. The maximum a posteriori (MAP) techniques are verified
using simulated and real data. The obtained results validate the proposed
method. The new approach guarantees independence of the initial parameters
estimation and theoretical convergence towards the global optimisation
solution. It is particularly useful in these situations, whenever a good
initial orbit estimation is difficult to get, whenever observations are not
well-sampled, and whenever the statistical behaviour of the observational
errors cannot be stated Gaussian like.
",Physics; Statistics,Mathematics; Statistics
"Stability of laminar Couette flow of compressible fluids   Cylindrical Couette flow is a subject where the main focus has long been on
the onset of turbulence or, more precisely, the limit of stability of the
simplest laminar flow. The theoretical framework of this paper is a recently
developed action principle for hydrodynamics. It incorporates Euler-Lagrange
equations that are in essential agreement with the Navier-Stokes equation, but
applicable to the general case of a compressible fluid. The variational
principle incorporates the equation of continuity, a canonical structure and a
conserved Hamiltonian. The density is compressible, characterized by a general
(non-polar) equation of state, and homogeneous. The onset of instability is
often accompanied by bubble formation. It is proposed that the limit of
stability of laminar Couette flow may some times be related to cavitation. In
contrast to traditional stability theory we are not looking for mathematical
instabilities of a system of differential equations, but instead for the
possibility that the system is driven to a metastable or unstable
configuration. The application of this idea to cylindrical Couette flow
reported here turns out to account rather well for the observations. The
failure of a famous criterion due to Rayleigh is well known. It is here shown
that it may be due to the use of methods that are appropriate only in the case
that the equations of motion are derived from an action principle.
",Physics,Physics
"Tractable and Scalable Schatten Quasi-Norm Approximations for Rank Minimization   The Schatten quasi-norm was introduced to bridge the gap between the trace
norm and rank function. However, existing algorithms are too slow or even
impractical for large-scale problems. Motivated by the equivalence relation
between the trace norm and its bilinear spectral penalty, we define two
tractable Schatten norms, i.e.\ the bi-trace and tri-trace norms, and prove
that they are in essence the Schatten-$1/2$ and $1/3$ quasi-norms,
respectively. By applying the two defined Schatten quasi-norms to various rank
minimization problems such as MC and RPCA, we only need to solve much smaller
factor matrices. We design two efficient linearized alternating minimization
algorithms to solve our problems and establish that each bounded sequence
generated by our algorithms converges to a critical point. We also provide the
restricted strong convexity (RSC) based and MC error bounds for our algorithms.
Our experimental results verified both the efficiency and effectiveness of our
algorithms compared with the state-of-the-art methods.
",Statistics,Computer Science; Mathematics
"The Word Problem of $\mathbb{Z}^n$ Is a Multiple Context-Free Language   The \emph{word problem} of a group $G = \langle \Sigma \rangle$ can be
defined as the set of formal words in $\Sigma^*$ that represent the identity in
$G$. When viewed as formal languages, this gives a strong connection between
classes of groups and classes of formal languages. For example, Anisimov showed
that a group is finite if and only if its word problem is a regular language,
and Muller and Schupp showed that a group is virtually-free if and only if its
word problem is a context-free language. Above this, not much was known, until
Salvati showed recently that the word problem of $\mathbb{Z}^2$ is a multiple
context-free language, giving first such example. We generalize Salvati's
result to show that the word problem of $\mathbb{Z}^n$ is a multiple
context-free language for any $n$.
",Computer Science; Mathematics,Computer Science
"Using Matching to Detect Infeasibility of Some Integer Programs   A novel matching based heuristic algorithm designed to detect specially
formulated infeasible zero-one IPs is presented. The algorithm input is a set
of nested doubly stochastic subsystems and a set E of instance defining
variables set at zero level. The algorithm deduces additional variables at zero
level until either a constraint is violated (the IP is infeasible), or no more
variables can be deduced zero (the IP is undecided). All feasible IPs, and all
infeasible IPs not detected infeasible are undecided. We successfully apply the
algorithm to a small set of specially formulated infeasible zero-one IP
instances of the Hamilton cycle decision problem. We show how to model both the
graph and subgraph isomorphism decision problems for input to the algorithm.
Increased levels of nested doubly stochastic subsystems can be implemented
dynamically. The algorithm is designed for parallel processing, and for
inclusion of techniques in addition to matching.
",Computer Science,Computer Science
"Baryon acoustic oscillations from the complete SDSS-III Ly$α$-quasar cross-correlation function at $z=2.4$   We present a measurement of baryon acoustic oscillations (BAO) in the
cross-correlation of quasars with the Ly$\alpha$-forest flux-transmission at a
mean redshift $z=2.40$. The measurement uses the complete SDSS-III data sample:
168,889 forests and 234,367 quasars from the SDSS Data Release DR12. In
addition to the statistical improvement on our previous study using DR11, we
have implemented numerous improvements at the analysis level allowing a more
accurate measurement of this cross-correlation. We also developed the first
simulations of the cross-correlation allowing us to test different aspects of
our data analysis and to search for potential systematic errors in the
determination of the BAO peak position. We measure the two ratios
$D_{H}(z=2.40)/r_{d} = 9.01 \pm 0.36$ and $D_{M}(z=2.40)/r_{d} = 35.7 \pm 1.7$,
where the errors include marginalization over the non-linear velocity of
quasars and the metal - quasar cross-correlation contribution, among other
effects. These results are within $1.8\sigma$ of the prediction of the
flat-$\Lambda$CDM model describing the observed CMB anisotropies. We combine
this study with the Ly$\alpha$-forest auto-correlation function
[2017A&A...603A..12B], yielding $D_{H}(z=2.40)/r_{d} = 8.94 \pm 0.22$ and
$D_{M}(z=2.40)/r_{d} = 36.6 \pm 1.2$, within $2.3\sigma$ of the same
flat-$\Lambda$CDM model.
",Physics,Physics
"On the Power Spectral Density Applied to the Analysis of Old Canvases   A routine task for art historians is painting diagnostics, such as dating or
attribution. Signal processing of the X-ray image of a canvas provides useful
information about its fabric. However, previous methods may fail when very old
and deteriorated artworks or simply canvases of small size are studied. We
present a new framework to analyze and further characterize the paintings from
their radiographs. First, we start from a general analysis of lattices and
provide new unifying results about the theoretical spectra of weaves. Then, we
use these results to infer the main structure of the fabric, like the type of
weave and the thread densities. We propose a practical estimation of these
theoretical results from paintings with the averaged power spectral density
(PSD), which provides a more robust tool. Furthermore, we found that the PSD
provides a fingerprint that characterizes the whole canvas. We search and
discuss some distinctive features we may find in that fingerprint. We apply
these results to several masterpieces of the 17th and 18th centuries from the
Museo Nacional del Prado to show that this approach yields accurate results in
thread counting and is very useful for paintings comparison, even in situations
where previous methods fail.
",Computer Science; Mathematics,Computer Science
"Ground-state properties of unitary bosons: from clusters to matter   The properties of cold Bose gases at unitarity have been extensively
investigated in the last few years both theoretically and experimentally. In
this paper we use a family of interactions tuned to two-body unitarity and very
weak three-body binding to demonstrate the universal properties of both
clusters and matter. We determine the universal properties of finite clusters
up to 60 particles and, for the first time, explicitly demonstrate the
saturation of energy and density with particle number and compare with bulk
properties. At saturation in the bulk we determine the energy, density, two-
and three-body contacts and the condensate fraction. We find that uniform
matter is more bound than three-body clusters by nearly two orders of
magnitude, the two-body contact is very large in absolute terms, and yet the
condensate fraction is also very large, greater than 90%. Equilibrium
properties of these systems may be experimentally accessible through rapid
quenching of weakly-interacting boson superfluids.
",Physics,Physics
"Performance Analysis of Robust Stable PID Controllers Using Dominant Pole Placement for SOPTD Process Models   This paper derives new formulations for designing dominant pole placement
based proportional-integral-derivative (PID) controllers to handle second order
processes with time delays (SOPTD). Previously, similar attempts have been made
for pole placement in delay-free systems. The presence of the time delay term
manifests itself as a higher order system with variable number of interlaced
poles and zeros upon Pade approximation, which makes it difficult to achieve
precise pole placement control. We here report the analytical expressions to
constrain the closed loop dominant and non-dominant poles at the desired
locations in the complex s-plane, using a third order Pade approximation for
the delay term. However, invariance of the closed loop performance with
different time delay approximation has also been verified using increasing
order of Pade, representing a closed to reality higher order delay dynamics.
The choice of the nature of non-dominant poles e.g. all being complex, real or
a combination of them modifies the characteristic equation and influences the
achievable stability regions. The effect of different types of non-dominant
poles and the corresponding stability regions are obtained for nine test-bench
processes indicating different levels of open-loop damping and lag to delay
ratio. Next, we investigate which expression yields a wider stability region in
the design parameter space by using Monte Carlo simulations while uniformly
sampling a chosen design parameter space. Various time and frequency domain
control performance parameters are investigated next, as well as their
deviations with uncertain process parameters, using thousands of Monte Carlo
simulations, around the robust stable solution for each of the nine test-bench
processes.
",Statistics,Computer Science
"Attack Analysis for Distributed Control Systems: An Internal Model Principle Approach   Although adverse effects of attacks have been acknowledged in many
cyber-physical systems, there is no system-theoretic comprehension of how a
compromised agent can leverage communication capabilities to maximize the
damage in distributed multi-agent systems. A rigorous analysis of
cyber-physical attacks enables us to increase the system awareness against
attacks and design more resilient control protocols. To this end, we will take
the role of the attacker to identify the worst effects of attacks on root nodes
and non-root nodes in a distributed control system. More specifically, we show
that a stealthy attack on root nodes can mislead the entire network to a wrong
understanding of the situation and even destabilize the synchronization
process. This will be called the internal model principle for the attacker and
will intensify the urgency of designing novel control protocols to mitigate
these types of attacks.
",Computer Science,Computer Science
"Information Diffusion in Social Networks: Friendship Paradox based Models and Statistical Inference   Dynamic models and statistical inference for the diffusion of information in
social networks is an area which has witnessed remarkable progress in the last
decade due to the proliferation of social networks. Modeling and inference of
diffusion of information has applications in targeted advertising and
marketing, forecasting elections, predicting investor sentiment and identifying
epidemic outbreaks. This chapter discusses three important aspects related to
information diffusion in social networks: (i) How does observation bias named
friendship paradox (a graph theoretic consequence) and monophilic contagion
(influence of friends of friends) affect information diffusion dynamics. (ii)
How can social networks adapt their structural connectivity depending on the
state of information diffusion. (iii) How one can estimate the state of the
network induced by information diffusion. The motivation for all three topics
considered in this chapter stems from recent findings in network science and
social sensing. Further, several directions for future research that arise from
these topics are also discussed.
",Computer Science,Computer Science; Statistics
"The Multi-layer Information Bottleneck Problem   The muti-layer information bottleneck (IB) problem, where information is
propagated (or successively refined) from layer to layer, is considered. Based
on information forwarded by the preceding layer, each stage of the network is
required to preserve a certain level of relevance with regards to a specific
hidden variable, quantified by the mutual information. The hidden variables and
the source can be arbitrarily correlated. The optimal trade-off between rates
of relevance and compression (or complexity) is obtained through a
single-letter characterization, referred to as the rate-relevance region.
Conditions of successive refinabilty are given. Binary source with BSC hidden
variables and binary source with BSC/BEC mixed hidden variables are both proved
to be successively refinable. We further extend our result to Guassian models.
A counterexample of successive refinability is also provided.
",Computer Science; Statistics,Computer Science; Mathematics
"The GBT Beam Shape at 109 GHz   With the installation of the Argus 16-pixel receiver covering 75-115 GHz on
the Green Bank Telescope (GBT), it is now possible to characterize the antenna
beam at very high frequencies, where the use of the active surface and
out-of-focus holography are critical to the telescope's performance. A recent
measurement in good weather conditions (low atmospheric opacity, low winds, and
stable night-time thermal conditions) at 109.4 GHz yielded a FWHM beam of
6.7""x6.4"" in azimuth and elevation, respectively. This corresponds to
1.16+/-0.03 Lambda/D at 109.4 GHz. The derived ratio agrees well with the
low-frequency value of 1.18+/-0.03 Lambda/D measured at 9.0 GHz. There are no
detectable side-lobes at either frequency. In good weather conditions and after
applying the standard antenna corrections (pointing, focus, and the active
surface corrections for gravity and thermal effects), there is no measurable
degradation of the beam of the GBT at its highest operational frequencies.
",Physics,Physics
"Structure Preserving Model Reduction of Parametric Hamiltonian Systems   While reduced-order models (ROMs) have been popular for efficiently solving
large systems of differential equations, the stability of reduced models over
long-time integration is of present challenges. We present a greedy approach
for ROM generation of parametric Hamiltonian systems that captures the
symplectic structure of Hamiltonian systems to ensure stability of the reduced
model. Through the greedy selection of basis vectors, two new vectors are added
at each iteration to the linear vector space to increase the accuracy of the
reduced basis. We use the error in the Hamiltonian due to model reduction as an
error indicator to search the parameter space and identify the next best basis
vectors. Under natural assumptions on the set of all solutions of the
Hamiltonian system under variation of the parameters, we show that the greedy
algorithm converges with exponential rate. Moreover, we demonstrate that
combining the greedy basis with the discrete empirical interpolation method
also preserves the symplectic structure. This enables the reduction of the
computational cost for nonlinear Hamiltonian systems. The efficiency, accuracy,
and stability of this model reduction technique is illustrated through
simulations of the parametric wave equation and the parametric Schrodinger
equation.
",Mathematics,Computer Science; Mathematics
"An Exploratory Study of Field Failures   Field failures, that is, failures caused by faults that escape the testing
phase leading to failures in the field, are unavoidable. Improving verification
and validation activities before deployment can identify and timely remove many
but not all faults, and users may still experience a number of annoying
problems while using their software systems. This paper investigates the nature
of field failures, to understand to what extent further improving in-house
verification and validation activities can reduce the number of failures in the
field, and frames the need of new approaches that operate in the field. We
report the results of the analysis of the bug reports of five applications
belonging to three different ecosystems, propose a taxonomy of field failures,
and discuss the reasons why failures belonging to the identified classes cannot
be detected at design time but shall be addressed at runtime. We observe that
many faults (70%) are intrinsically hard to detect at design-time.
",Computer Science,Computer Science
"Neural Architecture Search with Bayesian Optimisation and Optimal Transport   Bayesian Optimisation (BO) refers to a class of methods for global
optimisation of a function $f$ which is only accessible via point evaluations.
It is typically used in settings where $f$ is expensive to evaluate. A common
use case for BO in machine learning is model selection, where it is not
possible to analytically model the generalisation performance of a statistical
model, and we resort to noisy and expensive training and validation procedures
to choose the best model. Conventional BO methods have focused on Euclidean and
categorical domains, which, in the context of model selection, only permits
tuning scalar hyper-parameters of machine learning algorithms. However, with
the surge of interest in deep learning, there is an increasing demand to tune
neural network \emph{architectures}. In this work, we develop NASBOT, a
Gaussian process based BO framework for neural architecture search. To
accomplish this, we develop a distance metric in the space of neural network
architectures which can be computed efficiently via an optimal transport
program. This distance might be of independent interest to the deep learning
community as it may find applications outside of BO. We demonstrate that NASBOT
outperforms other alternatives for architecture search in several cross
validation based model selection tasks on multi-layer perceptrons and
convolutional neural networks.
",Statistics,Computer Science; Statistics
"Landau levels from neutral Bogoliubov particles in two-dimensional nodal superconductors under strain and doping gradients   Motivated by recent work on strain-induced pseudo-magnetic fields in Dirac
and Weyl semimetals, we analyze the possibility of analogous fields in
two-dimensional nodal superconductors. We consider the prototypical case of a
d-wave superconductor, a representative of the cuprate family, and find that
the presence of weak strain leads to pseudo-magnetic fields and Landau
quantization of Bogoliubov quasiparticles in the low-energy sector. A similar
effect is induced by the presence of generic, weak doping gradients. In
contrast to genuine magnetic fields in superconductors, the strain- and doping
gradient-induced pseudo-magnetic fields couple in a way that preserves
time-reversal symmetry and is not subject to the screening associated with the
Meissner effect. These effects can be probed by tuning weak applied
supercurrents which lead to shifts in the energies of the Landau levels and
hence to quantum oscillations in thermodynamic and transport quantities.
",Physics,Physics
"Space dependent adhesion forces mediated by transient elastic linkages : new convergence and global existence results   In the first part of this work we show the convergence with respect to an
asymptotic parameter {\epsilon} of a delayed heat equation. It represents a
mathematical extension of works considered previously by the authors [Milisic
et al. 2011, Milisic et al. 2016]. Namely, this is the first result involving
delay operators approximating protein linkages coupled with a spatial elliptic
second order operator. For the sake of simplicity we choose the Laplace
operator, although more general results could be derived. The main arguments
are (i) new energy estimates and (ii) a stability result extended from the
previous work to this more involved context. They allow to prove convergence of
the delay operator to a friction term together with the Laplace operator in the
same asymptotic regime considered without the space dependence in [Milisic et
al, 2011]. In a second part we extend fixed-point results for the fully
non-linear model introduced in [Milisic et al, 2016] and prove global existence
in time. This shows that the blow-up scenario observed previously does not
occur. Since the latter result was interpreted as a rupture of adhesion forces,
we discuss the possibility of bond breaking both from the analytic and
numerical point of view.
",Mathematics,Mathematics
"The dependence of cluster galaxy properties on the central entropy of their host cluster   We present a study of the connection between brightest cluster galaxies
(BCGs) and their host galaxy clusters. Using galaxy clusters at $0.1<z<0.3$
from the Hectospec Cluster Survey (HeCS) with X-ray information from the
Archive of {\it Chandra} Cluster Entropy Profile Tables (ACCEPT), we confirm
that BCGs in low central entropy clusters are well aligned with the X-ray
center. Additionally, the magnitude difference between BCG and the 2nd
brightest one also correlates with the central entropy of the intracluster
medium. From the red-sequence (RS) galaxies, we cannot find significant
dependence of RS color scatter and stellar population on the central entropy of
the intracluster medium of their host cluster. However, BCGs in low entropy
clusters are systematically less massive than those in high entropy clusters,
although this is dependent on the method used to derive the stellar mass of
BCGs. In contrast, the stellar velocity dispersion of BCGs shows no dependence
on BCG activity and cluster central entropy. This implies that the potential of
the BCG is established earlier and the activity leading to optical emission
lines is dictated by the properties of the intracluster medium in the cluster
core.
",Physics,Physics
"Real-World Modeling of a Pathfinding Robot Using Robot Operating System (ROS)   This paper presents a practical approach towards implementing pathfinding
algorithms on real-world and low-cost non- commercial hardware platforms. While
using robotics simulation platforms as a test-bed for our algorithms we easily
overlook real- world exogenous problems that are developed by external factors.
Such problems involve robot wheel slips, asynchronous motors, abnormal sensory
data or unstable power sources. The real-world dynamics tend to be very painful
even for executing simple algorithms like a Wavefront planner or A-star search.
This paper addresses designing techniques that tend to be robust as well as
reusable for any hardware platforms; covering problems like controlling
asynchronous drives, odometry offset issues and handling abnormal sensory
feedback. The algorithm implementation medium and hardware design tools have
been kept general in order to present our work as a serving platform for future
researchers and robotics enthusiast working in the field of path planning
robotics.
",Computer Science,Computer Science
"On the Casas-Alvero conjecture   The conjecture is formulated in an affine structure and linked with
dimension=1 of the defined CA sets. Then some known results are proved in this
context. The short intended proof relies on a direct yet unclear statement
about homogeneous dependence of algebraic equations. This might not be a
complete proof or even one on the right track, but it may provoke more thoughts
in this respect as expected.
",Mathematics,Mathematics
"Computer-assisted proof of heteroclinic connections in the one-dimensional Ohta-Kawasaki model   We present a computer-assisted proof of heteroclinic connections in the
one-dimensional Ohta-Kawasaki model of diblock copolymers. The model is a
fourth-order parabolic partial differential equation subject to homogeneous
Neumann boundary conditions, which contains as a special case the celebrated
Cahn-Hilliard equation. While the attractor structure of the latter model is
completely understood for one-dimensional domains, the diblock copolymer
extension exhibits considerably richer long-term dynamical behavior, which
includes a high level of multistability. In this paper, we establish the
existence of certain heteroclinic connections between the homogeneous
equilibrium state, which represents a perfect copolymer mixture, and all local
and global energy minimizers. In this way, we show that not every solution
originating near the homogeneous state will converge to the global energy
minimizer, but rather is trapped by a stable state with higher energy. This
phenomenon can not be observed in the one-dimensional Cahn-Hillard equation,
where generic solutions are attracted by a global minimizer.
",Computer Science; Mathematics,Mathematics
"AC-Biased Shift Registers as Fabrication Process Benchmark Circuits and Flux Trapping Diagnostic Tool   We develop an ac-biased shift register introduced in our previous work (V.K.
Semenov et al., IEEE Trans. Appl. Supercond., vol. 25, no. 3, 1301507, June
2015) into a benchmark circuit for evaluation of superconductor electronics
fabrication technology. The developed testing technique allows for extracting
margins of all individual cells in the shift register, which in turn makes it
possible to estimate statistical distribution of Josephson junctions in the
circuit. We applied this approach to successfully test registers having 8, 16,
36, and 202 thousand cells and, respectively, about 33000, 65000, 144000, and
809000 Josephson junctions. The circuits were fabricated at MIT Lincoln
Laboratory, using a fully planarized process, 0.4 {\mu}m inductor linewidth,
and 1.33x10^6 cm^-2 junction density. They are presently the largest
operational superconducting SFQ circuits ever made. The developed technique
distinguishes between hard defects (fabrication-related) and soft defects
(measurement-related) and locates them in the circuit. The soft defects are
specific to superconducting circuits and caused by magnetic flux trapping
either inside the active cells or in the dedicated flux-trapping moats near the
cells. The number and distribution of soft defects depend on the ambient
magnetic field and vary with thermal cycling even if done in the same magnetic
environment.
",Physics,Physics
"A Survey of Riccati Equation Results in Negative Imaginary Systems Theory and Quantum Control Theory   This paper presents a survey of some new applications of algebraic Riccati
equations. In particular, the paper surveys some recent results on the use of
algebraic Riccati equations in testing whether a system is negative imaginary
and in synthesizing state feedback controllers which make the closed loop
system negative imaginary. The paper also surveys the use of Riccati equation
methods in the control of quantum linear systems including coherent $H^\infty$
control.
",Computer Science; Mathematics,Computer Science
"Poisoning Attacks to Graph-Based Recommender Systems   Recommender system is an important component of many web services to help
users locate items that match their interests. Several studies showed that
recommender systems are vulnerable to poisoning attacks, in which an attacker
injects fake data to a given system such that the system makes recommendations
as the attacker desires. However, these poisoning attacks are either agnostic
to recommendation algorithms or optimized to recommender systems that are not
graph-based. Like association-rule-based and matrix-factorization-based
recommender systems, graph-based recommender system is also deployed in
practice, e.g., eBay, Huawei App Store. However, how to design optimized
poisoning attacks for graph-based recommender systems is still an open problem.
In this work, we perform a systematic study on poisoning attacks to graph-based
recommender systems. Due to limited resources and to avoid detection, we assume
the number of fake users that can be injected into the system is bounded. The
key challenge is how to assign rating scores to the fake users such that the
target item is recommended to as many normal users as possible. To address the
challenge, we formulate the poisoning attacks as an optimization problem,
solving which determines the rating scores for the fake users. We also propose
techniques to solve the optimization problem. We evaluate our attacks and
compare them with existing attacks under white-box (recommendation algorithm
and its parameters are known), gray-box (recommendation algorithm is known but
its parameters are unknown), and black-box (recommendation algorithm is
unknown) settings using two real-world datasets. Our results show that our
attack is effective and outperforms existing attacks for graph-based
recommender systems. For instance, when 1% fake users are injected, our attack
can make a target item recommended to 580 times more normal users in certain
scenarios.
",Statistics,Computer Science
"Minimax Rényi Redundancy   The redundancy for universal lossless compression of discrete memoryless
sources in Campbell's setting is characterized as a minimax Rényi divergence,
which is shown to be equal to the maximal $\alpha$-mutual information via a
generalized redundancy-capacity theorem. Special attention is placed on the
analysis of the asymptotics of minimax Rényi divergence, which is determined
up to a term vanishing in blocklength.
",Computer Science; Mathematics,Computer Science; Mathematics
"Reverse Quantum Annealing Approach to Portfolio Optimization Problems   We investigate a hybrid quantum-classical solution method to the
mean-variance portfolio optimization problems. Starting from real financial
data statistics and following the principles of the Modern Portfolio Theory, we
generate parametrized samples of portfolio optimization problems that can be
related to quadratic binary optimization forms programmable in the analog
D-Wave Quantum Annealer 2000Q. The instances are also solvable by an
industry-established Genetic Algorithm approach, which we use as a classical
benchmark. We investigate several options to run the quantum computation
optimally, ultimately discovering that the best results in terms of expected
time-to-solution as a function of number of variables for the hardest instances
set are obtained by seeding the quantum annealer with a solution candidate
found by a greedy local search and then performing a reverse annealing
protocol. The optimized reverse annealing protocol is found to be more than 100
times faster than the corresponding forward quantum annealing on average.
",Quantitative Finance,Statistics
"PS-DBSCAN: An Efficient Parallel DBSCAN Algorithm Based on Platform Of AI (PAI)   We present PS-DBSCAN, a communication efficient parallel DBSCAN algorithm
that combines the disjoint-set data structure and Parameter Server framework in
Platform of AI (PAI). Since data points within the same cluster may be
distributed over different workers which result in several disjoint-sets,
merging them incurs large communication costs. In our algorithm, we employ a
fast global union approach to union the disjoint-sets to alleviate the
communication burden. Experiments over the datasets of different scales
demonstrate that PS-DBSCAN outperforms the PDSDBSCAN with 2-10 times speedup on
communication efficiency.
We have released our PS-DBSCAN in an algorithm platform called Platform of AI
(PAI - this https URL) in Alibaba Cloud. We have also
demonstrated how to use the method in PAI.
",Computer Science,Computer Science
"Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments   Eliminating the negative effect of non-stationary environmental noise is a
long-standing research topic for automatic speech recognition that stills
remains an important challenge. Data-driven supervised approaches, including
ones based on deep neural networks, have recently emerged as potential
alternatives to traditional unsupervised approaches and with sufficient
training, can alleviate the shortcomings of the unsupervised methods in various
real-life acoustic environments. In this light, we review recently developed,
representative deep learning approaches for tackling non-stationary additive
and convolutional degradation of speech with the aim of providing guidelines
for those involved in the development of environmentally robust speech
recognition systems. We separately discuss single- and multi-channel techniques
developed for the front-end and back-end of speech recognition systems, as well
as joint front-end and back-end training frameworks.
",Computer Science,Computer Science
"Attention-Based Models for Text-Dependent Speaker Verification   Attention-based models have recently shown great performance on a range of
tasks, such as speech recognition, machine translation, and image captioning
due to their ability to summarize relevant information that expands through the
entire length of an input sequence. In this paper, we analyze the usage of
attention mechanisms to the problem of sequence summarization in our end-to-end
text-dependent speaker recognition system. We explore different topologies and
their variants of the attention layer, and compare different pooling methods on
the attention weights. Ultimately, we show that attention-based models can
improves the Equal Error Rate (EER) of our speaker verification system by
relatively 14% compared to our non-attention LSTM baseline model.
",Computer Science; Statistics,Computer Science
"Overcomplete Frame Thresholding for Acoustic Scene Analysis   In this work, we derive a generic overcomplete frame thresholding scheme
based on risk minimization. Overcomplete frames being favored for analysis
tasks such as classification, regression or anomaly detection, we provide a way
to leverage those optimal representations in real-world applications through
the use of thresholding. We validate the method on a large scale bird activity
detection task via the scattering network architecture performed by means of
continuous wavelets, known for being an adequate dictionary in audio
environments.
",Computer Science; Statistics,Computer Science
"Statistical Physics of the Symmetric Group   Ordered chains (such as chains of amino acids) are ubiquitous in biological
cells, and these chains perform specific functions contingent on the sequence
of their components. Using the existence and general properties of such
sequences as a theoretical motivation, we study the statistical physics of
systems whose state space is defined by the possible permutations of an ordered
list, i.e., the symmetric group, and whose energy is a function of how certain
permutations deviate from some chosen correct ordering. Such a non-factorizable
state space is quite different from the state spaces typically considered in
statistical physics systems and consequently has novel behavior in systems with
interacting and even non-interacting Hamiltonians. Various parameter choices of
a mean-field model reveal the system to contain five different physical regimes
defined by two transition temperatures, a triple point, and a quadruple point.
Finally, we conclude by discussing how the general analysis can be extended to
state spaces with more complex combinatorial properties and to other standard
questions of statistical mechanics models.
",Physics,Physics
"Normalized Maximum Likelihood with Luckiness for Multivariate Normal Distributions   The normalized maximum likelihood (NML) is one of the most important
distribution in coding theory and statistics. NML is the unique solution (if
exists) to the pointwise minimax regret problem. However, NML is not defined
even for simple family of distributions such as the normal distributions. Since
there does not exist any meaningful minimax-regret distribution for such case,
it is pointed out that NML with luckiness (LNML) can be employed as an
alternative to NML. In this paper, we develop the closed form of LNMLs for
multivariate normal distributions.
",Mathematics; Statistics,Mathematics; Statistics
"Replicator equation on networks with degree regular communities   The replicator equation is one of the fundamental tools to study evolutionary
dynamics in well-mixed populations. This paper contributes to the literature on
evolutionary graph theory, providing a version of the replicator equation for a
family of connected networks with communities, where nodes in the same
community have the same degree. This replicator equation is applied to the
study of different classes of games, exploring the impact of the graph
structure on the equilibria of the evolutionary dynamics.
",Quantitative Biology,Computer Science; Physics
"Who Said What: Modeling Individual Labelers Improves Classification   Data are often labeled by many different experts with each expert only
labeling a small fraction of the data and each data point being labeled by
several experts. This reduces the workload on individual experts and also gives
a better estimate of the unobserved ground truth. When experts disagree, the
standard approaches are to treat the majority opinion as the correct label or
to model the correct label as a distribution. These approaches, however, do not
make any use of potentially valuable information about which expert produced
which label. To make use of this extra information, we propose modeling the
experts individually and then learning averaging weights for combining them,
possibly in sample-specific ways. This allows us to give more weight to more
reliable experts and take advantage of the unique strengths of individual
experts at classifying certain types of data. Here we show that our approach
leads to improvements in computer-aided diagnosis of diabetic retinopathy. We
also show that our method performs better than competing algorithms by Welinder
and Perona (2010), and by Mnih and Hinton (2012). Our work offers an innovative
approach for dealing with the myriad real-world settings that use expert
opinions to define labels for training.
",Computer Science,Computer Science; Statistics
"Graph Attention Networks   We present graph attention networks (GATs), novel neural network
architectures that operate on graph-structured data, leveraging masked
self-attentional layers to address the shortcomings of prior methods based on
graph convolutions or their approximations. By stacking layers in which nodes
are able to attend over their neighborhoods' features, we enable (implicitly)
specifying different weights to different nodes in a neighborhood, without
requiring any kind of costly matrix operation (such as inversion) or depending
on knowing the graph structure upfront. In this way, we address several key
challenges of spectral-based graph neural networks simultaneously, and make our
model readily applicable to inductive as well as transductive problems. Our GAT
models have achieved or matched state-of-the-art results across four
established transductive and inductive graph benchmarks: the Cora, Citeseer and
Pubmed citation network datasets, as well as a protein-protein interaction
dataset (wherein test graphs remain unseen during training).
",Computer Science; Statistics,Computer Science; Statistics
"Siamese Networks with Location Prior for Landmark Tracking in Liver Ultrasound Sequences   Image-guided radiation therapy can benefit from accurate motion tracking by
ultrasound imaging, in order to minimize treatment margins and radiate moving
anatomical targets, e.g., due to breathing. One way to formulate this tracking
problem is the automatic localization of given tracked anatomical landmarks
throughout a temporal ultrasound sequence. For this, we herein propose a
fully-convolutional Siamese network that learns the similarity between pairs of
image regions containing the same landmark. Accordingly, it learns to localize
and thus track arbitrary image features, not only predefined anatomical
structures. We employ a temporal consistency model as a location prior, which
we combine with the network-predicted location probability map to track a
target iteratively in ultrasound sequences. We applied this method on the
dataset of the Challenge on Liver Ultrasound Tracking (CLUST) with competitive
results, where our work is the first to effectively apply CNNs on this tracking
problem, thanks to our temporal regularization.
",Computer Science,Computer Science
"Three principles of data science: predictability, computability, and stability (PCS)   We propose the predictability, computability, and stability (PCS) framework
to extract reproducible knowledge from data that can guide scientific
hypothesis generation and experimental design. The PCS framework builds on key
ideas in machine learning, using predictability as a reality check and
evaluating computational considerations in data collection, data storage, and
algorithm design. It augments PC with an overarching stability principle, which
largely expands traditional statistical uncertainty considerations. In
particular, stability assesses how results vary with respect to choices (or
perturbations) made across the data science life cycle, including problem
formulation, pre-processing, modeling (data and algorithm perturbations), and
exploratory data analysis (EDA) before and after modeling.
Furthermore, we develop PCS inference to investigate the stability of data
results and identify when models are consistent with relatively simple
phenomena. We compare PCS inference with existing methods, such as selective
inference, in high-dimensional sparse linear model simulations to demonstrate
that our methods consistently outperform others in terms of ROC curves over a
wide range of simulation settings. Finally, we propose a PCS documentation
based on Rmarkdown, iPython, or Jupyter Notebook, with publicly available,
reproducible codes and narratives to back up human choices made throughout an
analysis. The PCS workflow and documentation are demonstrated in a genomics
case study available on Zenodo.
",Computer Science; Statistics,Statistics
"Application of Coulomb energy density functional for atomic nuclei: Case studies of local density approximation and generalized gradient approximation   We test the Coulomb exchange and correlation energy density functionals of
electron systems for atomic nuclei in the local density approximation (LDA) and
the generalized gradient approximation (GGA). For the exchange Coulomb
energies, it is found that the deviation between the LDA and GGA ranges from
around $ 11 \, \% $ in $ {}^{4} \mathrm{He} $ to around $ 2.2 \, \% $ in $
{}^{208} \mathrm{Pb} $, by taking the Perdew-Burke-Ernzerhof (PBE) functional
as an example of the GGA\@. For the correlation Coulomb energies, it is shown
that those functionals of electron systems are not suitable for atomic nuclei.
",Physics,Physics
"Distributed Policy Iteration for Scalable Approximation of Cooperative Multi-Agent Policies   Decision making in multi-agent systems (MAS) is a great challenge due to
enormous state and joint action spaces as well as uncertainty, making
centralized control generally infeasible. Decentralized control offers better
scalability and robustness but requires mechanisms to coordinate on joint tasks
and to avoid conflicts. Common approaches to learn decentralized policies for
cooperative MAS suffer from non-stationarity and lacking credit assignment,
which can lead to unstable and uncoordinated behavior in complex environments.
In this paper, we propose Strong Emergent Policy approximation (STEP), a
scalable approach to learn strong decentralized policies for cooperative MAS
with a distributed variant of policy iteration. For that, we use function
approximation to learn from action recommendations of a decentralized
multi-agent planning algorithm. STEP combines decentralized multi-agent
planning with centralized learning, only requiring a generative model for
distributed black box optimization. We experimentally evaluate STEP in two
challenging and stochastic domains with large state and joint action spaces and
show that STEP is able to learn stronger policies than standard multi-agent
reinforcement learning algorithms, when combining multi-agent open-loop
planning with centralized function approximation. The learned policies can be
reintegrated into the multi-agent planning process to further improve
performance.
",Computer Science,Computer Science; Statistics
"Observational Equivalence in System Estimation: Contractions in Complex Networks   Observability of complex systems/networks is the focus of this paper, which
is shown to be closely related to the concept of contraction. Indeed, for
observable network tracking it is necessary/sufficient to have one node in each
contraction measured. Therefore, nodes in a contraction are equivalent to
recover for loss of observability, implying that contraction size is a key
factor for observability recovery. Here, using a polynomial order contraction
detection algorithm, we analyze the distribution of contractions, studying its
relation with key network properties. Our results show that contraction size is
related to network clustering coefficient and degree heterogeneity.
Particularly, in networks with power-law degree distribution, if the clustering
coefficient is high there are less contractions with smaller size on average.
The implication is that estimation/tracking of such systems requires less
number of measurements, while their observational recovery is more restrictive
in case of sensor failure. Further, in Small-World networks higher degree
heterogeneity implies that there are more contractions with smaller size on
average. Therefore, the estimation of representing system requires more
measurements, and also the recovery of measurement failure is more limited.
These results imply that one can tune the properties of synthetic networks to
alleviate their estimation/observability recovery.
",Computer Science,Computer Science
"Bouncy Hybrid Sampler as a Unifying Device   This work introduces a class of rejection-free Markov chain Monte Carlo
(MCMC) samplers, named the Bouncy Hybrid Sampler, which unifies several
existing methods from the literature. Examples include the Bouncy Particle
Sampler of Peters and de With (2012), Bouchard-Cote et al. (2015) and the
Hamiltonian MCMC. Following the introduced general framework, we derive a new
sampler called the Quadratic Bouncy Hybrid Sampler. We apply this novel sampler
to the problem of sampling from a truncated Gaussian distribution.
",Statistics,Computer Science; Statistics
"Simulation assisted machine learning   Predicting how a proposed cancer treatment will affect a given tumor can be
cast as a machine learning problem, but the complexity of biological systems,
the number of potentially relevant genomic and clinical features, and the lack
of very large scale patient data repositories make this a unique challenge.
""Pure data"" approaches to this problem are underpowered to detect
combinatorially complex interactions and are bound to uncover false
correlations despite statistical precautions taken (1). To investigate this
setting, we propose a method to integrate simulations, a strong form of prior
knowledge, into machine learning, a combination which to date has been largely
unexplored. The results of multiple simulations (under various uncertainty
scenarios) are used to compute similarity measures between every pair of
samples: sample pairs are given a high similarity score if they behave
similarly under a wide range of simulation parameters. These similarity values,
rather than the original high dimensional feature data, are used to train
kernelized machine learning algorithms such as support vector machines, thus
handling the curse-of-dimensionality that typically affects genomic machine
learning. Using four synthetic datasets of complex systems--three biological
models and one network flow optimization model--we demonstrate that when the
number of training samples is small compared to the number of features, the
simulation kernel approach dominates over no-prior-knowledge methods. In
addition to biology and medicine, this approach should be applicable to other
disciplines, such as weather forecasting, financial markets, and agricultural
management, where predictive models are sought and informative yet approximate
simulations are available. The Python SimKern software, the models (in MATLAB,
Octave, and R), and the datasets are made freely available at
this https URL .
",Statistics; Quantitative Biology,Statistics
"Really? Well. Apparently Bootstrapping Improves the Performance of Sarcasm and Nastiness Classifiers for Online Dialogue   More and more of the information on the web is dialogic, from Facebook
newsfeeds, to forum conversations, to comment threads on news articles. In
contrast to traditional, monologic Natural Language Processing resources such
as news, highly social dialogue is frequent in social media, making it a
challenging context for NLP. This paper tests a bootstrapping method,
originally proposed in a monologic domain, to train classifiers to identify two
different types of subjective language in dialogue: sarcasm and nastiness. We
explore two methods of developing linguistic indicators to be used in a first
level classifier aimed at maximizing precision at the expense of recall. The
best performing classifier for the first phase achieves 54% precision and 38%
recall for sarcastic utterances. We then use general syntactic patterns from
previous work to create more general sarcasm indicators, improving precision to
62% and recall to 52%. To further test the generality of the method, we then
apply it to bootstrapping a classifier for nastiness dialogic acts. Our first
phase, using crowdsourced nasty indicators, achieves 58% precision and 49%
recall, which increases to 75% precision and 62% recall when we bootstrap over
the first level with generalized syntactic patterns.
",Computer Science,Computer Science
"Comprehension-guided referring expressions   We consider generation and comprehension of natural language referring
expression for objects in an image. Unlike generic ""image captioning"" which
lacks natural standard evaluation criteria, quality of a referring expression
may be measured by the receiver's ability to correctly infer which object is
being described. Following this intuition, we propose two approaches to utilize
models trained for comprehension task to generate better expressions. First, we
use a comprehension module trained on human-generated expressions, as a
""critic"" of referring expression generator. The comprehension module serves as
a differentiable proxy of human evaluation, providing training signal to the
generation module. Second, we use the comprehension module in a
generate-and-rerank pipeline, which chooses from candidate expressions
generated by a model according to their performance on the comprehension task.
We show that both approaches lead to improved referring expression generation
on multiple benchmark datasets.
",Computer Science,Computer Science
"Algorithms for solving optimization problems arising from deep neural net models: nonsmooth problems   Machine Learning models incorporating multiple layered learning networks have
been seen to provide effective models for various classification problems. The
resulting optimization problem to solve for the optimal vector minimizing the
empirical risk is, however, highly nonconvex. This alone presents a challenge
to application and development of appropriate optimization algorithms for
solving the problem. However, in addition, there are a number of interesting
problems for which the objective function is non- smooth and nonseparable. In
this paper, we summarize the primary challenges involved, the state of the art,
and present some numerical results on an interesting and representative class
of problems.
",Statistics,Computer Science; Statistics
"Parameter and State Estimation in Queues and Related Stochastic Models: A Bibliography   This is an annotated bibliography on estimation and inference results for
queues and related stochastic models. The purpose of this document is to
collect and categorise works in the field, allowing for researchers and
practitioners to explore the various types of results that exist. This
bibliography attempts to include all known works that satisfy both of these
requirements: -Works that deal with queueing models. -Works that contain
contributions related to the methodology of parameter estimation, state
estimation, hypothesis testing, confidence interval and/or actual datasets of
application areas. Our attempt is to make this bibliography exhaustive, yet
there are possibly some papers that we have missed. As it is updated
continuously, additions and comments are welcomed. The sections below
categorise the works based on several categories. A single paper may appear in
several categories simultaneously. The final section lists all works in
chronological order along with short descriptions of the contributions. This
bibliography is maintained at
this http URL and may be cited as such.
We welcome additions and corrections.
",Computer Science; Mathematics; Statistics,Computer Science; Statistics
"Turbulent gas accretion between supermassive black holes and star-forming rings in the circumnuclear disk   While supermassive black holes are known to co-evolve with their host galaxy,
the precise nature and origin of this co-evolution is not clear. We here
explore the possible connection between star formation and black hole growth in
the circumnuclear disk (CND) to probe this connection in the vicinity close to
the black hole. We adopt here the circumnuclear disk model developed by
Kawakatu & Wada (2008) and Wutschik et al. (2013), and explore both the
dependence on the star formation recipe as well as the role of the
gravitational field, which can be dominated by the central black hole, the CND
itself or the host galaxy. A specific emphasis is put on the turbulence
regulated star formation model by Krumholz et al. (2005) to explore the impact
of a realistic star formation recipe. It is shown that this model helps to
introduce realistic fluctuations in the black hole and star formation rate,
without overestimating them. Consistent with previous works, we show that the
final black hole masses are rather insensitive to the masses of the initial
seeds, even for seed masses of up to 10^6 M_sol. In addition, we apply our
model to the formation of high-redshift quasars, as well as to the nearby
system NGC 6951, where a tentative comparison is made in spite of the presence
of a bar in the galaxy. We show that our model can reproduce the high black
hole masses of the high-redshift quasars within a sufficiently short time,
provided a high mass supply rate from the host galaxy. In addition, it
reproduces several of the properties observed in NGC 6951. With respect to the
latter system, our analysis suggests that supernova feedback may be important
to create the observed fluctuations in the star formation history as a result
of negative feedback effects.
",Physics,Physics
"Spin Transport and Accumulation in 2D Weyl Fermion System   In this work, we study the spin Hall effect and Rashba-Edelstein effect of a
2D Weyl fermion system in the clean limit using the Kubo formalism. Spin
transport is solely due to the spin-torque current in this strongly spin-orbit
coupled (SOC) system, and chiral spin-flip scattering off non-SOC scalar
impurities, with potential strength $V$ and size $a$, gives rise to a
skew-scattering mechanism for the spin Hall effect. The key result is that the
resultant spin-Hall angle has a fixed sign, with $\theta^{SH} \sim O
\left(\tfrac{V^2}{v_F^2/a^2} (k_F a)^4 \right)$ being a strongly-dependent
function of $k_F a$, with $k_F$ and $v_F$ being the Fermi wave-vector and Fermi
velocity respectively. This, therefore, allows for the possibility of tuning
the SHE by adjusting the Fermi energy or impurity size.
",Physics,Physics
"Wasserstein Introspective Neural Networks   We present Wasserstein introspective neural networks (WINN) that are both a
generator and a discriminator within a single model. WINN provides a
significant improvement over the recent introspective neural networks (INN)
method by enhancing INN's generative modeling capability. WINN has three
interesting properties: (1) A mathematical connection between the formulation
of the INN algorithm and that of Wasserstein generative adversarial networks
(WGAN) is made. (2) The explicit adoption of the Wasserstein distance into INN
results in a large enhancement to INN, achieving compelling results even with a
single classifier --- e.g., providing nearly a 20 times reduction in model size
over INN for unsupervised generative modeling. (3) When applied to supervised
classification, WINN also gives rise to improved robustness against adversarial
examples in terms of the error reduction. In the experiments, we report
encouraging results on unsupervised learning problems including texture, face,
and object modeling, as well as a supervised classification task against
adversarial attacks.
",Computer Science,Computer Science; Statistics
"Nonlinear parametric excitation effect induces stability transitions in swimming direction of flexible superparamagnetic microswimmers   Microscopic artificial swimmers have recently become highly attractive due to
their promising potential for biomedical applications. The pioneering work of
Dreyfus et al (2005) has demonstrated the motion of a microswimmer with an
undulating chain of superparamagnetic beads, which is actuated by an
oscillating external magnetic field. Interestingly, it has also been
theoretically predicted that the swimming direction of this swimmer will
undergo a $90^\circ$-transition when the magnetic field's oscillations
amplitude is increased above a critical value of $\sqrt{2}$. In this work, we
further investigate this transition both theoretically and experimentally by
using numerical simulations and presenting a novel flexible microswimmer with a
superparamagnetic head. We realize the $90^\circ$-transition in swimming
direction, prove that this effect depends on both frequency and amplitude of
the oscillating magnetic field, and demonstrate the existence of an optimal
amplitude, under which, maximal swimming speed can be achieved. By
asymptotically analyzing the dynamic motion of microswimmer with a minimal
two-link model, we reveal that the stability transitions representing the
changes in the swimming direction are induced by the effect of nonlinear
parametric excitation.
",Physics,Physics
"A Bootstrap Method for Error Estimation in Randomized Matrix Multiplication   In recent years, randomized methods for numerical linear algebra have
received growing interest as a general approach to large-scale problems.
Typically, the essential ingredient of these methods is some form of randomized
dimension reduction, which accelerates computations, but also creates random
approximation error. In this way, the dimension reduction step encodes a
tradeoff between cost and accuracy. However, the exact numerical relationship
between cost and accuracy is typically unknown, and consequently, it may be
difficult for the user to precisely know (1) how accurate a given solution is,
or (2) how much computation is needed to achieve a given level of accuracy. In
the current paper, we study randomized matrix multiplication (sketching) as a
prototype setting for addressing these general problems. As a solution, we
develop a bootstrap method for {directly estimating} the accuracy as a function
of the reduced dimension (as opposed to deriving worst-case bounds on the
accuracy in terms of the reduced dimension). From a computational standpoint,
the proposed method does not substantially increase the cost of standard
sketching methods, and this is made possible by an ""extrapolation"" technique.
In addition, we provide both theoretical and empirical results to demonstrate
the effectiveness of the proposed method.
",Computer Science; Statistics,Computer Science
"Force and torque of a string on a pulley   Every university introductory physics course considers the problem of
Atwood's machine taking into account the mass of the pulley. In the usual
treatment the tensions at the two ends of the string are offhandedly taken to
act on the pulley and be responsible for its rotation. However such a free-body
diagram of the forces on the pulley is not {\it a priori} justified, inducing
students to construct wrong hypotheses such as that the string transfers its
tension to the pulley or that some symmetry is in operation. We reexamine this
problem by integrating the contact forces between each element of the string
and the pulley and show that although the pulley does behave as if the tensions
were acting on it, this comes only as the end result of a detailed analysis. We
also address the question of how much friction is needed to prevent the string
from slipping over the pulley. Finally, we deal with the case in which the
string is on the verge of sliding and show that this will never happen unless
certain conditions are met by the coefficient of friction and the masses
involved.
",Physics,Physics
"Multi-User Multi-Armed Bandits for Uncoordinated Spectrum Access   A multi-user multi-armed bandit (MAB) framework is used to develop algorithms
for uncoordinated spectrum access. The number of users is assumed to be unknown
to each user. A stochastic setting is first considered, where the rewards on a
channel are the same for each user. In contrast to prior work, it is assumed
that the number of users can possibly exceed the number of channels, and that
rewards can be non-zero even under collisions. The proposed algorithm consists
of an estimation phase and an allocation phase. It is shown that if every user
adopts the algorithm, the system wide regret is constant with time with high
probability. The regret guarantees hold for any number of users and channels,
in particular, even when the number of users is less than the number of
channels. Next, an adversarial multi-user MAB framework is considered, where
the rewards on the channels are user-dependent. It is assumed that the number
of users is less than the number of channels, and that the users receive zero
reward on collision. The proposed algorithm combines the Exp3.P algorithm
developed in prior work for single user adversarial bandits with a collision
resolution mechanism to achieve sub-linear regret. It is shown that if every
user employs the proposed algorithm, the system wide regret is of the order
$O(T^\frac{3}{4})$ over a horizon of time $T$. The algorithms in both
stochastic and adversarial scenarios are extended to the dynamic case where the
number of users in the system evolves over time and are shown to lead to
sub-linear regret.
",Statistics,Computer Science
"Mechanism of the double heterostructure TiO2/ZnO/TiO2 for photocatalytic and photovoltaic applications: A theoretical study   Understanding the mechanism of the heterojunction is an important step
towards controllable and tunable interfaces for photocatalytic and photovoltaic
based devices. To this aim, we propose a thorough study of a double
heterostructure system consisting of two semiconductors with large band gap,
namely, wurtzite ZnO and anatase TiO2. We demonstrate via first-principle
calculations two stable configurations of ZnO/TiO2 interfaces. Our structural
analysis provides a key information on the nature of the complex interface and
lattice distortions occurring when combining these materials. The study of the
electronic properties of the sandwich nanostructure TiO2/ZnO/TiO2 reveals that
conduction band arises mainly from Ti3d orbitals, while valence band is
maintained by O2p of ZnO, and that the trapped states within the gap region
frequent in single heterostructure are substantially reduced in the double
interface system. Moreover, our work explains the origin of certain optical
transitions observed in the experimental studies. Unexpectedly, as a
consequence of different bond distortions, the results on the band alignments
show electron accumulation in the left shell of TiO2 rather than the right one.
Such behavior provides more choice for the sensitization and functionalization
of TiO2 surfaces.
",Physics,Physics
"Ultracold Atomic Gases in Artificial Magnetic Fields (PhD thesis)   A phenomenon can hardly be found that accompanied physical paradigms and
theoretical concepts in a more reflecting way than magnetism. From the
beginnings of metaphysics and the first classical approaches to magnetic poles
and streamlines of the field, it has inspired modern physics on its way to the
classical field description of electrodynamics, and further to the quantum
mechanical description of internal degrees of freedom of elementary particles.
Meanwhile, magnetic manifestations have posed and still do pose complex and
often controversially debated questions. This regards so various and utterly
distinct topics as quantum spin systems and the grand unification theory. This
may be foremost caused by the fact that all of these effects are based on
correlated structures, which are induced by the interplay of dynamics and
elementary interactions. It is strongly correlated systems that certainly
represent one of the most fascinating and universal fields of research. In
particular, low dimensional systems are in the focus of interest, as they
reveal strongly pronounced correlations of counterintuitive nature. As regards
this framework, the quantum Hall effect must be seen as one of the most
intriguing and complex problems of modern solid state physics. Even after two
decades and the same number of Nobel prizes, it still keeps researchers of
nearly all fields of physics occupied. In spite of seminal progress, its
inherent correlated order still lacks understanding on a microscopic level.
Despite this, it is obvious that the phenomenon is thoroughly fundamental of
nature. To resolve some puzzles of this nature is a key topic of this thesis.
(excerpt from abstract)
",Physics,Physics
"Defect entropies and enthalpies in Barium Fluoride   Various experimental techniques, have revealed that the predominant intrinsic
point defects in BaF$_2$ are anion Frenkel defects. Their formation enthalpy
and entropy as well as the corresponding parameters for the fluorine vacancy
and fluorine interstitial motion have been determined. In addition, low
temperature dielectric relaxation measurements in BaF$_2$ doped with uranium
leads to the parameters {\tau}$_0$, E in the Arrhenius relation
{\tau}={\tau}$_0$exp(E/kBT) for the relaxation time {\tau}. For the relaxation
peak associated with a single tetravalent uranium, the migration entropy
deduced from the pre-exponential factor {\tau}$_0$, is smaller than the anion
Frenkel defect formation entropy by almost two orders of magnitude. We show
that, despite their great variation, the defect entropies and enthalpies are
interconnected through a model based on anharmonic properties of the bulk
material that have been recently studied by employing density-functional theory
and density-functional perturbation theory.
",Physics,Physics
"Multipath IP Routing on End Devices: Motivation, Design, and Performance   Most end devices are now equipped with multiple network interfaces.
Applications can exploit all available interfaces and benefit from multipath
transmission. Recently Multipath TCP (MPTCP) was proposed to implement
multipath transmission at the transport layer and has attracted lots of
attention from academia and industry. However, MPTCP only supports TCP-based
applications and its multipath routing flexibility is limited. In this paper,
we investigate the possibility of orchestrating multipath transmission from the
network layer of end devices, and develop a Multipath IP (MPIP) design
consisting of signaling, session and path management, multipath routing, and
NAT traversal. We implement MPIP in Linux and Android kernels. Through
controlled lab experiments and Internet experiments, we demonstrate that MPIP
can effectively achieve multipath gains at the network layer. It not only
supports the legacy TCP and UDP protocols, but also works seamlessly with
MPTCP. By facilitating user-defined customized routing, MPIP can route traffic
from competing applications in a coordinated fashion to maximize the aggregate
user Quality-of-Experience.
",Computer Science,Computer Science
"Sylvester's Problem and Mock Heegner Points   We prove that if $p \equiv 4,7 \pmod{9}$ is prime and $3$ is not a cube
modulo $p$, then both of the equations $x^3+y^3=p$ and $x^3+y^3=p^2$ have a
solution with $x,y \in \mathbb{Q}$.
",Mathematics,Mathematics
"Commissioning and performance results of the WFIRST/PISCES integral field spectrograph   The Prototype Imaging Spectrograph for Coronagraphic Exoplanet Studies
(PISCES) is a high contrast integral field spectrograph (IFS) whose design was
driven by WFIRST coronagraph instrument requirements. We present commissioning
and operational results using PISCES as a camera on the High Contrast Imaging
Testbed at JPL. PISCES has demonstrated ability to achieve high contrast
spectral retrieval with flight-like data reduction and analysis techniques.
",Physics,Computer Science
"Classification of rank two Lie conformal algebras   We give a complete classification (up to isomorphism) of Lie conformal
algebras which are free of rank two as $\C[\partial]$-modules, and determine
their automorphism groups.
",Mathematics,Mathematics
"Non-Spherical Szekeres models in the language of Cosmological Perturbations   We study the differences and equivalences between the non-perturbative
description of the evolution of cosmic structure furnished by the Szekeres dust
models (a non-spherical exact solution of Einstein's equations) and the
dynamics of Cosmological Perturbation Theory (CPT) for dust sources in a
$\Lambda$CDM background. We show how the dynamics of Szekeres models can be
described by evolution equations given in terms of ""exact fluctuations"" that
identically reduce (at all orders) to evolution equations of CPT in the
comoving isochronous gauge. We explicitly show how Szekeres linearised exact
fluctuations are specific (deterministic) realisations of standard linear
perturbations of CPT given as random fields but, as opposed to the latter
perturbations, they can be evolved exactly into the full non-linear regime. We
prove two important results: (i) the conservation of the curvature perturbation
(at all scales) also holds for the appropriate approximation of the exact
Szekeres fluctuations in a $\Lambda$CDM background, and (ii) the different
collapse morphologies of Szekeres models yields, at nonlinear order, different
functional forms for the growth factor that follows from the study of redshift
space distortions. The metric based potentials used in linear CPT are computed
in terms of the parameters of the linearised Szekeres models, thus allowing us
to relate our results to linear CPT results in other gauges. We believe that
these results provide a solid starting stage to examine the role of
non-perturbative General Relativity in current cosmological research.
",Physics,Physics
"Effects of tunnelling and asymmetry for system-bath models of electron transfer   We apply the newly derived nonadiabatic golden-rule instanton theory to
asymmetric models describing electron-transfer in solution. The models go
beyond the usual spin-boson description and have anharmonic free-energy
surfaces with different values for the reactant and product reorganization
energies. The instanton method gives an excellent description of the behaviour
of the rate constant with respect to asymmetry for the whole range studied. We
derive a general formula for an asymmetric version of Marcus theory based on
the classical limit of the instanton and find that this gives significant
corrections to the standard Marcus theory. A scheme is given to compute this
rate based only on equilibrium simulations. We also compare the rate constants
obtained by the instanton method with its classical limit to study the effect
of tunnelling and other quantum nuclear effects. These quantum effects can
increase the rate constant by orders of magnitude.
",Physics,Physics
"A Procedural Texture Generation Framework Based on Semantic Descriptions   Procedural textures are normally generated from mathematical models with
parameters carefully selected by experienced users. However, for naive users,
the intuitive way to obtain a desired texture is to provide semantic
descriptions such as ""regular,"" ""lacelike,"" and ""repetitive"" and then a
procedural model with proper parameters will be automatically suggested to
generate the corresponding textures. By contrast, it is less practical for
users to learn mathematical models and tune parameters based on multiple
examinations of large numbers of generated textures. In this study, we propose
a novel framework that generates procedural textures according to user-defined
semantic descriptions, and we establish a mapping between procedural models and
semantic texture descriptions. First, based on a vocabulary of semantic
attributes collected from psychophysical experiments, a multi-label learning
method is employed to annotate a large number of textures with semantic
attributes to form a semantic procedural texture dataset. Then, we derive a low
dimensional semantic space in which the semantic descriptions can be separated
from one other. Finally, given a set of semantic descriptions, the diverse
properties of the samples in the semantic space can lead the framework to find
an appropriate generation model that uses appropriate parameters to produce a
desired texture. The experimental results show that the proposed framework is
effective and that the generated textures closely correlate with the input
semantic descriptions.
",Computer Science,Computer Science
"Game Theory for Multi-Access Edge Computing: Survey, Use Cases, and Future Trends   Game Theory (GT) has been used with significant success to formulate, and
either design or optimize, the operation of many representative communications
and networking scenarios. The games in these scenarios involve, as usual,
diverse players with conflicting goals. This paper primarily surveys the
literature that has applied theoretical games to wireless networks, emphasizing
use cases of upcoming Multi-Access Edge Computing (MEC). MEC is relatively new
and offers cloud services at the network periphery, aiming to reduce service
latency backhaul load, and enhance relevant operational aspects such as Quality
of Experience or security. Our presentation of GT is focused on the major
challenges imposed by MEC services over the wireless resources. The survey is
divided into classical and evolutionary games. Then, our discussion proceeds to
more specific aspects which have a considerable impact on the game usefulness,
namely: rational vs. evolving strategies, cooperation among players, available
game information, the way the game is played (single turn, repeated), the game
model evaluation, and how the model results can be applied for both optimizing
resource-constrained resources and balancing diverse trade-offs in real edge
networking scenarios. Finally, we reflect on lessons learned, highlighting
future trends and research directions for applying theoretical model games in
upcoming MEC services, considering both network design issues and usage
scenarios.
",Computer Science,Computer Science
"Forecasting Internally Displaced Population Migration Patterns in Syria and Yemen   Armed conflict has led to an unprecedented number of internally displaced
persons (IDPs) - individuals who are forced out of their homes but remain
within their country. IDPs often urgently require shelter, food, and
healthcare, yet prediction of when large fluxes of IDPs will cross into an area
remains a major challenge for aid delivery organizations. Accurate forecasting
of IDP migration would empower humanitarian aid groups to more effectively
allocate resources during conflicts. We show that monthly flow of IDPs from
province to province in both Syria and Yemen can be accurately forecasted one
month in advance, using publicly available data. We model monthly IDP flow
using data on food price, fuel price, wage, geospatial, and news data. We find
that machine learning approaches can more accurately forecast migration trends
than baseline persistence models. Our findings thus potentially enable
proactive aid allocation for IDPs in anticipation of forecasted arrivals.
",Statistics,Statistics
"Thermal and non-thermal emission from the cocoon of a gamma-ray burst jet   We present hydrodynamic simulations of the hot cocoon produced when a
relativistic jet passes through the gamma-ray burst (GRB) progenitor star and
its environment, and we compute the lightcurve and spectrum of the radiation
emitted by the cocoon. The radiation from the cocoon has a nearly thermal
spectrum with a peak in the X-ray band, and it lasts for a few minutes in the
observer frame; the cocoon radiation starts at roughly the same time as when
$\gamma$-rays from a burst trigger detectors aboard GRB satellites. The
isotropic cocoon luminosity ($\sim 10^{47}$ erg s$^{-1}$) is of the same order
of magnitude as the X-ray luminosity of a typical long-GRB afterglow during the
plateau phase. This radiation should be identifiable in the Swift data because
of its nearly thermal spectrum which is distinct from the somewhat brighter
power-law component. The detection of this thermal component would provide
information regarding the size and density stratification of the GRB progenitor
star. Photons from the cocoon are also inverse-Compton (IC) scattered by
electrons in the relativistic jet. We present the IC lightcurve and spectrum,
by post-processing the results of the numerical simulations. The IC spectrum
lies in 10 keV--MeV band for typical GRB parameters. The detection of this IC
component would provide an independent measurement of GRB jet Lorentz factor
and it would also help to determine the jet magnetisation parameter.
",Physics,Physics
"Crowdsourcing Multiple Choice Science Questions   We present a novel method for obtaining high-quality, domain-targeted
multiple choice questions from crowd workers. Generating these questions can be
difficult without trading away originality, relevance or diversity in the
answer options. Our method addresses these problems by leveraging a large
corpus of domain-specific text and a small set of existing questions. It
produces model suggestions for document selection and answer distractor choice
which aid the human question generation process. With this method we have
assembled SciQ, a dataset of 13.7K multiple choice science exam questions
(Dataset available at this http URL). We demonstrate that the
method produces in-domain questions by providing an analysis of this new
dataset and by showing that humans cannot distinguish the crowdsourced
questions from original questions. When using SciQ as additional training data
to existing questions, we observe accuracy improvements on real science exams.
",Computer Science; Statistics,Computer Science
"Entanglement Entropy of Eigenstates of Quadratic Fermionic Hamiltonians   In a seminal paper [D. N. Page, Phys. Rev. Lett. 71, 1291 (1993)], Page
proved that the average entanglement entropy of subsystems of random pure
states is $S_{\rm ave}\simeq\ln{\cal D}_{\rm A} - (1/2) {\cal D}_{\rm
A}^2/{\cal D}$ for $1\ll{\cal D}_{\rm A}\leq\sqrt{\cal D}$, where ${\cal
D}_{\rm A}$ and ${\cal D}$ are the Hilbert space dimensions of the subsystem
and the system, respectively. Hence, typical pure states are (nearly) maximally
entangled. We develop tools to compute the average entanglement entropy
$\langle S\rangle$ of all eigenstates of quadratic fermionic Hamiltonians. In
particular, we derive exact bounds for the most general translationally
invariant models $\ln{\cal D}_{\rm A} - (\ln{\cal D}_{\rm A})^2/\ln{\cal D}
\leq \langle S \rangle \leq \ln{\cal D}_{\rm A} - [1/(2\ln2)] (\ln{\cal D}_{\rm
A})^2/\ln{\cal D}$. Consequently we prove that: (i) if the subsystem size is a
finite fraction of the system size then $\langle S\rangle<\ln{\cal D}_{\rm A}$
in the thermodynamic limit, i.e., the average over eigenstates of the
Hamiltonian departs from the result for typical pure states, and (ii) in the
limit in which the subsystem size is a vanishing fraction of the system size,
the average entanglement entropy is maximal, i.e., typical eigenstates of such
Hamiltonians exhibit eigenstate thermalization.
",Physics,Physics
"Facets of a mixed-integer bilinear covering set with bounds on variables   We derive a closed form description of the convex hull of mixed-integer
bilinear covering set with bounds on the integer variables. This convex hull
description is completely determined by considering some orthogonal disjunctive
sets defined in a certain way. Our description does not introduce any new
variables. We also derive a linear time separation algorithm for finding the
facet defining inequalities of this convex hull. We show the effectiveness of
the new inequalities using some examples.
",Mathematics,Mathematics
"Evolution Strategies as a Scalable Alternative to Reinforcement Learning   We explore the use of Evolution Strategies (ES), a class of black box
optimization algorithms, as an alternative to popular MDP-based RL techniques
such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show
that ES is a viable solution strategy that scales extremely well with the
number of CPUs available: By using a novel communication strategy based on
common random numbers, our ES implementation only needs to communicate scalars,
making it possible to scale to over a thousand parallel workers. This allows us
to solve 3D humanoid walking in 10 minutes and obtain competitive results on
most Atari games after one hour of training. In addition, we highlight several
advantages of ES as a black box optimization technique: it is invariant to
action frequency and delayed rewards, tolerant of extremely long horizons, and
does not need temporal discounting or value function approximation.
",Computer Science; Statistics,Computer Science; Statistics
"Regularizing nonlinear Schroedinger equations through partial off-axis variations   We study a class of focusing nonlinear Schroedinger-type equations derived
recently by Dumas, Lannes and Szeftel within the mathematical description of
high intensity laser beams [7]. These equations incorporate the possibility of
a (partial) off-axis variation of the group velocity of such laser beams
through a second order partial differential operator acting in some, but not
necessarily all, spatial directions. We study the well-posedness theory for
such models and obtain a regularizing effect, even in the case of only partial
off-axis dependence. This provides an answer to an open problem posed in [7].
",Mathematics,Physics
"Unoriented Cobordism Maps on Link Floer Homology   We study the problem of defining maps on link Floer homology induced by
unoriented link cobordisms. We provide a natural notion of link cobordism,
disoriented link cobordism, which tracks the motion of index zero and index
three critical points. Then we construct a map on unoriented link Floer
homology associated to a disoriented link cobordism. Furthermore, we give a
comparison with Oszváth-Stipsicz-Szabó's and Manolescu's constructions of
link cobordism maps for an unoriented band move.
",Mathematics,Mathematics
"Homogeneity Pursuit in Single Index Models based Panel Data Analysis   Panel data analysis is an important topic in statistics and econometrics.
Traditionally, in panel data analysis, all individuals are assumed to share the
same unknown parameters, e.g. the same coefficients of covariates when the
linear models are used, and the differences between the individuals are
accounted for by cluster effects. This kind of modelling only makes sense if
our main interest is on the global trend, this is because it would not be able
to tell us anything about the individual attributes which are sometimes very
important. In this paper, we proposed a modelling based on the single index
models embedded with homogeneity for panel data analysis, which builds the
individual attributes in the model and is parsimonious at the same time. We
develop a data driven approach to identify the structure of homogeneity, and
estimate the unknown parameters and functions based on the identified
structure. Asymptotic properties of the resulting estimators are established.
Intensive simulation studies conducted in this paper also show the resulting
estimators work very well when sample size is finite. Finally, the proposed
modelling is applied to a public financial dataset and a UK climate dataset,
the results reveal some interesting findings.
",Mathematics; Statistics,Mathematics; Statistics
"Impact of Detour-Aware Policies on Maximizing Profit in Ridesharing   This paper provides efficient solutions to maximize profit for commercial
ridesharing services, under a pricing model with detour-based discounts for
passengers. We propose greedy heuristics for real-time ride matching that offer
different trade-offs between optimality and speed. Simulations on New York City
(NYC) taxi trip data show that our heuristics are up to 90% optimal and 10^5
times faster than the (necessarily) exponential-time optimal algorithm.
Commercial ridesharing service providers generate significant savings by
matching multiple ride requests using heuristic methods. The resulting savings
are typically shared between the service provider (in the form of increased
profit) and the ridesharing passengers (in the form of discounts). It is not
clear a priori how this split should be effected, since higher discounts would
encourage more ridesharing, thereby increasing total savings, but the fraction
of savings taken as profit is reduced. We simulate a scenario where the
decisions of the passengers to opt for ridesharing depend on the discount
offered by the service provider. We provide an adaptive learning algorithm
IDFLA that learns the optimal profit-maximizing discount factor for the
provider. An evaluation over NYC data shows that IDFLA, on average, learns the
optimal discount factor in under 16 iterations.
Finally, we investigate the impact of imposing a detour-aware routing policy
based on sequential individual rationality, a recently proposed concept. Such
restricted policies offer a better ride experience, increasing the provider's
market share, but at the cost of decreased average per-ride profit due to the
reduced number of matched rides. We construct a model that captures these
opposing effects, wherein simulations based on NYC data show that a 7% increase
in market share would suffice to offset the decreased average per-ride profit.
",Computer Science; Mathematics,Computer Science
"Ensemble representation learning: an analysis of fitness and survival for wrapper-based genetic programming methods   Recently we proposed a general, ensemble-based feature engineering wrapper
(FEW) that was paired with a number of machine learning methods to solve
regression problems. Here, we adapt FEW for supervised classification and
perform a thorough analysis of fitness and survival methods within this
framework. Our tests demonstrate that two fitness metrics, one introduced as an
adaptation of the silhouette score, outperform the more commonly used Fisher
criterion. We analyze survival methods and demonstrate that $\epsilon$-lexicase
survival works best across our test problems, followed by random survival which
outperforms both tournament and deterministic crowding. We conduct a benchmark
comparison to several classification methods using a large set of problems and
show that FEW can improve the best classifier performance in several cases. We
show that FEW generates consistent, meaningful features for a biomedical
problem with different ML pairings.
",Computer Science; Statistics,Statistics
"Design Considerations for Proposed Fermilab Integrable RCS   Integrable optics is an innovation in particle accelerator design that
provides strong nonlinear focusing while avoiding parametric resonances. One
promising application of integrable optics is to overcome the traditional
limits on accelerator intensity imposed by betatron tune-spread and collective
instabilities. The efficacy of high-intensity integrable accelerators will be
undergo comprehensive testing over the next several years at the Fermilab
Integrable Optics Test Accelerator (IOTA) and the University of Maryland
Electron Ring (UMER). We propose an integrable Rapid-Cycling Synchrotron (iRCS)
as a replacement for the Fermilab Booster to achieve multi-MW beam power for
the Fermilab high-energy neutrino program. We provide a overview of the machine
parameters and discuss an approach to lattice optimization. Integrable optics
requires arcs with integer-pi phase advance followed by drifts with matched
beta functions. We provide an example integrable lattice with features of a
modern RCS - long dispersion-free drifts, low momentum compaction,
superperiodicity, chromaticity correction, separate-function magnets, and
bounded beta functions.
",Physics,Physics
"Surrogate-Based Bayesian Inverse Modeling of the Hydrological System: An Adaptive Approach Considering Surrogate Approximation Erro   Bayesian inverse modeling is important for a better understanding of
hydrological processes. However, this approach can be computationally demanding
as it usually requires a large number of model evaluations. To address this
issue, one can take advantage of surrogate modeling techniques. Nevertheless,
when approximation error of the surrogate model is neglected in inverse
modeling, the inversion result will be biased. In this paper, we develop a
surrogate-based Bayesian inversion framework that explicitly quantifies and
gradually reduces the approximation error of the surrogate. Specifically, two
strategies are proposed and compared. The first strategy works by obtaining an
ensemble of sparse polynomial chaos expansion (PCE) surrogates with Markov
chain Monte Carlo sampling, while the second one uses Gaussian process (GP) to
simulate the approximation error of a single sparse PCE surrogate. The two
strategies can also be applied with other surrogates, thus they have general
applicability. By adaptively refining the surrogate over the posterior
distribution, we can gradually reduce the surrogate approximation error to a
small level. Demonstrated with three case studies involving
high-dimensionality, multi-modality and a real-world application, respectively,
it is found that both strategies can reduce the bias introduced by surrogate
modeling, while the second strategy has a better performance as it integrates
two methods (i.e., sparse PCE and GP) that complement each other.
",Statistics,Computer Science; Statistics
"Perception-in-the-Loop Adversarial Examples   We present a scalable, black box, perception-in-the-loop technique to find
adversarial examples for deep neural network classifiers. Black box means that
our procedure only has input-output access to the classifier, and not to the
internal structure, parameters, or intermediate confidence values.
Perception-in-the-loop means that the notion of proximity between inputs can be
directly queried from human participants rather than an arbitrarily chosen
metric. Our technique is based on covariance matrix adaptation evolution
strategy (CMA-ES), a black box optimization approach. CMA-ES explores the
search space iteratively in a black box manner, by generating populations of
candidates according to a distribution, choosing the best candidates according
to a cost function, and updating the posterior distribution to favor the best
candidates. We run CMA-ES using human participants to provide the fitness
function, using the insight that the choice of best candidates in CMA-ES can be
naturally modeled as a perception task: pick the top $k$ inputs perceptually
closest to a fixed input. We empirically demonstrate that finding adversarial
examples is feasible using small populations and few iterations. We compare the
performance of CMA-ES on the MNIST benchmark with other black-box approaches
using $L_p$ norms as a cost function, and show that it performs favorably both
in terms of success in finding adversarial examples and in minimizing the
distance between the original and the adversarial input. In experiments on the
MNIST, CIFAR10, and GTSRB benchmarks, we demonstrate that CMA-ES can find
perceptually similar adversarial inputs with a small number of iterations and
small population sizes when using perception-in-the-loop. Finally, we show that
networks trained specifically to be robust against $L_\infty$ norm can still be
susceptible to perceptually similar adversarial examples.
",Computer Science; Statistics,Computer Science; Statistics
"EasyInterface: A toolkit for rapid development of GUIs for research prototype tools   In this paper we describe EasyInterface, an open-source toolkit for rapid
development of web-based graphical user interfaces (GUIs). This toolkit
addresses the need of researchers to make their research prototype tools
available to the community, and integrating them in a common environment,
rapidly and without being familiar with web programming or GUI libraries in
general. If a tool can be executed from a command-line and its output goes to
the standard output, then in few minutes one can make it accessible via a
web-interface or within Eclipse. Moreover, the toolkit defines a text-based
language that can be used to get more sophisticated GUIs, e.g., syntax
highlighting, dialog boxes, user interactions, etc. EasyInterface was
originally developed for building a common frontend for tools developed in the
Envisage project.
",Computer Science,Computer Science
"A New Torsion Pendulum for Gravitational Reference Sensor Technology Development   We report on the design and sensitivity of a new torsion pendulum for
measuring the performance of ultra-precise inertial sensors and for the
development of associated technologies for space-based gravitational wave
observatories and geodesy missions. The apparatus comprises a 1 m-long, 50
um-diameter, tungsten fiber that supports an inertial member inside a vacuum
system. The inertial member is an aluminum crossbar with four hollow cubic test
masses at each end. This structure converts the rotation of the torsion
pendulum into translation of the test masses. Two test masses are enclosed in
capacitive sensors which provide readout and actuation. These test masses are
electrically insulated from the rest of the cross-bar and their electrical
charge is controlled by photoemission using fiber-coupled ultraviolet light
emitting diodes. The capacitive readout measures the test mass displacement
with a broadband sensitivity of 30 nm / sqrt(Hz), and is complemented by a
laser interferometer with a sensitivity of about 0.5 nm / sqrt(Hz). The
performance of the pendulum, as determined by the measured residual torque
noise and expressed in terms of equivalent force acting on a single test mass,
is roughly 200 fN / sqrt(Hz) around 2 mHz, which is about a factor of 20 above
the thermal noise limit of the fiber.
",Physics,Physics
"Lax orthogonal factorisations in monad-quantale-enriched categories   We show that, for a quantale $V$ and a $\mathsf{Set}$-monad $\mathbb{T}$
laxly extended to $V$-$\mathsf{Rel}$, the presheaf monad on the category of
$(\mathbb{T},V)$-categories is simple, giving rise to a lax orthogonal
factorisation system (lofs) whose corresponding weak factorisation system has
embeddings as left part. In addition, we present presheaf submonads and study
the LOFSs they define. This provides a method of constructing weak
factorisation systems on some well-known examples of topological categories
over $\mathsf{Set}$.
",Mathematics,Mathematics
"Tunneling of Glashow-Weinberg-Salam model particles from Black Hole Solutions in Rastall Theory   Using the semiclassical WKB approximation and Hamilton-Jacobi method, we
solve an equation of motion for the Glashow-Weinberg-Salam model, which is
important for understanding the unified gauge-theory of weak and
electromagnetic interactions. We calculate the tunneling rate of the massive
charged W-bosons in a background of electromagnetic field to investigate the
Hawking temperature of black holes surrounded by perfect fluid in Rastall
theory. Then, we study the quantum gravity effects on the generalized Proca
equation with generalized uncertainty principle (GUP) on this background. We
show that quantum gravity effects leave the remnants on the Hawking temperature
and the Hawking radiation becomes nonthermal.
",Physics,Physics
"Virtual plane-wave imaging via Marchenko redatuming   Marchenko redatuming is a novel scheme used to retrieve up- and down-going
Green's functions in an unknown medium. Marchenko equations are based on
reciprocity theorems and are derived on the assumption of the existence of so
called focusing functions, i.e. functions which exhibit time-space focusing
properties once injected in the subsurface. In contrast to interferometry but
similarly to standard migration methods, Marchenko redatuming only requires an
estimate of the direct wave from the virtual source (or to the virtual
receiver), illumination from only one side of the medium, and no physical
sources (or receivers) inside the medium. In this contribution we consider a
different time-focusing condition within the frame of Marchenko redatuming and
show how this can lead to the retrieval of virtual plane-wave responses, thus
allowing multiple-free imaging using only a 1 dimensional sampling of the
targeted model. The potential of the new method is demonstrated on a 2D
synthetic model.
",Physics,Physics
"Autonomous drone cinematographer: Using artistic principles to create smooth, safe, occlusion-free trajectories for aerial filming   Autonomous aerial cinematography has the potential to enable automatic
capture of aesthetically pleasing videos without requiring human intervention,
empowering individuals with the capability of high-end film studios. Current
approaches either only handle off-line trajectory generation, or offer
strategies that reason over short time horizons and simplistic representations
for obstacles, which result in jerky movement and low real-life applicability.
In this work we develop a method for aerial filming that is able to trade off
shot smoothness, occlusion, and cinematography guidelines in a principled
manner, even under noisy actor predictions. We present a novel algorithm for
real-time covariant gradient descent that we use to efficiently find the
desired trajectories by optimizing a set of cost functions. Experimental
results show that our approach creates attractive shots, avoiding obstacles and
occlusion 65 times over 1.25 hours of flight time, re-planning at 5 Hz with a
10 s time horizon. We robustly film human actors, cars and bicycles performing
different motion among obstacles, using various shot types.
",Computer Science,Computer Science
"Polarisation of submillimetre lines from interstellar medium   Magnetic fields play important roles in many astrophysical processes.
However, there is no universal diagnostic for the magnetic fields in the
interstellar medium (ISM) and each magnetic tracer has its limitation. Any new
detection method is thus valuable. Theoretical studies have shown that
submillimetre fine-structure lines are polarised due to atomic alignment by
Ultraviolet (UV) photon-excitation, which opens up a new avenue to probe
interstellar magnetic fields. We will, for the first time, perform synthetic
observations on the simulated three-dimensional ISM to demonstrate the
measurability of the polarisation of submillimetre atomic lines. The maximum
polarisation for different absorption and emission lines expected from various
sources, including Star-Forming Regions (SFRs) are provided. Our results
demonstrate that the polarisation of submillimetre atomic lines is a powerful
magnetic tracer and add great value to the observational studies of the
submilimetre astronomy.
",Physics,Physics
"On the structure of radial solutions for some quasilinear elliptic equations   In this paper we study entire radial solutions for the quasilinear
$p$-Laplace equation $\Delta_p u + k(x) f(u) = 0$ where $k$ is a radial
positive weight and the nonlinearity behaves e.g. as
$f(u)=u|u|^{q-2}-u|u|^{Q-2}$ with $q<Q$. In particular we focus our attention
on solutions (positive and sign changing) which are infinitesimal at infinity,
thus providing an extension of a previous result by Tang (2001).
",Mathematics,Mathematics
"Resilience of Complex Networks   This article determines and characterizes the minimal number of actuators
needed to ensure structural controllability of a linear system under structural
alterations that can severe the connection between any two states. We assume
that initially the system is structurally controllable with respect to a given
set of controls, and propose an efficient system-synthesis mechanism to find
the minimal number of additional actuators required for resilience of the
system w.r.t such structural changes. The effectiveness of this approach is
demonstrated by using standard IEEE power networks.
",Computer Science; Mathematics,Computer Science
"Heavy Traffic Limit for a Tandem Queue with Identical Service Times   We consider a two-node tandem queueing network in which the upstream queue is
M/G/1 and each job reuses its upstream service requirement when moving to the
downstream queue. Both servers employ the first-in-first-out policy. We
investigate the amount of work in the second queue at certain embedded arrival
time points, namely when the upstream queue has just emptied. We focus on the
case of infinite-variance service times and obtain a heavy traffic process
limit for the embedded Markov chain.
",Mathematics,Computer Science
"Multi-Kernel LS-SVM Based Bio-Clinical Data Integration: Applications to Ovarian Cancer   The medical research facilitates to acquire a diverse type of data from the
same individual for particular cancer. Recent studies show that utilizing such
diverse data results in more accurate predictions. The major challenge faced is
how to utilize such diverse data sets in an effective way. In this paper, we
introduce a multiple kernel based pipeline for integrative analysis of
high-throughput molecular data (somatic mutation, copy number alteration, DNA
methylation and mRNA) and clinical data. We apply the pipeline on Ovarian
cancer data from TCGA. After multiple kernels have been generated from the
weighted sum of individual kernels, it is used to stratify patients and predict
clinical outcomes. We examine the survival time, vital status, and neoplasm
cancer status of each subtype to verify how well they cluster. We have also
examined the power of molecular and clinical data in predicting dichotomized
overall survival data and to classify the tumor grade for the cancer samples.
It was observed that the integration of various data types yields higher
log-rank statistics value. We were also able to predict clinical status with
higher accuracy as compared to using individual data types.
",Statistics,Statistics
"Quantum Klein Space and Superspace   We give an algebraic quantization, in the sense of quantum groups, of the
complex Minkowski space, and we examine the real forms corresponding to the
signatures $(3,1)$, $(2,2)$, $(4,0)$, constructing the corresponding quantum
metrics and providing an explicit presentation of the quantized coordinate
algebras. In particular, we focus on the Kleinian signature $(2,2)$. The
quantizations of the complex and real spaces come together with a coaction of
the quantizations of the respective symmetry groups. We also extend such
quantizations to the $\mathcal{N}=1$ supersetting.
",Mathematics,Mathematics
"The Structure of the Broad-Line Region In Active Galactic Nuclei. II. Dynamical Modeling of Data from the AGN10 Reverberation Mapping Campaign   We present inferences on the geometry and kinematics of the broad-Hbeta
line-emitting region in four active galactic nuclei monitored as a part of the
fall 2010 reverberation mapping campaign at MDM Observatory led by the Ohio
State University. From modeling the continuum variability and response in
emission-line profile changes as a function of time, we infer the geometry of
the Hbeta- emitting broad line regions to be thick disks that are close to
face-on to the observer with kinematics that are well-described by either
elliptical orbits or inflowing gas. We measure the black hole mass to be log
(MBH) = 7.25 (+/-0.10) for Mrk 335, 7.86 (+0.20, -0.17) for Mrk 1501, 7.84
(+0.14, -0.19) for 3C 120, and 6.92 (+0.24, -0.23) for PG 2130+099. These black
hole mass measurements are not based on a particular assumed value of the
virial scale factor f, allowing us to compute individual f factors for each
target. Our results nearly double the number of targets that have been modeled
in this manner, and investigate the properties of a more diverse sample by
including previously modeled objects. We measure an average scale factor f in
the entire sample to be log10(f) = 0.54 +/- 0.17 when the line dispersion is
used to characterize the line width, which is consistent with values derived
using the normalization of the MBH-sigma relation. We find that the scale
factor f for individual targets is likely correlated with the black hole mass,
inclination angle, and opening angle of the broad line region but we do not
find any correlation with the luminosity.
",Physics,Physics
"SMT Solving for Vesicle Traffic Systems in Cells   In biology, there are several questions that translate to combinatorial
search. For example, vesicle traffic systems that move cargo within eukaryotic
cells have been proposed to exhibit several graph properties such as three
connectivity. These properties are consequences of underlying biophysical
constraints. A natural question for biologists is: what are the possible
networks for various combinations of those properties? In this paper, we
present novel SMT based encodings of the properties over vesicle traffic
systems and a tool that searches for the networks that satisfies the properties
using SMT solvers. In our experiments, we show that our tool can search for
networks of sizes that are considered to be relevant by biologists.
",Computer Science,Computer Science
"Interactive Certificates for Polynomial Matrices with Sub-Linear Communication   We develop and analyze new protocols to verify the correctness of various
computations on matrices over F[x], where F is a field. The properties we
verify concern an F[x]-module and therefore cannot simply rely on
previously-developed linear algebra certificates which work only for vector
spaces. Our protocols are interactive certificates, often randomized, and
featuring a constant number of rounds of communication between the prover and
verifier. We seek to minimize the communication cost so that the amount of data
sent during the protocol is significantly smaller than the size of the result
being verified, which can be useful when combining protocols or in some
multi-party settings. The main tools we use are reductions to existing linear
algebra certificates and a new protocol to verify that a given vector is in the
F[x]-linear span of a given matrix.
",Computer Science,Computer Science
"A Simple Convolutional Generative Network for Next Item Recommendation   Convolutional Neural Networks (CNNs) have been recently introduced in the
domain of session-based next item recommendation. An ordered collection of past
items the user has interacted with in a session (or sequence) are embedded into
a 2-dimensional latent matrix, and treated as an image. The convolution and
pooling operations are then applied to the mapped item embeddings. In this
paper, we first examine the typical session-based CNN recommender and show that
both the generative model and network architecture are suboptimal when modeling
long-range dependencies in the item sequence. To address the issues, we
introduce a simple, but very effective generative model that is capable of
learning high-level representation from both short- and long-range item
dependencies. The network architecture of the proposed model is formed of a
stack of \emph{holed} convolutional layers, which can efficiently increase the
receptive fields without relying on the pooling operation. Another contribution
is the effective use of residual block structure in recommender systems, which
can ease the optimization for much deeper networks. The proposed generative
model attains state-of-the-art accuracy with less training time in the next
item recommendation task. It accordingly can be used as a powerful
recommendation baseline to beat in future, especially when there are long
sequences of user feedback.
",Statistics,Computer Science
"Instrument-Armed Bandits   We extend the classic multi-armed bandit (MAB) model to the setting of
noncompliance, where the arm pull is a mere instrument and the treatment
applied may differ from it, which gives rise to the instrument-armed bandit
(IAB) problem. The IAB setting is relevant whenever the experimental units are
human since free will, ethics, and the law may prohibit unrestricted or forced
application of treatment. In particular, the setting is relevant in bandit
models of dynamic clinical trials and other controlled trials on human
interventions. Nonetheless, the setting has not been fully investigate in the
bandit literature. We show that there are various and divergent notions of
regret in this setting, all of which coincide only in the classic MAB setting.
We characterize the behavior of these regrets and analyze standard MAB
algorithms. We argue for a particular kind of regret that captures the causal
effect of treatments but show that standard MAB algorithms cannot achieve
sublinear control on this regret. Instead, we develop new algorithms for the
IAB problem, prove new regret bounds for them, and compare them to standard MAB
algorithms in numerical examples.
",Computer Science; Statistics,Computer Science; Statistics
"Brief Notes on Hard Takeoff, Value Alignment, and Coherent Extrapolated Volition   I make some basic observations about hard takeoff, value alignment, and
coherent extrapolated volition, concepts which have been central in analyses of
superintelligent AI systems.
",Computer Science,Computer Science; Mathematics
"Massively-Parallel Feature Selection for Big Data   We present the Parallel, Forward-Backward with Pruning (PFBP) algorithm for
feature selection (FS) in Big Data settings (high dimensionality and/or sample
size). To tackle the challenges of Big Data FS PFBP partitions the data matrix
both in terms of rows (samples, training examples) as well as columns
(features). By employing the concepts of $p$-values of conditional independence
tests and meta-analysis techniques PFBP manages to rely only on computations
local to a partition while minimizing communication costs. Then, it employs
powerful and safe (asymptotically sound) heuristics to make early, approximate
decisions, such as Early Dropping of features from consideration in subsequent
iterations, Early Stopping of consideration of features within the same
iteration, or Early Return of the winner in each iteration. PFBP provides
asymptotic guarantees of optimality for data distributions faithfully
representable by a causal network (Bayesian network or maximal ancestral
graph). Our empirical analysis confirms a super-linear speedup of the algorithm
with increasing sample size, linear scalability with respect to the number of
features and processing cores, while dominating other competitive algorithms in
its class.
",Computer Science; Statistics,Computer Science; Mathematics; Statistics
"Towards a Unified Taxonomy of Biclustering Methods   Being an unsupervised machine learning and data mining technique,
biclustering and its multimodal extensions are becoming popular tools for
analysing object-attribute data in different domains. Apart from conventional
clustering techniques, biclustering is searching for homogeneous groups of
objects while keeping their common description, e.g., in binary setting, their
shared attributes. In bioinformatics, biclustering is used to find genes, which
are active in a subset of situations, thus being candidates for biomarkers.
However, the authors of those biclustering techniques that are popular in gene
expression analysis, may overlook the existing methods. For instance, BiMax
algorithm is aimed at finding biclusters, which are well-known for decades as
formal concepts. Moreover, even if bioinformatics classify the biclustering
methods according to reasonable domain-driven criteria, their classification
taxonomies may be different from survey to survey and not full as well. So, in
this paper we propose to use concept lattices as a tool for taxonomy building
(in the biclustering domain) and attribute exploration as means for
cross-domain taxonomy completion.
",Computer Science; Statistics,Computer Science; Statistics
"Improved Fixed-Rank Nyström Approximation via QR Decomposition: Practical and Theoretical Aspects   The Nyström method is a popular technique for computing fixed-rank
approximations of large kernel matrices using a small number of landmark
points. In practice, to ensure high quality approximations, the number of
landmark points is chosen to be greater than the target rank. However, the
standard Nyström method uses a sub-optimal procedure for rank reduction
mainly due to its simplicity. In this paper, we highlight the drawbacks of
standard Nyström in terms of poor performance and lack of theoretical
guarantees. To address these issues, we present an efficient method for
generating improved fixed-rank Nyström approximations. Theoretical analysis
and numerical experiments are provided to demonstrate the advantages of the
modified method over the standard Nyström method. Overall, the aim of this
paper is to convince researchers to use the modified method, as it has nearly
identical computational complexity, is easy to code, and has greatly improved
accuracy in many cases.
",Computer Science; Statistics,Computer Science
"End-to-end semantic face segmentation with conditional random fields as convolutional, recurrent and adversarial networks   Recent years have seen a sharp increase in the number of related yet distinct
advances in semantic segmentation. Here, we tackle this problem by leveraging
the respective strengths of these advances. That is, we formulate a conditional
random field over a four-connected graph as end-to-end trainable convolutional
and recurrent networks, and estimate them via an adversarial process.
Importantly, our model learns not only unary potentials but also pairwise
potentials, while aggregating multi-scale contexts and controlling higher-order
inconsistencies. We evaluate our model on two standard benchmark datasets for
semantic face segmentation, achieving state-of-the-art results on both of them.
",Computer Science,Computer Science; Statistics
"Viscosity solutions and the minimal surface system   We give a definition of viscosity solution for the minimal surface system and
prove a version of Allard regularity theorem in this setting.
",Mathematics,Mathematics
"Sharper and Simpler Nonlinear Interpolants for Program Verification   Interpolation of jointly infeasible predicates plays important roles in
various program verification techniques such as invariant synthesis and CEGAR.
Intrigued by the recent result by Dai et al.\ that combines real algebraic
geometry and SDP optimization in synthesis of polynomial interpolants, the
current paper contributes its enhancement that yields sharper and simpler
interpolants. The enhancement is made possible by: theoretical observations in
real algebraic geometry; and our continued fraction-based algorithm that rounds
off (potentially erroneous) numerical solutions of SDP solvers. Experiment
results support our tool's effectiveness; we also demonstrate the benefit of
sharp and simple interpolants in program verification examples.
",Computer Science,Computer Science; Mathematics
"The Rice-Shapiro theorem in Computable Topology   We provide requirements on effectively enumerable topological spaces which
guarantee that the Rice-Shapiro theorem holds for the computable elements of
these spaces. We show that the relaxation of these requirements leads to the
classes of effectively enumerable topological spaces where the Rice-Shapiro
theorem does not hold. We propose two constructions that generate effectively
enumerable topological spaces with particular properties from wn--families and
computable trees without computable infinite paths. Using them we propose
examples that give a flavor of this class.
",Computer Science; Mathematics,Mathematics
"Controlling Sources of Inaccuracy in Stochastic Kriging   Scientists and engineers commonly use simulation models to study real systems
for which actual experimentation is costly, difficult, or impossible. Many
simulations are stochastic in the sense that repeated runs with the same input
configuration will result in different outputs. For expensive or time-consuming
simulations, stochastic kriging \citep{ankenman} is commonly used to generate
predictions for simulation model outputs subject to uncertainty due to both
function approximation and stochastic variation. Here, we develop and justify a
few guidelines for experimental design, which ensure accuracy of stochastic
kriging emulators. We decompose error in stochastic kriging predictions into
nominal, numeric, parameter estimation and parameter estimation numeric
components and provide means to control each in terms of properties of the
underlying experimental design. The design properties implied for each source
of error are weakly conflicting and broad principles are proposed. In brief,
space-filling properties ""small fill distance"" and ""large separation distance""
should balance with replication at distinct input configurations, with number
of replications depending on the relative magnitudes of stochastic and process
variability. Non-stationarity implies higher input density in more active
regions, while regression functions imply a balance with traditional design
properties. A few examples are presented to illustrate the results.
",Mathematics; Statistics,Computer Science; Mathematics
"Correspondence Theorem between Holomorphic Discs and Tropical Discs on K3 Surfaces   We prove that the open Gromov-Witten invariants on K3 surfaces satisfy the
Kontsevich-Soibelman wall-crossing formula. One one hand, this gives a
geometric interpretation of the slab functions in Gross-Siebert program. On the
other hands, the open Gromov-Witten invariants coincide with the weighted
counting of tropical discs. This is an analog of the corresponding theorem on
toric varieties \cite{M2}\cite{NS} but on compact Calabi-Yau surfaces.
",Mathematics,Mathematics
"Visual Analogies between Atari Games for Studying Transfer Learning in RL   In this work, we ask the following question: Can visual analogies, learned in
an unsupervised way, be used in order to transfer knowledge between pairs of
games and even play one game using an agent trained for another game? We
attempt to answer this research question by creating visual analogies between a
pair of games: a source game and a target game. For example, given a video
frame in the target game, we map it to an analogous state in the source game
and then attempt to play using a trained policy learned for the source game. We
demonstrate convincing visual mapping between four pairs of games (eight
mappings), which are used to evaluate three transfer learning approaches.
",Statistics,Computer Science; Statistics
"In search of a new economic model determined by logistic growth   In this paper we extend the work by Ryuzo Sato devoted to the development of
economic growth models within the framework of the Lie group theory. We propose
a new growth model based on the assumption of logistic growth in factors. It is
employed to derive new production functions and introduce a new notion of wage
share. In the process it is shown that the new functions compare reasonably
well against relevant economic data. The corresponding problem of maximization
of profit under conditions of perfect competition is solved with the aid of one
of these functions. In addition, it is explained in reasonably rigorous
mathematical terms why Bowley's law no longer holds true in post-1960 data.
",Mathematics,Mathematics; Statistics
"On Axiomatizability of the Multiplicative Theory of Numbers   The multiplicative theory of a set of numbers (which could be natural,
integer, rational, real or complex numbers) is the first-order theory of the
structure of that set with (solely) the multiplication operation (that set is
taken to be multiplicative, i.e., closed under multiplication). In this paper
we study the multiplicative theories of the complex, real and (positive)
rational numbers. These theories (and also the multiplicative theories of
natural and integer numbers) are known to be decidable (i.e., there exists an
algorithm that decides whether a given sentence is derivable form the theory);
here we present explicit axiomatizations for them and show that they are not
finitely axiomatizable. For each of these sets (of complex, real and [positive]
rational numbers) a language, including the multiplication operation, is
introduced in a way that it allows quantifier elimination (for the theory of
that set).
",Computer Science; Mathematics,Computer Science
"Deep Health Care Text Classification   Health related social media mining is a valuable apparatus for the early
recognition of the diverse antagonistic medicinal conditions. Mostly, the
existing methods are based on machine learning with knowledge-based learning.
This working note presents the Recurrent neural network (RNN) and Long
short-term memory (LSTM) based embedding for automatic health text
classification in the social media mining. For each task, two systems are built
and that classify the tweet at the tweet level. RNN and LSTM are used for
extracting features and non-linear activation function at the last layer
facilitates to distinguish the tweets of different categories. The experiments
are conducted on 2nd Social Media Mining for Health Applications Shared Task at
AMIA 2017. The experiment results are considerable; however the proposed method
is appropriate for the health text classification. This is primarily due to the
reason that, it doesn't rely on any feature engineering mechanisms.
",Computer Science,Computer Science
"Optimal compromise between incompatible conditional probability distributions, with application to Objective Bayesian Kriging   Models are often defined through conditional rather than joint distributions,
but it can be difficult to check whether the conditional distributions are
compatible, i.e. whether there exists a joint probability distribution which
generates them. When they are compatible, a Gibbs sampler can be used to sample
from this joint distribution. When they are not, the Gibbs sampling algorithm
may still be applied, resulting in a ""pseudo-Gibbs sampler"". We show its
stationary probability distribution to be the optimal compromise between the
conditional distributions, in the sense that it minimizes a mean squared misfit
between them and its own conditional distributions. This allows us to perform
Objective Bayesian analysis of correlation parameters in Kriging models by
using univariate conditional Jeffreys-rule posterior distributions instead of
the widely used multivariate Jeffreys-rule posterior. This strategy makes the
full-Bayesian procedure tractable. Numerical examples show it has near-optimal
frequentist performance in terms of prediction interval coverage.
",Mathematics; Statistics,Statistics
"Jamming transitions induced by an attraction in pedestrian flow   We numerically study jamming transitions in pedestrian flow interacting with
an attraction, mostly based on the social force model for pedestrians who can
join the attraction. We formulate the joining probability as a function of
social influence from others, reflecting that individual choice behavior is
likely influenced by others. By controlling pedestrian influx and the social
influence parameter, we identify various pedestrian flow patterns. For the
bidirectional flow scenario, we observe a transition from the free flow phase
to the freezing phase, in which oppositely walking pedestrians reach a complete
stop and block each other. On the other hand, a different transition behavior
appears in the unidirectional flow scenario, i.e., from the free flow phase to
the localized jam phase and then to the extended jam phase. It is also observed
that the extended jam phase can end up in freezing phenomena with a certain
probability when pedestrian flux is high with strong social influence. This
study highlights that attractive interactions between pedestrians and an
attraction can trigger jamming transitions by increasing the number of
conflicts among pedestrians near the attraction. In order to avoid excessive
pedestrian jams, we suggest suppressing the number of conflicts under a certain
level by moderating pedestrian influx especially when the social influence is
strong.
",Physics,Physics
"Robust Guaranteed-Cost Adaptive Quantum Phase Estimation   Quantum parameter estimation plays a key role in many fields like quantum
computation, communication and metrology. Optimal estimation allows one to
achieve the most precise parameter estimates, but requires accurate knowledge
of the model. Any inevitable uncertainty in the model parameters may heavily
degrade the quality of the estimate. It is therefore desired to make the
estimation process robust to such uncertainties. Robust estimation was
previously studied for a varying phase, where the goal was to estimate the
phase at some time in the past, using the measurement results from both before
and after that time within a fixed time interval up to current time. Here, we
consider a robust guaranteed-cost filter yielding robust estimates of a varying
phase in real time, where the current phase is estimated using only past
measurements. Our filter minimizes the largest (worst-case) variance in the
allowable range of the uncertain model parameter(s) and this determines its
guaranteed cost. It outperforms in the worst case the optimal Kalman filter
designed for the model with no uncertainty, that corresponds to the center of
the possible range of the uncertain parameter(s). Moreover, unlike the Kalman
filter, our filter in the worst case always performs better than the best
achievable variance for heterodyne measurements, that we consider as the
tolerable threshold for our system. Furthermore, we consider effective quantum
efficiency and effective noise power, and show that our filter provides the
best results by these measures in the worst case.
",Computer Science; Mathematics,Computer Science; Physics
"Global well-posedness for 2-D Boussinesq system with the temperature-dependent viscosity and supercritical dissipation   The present paper is dedicated to the global well-posedness issue for the
Boussinesq system with the temperature-dependent viscosity in $\mathbb{R}^2.$
We aim at extending the work by Abidi and Zhang ( Adv. Math. 2017 (305)
1202--1249 ) to a supercritical dissipation for temperature.
",Mathematics,Mathematics
"User-friendly guarantees for the Langevin Monte Carlo with inaccurate gradient   In this paper, we study the problem of sampling from a given probability
density function that is known to be smooth and strongly log-concave. We
analyze several methods of approximate sampling based on discretizations of the
(highly overdamped) Langevin diffusion and establish guarantees on its error
measured in the Wasserstein-2 distance. Our guarantees improve or extend the
state-of-the-art results in three directions. First, we provide an upper bound
on the error of the first-order Langevin Monte Carlo (LMC) algorithm with
optimized varying step-size. This result has the advantage of being horizon
free (we do not need to know in advance the target precision) and to improve by
a logarithmic factor the corresponding result for the constant step-size.
Second, we study the case where accurate evaluations of the gradient of the
log-density are unavailable, but one can have access to approximations of the
aforementioned gradient. In such a situation, we consider both deterministic
and stochastic approximations of the gradient and provide an upper bound on the
sampling error of the first-order LMC that quantifies the impact of the
gradient evaluation inaccuracies. Third, we establish upper bounds for two
versions of the second-order LMC, which leverage the Hessian of the
log-density. We nonasymptotic guarantees on the sampling error of these
second-order LMCs. These guarantees reveal that the second-order LMC algorithms
improve on the first-order LMC in ill-conditioned settings.
",Computer Science; Mathematics; Statistics,Mathematics; Statistics
"Nonparametric estimation of locally stationary Hawkes processe   In this paper we consider multivariate Hawkes processes with baseline hazard
and kernel functions that depend on time. This defines a class of locally
stationary processes. We discuss estimation of the time-dependent baseline
hazard and kernel functions based on a localized criterion. Theory on
stationary Hawkes processes is extended to develop asymptotic theory for the
estimator in the locally stationary model.
",Mathematics; Statistics,Mathematics; Statistics
"Excitonic gap generation in thin-film topological insulators   In this work, we analyze the excitonic gap generation in the strong-coupling
regime of thin films of three-dimensional time-reversal-invariant topological
insulators. We start by writing down the effective gauge theory in
2+1-dimensions from the projection of the 3+1-dimensional quantum
electrodynamics. Within this method, we obtain a short-range interaction, which
has the form of a Thirring-like term, and a long-range one. The interaction
between the two surface states of the material induces an excitonic gap. By
using the large-$N$ approximation in the strong-coupling limit, we find that
there is a dynamical mass generation for the excitonic states that preserves
time-reversal symmetry and is related to the dynamical chiral-symmetry breaking
of our model. This symmetry breaking occurs only for values of the
fermion-flavor number smaller than $N_{c}\approx 11.8$. Our results show that
the inclusion of the full dynamical interaction strongly modifies the critical
number of flavors for the occurrence of exciton condensation, and therefore,
cannot be neglected.
",Physics,Physics
"Lion and man in non-metric spaces   A lion and a man move continuously in a space $X$. The aim of the lion is to
capture his prey while the man wants to escape forever. Which of them has a
strategy? This question has been studied for different metric domains. In this
article we consider the case of general topological spaces.
",Mathematics,Mathematics
"Nonzero positive solutions of a multi-parameter elliptic system with functional BCs   We prove, by topological methods, new results on the existence of nonzero
positive weak solutions for a class of multi-parameter second order elliptic
systems subject to functional boundary conditions. The setting is fairly
general and covers the case of multi-point, integral and nonlinear boundary
conditions. We also present a non-existence result. We provide some examples to
illustrate the applicability our theoretical results.
",Mathematics,Mathematics
"Discretization error estimates for penalty formulations of a linearized Canham-Helfrich type energy   This paper is concerned with minimization of a fourth-order linearized
Canham-Helfrich energy subject to Dirichlet boundary conditions on curves
inside the domain. Such problems arise in the modeling of the mechanical
interaction of biomembranes with embedded particles. There, the curve
conditions result from the imposed particle--membrane coupling. We prove
almost-$H^{\frac{5}{2}}$ regularity of the solution and then consider two
possible penalty formulations. For the combination of these penalty
formulations with a Bogner-Fox-Schmit finite element discretization we prove
discretization error estimates which are optimal in view of the solution's
reduced regularity. The error estimates are based on a general estimate for
linear penalty problems in Hilbert spaces. Finally, we illustrate the
theoretical results by numerical computations. An important feature of the
presented discretization is that it does not require to resolve the particle
boundary. This is crucial in order to avoid re-meshing if the presented problem
arises as subproblem in a model where particles are allowed to move or rotate.
",Mathematics,Mathematics
"Birth of isolated nested cylinders and limit cycles in 3D piecewise smooth vector fields with symmetry   Our start point is a 3D piecewise smooth vector field defined in two zones
and presenting a shared fold curve for the two smooth vector fields considered.
Moreover, these smooth vector fields are symmetric relative to the fold curve,
giving raise to a continuum of nested topological cylinders such that each
orthogonal section of these cylinders is filled by centers. First we prove that
the normal form considered represents a whole class of piecewise smooth vector
fields. After we perturb the initial model in order to obtain exactly
$\mathcal{L}$ invariant planes containing centers. A second perturbation of the
initial model also is considered in order to obtain exactly $k$ isolated
cylinders filled by periodic orbits. Finally, joining the two previous
bifurcations we are able to exhibit a model, preserving the symmetry relative
to the fold curve, and having exactly $k.\mathcal{L}$ limit cycles.
",Mathematics,Physics
"The James construction and $π_4(\mathbb{S}^3)$ in homotopy type theory   In the first part of this paper we present a formalization in Agda of the
James construction in homotopy type theory. We include several fragments of
code to show what the Agda code looks like, and we explain several techniques
that we used in the formalization. In the second part, we use the James
construction to give a constructive proof that $\pi_4(\mathbb{S}^3)$ is of the
form $\mathbb{Z}/n\mathbb{Z}$ (but we do not compute the $n$ here).
",Computer Science; Mathematics,Mathematics
"Variants of RMSProp and Adagrad with Logarithmic Regret Bounds   Adaptive gradient methods have become recently very popular, in particular as
they have been shown to be useful in the training of deep neural networks. In
this paper we have analyzed RMSProp, originally proposed for the training of
deep neural networks, in the context of online convex optimization and show
$\sqrt{T}$-type regret bounds. Moreover, we propose two variants SC-Adagrad and
SC-RMSProp for which we show logarithmic regret bounds for strongly convex
functions. Finally, we demonstrate in the experiments that these new variants
outperform other adaptive gradient techniques or stochastic gradient descent in
the optimization of strongly convex functions as well as in training of deep
neural networks.
",Computer Science; Statistics,Computer Science; Statistics
"Gravitational radiation from compact binary systems in screened modified gravity   Screened modified gravity (SMG) is a kind of scalar-tensor theory with
screening mechanisms, which can suppress the fifth force in dense regions and
allow theories to evade the solar system and laboratory tests. In this paper,
we investigate how the screening mechanisms in SMG affect the gravitational
radiation damping effects, calculate in detail the rate of the energy loss due
to the emission of tensor and scalar gravitational radiations, and derive their
contributions to the change in the orbital period of the binary system. We find
that the scalar radiation depends on the screened parameters and the
propagation speed of scalar waves, and the scalar dipole radiation dominates
the orbital decay of the binary system. For strongly self-gravitating bodies,
all effects of scalar sector are strongly suppressed by the screening
mechanisms in SMG. By comparing our results to observations of binary system
PSR J1738+0333, we place the stringent constraints on the screening mechanisms
in SMG. As an application of these results, we focus on three specific models
of SMG (chameleon, symmetron, and dilaton), and derive the constraints on the
model parameters, respectively.
",Physics,Physics
"Stochastic Dynamic Optimal Power Flow in Distribution Network with Distributed Renewable Energy and Battery Energy Storage   The penetration of distributed renewable energy (DRE) greatly raises the risk
of distribution network operation such as peak shaving and voltage stability.
Battery energy storage (BES) has been widely accepted as the most potential
application to cope with the challenge of high penetration of DRE. To cope with
the uncertainties and variability of DRE, a stochastic day-ahead dynamic
optimal power flow (DOPF) and its algorithm are proposed. The overall economy
is achieved by fully considering the DRE, BES, electricity purchasing and
active power losses. The rainflow algorithm-based cycle counting method of BES
is incorporated in the DOPF model to capture the cell degradation, greatly
extending the expected BES lifetime and achieving a better economy. DRE
scenarios are generated to consider the uncertainties and correlations based on
the Copula theory. To solve the DOPF model, we propose a Lagrange
relaxation-based algorithm, which has a significantly reduced complexity with
respect to the existing techniques. For this reason, the proposed algorithm
enables much more scenarios incorporated in the DOPF model and better captures
the DRE uncertainties and correlations. Finally, numerical studies for the
day-ahead DOPF in the IEEE 123-node test feeder are presented to demonstrate
the merits of the proposed method. Results show that the actual BES life
expectancy of the proposed model has increased to 4.89 times compared with the
traditional ones. The problems caused by DRE are greatly alleviated by fully
capturing the uncertainties and correlations with the proposed method.
",Mathematics,Computer Science
"NodeTrix Planarity Testing with Small Clusters   We study the NodeTrix planarity testing problem for flat clustered graphs
when the maximum size of each cluster is bounded by a constant $k$. We consider
both the case when the sides of the matrices to which the edges are incident
are fixed and the case when they can be arbitrarily chosen. We show that
NodeTrix planarity testing with fixed sides can be solved in
$O(k^{3k+\frac{3}{2}} n^3)$ time for every flat clustered graph that can be
reduced to a partial 2-tree by collapsing its clusters into single vertices. In
the general case, NodeTrix planarity testing with fixed sides can be solved in
$O(n^3)$ time for $k = 2$, but it is NP-complete for any $k \geq 3$. NodeTrix
planarity testing remains NP-complete also in the free side model when $k > 4$.
",Computer Science,Mathematics
"Inverse problems in models of resource distribution   We continue to study the problem of modeling of substitution of production
factors motivated by the need for computable mathematical models of economics
that could be used as a basis in applied developments. This problem has been
studied for several decades, and several connections to complex analysis and
geometry has been established. We describe several models of resource
distribution and discuss the inverse problems for the generalized Radon
transform arising is these models. We give a simple explicit range
characterization for a particular case of the generalized Radon transform, and
we apply it to show that the most popular production functions are compatible
with these models. Besides, we give a necessary condition and a sufficient
condition for solvability of the model identification problem in the form of an
appropriate moment problem. These conditions are formulated in terms of rhombic
tilings.
",Mathematics,Mathematics; Statistics
"KZ-Calogero correspondence revisited   We discuss the correspondence between the Knizhnik-Zamolodchikov equations
associated with $GL(N)$ and the $n$-particle quantum Calogero model in the case
when $n$ is not necessarily equal to $N$. This can be viewed as a natural
""quantization"" of the quantum-classical correspondence between quantum Gaudin
and classical Calogero models.
",Physics; Mathematics,Mathematics
"Just-infinite C*-algebras and their invariants   Just-infinite C*-algebras, i.e., infinite dimensional C*-algebras, whose
proper quotients are finite dimensional, were investigated in
[Grigorchuk-Musat-Rordam, 2016]. One particular example of a just-infinite
residually finite dimensional AF-algebras was constructed in that article. In
this paper we extend that construction by showing that each infinite
dimensional metrizable Choquet simplex is affinely homeomorphic to the trace
simplex of a just-infinite residually finite dimensional C*-algebras. The trace
simplex of any unital residually finite dimensional C*-algebra is hence
realized by a just-infinite one. We determine the trace simplex of the
particular residually finite dimensional AF-algebras constructed in the above
mentioned article, and we show that it has precisely one extremal trace of type
II_1.
We give a complete description of the Bratteli diagrams corresponding to
residually finite dimensional AF-algebras. We show that a modification of any
such Bratteli diagram, similar to the modification that makes an arbitrary
Bratteli diagram simple, will yield a just-infinite residually finite
dimensional AF-algebra.
",Mathematics,Mathematics
"Never Forget: Balancing Exploration and Exploitation via Learning Optical Flow   Exploration bonus derived from the novelty of the states in an environment
has become a popular approach to motivate exploration for deep reinforcement
learning agents in the past few years. Recent methods such as curiosity-driven
exploration usually estimate the novelty of new observations by the prediction
errors of their system dynamics models. Due to the capacity limitation of the
models and difficulty of performing next-frame prediction, however, these
methods typically fail to balance between exploration and exploitation in
high-dimensional observation tasks, resulting in the agents forgetting the
visited paths and exploring those states repeatedly. Such inefficient
exploration behavior causes significant performance drops, especially in large
environments with sparse reward signals. In this paper, we propose to introduce
the concept of optical flow estimation from the field of computer vision to
deal with the above issue. We propose to employ optical flow estimation errors
to examine the novelty of new observations, such that agents are able to
memorize and understand the visited states in a more comprehensive fashion. We
compare our method against the previous approaches in a number of experimental
experiments. Our results indicate that the proposed method appears to deliver
superior and long-lasting performance than the previous methods. We further
provide a set of comprehensive ablative analysis of the proposed method, and
investigate the impact of optical flow estimation on the learning curves of the
DRL agents.
",Computer Science,Computer Science; Statistics
"Combating Fake News: A Survey on Identification and Mitigation Techniques   The proliferation of fake news on social media has opened up new directions
of research for timely identification and containment of fake news, and
mitigation of its widespread impact on public opinion. While much of the
earlier research was focused on identification of fake news based on its
contents or by exploiting users' engagements with the news on social media,
there has been a rising interest in proactive intervention strategies to
counter the spread of misinformation and its impact on society. In this survey,
we describe the modern-day problem of fake news and, in particular, highlight
the technical challenges associated with it. We discuss existing methods and
techniques applicable to both identification and mitigation, with a focus on
the significant advances in each method and their advantages and limitations.
In addition, research has often been limited by the quality of existing
datasets and their specific application contexts. To alleviate this problem, we
comprehensively compile and summarize characteristic features of available
datasets. Furthermore, we outline new directions of research to facilitate
future development of effective and interdisciplinary solutions.
",Computer Science; Statistics,Computer Science
"Symmetries and multipeakon solutions for the modified two-component Camassa-Holm system   Compared with the two-component Camassa-Holm system, the modified
two-component Camassa-Holm system introduces a regularized density which makes
possible the existence of solutions of lower regularity, and in particular of
multipeakon solutions. In this paper, we derive a new pointwise invariant for
the modified two-component Camassa-Holm system. The derivation of the invariant
uses directly the symmetry of the system, following the classical argument of
Noether's theorem. The existence of the multipeakon solutions can be directly
inferred from this pointwise invariant. This derivation shows the strong
connection between symmetries and the existence of special solutions. The
observation also holds for the scalar Camassa-Holm equation and, for
comparison, we have also included the corresponding derivation. Finally, we
compute explicitly the solutions obtained for the peakon-antipeakon case. We
observe the existence of a periodic solution which has not been reported in the
literature previously. This case shows the attractive effect that the
introduction of an elastic potential can have on the solutions.
",Mathematics,Mathematics
"Zeroth order regular approximation approach to electric dipole moment interactions of the electron   A quasi-relativistic two-component approach for an efficient calculation of
$\mathcal{P,T}$-odd interactions caused by a permanent electric dipole moment
of the electron (eEDM) is presented. The approach uses a (two-component)
complex generalized Hartree-Fock (cGHF) and a complex generalized Kohn-Sham
(cGKS) scheme within the zeroth order regular approximation (ZORA). In
applications to select heavy-elemental polar diatomic molecular radicals, which
are promising candidates for an eEDM experiment, the method is compared to
relativistic four-component electron-correlation calculations and confirms
values for the effective electrical field acting on the unpaired electron for
RaF, BaF, YbF and HgF. The calculations show that purely relativistic effects,
involving only the lower component of the Dirac bi-spinor, are well described
by treating only the upper component explicitly.
",Physics,Physics
"Systems of cubic forms in many variables   We consider a system of $R$ cubic forms in $n$ variables, with integer
coefficients, which define a smooth complete intersection in projective space.
Provided $n\geq 25R$, we prove an asymptotic formula for the number of integer
points in an expanding box at which these forms simultaneously vanish. In
particular we can handle systems of forms in $O(R)$ variables, previous work
having required that $n \gg R^2$. One conjectures that $n \geq 6R+1$ should be
sufficient. We reduce the problem to an upper bound for the number of solutions
to a certain auxiliary inequality. To prove this bound we adapt a method of
Davenport.
",Mathematics,Mathematics
"Unsupervised prototype learning in an associative-memory network   Unsupervised learning in a generalized Hopfield associative-memory network is
investigated in this work. First, we prove that the (generalized) Hopfield
model is equivalent to a semi-restricted Boltzmann machine with a layer of
visible neurons and another layer of hidden binary neurons, so it could serve
as the building block for a multilayered deep-learning system. We then
demonstrate that the Hopfield network can learn to form a faithful internal
representation of the observed samples, with the learned memory patterns being
prototypes of the input data. Furthermore, we propose a spectral method to
extract a small set of concepts (idealized prototypes) as the most concise
summary or abstraction of the empirical data.
",Computer Science; Physics,Computer Science; Statistics
"Searching for the Transit of the Earth--mass exoplanet Proxima~Centauri~b in Antarctica: Preliminary Result   Proxima Centauri is known as the closest star from the Sun. Recently, radial
velocity observations revealed the existence of an Earth-mass planet around it.
With an orbital period of ~11 days, the surface of Proxima Centauri b is
temperate and might be habitable. We took a photometric monitoring campaign to
search for its transit, using the Bright Star Survey Telescope at the Zhongshan
Station in Antarctica. A transit-like signal appearing on 2016 September 8th,
is identified tentatively. Its midtime, $T_{C}=2,457,640.1990\pm0.0017$ HJD, is
consistent with the predicted ephemeris based on RV orbit in a 1$\sigma$
confidence interval. Time-correlated noise is pronounced in the light curve of
Proxima Centauri, affecting detection of transits. We develop a technique, in a
Gaussian process framework, to gauge the statistical significance of potential
transit detection. The tentative transit signal reported here, has a confidence
level of $2.5\sigma$. Further detection of its periodic signals is necessary to
confirm the planetary transit of Proxima Centauri b. We plan to monitor Proxima
Centauri in next Polar night at Dome A in Antarctica, taking the advantage of
continuous darkness. \citet{Kipping17} reported two tentative transit-like
signals of Proxima Centauri b, observed by the Microvariability and Oscillation
of Stars space Telescope in 2014 and 2015, respectively. The midtransit time of
our detection is 138 minutes later than that predicted by their transit
ephemeris. If all the signals are real transits, the misalignment of the epochs
plausibly suggests transit timing variations of Proxima Centauri b induced by
an outer planet in this system.
",Physics,Physics
"Indoor Office Wideband Penetration Loss Measurements at 73 GHz   This paper presents millimeter wave (mmWave) penetration loss measurements
and analysis at 73 GHz using a wideband sliding correlator channel sounder in
an indoor office environment. Penetration loss was measured using a carefully
controlled measurement setup for many common indoor building materials such as
glass doors, glass windows, closet doors, steel doors, and whiteboard writing
walls. Measurements were conducted using narrowbeam transmitter (TX) and
receiver (RX) horn antennas that were boresight-aligned with a test material
between the antennas. Overall, 21 different locations were measured for 6
different materials such that the same type of material was tested in at least
two locations in order to characterize the effect of penetration loss for
materials with similar composition. As shown here, attenuation through common
materials ranged between 0.8 dB/cm and 9.9 dB/cm for co-polarized antennas,
while cross-polarized antennas exhibited similar attenuation for most
materials, but up to 23.4 dB/cm of attenuation for others. The penetration loss
results presented here are useful for site-specific planning tools that will
model indoor mmWave networks, without the need for expensive measurement
campaigns.
",Computer Science,Computer Science
"On types of degenerate critical points of real polynomial functions   In this paper, we consider the problem of identifying the type (local
minimizer, maximizer or saddle point) of a given isolated real critical point
$c$, which is degenerate, of a multivariate polynomial function $f$. To this
end, we introduce the definition of faithful radius of $c$ by means of the
curve of tangency of $f$. We show that the type of $c$ can be determined by the
global extrema of $f$ over the Euclidean ball centered at $c$ with a faithful
radius.We propose algorithms to compute a faithful radius of $c$ and determine
its type.
",Mathematics,Mathematics
"Projection-Free Bandit Convex Optimization   In this paper, we propose the first computationally efficient projection-free
algorithm for bandit convex optimization (BCO). We show that our algorithm
achieves a sublinear regret of $O(nT^{4/5})$ (where $T$ is the horizon and $n$
is the dimension) for any bounded convex functions with uniformly bounded
gradients. We also evaluate the performance of our algorithm against baselines
on both synthetic and real data sets for quadratic programming, portfolio
selection and matrix completion problems.
",Statistics,Computer Science; Statistics
"Geometric Fluctuation Theorem   We derive an extended fluctuation theorem for a geometric pumping in a
spin-boson system under a periodic control of environmental temperatures by
using a Markovian quantum master equation. We perform the Monte-Carlo
simulation and obtain the current distribution, the average current and the
fluctuation. Using the extended fluctuation theorem we try to explain the
results of our simulation. The fluctuation theorem leads to the fluctuation
dissipation relations but the absence of the conventional reciprocal relation.
",Physics,Physics
"More investment in Research and Development for better Education in the future?   The question in this paper is whether R&D efforts affect education
performance in small classes. Merging two datasets collected from the PISA
studies and the World Development Indicators and using Learning Bayesian
Networks, we prove the existence of a statistical causal relationship between
investment in R&D of a country and its education performance (PISA scores). We
also prove that the effect of R\&D on Education is long term as a country has
to invest at least 10 years before beginning to improve the level of young
pupils.
",Statistics,Statistics
"Exploring light mediators with low-threshold direct detection experiments   We explore the potential of future cryogenic direct detection experiments to
determine the properties of the mediator that communicates the interactions
between dark matter and nuclei. Due to their low thresholds and large
exposures, experiments like CRESST-III, SuperCDMS SNOLAB and EDELWEISS-III will
have excellent capability to reconstruct mediator masses in the MeV range for a
large class of models. Combining the information from several experiments
further improves the parameter reconstruction, even when taking into account
additional nuisance parameters related to background uncertainties and the dark
matter velocity distribution. These observations may offer the intriguing
possibility of studying dark matter self-interactions with direct detection
experiments.
",Physics,Physics
"On Geometry of Manifolds with Some Tensor Structures and Metrics of Norden Type   The object of study in the present dissertation are some topics in
differential geometry of smooth manifolds with additional tensor structures and
metrics of Norden type. There are considered four cases depending on the
dimension of the manifold: 2n, 2n + 1, 4n and 4n + 3. The studied tensor
structures, which are counterparts in the different related dimensions, are the
almost complex/contact/hypercomplex structure and the almost contact
3-structure. The considered metric on the 2n-dimensional case is the Norden
metric, and the metrics in the other three cases are generated by it. The
purpose of the dissertation is to carry out the following: 1. Further
investigations of almost complex manifolds with Norden metric including
studying of natural connections with conditions for their torsion and invariant
tensors under the twin interchange of Norden metrics. 2. Further investigations
of almost contact manifolds with B-metric including studying of natural
connections with conditions for their torsion and associated Schouten-van
Kampen connections as well as a classification of affine connections. 3.
Introducing and studying of Sasaki-like almost contact complex Riemannian
manifolds. 4. Further investigations of almost hypercomplex manifolds with
Hermitian-Norden metrics including studying of integrable structures of the
considered type on 4-dimensional Lie algebra and tangent bundles with the
complete lift of the base metric; introducing of associated Nijenhuis tensors
in relation with natural connections having totally skew-symmetric torsion as
well as quaternionic Kähler manifolds with Hermitian-Norden metrics. 5.
Introducing and studying of manifolds with almost contact 3-structures and
metrics of Hermitian-Norden type and, in particular, associated Nijenhuis
tensors and their relationship with natural connections having totally
skew-symmetric torsion.
",Mathematics,Mathematics
"Hierarchical Adversarially Learned Inference   We propose a novel hierarchical generative model with a simple Markovian
structure and a corresponding inference model. Both the generative and
inference model are trained using the adversarial learning paradigm. We
demonstrate that the hierarchical structure supports the learning of
progressively more abstract representations as well as providing semantically
meaningful reconstructions with different levels of fidelity. Furthermore, we
show that minimizing the Jensen-Shanon divergence between the generative and
inference network is enough to minimize the reconstruction error. The resulting
semantically meaningful hierarchical latent structure discovery is exemplified
on the CelebA dataset. There, we show that the features learned by our model in
an unsupervised way outperform the best handcrafted features. Furthermore, the
extracted features remain competitive when compared to several recent deep
supervised approaches on an attribute prediction task on CelebA. Finally, we
leverage the model's inference network to achieve state-of-the-art performance
on a semi-supervised variant of the MNIST digit classification task.
",Statistics,Statistics
"A gradient estimate for nonlocal minimal graphs   We consider the class of measurable functions defined in all of
$\mathbb{R}^n$ that give rise to a nonlocal minimal graph over a ball of
$\mathbb{R}^n$. We establish that the gradient of any such function is bounded
in the interior of the ball by a power of its oscillation. This estimate,
together with previously known results, leads to the $C^\infty$ regularity of
the function in the ball. While the smoothness of nonlocal minimal graphs was
known for $n = 1, 2$ (but without a quantitative bound), in higher dimensions
only their continuity had been established.
To prove the gradient bound, we show that the normal to a nonlocal minimal
graph is a supersolution of a truncated fractional Jacobi operator, for which
we prove a weak Harnack inequality. To this end, we establish a new universal
fractional Sobolev inequality on nonlocal minimal surfaces.
Our estimate provides an extension to the fractional setting of the
celebrated gradient bounds of Finn and of Bombieri, De Giorgi & Miranda for
solutions of the classical mean curvature equation.
",Mathematics,Mathematics
"Switching Isotropic and Directional Exploration with Parameter Space Noise in Deep Reinforcement Learning   This paper proposes an exploration method for deep reinforcement learning
based on parameter space noise. Recent studies have experimentally shown that
parameter space noise results in better exploration than the commonly used
action space noise. Previous methods devised a way to update the diagonal
covariance matrix of a noise distribution and did not consider the direction of
the noise vector and its correlation. In addition, fast updates of the noise
distribution are required to facilitate policy learning. We propose a method
that deforms the noise distribution according to the accumulated returns and
the noises that have led to the returns. Moreover, this method switches
isotropic exploration and directional exploration in parameter space with
regard to obtained rewards. We validate our exploration strategy in the OpenAI
Gym continuous environments and modified environments with sparse rewards. The
proposed method achieves results that are competitive with a previous method at
baseline tasks. Moreover, our approach exhibits better performance in sparse
reward environments by exploration with the switching strategy.
",Statistics,Computer Science; Statistics
"Quantum critical response: from conformal perturbation theory to holography   We discuss dynamical response functions near quantum critical points,
allowing for both a finite temperature and detuning by a relevant operator.
When the quantum critical point is described by a conformal field theory (CFT),
conformal perturbation theory and the operator product expansion can be used to
fix the first few leading terms at high frequencies. Knowledge of the high
frequency response allows us then to derive non-perturbative sum rules. We
show, via explicit computations, how holography recovers the general results of
CFT, and the associated sum rules, for any holographic field theory with a
conformal UV completion -- regardless of any possible new ordering and/or
scaling physics in the IR. We numerically obtain holographic response functions
at all frequencies, allowing us to probe the breakdown of the asymptotic
high-frequency regime. Finally, we show that high frequency response functions
in holographic Lifshitz theories are quite similar to their conformal
counterparts, even though they are not strongly constrained by symmetry.
",Physics,Physics
"Optimal Invariant Tests in an Instrumental Variables Regression With Heteroskedastic and Autocorrelated Errors   This paper uses model symmetries in the instrumental variable (IV) regression
to derive an invariant test for the causal structural parameter. Contrary to
popular belief, we show there exist model symmetries when equation errors are
heteroskedastic and autocorrelated (HAC). Our theory is consistent with
existing results for the homoskedastic model (Andrews, Moreira and Stock(2006}
and Chamberlain (2007}), but in general uses information on the structural
parameter beyond the Anderson-Rubin, score, and rank statistics. This suggests
that tests based only the Anderson-Rubin and score statistics discard
information on the causal parameter of interest. We apply our theory to
construct designs in which these tests indeed have power arbitrarily close to
size. Other tests, including other adaptations to the CLR test, do not suffer
the same deficiencies. Finally, we use the model symmetries to propose novel
weighted-average power tests for the HAC-IV model.
",Mathematics; Statistics,Mathematics; Statistics
"Warped Product Space-times   Many classical results in relativity theory concerning spherically symmetric
space-times have easy generalizations to warped product space-times, with a
two-dimensional Lorentzian base and arbitrary dimensional Riemannian fibers. We
first give a systematic presentation of the main geometric constructions, with
emphasis on the Kodama vector field and the Hawking energy; the construction is
signature independent. This leads to proofs of general Birkhoff-type theorems
for warped product manifolds; our theorems in particular apply to situations
where the warped product manifold is not necessarily Einstein, and thus can be
applied to solutions with matter content in general relativity. Next we
specialize to the Lorentzian case and study the propagation of null expansions
under the assumption of the dominant energy condition. We prove several
non-existence results relating to the Yamabe class of the fibers, in the spirit
of the black-hole topology theorem of Hawking-Galloway-Schoen. Finally we
discuss the effect of the warped product ansatz on matter models. In particular
we construct several cosmological solutions to the Einstein-Euler equations
whose spatial geometry is generally not isotropic.
",Mathematics,Physics
"Robust Regression via Mutivariate Regression Depth   This paper studies robust regression in the settings of Huber's
$\epsilon$-contamination models. We consider estimators that are maximizers of
multivariate regression depth functions. These estimators are shown to achieve
minimax rates in the settings of $\epsilon$-contamination models for various
regression problems including nonparametric regression, sparse linear
regression, reduced rank regression, etc. We also discuss a general notion of
depth function for linear operators that has potential applications in robust
functional linear regression.
",Mathematics; Statistics,Mathematics; Statistics
"The Galaxy-Halo Connection Over The Last 13.3 Gyrs   We present new determinations of the stellar-to-halo mass relation (SHMR) at
$z=0-10$ that match the evolution of the galaxy stellar mass function, the
SFR$-M_*$ relation,and the cosmic star formation rate. We utilize a compilation
of 40 observational studies from the literature and correct them for potential
biases. Using our robust determinations of halo mass assembly and the SHMR, we
infer star formation histories, merger rates, and structural properties for
average galaxies, combining star-forming and quenched galaxies. Our main
findings: (1) The halo mass $M_{50}$ above which 50\% of galaxies are quenched
coincides with sSFR/sMAR$\sim1$, where sMAR is the specific halo mass accretion
rate. (2) $M_{50}$ increases with redshift, presumably due to cold streams
being more efficient at high redshift while virial shocks and AGN feedback
become more relevant at lower redshifts. (3) The ratio sSFR/sMAR has a peak
value, which occurs around $M_{\rm vir}\sim2\times10^{11}M_{\odot}$. (4) The
stellar mass density within 1 kpc, $\Sigma_1$, is a good indicator of the
galactic global sSFR. (5) Galaxies are statistically quenched after they reach
a maximum in $\Sigma_1$, consistent with theoretical expectations of the gas
compaction model; this maximum depends on redshift. (6) In-situ star formation
is responsible for most galactic stellar mass growth, especially for lower-mass
galaxies. (7) Galaxies grow inside out. The marked change in the slope of the
size--mass relation when galaxies became quenched, from $d\log R_{\rm
eff}/d\log M_*\sim0.35$ to $\sim2.5$, could be the result of dry minor mergers.
",Physics,Physics
"Slow Spin Dynamics and Self-Sustained Clusters in Sparsely Connected Systems   To identify emerging microscopic structures in low temperature spin glasses,
we study self-sustained clusters (SSC) in spin models defined on sparse random
graphs. A message-passing algorithm is developed to determine the probability
of individual spins to belong to SSC. Results for specific instances, which
compare the predicted SSC associations with the dynamical properties of spins
obtained from numerical simulations, show that SSC association identifies
individual slow-evolving spins. This insight gives rise to a powerful approach
for predicting individual spin dynamics from a single snapshot of an
equilibrium spin configuration, namely from limited static information, which
can be used to devise generic prediction tools applicable to a wide range of
areas.
",Physics,Physics
"Convergence of extreme value statistics in a two-layer quasi-geostrophic atmospheric model   We search for the signature of universal properties of extreme events,
theoretically predicted for Axiom A flows, in a chaotic and high dimensional
dynamical system by studying the convergence of GEV (Generalized Extreme Value)
and GP (Generalized Pareto) shape parameter estimates to a theoretical value,
expressed in terms of partial dimensions of the attractor, which are global
properties. We consider a two layer quasi-geostrophic (QG) atmospheric model
using two forcing levels, and analyse extremes of different types of physical
observables (local, zonally-averaged energy, and the average value of energy
over the mid-latitudes). Regarding the predicted universality, we find closer
agreement in the shape parameter estimates only in the case of strong forcing,
producing a highly chaotic behaviour, for some observables (the local energy at
every latitude). Due to the limited (though very large) data size and the
presence of serial correlations, it is difficult to obtain robust statistics of
extremes in case of the other observables. In the case of weak forcing,
inducing a less pronounced chaotic flow with regime behaviour, we find worse
agreement with the theory developed for Axiom A flows, which is unsurprising
considering the properties of the system.
",Physics; Statistics,Physics
"One-shot and few-shot learning of word embeddings   Standard deep learning systems require thousands or millions of examples to
learn a concept, and cannot integrate new concepts easily. By contrast, humans
have an incredible ability to do one-shot or few-shot learning. For instance,
from just hearing a word used in a sentence, humans can infer a great deal
about it, by leveraging what the syntax and semantics of the surrounding words
tells us. Here, we draw inspiration from this to highlight a simple technique
by which deep recurrent networks can similarly exploit their prior knowledge to
learn a useful representation for a new word from little data. This could make
natural language processing systems much more flexible, by allowing them to
learn continually from the new words they encounter.
",Computer Science; Statistics,Computer Science
"Controlling Physical Attributes in GAN-Accelerated Simulation of Electromagnetic Calorimeters   High-precision modeling of subatomic particle interactions is critical for
many fields within the physical sciences, such as nuclear physics and high
energy particle physics. Most simulation pipelines in the sciences are
computationally intensive -- in a variety of scientific fields, Generative
Adversarial Networks have been suggested as a solution to speed up the forward
component of simulation, with promising results. An important component of any
simulation system for the sciences is the ability to condition on any number of
physically meaningful latent characteristics that can effect the forward
generation procedure. We introduce an auxiliary task to the training of a
Generative Adversarial Network on particle showers in a multi-layer
electromagnetic calorimeter, which allows our model to learn an attribute-aware
conditioning mechanism.
",Computer Science,Physics
"Feature Model-to-Ontology for SPL Application Realisation   Feature model are widely used to capture commonalities and variabilities of
artefacts in Software Product Line (SPL). Several studies have discussed the
formal representation of feature diagram using ontologies with different styles
of mapping. However, they still focused on the ontology approach for problem
space and keep the solution space aside. In this paper, we present the
modelling of feature model using OWL ontology and produce an application based
on the ontology. Firstly, we map the features in a running example feature
diagram to OWL classes and properties. Secondly, we verify the consistency of
the OWL ontology by using reasoning engines. Finally, we use the ontology as an
input of Zotonic framework for application realisation.
",Computer Science,Computer Science
"Intrinsic geometry and analysis of Finsler structures   In this short note, we prove that if $F$ is a weak upper semicontinuous
admissible Finsler structure on a domain in $\mathbb{R}^n$, $n\geq 2$, then the
intrinsic distance and differential structures coincide.
",Mathematics,Mathematics
"Critical neural networks with short and long term plasticity   In recent years self organised critical neuronal models have provided
insights regarding the origin of the experimentally observed avalanching
behaviour of neuronal systems. It has been shown that dynamical synapses, as a
form of short-term plasticity, can cause critical neuronal dynamics. Whereas
long-term plasticity, such as hebbian or activity dependent plasticity, have a
crucial role in shaping the network structure and endowing neural systems with
learning abilities. In this work we provide a model which combines both
plasticity mechanisms, acting on two different time-scales. The measured
avalanche statistics are compatible with experimental results for both the
avalanche size and duration distribution with biologically observed percentages
of inhibitory neurons. The time-series of neuronal activity exhibits temporal
bursts leading to 1/f decay in the power spectrum. The presence of long-term
plasticity gives the system the ability to learn binary rules such as XOR,
providing the foundation of future research on more complicated tasks such as
pattern recognition.
",Physics,Quantitative Biology
"Parallel Simultaneous Perturbation Optimization   Stochastic computer simulations enable users to gain new insights into
complex physical systems. Optimization is a common problem in this context:
users seek to find model inputs that maximize the expected value of an
objective function. The objective function, however, is time-intensive to
evaluate, and cannot be directly measured. Instead, the stochastic nature of
the model means that individual realizations are corrupted by noise. More
formally, we consider the problem of optimizing the expected value of an
expensive black-box function with continuously-differentiable mean, from which
observations are corrupted by Gaussian noise. We present Parallel Simultaneous
Perturbation Optimization (PSPO), which extends a well-known stochastic
optimization algorithm, simultaneous perturbation stochastic approximation, in
several important ways. Our modifications allow the algorithm to fully take
advantage of parallel computing resources, like high-performance cloud
computing. The resulting PSPO algorithm takes fewer time-consuming iterations
to converge, automatically chooses the step size, and can vary the error
tolerance by step. Theoretical results are supported by a numerical example. To
demonstrate the performance of the algorithm, we implemented the algorithm to
maximize the pseudo-likelihood of a stochastic epidemiological model to data of
a measles outbreak.
",Mathematics,Computer Science; Statistics
"Some Ultraspheroidal Monogenic Clifford Gegenbauer Jacobi Polynomials and Associated Wavelets   In the present paper, new classes of wavelet functions are presented in the
framework of Clifford analysis. Firstly, some classes of orthogonal polynomials
are provided based on 2-parameters weight functions. Such classes englobe the
well known ones of Jacobi and Gegenbauer polynomials when relaxing one of the
parameters. The discovered polynomial sets are next applied to introduce new
wavelet functions. Reconstruction formula as well as Fourier-Plancherel rules
have been proved.
",Mathematics,Mathematics
"Nef partitions for codimension 2 weighted complete intersections   We prove that a smooth well formed Fano weighted complete intersection of
codimension 2 has a nef partition. We discuss applications of this fact to
Mirror Symmetry. In particular we list all nef partitions for smooth well
formed Fano weighted complete intersections of dimensions 4 and 5 and present
weak Landau--Ginzburg models for them.
",Mathematics,Mathematics
"Distributed Estimation of Principal Eigenspaces   Principal component analysis (PCA) is fundamental to statistical machine
learning. It extracts latent principal factors that contribute to the most
variation of the data. When data are stored across multiple machines, however,
communication cost can prohibit the computation of PCA in a central location
and distributed algorithms for PCA are thus needed. This paper proposes and
studies a distributed PCA algorithm: each node machine computes the top $K$
eigenvectors and transmits them to the central server; the central server then
aggregates the information from all the node machines and conducts a PCA based
on the aggregated information. We investigate the bias and variance for the
resulting distributed estimator of the top $K$ eigenvectors. In particular, we
show that for distributions with symmetric innovation, the empirical top
eigenspaces are unbiased and hence the distributed PCA is ""unbiased"". We derive
the rate of convergence for distributed PCA estimators, which depends
explicitly on the effective rank of covariance, eigen-gap, and the number of
machines. We show that when the number of machines is not unreasonably large,
the distributed PCA performs as well as the whole sample PCA, even without full
access of whole data. The theoretical results are verified by an extensive
simulation study. We also extend our analysis to the heterogeneous case where
the population covariance matrices are different across local machines but
share similar top eigen-structures.
",Mathematics; Statistics,Statistics
"Impact of Feature Selection on Micro-Text Classification   Social media datasets, especially Twitter tweets, are popular in the field of
text classification. Tweets are a valuable source of micro-text (sometimes
referred to as ""micro-blogs""), and have been studied in domains such as
sentiment analysis, recommendation systems, spam detection, clustering, among
others. Tweets often include keywords referred to as ""Hashtags"" that can be
used as labels for the tweet. Using tweets encompassing 50 labels, we studied
the impact of word versus character-level feature selection and extraction on
different learners to solve a multi-class classification task. We show that
feature extraction of simple character-level groups performs better than simple
word groups and pre-processing methods like normalizing using Porter's Stemming
and Part-of-Speech (""POS"")-Lemmatization.
",Computer Science,Computer Science
"High-frequency approximation of the interior dirichlet-to-neumann map and applications to the transmission eigenvalues   We study the high-frequency behavior of the Dirichlet-to-Neumann map for an
arbitrary compact Riemannian manifold with a non-empty smooth boundary. We show
that far from the real axis it can be approximated by a simpler operator. We
use this fact to get new results concerning the location of the transmission
eigenvalues on the complex plane. In some cases we obtain optimal transmission
eigenvalue-free regions.
",Mathematics,Mathematics
"Accelerating Permutation Testing in Voxel-wise Analysis through Subspace Tracking: A new plugin for SnPM   Permutation testing is a non-parametric method for obtaining the max null
distribution used to compute corrected $p$-values that provide strong control
of false positives. In neuroimaging, however, the computational burden of
running such an algorithm can be significant. We find that by viewing the
permutation testing procedure as the construction of a very large permutation
testing matrix, $T$, one can exploit structural properties derived from the
data and the test statistics to reduce the runtime under certain conditions. In
particular, we see that $T$ is low-rank plus a low-variance residual. This
makes $T$ a good candidate for low-rank matrix completion, where only a very
small number of entries of $T$ ($\sim0.35\%$ of all entries in our experiments)
have to be computed to obtain a good estimate. Based on this observation, we
present RapidPT, an algorithm that efficiently recovers the max null
distribution commonly obtained through regular permutation testing in
voxel-wise analysis. We present an extensive validation on a synthetic dataset
and four varying sized datasets against two baselines: Statistical
NonParametric Mapping (SnPM13) and a standard permutation testing
implementation (referred as NaivePT). We find that RapidPT achieves its best
runtime performance on medium sized datasets ($50 \leq n \leq 200$), with
speedups of 1.5x - 38x (vs. SnPM13) and 20x-1000x (vs. NaivePT). For larger
datasets ($n \geq 200$) RapidPT outperforms NaivePT (6x - 200x) on all
datasets, and provides large speedups over SnPM13 when more than 10000
permutations (2x - 15x) are needed. The implementation is a standalone toolbox
and also integrated within SnPM13, able to leverage multi-core architectures
when available.
",Computer Science; Statistics,Statistics
"Enhanced clustering tendency of Cu-impurities with a number of oxygen vacancies in heavy carbon-loaded TiO2 - the bulk and surface morphologies   The over threshold carbon-loadings (~50 at.%) of initial TiO2-hosts and
posterior Cu-sensitization (~7 at.%) was made using pulsed ion-implantation
technique in sequential mode with 1 hour vacuum-idle cycle between sequential
stages of embedding. The final Cx-TiO2:Cu samples were qualified using XPS
wide-scan elemental analysis, core-levels and valence band mappings. The
results obtained were discussed on the theoretic background employing
DFT-calculations. The combined XPS and DFT analysis allows to establish and
prove the final formula of the synthesized samples as Cx-TiO2:[Cu+][Cu2+] for
the bulk and Cx-TiO2:[Cu+][Cu0] for thin-films. It was demonstrated the in the
mode of heavy carbon-loadings the remaining majority of neutral C-C bonds
(sp3-type) is dominating and only a lack of embedded carbon is fabricating the
O-C=O clusters. No valence base-band width altering was established after
sequential carbon-copper modification of the atomic structure of initial
TiO2-hosts except the dominating majority of Cu 3s states after
Cu-sensitization. The crucial role of neutral carbon low-dimensional impurities
as the precursors for the new phases growth was shown for Cu-sensitized Cx-TiO2
intermediate-state hosts.
",Physics,Physics
"Mutation invariance for the zeroth coefficients of the colored HOMFLY polynomial   We show that the zeroth coefficient of the cables of the HOMFLY polynomial
(colored HOMFLY polynomials) does not distinguish mutants. This makes a sharp
contrast with the total HOMFLY polynomial whose 3-cables can distinguish
mutants.
",Mathematics,Mathematics
"Asymptotic Normality of Extensible Grid Sampling   Recently, He and Owen (2016) proposed the use of Hilbert's space filling
curve (HSFC) in numerical integration as a way of reducing the dimension from
$d>1$ to $d=1$. This paper studies the asymptotic normality of the HSFC-based
estimate when using scrambled van der Corput sequence as input. We show that
the estimate has an asymptotic normal distribution for functions in
$C^1([0,1]^d)$, excluding the trivial case of constant functions. The
asymptotic normality also holds for discontinuous functions under mild
conditions. It was previously known only that scrambled $(0,m,d)$-net
quadratures enjoy the asymptotic normality for smooth enough functions, whose
mixed partial gradients satisfy a Hölder condition. As a by-product, we find
lower bounds for the variance of the HSFC-based estimate. Particularly, for
nontrivial functions in $C^1([0,1]^d)$, the low bound is of order $n^{-1-2/d}$,
which matches the rate of the upper bound established in He and Owen (2016).
",Mathematics; Statistics,Mathematics; Statistics
"Some new bounds of placement delivery arrays   Coded caching scheme is a technique which reduce the load during peak traffic
times in a wireless network system. Placement delivery array (PDA in short) was
first introduced by Yan et al.. It can be used to design coded caching scheme.
In this paper, we prove some lower bounds of PDA on the element and some lower
bounds of PDA on the column. We also give some constructions for optimal PDA.
",Computer Science,Computer Science
"Adsorption and desorption of hydrogen at nonpolar GaN(1-100) surfaces: Kinetics and impact on surface vibrational and electronic properties   The adsorption of hydrogen at nonpolar GaN(1-100) surfaces and its impact on
the electronic and vibrational properties is investigated using surface
electron spectroscopy in combination with density functional theory (DFT)
calculations. For the surface mediated dissociation of H2 and the subsequent
adsorption of H, an energy barrier of 0.55 eV has to be overcome. The
calculated kinetic surface phase diagram indicates that the reaction is
kinetically hindered at low pressures and low temperatures. At higher
temperatures ab-initio thermodynamics show, that the H-free surface is
energetically favored. To validate these theoretical predictions experiments at
room temperature and under ultrahigh vacuum conditions were performed. They
reveal that molecular hydrogen does not dissociatively adsorb at the GaN(1-100)
surface. Only activated atomic hydrogen atoms attach to the surface. At
temperatures above 820 K, the attached hydrogen gets desorbed. The adsorbed
hydrogen atoms saturate the dangling bonds of the gallium and nitrogen surface
atoms and result in an inversion of the Ga-N surface dimer buckling. The
signatures of the Ga-H and N-H vibrational modes on the H-covered surface have
experimentally been identified and are in good agreement with the DFT
calculations of the surface phonon modes. Both theory and experiment show that
H adsorption results in a removal of occupied and unoccupied intragap electron
states of the clean GaN(1-100) surface and a reduction of the surface upward
band bending by 0.4 eV. The latter mechanism largely reduces surface electron
depletion.
",Physics,Physics
"SegMap: 3D Segment Mapping using Data-Driven Descriptors   When performing localization and mapping, working at the level of structure
can be advantageous in terms of robustness to environmental changes and
differences in illumination. This paper presents SegMap: a map representation
solution to the localization and mapping problem based on the extraction of
segments in 3D point clouds. In addition to facilitating the computationally
intensive task of processing 3D point clouds, working at the level of segments
addresses the data compression requirements of real-time single- and
multi-robot systems. While current methods extract descriptors for the single
task of localization, SegMap leverages a data-driven descriptor in order to
extract meaningful features that can also be used for reconstructing a dense 3D
map of the environment and for extracting semantic information. This is
particularly interesting for navigation tasks and for providing visual feedback
to end-users such as robot operators, for example in search and rescue
scenarios. These capabilities are demonstrated in multiple urban driving and
search and rescue experiments. Our method leads to an increase of area under
the ROC curve of 28.3% over current state of the art using eigenvalue based
features. We also obtain very similar reconstruction capabilities to a model
specifically trained for this task. The SegMap implementation will be made
available open-source along with easy to run demonstrations at
www.github.com/ethz-asl/segmap. A video demonstration is available at
this https URL.
",Computer Science,Computer Science
"Proposal for a High Precision Tensor Processing Unit   This whitepaper proposes the design and adoption of a new generation of
Tensor Processing Unit which has the performance of Google's TPU, yet performs
operations on wide precision data. The new generation TPU is made possible by
implementing arithmetic circuits which compute using a new general purpose,
fractional arithmetic based on the residue number system.
",Computer Science,Computer Science
"Reward Maximization Under Uncertainty: Leveraging Side-Observations on Networks   We study the stochastic multi-armed bandit (MAB) problem in the presence of
side-observations across actions that occur as a result of an underlying
network structure. In our model, a bipartite graph captures the relationship
between actions and a common set of unknowns such that choosing an action
reveals observations for the unknowns that it is connected to. This models a
common scenario in online social networks where users respond to their friends'
activity, thus providing side information about each other's preferences. Our
contributions are as follows: 1) We derive an asymptotic lower bound (with
respect to time) as a function of the bi-partite network structure on the
regret of any uniformly good policy that achieves the maximum long-term average
reward. 2) We propose two policies - a randomized policy; and a policy based on
the well-known upper confidence bound (UCB) policies - both of which explore
each action at a rate that is a function of its network position. We show,
under mild assumptions, that these policies achieve the asymptotic lower bound
on the regret up to a multiplicative factor, independent of the network
structure. Finally, we use numerical examples on a real-world social network
and a routing example network to demonstrate the benefits obtained by our
policies over other existing policies.
",Computer Science; Statistics,Computer Science
"A Fast Interior Point Method for Atomic Norm Soft Thresholding   The atomic norm provides a generalization of the $\ell_1$-norm to continuous
parameter spaces. When applied as a sparse regularizer for line spectral
estimation the solution can be obtained by solving a convex optimization
problem. This problem is known as atomic norm soft thresholding (AST). It can
be cast as a semidefinite program and solved by standard methods. In the
semidefinite formulation there are $O(N^2)$ dual variables and a standard
primal-dual interior point method requires at least $O(N^6)$ flops per
iteration. That has lead researcher to consider alternating direction method of
multipliers (ADMM) for the solution of AST, but this method is still somewhat
slow for large problem sizes. To obtain a faster algorithm we reformulate AST
as a non-symmetric conic program. That has two properties of key importance to
its numerical solution: the conic formulation has only $O(N)$ dual variables
and the Toeplitz structure inherent to AST is preserved. Based on it we derive
FastAST which is a primal-dual interior point method for solving AST. Two
variants are considered with the fastest one requiring only $O(N^2)$ flops per
iteration. Extensive numerical experiments demonstrate that FastAST solves AST
significantly faster than a state-of-the-art solver based on ADMM.
",Computer Science,Computer Science; Mathematics
"Free fermions on a piecewise linear four-manifold. II: Pachner moves   This is the second in a series of papers where we construct an invariant of a
four-dimensional piecewise linear manifold $M$ with a given middle cohomology
class $h\in H^2(M,\mathbb C)$. This invariant is the square root of the torsion
of unusual chain complex introduced in Part I (arXiv:1605.06498) of our work,
multiplied by a correcting factor. Here we find this factor by studying the
behavior of our construction under all four-dimensional Pachner moves, and show
that it can be represented in a multiplicative form: a product of same-type
multipliers over all 2-faces, multiplied by a product of same-type multipliers
over all pentachora.
",Mathematics,Mathematics
"Convergence Analysis of Deterministic Kernel-Based Quadrature Rules in Misspecified Settings   This paper presents a convergence analysis of kernel-based quadrature rules
in misspecified settings, focusing on deterministic quadrature in Sobolev
spaces. In particular, we deal with misspecified settings where a test
integrand is less smooth than a Sobolev RKHS based on which a quadrature rule
is constructed. We provide convergence guarantees based on two different
assumptions on a quadrature rule: one on quadrature weights, and the other on
design points. More precisely, we show that convergence rates can be derived
(i) if the sum of absolute weights remains constant (or does not increase
quickly), or (ii) if the minimum distance between design points does not
decrease very quickly. As a consequence of the latter result, we derive a rate
of convergence for Bayesian quadrature in misspecified settings. We reveal a
condition on design points to make Bayesian quadrature robust to
misspecification, and show that, under this condition, it may adaptively
achieve the optimal rate of convergence in the Sobolev space of a lesser order
(i.e., of the unknown smoothness of a test integrand), under a slightly
stronger regularity condition on the integrand.
",Computer Science; Statistics,Computer Science
"Hydra: a C++11 framework for data analysis in massively parallel platforms   Hydra is a header-only, templated and C++11-compliant framework designed to
perform the typical bottleneck calculations found in common HEP data analyses
on massively parallel platforms. The framework is implemented on top of the
C++11 Standard Library and a variadic version of the Thrust library and is
designed to run on Linux systems, using OpenMP, CUDA and TBB enabled devices.
This contribution summarizes the main features of Hydra. A basic description of
the overall design, functionality and user interface is provided, along with
some code examples and measurements of performance.
",Computer Science; Physics,Computer Science
"Interacting superradiance samples: modified intensities and timescales, and frequency shifts   We consider the interaction between distinct superradiance (SR) systems and
use the dressed state formalism to solve the case of two interacting two-atom
SR samples at resonance. We show that the ensuing entanglement modifies the
transition rates and intensities of radiation, as well as introduces a
potentially measurable frequency chirp in the SR cascade, the magnitude of
which being a function of the separation between the samples. For the dominant
SR cascade we find a significant reduction in the duration and an increase of
the intensity of the SR pulse relative to the case of a single two-atom SR
sample.
",Physics,Physics
"Thermal diffusivity and chaos in metals without quasiparticles   We study the thermal diffusivity $D_T$ in models of metals without
quasiparticle excitations (`strange metals'). The many-body quantum chaos and
transport properties of such metals can be efficiently described by a
holographic representation in a gravitational theory in an emergent curved
spacetime with an additional spatial dimension. We find that at generic
infra-red fixed points $D_T$ is always related to parameters characterizing
many-body quantum chaos: the butterfly velocity $v_B$, and Lyapunov time
$\tau_L$ through $D_T \sim v_B^2 \tau_L$. The relationship holds independently
of the charge density, periodic potential strength or magnetic field at the
fixed point. The generality of this result follows from the observation that
the thermal conductivity of strange metals depends only on the metric near the
horizon of a black hole in the emergent spacetime, and is otherwise insensitive
to the profile of any matter fields.
",Physics,Physics
"General Refraction Problems with Phase Discontinuity   This paper provides a mathematical approach to study metasurfaces in non flat
geometries. Analytical conditions between the curvature of the surface and the
set of refracted directions are introduced to guarantee the existence of phase
discontinuities. The approach contains both the near and far field cases. A
starting point is the formulation of a vector Snell law in presence of abrupt
discontinuities on the interfaces.
",Physics; Mathematics,Mathematics
"Zero-temperature magnetic response of small fullerene molecules at the classical and full quantum limit   The ground-state magnetic response of fullerene molecules with up to 36
vertices is calculated, when spins classical or with magnitude $s=\frac{1}{2}$
are located on their vertices and interact according to the nearest-neighbor
antiferromagnetic Heisenberg model. The frustrated topology, which originates
in the pentagons of the fullerenes and is enhanced by their close proximity,
leads to a significant number of classical magnetization and susceptibility
discontinuities, something not expected for a model lacking magnetic
anisotropy. This establishes the classical discontinuities as a generic feature
of fullerene molecules irrespective of their symmetry. The largest number of
discontinuities have the molecule with 26 sites, four of the magnetization and
two of the susceptibility, and an isomer with 34 sites, which has three each.
In addition, for several of the fullerenes the classical zero-field lowest
energy configuration has finite magnetization, which is unexpected for
antiferromagnetic interactions between an even number of spins and with each
spin having the same number of nearest-neighbors. The molecules come in
different symmetries and topologies and there are only a few patterns of
magnetic behavior that can be detected from such a small sample of relatively
small fullerenes. Contrary to the classical case, in the full quantum limit
$s=\frac{1}{2}$ there are no discontinuities for a subset of the molecules that
was considered. This leaves the icosahedral symmetry fullerenes as the only
ones known supporting ground-state magnetization discontinuities for
$s=\frac{1}{2}$. It is also found that a molecule with 34 sites has a
doubly-degenerate ground state when $s=\frac{1}{2}$.
",Physics,Physics
"Leverage Score Sampling for Faster Accelerated Regression and ERM   Given a matrix $\mathbf{A}\in\mathbb{R}^{n\times d}$ and a vector $b
\in\mathbb{R}^{d}$, we show how to compute an $\epsilon$-approximate solution
to the regression problem $ \min_{x\in\mathbb{R}^{d}}\frac{1}{2} \|\mathbf{A} x
- b\|_{2}^{2} $ in time $ \tilde{O} ((n+\sqrt{d\cdot\kappa_{\text{sum}}})\cdot
s\cdot\log\epsilon^{-1}) $ where
$\kappa_{\text{sum}}=\mathrm{tr}\left(\mathbf{A}^{\top}\mathbf{A}\right)/\lambda_{\min}(\mathbf{A}^{T}\mathbf{A})$
and $s$ is the maximum number of non-zero entries in a row of $\mathbf{A}$. Our
algorithm improves upon the previous best running time of $ \tilde{O}
((n+\sqrt{n \cdot\kappa_{\text{sum}}})\cdot s\cdot\log\epsilon^{-1})$.
We achieve our result through a careful combination of leverage score
sampling techniques, proximal point methods, and accelerated coordinate
descent. Our method not only matches the performance of previous methods, but
further improves whenever leverage scores of rows are small (up to
polylogarithmic factors). We also provide a non-linear generalization of these
results that improves the running time for solving a broader class of ERM
problems.
",Computer Science; Statistics,Computer Science; Statistics
"KATE: K-Competitive Autoencoder for Text   Autoencoders have been successful in learning meaningful representations from
image datasets. However, their performance on text datasets has not been widely
studied. Traditional autoencoders tend to learn possibly trivial
representations of text documents due to their confounding properties such as
high-dimensionality, sparsity and power-law word distributions. In this paper,
we propose a novel k-competitive autoencoder, called KATE, for text documents.
Due to the competition between the neurons in the hidden layer, each neuron
becomes specialized in recognizing specific data patterns, and overall the
model can learn meaningful representations of textual data. A comprehensive set
of experiments show that KATE can learn better representations than traditional
autoencoders including denoising, contractive, variational, and k-sparse
autoencoders. Our model also outperforms deep generative models, probabilistic
topic models, and even word representation models (e.g., Word2Vec) in terms of
several downstream tasks such as document classification, regression, and
retrieval.
",Computer Science; Statistics,Computer Science; Statistics
"Small-scale Effects of Thermal Inflation on Halo Abundance at High-$z$, Galaxy Substructure Abundance and 21-cm Power Spectrum   We study the impact of thermal inflation on the formation of cosmological
structures and present astrophysical observables which can be used to constrain
and possibly probe the thermal inflation scenario. These are dark matter halo
abundance at high redshifts, satellite galaxy abundance in the Milky Way, and
fluctuation in the 21-cm radiation background before the epoch of reionization.
The thermal inflation scenario leaves a characteristic signature on the matter
power spectrum by boosting the amplitude at a specific wavenumber determined by
the number of e-foldings during thermal inflation ($N_{\rm bc}$), and strongly
suppressing the amplitude for modes at smaller scales. For a reasonable range
of parameter space, one of the consequences is the suppression of minihalo
formation at high redshifts and that of satellite galaxies in the Milky Way.
While this effect is substantial, it is degenerate with other cosmological or
astrophysical effects. The power spectrum of the 21-cm background probes this
impact more directly, and its observation may be the best way to constrain the
thermal inflation scenario due to the characteristic signature in the power
spectrum. The Square Kilometre Array (SKA) in phase 1 (SKA1) has sensitivity
large enough to achieve this goal for models with $N_{\rm bc}\gtrsim 26$ if a
10000-hr observation is performed. The final phase SKA, with anticipated
sensitivity about an order of magnitude higher, seems more promising and will
cover a wider parameter space.
",Physics,Physics
"A branch-and-bound algorithm for the minimum radius $k$-enclosing ball problem   The minimum $k$-enclosing ball problem seeks the ball with smallest radius
that contains at least~$k$ of~$m$ given points in a general $n$-dimensional
Euclidean space. This problem is NP-hard. We present a branch-and-bound
algorithm on the tree of the subsets of~$k$ points to solve this problem. The
nodes on the tree are ordered in a suitable way, which, complemented with a
last-in-first-out search strategy, allows for only a small fraction of nodes to
be explored. Additionally, an efficient dual algorithm to solve the subproblems
at each node is employed.
",Computer Science; Mathematics,Computer Science
"Reinforcement Learning of Speech Recognition System Based on Policy Gradient and Hypothesis Selection   Speech recognition systems have achieved high recognition performance for
several tasks. However, the performance of such systems is dependent on the
tremendously costly development work of preparing vast amounts of task-matched
transcribed speech data for supervised training. The key problem here is the
cost of transcribing speech data. The cost is repeatedly required to support
new languages and new tasks. Assuming broad network services for transcribing
speech data for many users, a system would become more self-sufficient and more
useful if it possessed the ability to learn from very light feedback from the
users without annoying them. In this paper, we propose a general reinforcement
learning framework for speech recognition systems based on the policy gradient
method. As a particular instance of the framework, we also propose a hypothesis
selection-based reinforcement learning method. The proposed framework provides
a new view for several existing training and adaptation methods. The
experimental results show that the proposed method improves the recognition
performance compared to unsupervised adaptation.
",Computer Science; Statistics,Computer Science
"A construction of trivial Beltrami coefficients   A measurable function $\mu$ on the unit disk $\mathbb{D}$ of the complex
plane with $\|\mu\|_\infty<1$ is sometimes called a Beltrami coefficient. We
say that $\mu$ is trivial if it is the complex dilatation $f_{\bar z}/f_z$ of a
quasiconformal automorphism $f$ of $\mathbb{D}$ satisfying the trivial boundary
condition $f(z)=z,~|z|=1.$ Since it is not easy to solve the Beltrami equation
explicitly, to detect triviality of a given Beltrami coefficient is a hard
problem, in general. In the present article, we offer a sufficient condition
for a Beltrami coefficient to be trivial. Our proof is based on Betker's
theorem on Löwner chains.
",Mathematics,Mathematics
"Four revolutions in physics and the second quantum revolution -- a unification of force and matter by quantum information   Newton's mechanical revolution unifies the motion of planets in the sky and
falling of apple on earth. Maxwell's electromagnetic revolution unifies
electricity, magnetism, and light. Einstein's relativity revolution unifies
space with time, and gravity with space-time distortion. The quantum revolution
unifies particle with waves, and energy with frequency. Each of those
revolution changes our world view. In this article, we will describe a
revolution that is happening now: the second quantum revolution which unifies
matter/space with information. In other words, the new world view suggests that
elementary particles (the bosonic force particles and fermionic matter
particles) all originated from quantum information (qubits): they are
collective excitations of an entangled qubit ocean that corresponds to our
space. The beautiful geometric Yang-Mills gauge theory and the strange Fermi
statistics of matter particles now have a common algebraic quantum
informational origin.
",Physics,Physics
"Convexity of level lines of Martin functions and applications   Let $\Omega$ be an unbounded domain in $\mathbb{R}\times\mathbb{R}^{d}.$ A
positive harmonic function $u$ on $\Omega$ that vanishes on the boundary of
$\Omega$ is called a Martin function. In this note, we show that, when $\Omega$
is convex, the superlevel sets of a Martin function are also convex. As a
consequence we obtain that if in addition $\Omega$ is symmetric, then the
maximum of any Martin function along a slice $\Omega\cap
(\{t\}\times\mathbb{R}^d)$ is attained at $(t,0).$
",Mathematics,Mathematics
"The statistical significance filter leads to overconfident expectations of replicability   We show that publishing results using the statistical significance
filter---publishing only when the p-value is less than 0.05---leads to a
vicious cycle of overoptimistic expectation of the replicability of results.
First, we show analytically that when true statistical power is relatively low,
computing power based on statistically significant results will lead to
overestimates of power. Then, we present a case study using 10 experimental
comparisons drawn from a recently published meta-analysis in psycholinguistics
(Jäger et al., 2017). We show that the statistically significant results
yield an illusion of replicability. This illusion holds even if the researcher
doesn't conduct any formal power analysis but just uses statistical
significance to informally assess robustness (i.e., replicability) of results.
",Mathematics; Statistics,Computer Science; Statistics
"Dynamical Tides in Highly Eccentric Binaries: Chaos, Dissipation and Quasi-Steady State   Highly eccentric binary systems appear in many astrophysical contexts,
ranging from tidal capture in dense star clusters, precursors of stellar
disruption by massive black holes, to high-eccentricity migration of giant
planets. In a highly eccentric binary, the tidal potential of one body can
excite oscillatory modes in the other during a pericenter passage, resulting in
energy exchange between the modes and the binary orbit. These modes exhibit one
of three behaviors over multiple passages: low-amplitude oscillations, large
amplitude oscillations corresponding to a resonance between the orbital
frequency and the mode frequency, and chaotic growth. We study these phenomena
with an iterative map, fully exploring how the mode evolution depends on the
pericenter distance and other parameters. In addition, we show that the
dissipation of mode energy results in a quasi-steady state, with gradual
orbital decay punctuated by resonances, even in systems where the mode
amplitude would initially grow stochastically. A newly captured star around a
black hole can experience significant orbital decay and heating due to the
chaotic growth of the mode amplitude and dissipation. A giant planet pushed
into a high-eccentricity orbit may experience a similar effect and become a hot
or warm Jupiter.
",Physics,Physics
"A Linear-time Algorithm for Orthogonal Watchman Route Problem with Minimum Bends   Given an orthogonal polygon $ P $ with $ n $ vertices, the goal of the
watchman route problem is finding a path $ S $ of the minimum length in $ P $
such that every point of the polygon $ P $ is visible from at least one of the
point of $ S $. In the other words, in the watchman route problem we must
compute a shortest watchman route inside a simple polygon of $ n $ vertices
such that all the points interior to the polygon and on its boundary are
visible to at least one point on the route. If route and polygon be orthogonal,
it is called orthogonal watchman route problem. One of the targets of this
problem is finding the orthogonal path with the minimum number of bends as
possible. We present a linear-time algorithm for the orthogonal watchman route
problem, in which the given polygon is monotone. Our algorithm can be used also
for the problem on simple orthogonal polygons $ P $ for which the dual graph
induced by the vertical decomposition of $ P $ is a path, which is called path
polygon.
",Computer Science,Computer Science
"$z^\circ$-ideals in intermediate rings of ordered field valued continuous functions   A proper ideal $I$ in a commutative ring with unity is called a
$z^\circ$-ideal if for each $a$ in $I$, the intersection of all minimal prime
ideals in $R$ which contain $a$ is contained in $I$. For any totally ordered
field $F$ and a completely $F$-regular topological space $X$, let $C(X,F)$ be
the ring of all $F$-valued continuous functions on $X$ and $B(X,F)$ the
aggregate of all those functions which are bounded over $X$. An explicit
formula for all the $z^\circ$-ideals in $A(X,F)$ in terms of ideals of closed
sets in $X$ is given. It turns out that an intermediate ring $A(X,F)\neq
C(X,F)$ is never regular in the sense of Von-Neumann. This property further
characterizes $C(X,F)$ amongst the intermediate rings within the class of
$P_F$-spaces $X$. It is also realized that $X$ is an almost $P_F$-space if and
only if each maximal ideal in $C(X,F)$ is $z^\circ$-ideal. Incidentally this
property also characterizes $C(X,F)$ amongst the intermediate rings within the
family of almost $P_F$-spaces.
",Mathematics,Mathematics
"Valley polarized relaxation and upconversion luminescence from Tamm-Plasmon Trion-Polaritons with a MoSe2 monolayer   Transition metal dichalcogenides represent an ideal testbed to study
excitonic effects, spin-related phenomena and fundamental light-matter coupling
in nanoscopic condensed matter systems. In particular, the valley degree of
freedom, which is unique to such direct band gap monolayers with broken
inversion symmetry, adds fundamental interest in these materials. Here, we
implement a Tamm-plasmon structure with an embedded MoSe2 monolayer and study
the formation of polaritonic quasi-particles. Strong coupling conditions
between the Tamm-mode and the trion resonance of MoSe2 are established,
yielding bright luminescence from the polaritonic ground state under
non-resonant optical excitation. We demonstrate, that tailoring the
electrodynamic environment of the monolayer results in a significantly
increased valley polarization. This enhancement can be related to change in
recombination dynamics shown in time-resolved photoluminescence measurements.
We furthermore observe strong upconversion luminescence from resonantly excited
polariton states in the lower polariton branch. This upconverted polariton
luminescence is shown to preserve the valley polarization of the
trion-polariton, which paves the way towards combining spin-valley physics and
exciton scattering experiments.
",Physics,Physics
"Status updates through M/G/1/1 queues with HARQ   We consider a system where randomly generated updates are to be transmitted
to a monitor, but only a single update can be in the transmission service at a
time. Therefore, the source has to prioritize between the two possible
transmission policies: preempting the current update or discarding the new one.
We consider Poisson arrivals and general service time, and refer to this system
as the M/G/1/1 queue. We start by studying the average status update age and
the optimal update arrival rate for these two schemes under general service
time distribution. We then apply these results on two practical scenarios in
which updates are sent through an erasure channel using (a) an infinite
incremental redundancy (IIR) HARQ system and (b) a fixed redundancy (FR) HARQ
system. We show that in both schemes the best strategy would be not to preempt.
Moreover, we also prove that, from an age point of view, IIR is better than FR.
",Computer Science,Computer Science
"Bayesian Hypernetworks   We study Bayesian hypernetworks: a framework for approximate Bayesian
inference in neural networks. A Bayesian hypernetwork $\h$ is a neural network
which learns to transform a simple noise distribution, $p(\vec\epsilon) =
\N(\vec 0,\mat I)$, to a distribution $q(\pp) := q(h(\vec\epsilon))$ over the
parameters $\pp$ of another neural network (the ""primary network"")\@. We train
$q$ with variational inference, using an invertible $\h$ to enable efficient
estimation of the variational lower bound on the posterior $p(\pp | \D)$ via
sampling. In contrast to most methods for Bayesian deep learning, Bayesian
hypernets can represent a complex multimodal approximate posterior with
correlations between parameters, while enabling cheap iid sampling of~$q(\pp)$.
In practice, Bayesian hypernets can provide a better defense against
adversarial examples than dropout, and also exhibit competitive performance on
a suite of tasks which evaluate model uncertainty, including regularization,
active learning, and anomaly detection.
",Computer Science; Statistics,Statistics
"Khovanov homology and periodic links   Based on the results of the second author, we define an equivariant version
of Lee and Bar-Natan homology for periodic links and show that there exists an
equivariant spectral sequence from the equivariant Khovanov homology to
equivariant Lee homology. As a result we obtain new obstructions for a link to
be periodic. These obstructions generalize previous results of Przytycki and of
the second author.
",Mathematics,Mathematics
"Modelling the descent of nitric oxide during the elevated stratopause event of January 2013   Using simulations with a whole-atmosphere chemistry-climate model nudged by
meteorological analyses, global satellite observations of nitrogen oxide (NO)
and water vapour by the Sub-Millimetre Radiometer instrument (SMR), of
temperature by the Microwave Limb Sounder (MLS), as well as local radar
observations, this study examines the recent major stratospheric sudden warming
accompanied by an elevated stratopause event (ESE) that occurred in January
2013. We examine dynamical processes during the ESE, including the role of
planetary wave, gravity wave and tidal forcing on the initiation of the descent
in the mesosphere-lower thermosphere (MLT) and its continuation throughout the
mesosphere and stratosphere, as well as the impact of model eddy diffusion. We
analyse the transport of NO and find the model underestimates the large descent
of NO compared to SMR observations. We demonstrate that the discrepancy arises
abruptly in the MLT region at a time when the resolved wave forcing and the
planetary wave activity increase, just before the elevated stratopause reforms.
The discrepancy persists despite doubling the model eddy diffusion. While the
simulations reproduce an enhancement of the semi-diurnal tide following the
onset of the 2013 SSW, corroborating new meteor radar observations at high
northern latitudes over Trondheim (63.4$^{\circ}$N), the modelled tidal
contribution to the forcing of the mean meridional circulation and to the
descent is a small portion of the resolved wave forcing, and lags it by about
ten days.
",Physics,Physics
"Scaling Limits for Super--replication with Transient Price Impact   We prove limit theorems for the super-replication cost of European options in
a Binomial model with transient price impact. We show that if the time step
goes to zero and the effective resilience between consecutive trading times
remains constant then the limit of the super--replication prices coincide with
the scaling limit for temporary price impact with a modified market depth.
",Quantitative Finance,Quantitative Finance
"Robust two-qubit gates in a linear ion crystal using a frequency-modulated driving force   In an ion trap quantum computer, collective motional modes are used to
entangle two or more qubits in order to execute multi-qubit logical gates. Any
residual entanglement between the internal and motional states of the ions
results in loss of fidelity, especially when there are many spectator ions in
the crystal. We propose using a frequency-modulated (FM) driving force to
minimize such errors. In simulation, we obtained an optimized FM two-qubit gate
that can suppress errors to less than 0.01\% and is robust against frequency
drifts over $\pm$1 kHz. Experimentally, we have obtained a two-qubit gate
fidelity of $98.3(4)\%$, a state-of-the-art result for two-qubit gates with 5
ions.
",Physics,Physics
"On the Origin of Deep Learning   This paper is a review of the evolutionary history of deep learning models.
It covers from the genesis of neural networks when associationism modeling of
the brain is studied, to the models that dominate the last decade of research
in deep learning like convolutional neural networks, deep belief networks, and
recurrent neural networks. In addition to a review of these models, this paper
primarily focuses on the precedents of the models above, examining how the
initial ideas are assembled to construct the early models and how these
preliminary models are developed into their current forms. Many of these
evolutionary paths last more than half a century and have a diversity of
directions. For example, CNN is built on prior knowledge of biological vision
system; DBN is evolved from a trade-off of modeling power and computation
complexity of graphical models and many nowadays models are neural counterparts
of ancient linear models. This paper reviews these evolutionary paths and
offers a concise thought flow of how these models are developed, and aims to
provide a thorough background for deep learning. More importantly, along with
the path, this paper summarizes the gist behind these milestones and proposes
many directions to guide the future research of deep learning.
",Computer Science; Statistics,Computer Science; Statistics
"Water sub-diffusion in membranes for fuel cells   We investigate the dynamics of water confined in soft ionic nano-assemblies,
an issue critical for a general understanding of the multi-scale
structure-function interplay in advanced materials. We focus in particular on
hydrated perfluoro-sulfonic acid compounds employed as electrolytes in fuel
cells. These materials form phase-separated morphologies that show outstanding
proton-conducting properties, directly related to the state and dynamics of the
absorbed water. We have quantified water motion and ion transport by combining
Quasi Elastic Neutron Scattering, Pulsed Field Gradient Nuclear Magnetic
Resonance, and Molecular Dynamics computer simulation. Effective water and ion
diffusion coefficients have been determined together with their variation upon
hydration at the relevant atomic, nanoscopic and macroscopic scales, providing
a complete picture of transport. We demonstrate that confinement at the
nanoscale and direct interaction with the charged interfaces produce anomalous
sub-diffusion, due to a heterogeneous space-dependent dynamics within the ionic
nanochannels. This is irrespective of the details of the chemistry of the
hydrophobic confining matrix, confirming the statistical significance of our
conclusions. Our findings turn out to indicate interesting connections and
possibilities of cross-fertilization with other domains, including biophysics.
They also establish fruitful correspondences with advanced topics in
statistical mechanics, resulting in new possibilities for the analysis of
Neutron scattering data.
",Physics,Physics
"Understanding News Outlets' Audience-Targeting Patterns   The power of the press to shape the informational landscape of a population
is unparalleled, even now in the era of democratic access to all information
outlets. However, it is known that news outlets (particularly more traditional
ones) tend to discriminate who they want to reach, and who to leave aside. In
this work, we attempt to shed some light on the audience targeting patterns of
newspapers, using the Chilean media ecosystem. First, we use the gravity model
to analyze geography as a factor in explaining audience reachability. This
shows that some newspapers are indeed driven by geographical factors (mostly
local news outlets) but some others are not (national-distribution outlets).
For those which are not, we use a regression model to study the influence of
socioeconomic and political characteristics in news outlets adoption. We
conclude that indeed larger, national-distribution news outlets target
populations based on these factors, rather than on geography or immediacy.
",Computer Science,Computer Science
"Summarized Network Behavior Prediction   This work studies the entity-wise topical behavior from massive network logs.
Both the temporal and the spatial relationships of the behavior are explored
with the learning architectures combing the recurrent neural network (RNN) and
the convolutional neural network (CNN). To make the behavioral data appropriate
for the spatial learning in CNN, several reduction steps are taken to form the
topical metrics and place them homogeneously like pixels in the images. The
experimental result shows both the temporal- and the spatial- gains when
compared to a multilayer perceptron (MLP) network. A new learning framework
called spatially connected convolutional networks (SCCN) is introduced to more
efficiently predict the behavior.
",Computer Science; Statistics,Computer Science; Statistics
"NIP formulas and Baire 1 definability   In this short note, using results of Bourgain, Fremlin, and Talagrand
\cite{BFT}, we show that for a countable structure $M$, a saturated elementary
extension $M^*$ of $M$ and a formula $\phi(x,y)$ the following are equivalent:
(i) $\phi(x,y)$ is NIP on $M$ (in the sense of Definition 2.1).
(ii) Whenever $p(x)\in S_\phi(M^*)$ is finitely satisfiable in $M$ then it is
Baire 1 definable over $M$ (in sense of Definition 2.5).
",Mathematics,Mathematics
"Ringel duality as an instance of Koszul duality   In their previous work, S. Koenig, S. Ovsienko and the second author showed
that every quasi-hereditary algebra is Morita equivalent to the right algebra,
i.e. the opposite algebra of the left dual, of a coring. Let $A$ be an
associative algebra and $V$ an $A$-coring whose right algebra $R$ is
quasi-hereditary. In this paper, we give a combinatorial description of an
associative algebra $B$ and a $B$-coring $W$ whose right algebra is the Ringel
dual of $R$. We apply our results in small examples to obtain restrictions on
the $A_\infty$-structure of the $\textrm{Ext}$-algebra of standard modules over
a class of quasi-hereditary algebras related to birational morphisms of smooth
surfaces.
",Mathematics,Mathematics
"A general framework for data-driven uncertainty quantification under complex input dependencies using vine copulas   Systems subject to uncertain inputs produce uncertain responses. Uncertainty
quantification (UQ) deals with the estimation of statistics of the system
response, given a computational model of the system and a probabilistic model
of its inputs. In engineering applications it is common to assume that the
inputs are mutually independent or coupled by a Gaussian or elliptical
dependence structure (copula). In this paper we overcome such limitations by
modelling the dependence structure of multivariate inputs as vine copulas. Vine
copulas are models of multivariate dependence built from simpler pair-copulas.
The vine representation is flexible enough to capture complex dependencies.
This paper formalises the framework needed to build vine copula models of
multivariate inputs and to combine them with virtually any UQ method. The
framework allows for a fully automated, data-driven inference of the
probabilistic input model on available input data. The procedure is exemplified
on two finite element models of truss structures, both subject to inputs with
non-Gaussian dependence structures. For each case, we analyse the moments of
the model response (using polynomial chaos expansions), and perform a
structural reliability analysis to calculate the probability of failure of the
system (using the first order reliability method and importance sampling).
Reference solutions are obtained by Monte Carlo simulation. The results show
that, while the Gaussian assumption yields biased statistics, the vine copula
representation achieves significantly more precise estimates, even when its
structure needs to be fully inferred from a limited amount of observations.
",Statistics,Computer Science; Statistics
"A Game of Life on Penrose tilings   We define rules for cellular automata played on quasiperiodic tilings of the
plane arising from the multigrid method in such a way that these cellular
automata are isomorphic to Conway's Game of Life. Although these tilings are
nonperiodic, determining the next state of each tile is a local computation,
requiring only knowledge of the local structure of the tiling and the states of
finitely many nearby tiles. As an example, we show a version of a ""glider""
moving through a region of a Penrose tiling. This constitutes a potential
theoretical framework for a method of executing computations in
non-periodically structured substrates such as quasicrystals.
",Physics; Mathematics,Computer Science
"Binary Voting with Delegable Proxy: An Analysis of Liquid Democracy   The paper provides an analysis of the voting method known as delegable proxy
voting, or liquid democracy. The analysis first positions liquid democracy
within the theory of binary aggregation. It then focuses on two issues of the
system: the occurrence of delegation cycles; and the effect of delegations on
individual rationality when voting on logically interdependent propositions. It
finally points to proposals on how the system may be modified in order to
address the above issues.
",Computer Science,Computer Science
"Distributed Nesterov gradient methods over arbitrary graphs   In this letter, we introduce a distributed Nesterov method, termed as
$\mathcal{ABN}$, that does not require doubly-stochastic weight matrices.
Instead, the implementation is based on a simultaneous application of both row-
and column-stochastic weights that makes this method applicable to arbitrary
(strongly-connected) graphs. Since constructing column-stochastic weights needs
additional information (the number of outgoing neighbors at each agent), not
available in certain communication protocols, we derive a variation, termed as
FROZEN, that only requires row-stochastic weights but at the expense of
additional iterations for eigenvector learning. We numerically study these
algorithms for various objective functions and network parameters and show that
the proposed distributed Nesterov methods achieve acceleration compared to the
current state-of-the-art methods for distributed optimization.
",Computer Science; Statistics,Computer Science; Statistics
"Routing in FRET-based Nanonetworks   Nanocommunications, understood as communications between nanoscale devices,
is commonly regarded as a technology essential for cooperation of large groups
of nanomachines and thus crucial for development of the whole area of
nanotechnology. While solutions for point-to-point nanocommunications have been
already proposed, larger networks cannot function properly without routing. In
this article we focus on the nanocommunications via Forster Resonance Energy
Transfer (FRET), which was found to be a technique with a very high signal
propagation speed, and discuss how to route signals through nanonetworks. We
introduce five new routing mechanisms, based on biological properties of
specific molecules. We experimentally validate one of these mechanisms.
Finally, we analyze open issues showing the technical challenges for signal
transmission and routing in FRET-based nanocommunications.
",Quantitative Biology,Computer Science
"Resonant Drag Instabilities in protoplanetary disks: the streaming instability and new, faster-growing instabilities   We identify and study a number of new, rapidly growing instabilities of dust
grains in protoplanetary disks, which may be important for planetesimal
formation. The study is based on the recognition that dust-gas mixtures are
generically unstable to a Resonant Drag Instability (RDI), whenever the gas,
absent dust, supports undamped linear modes. We show that the ""streaming
instability"" is an RDI associated with epicyclic oscillations; this provides
simple interpretations for its mechanisms and accurate analytic expressions for
its growth rates and fastest-growing wavelengths. We extend this analysis to
more general dust streaming motions and other waves, including buoyancy and
magnetohydrodynamic oscillations, finding various new instabilities. Most
importantly, we identify the disk ""settling instability,"" which occurs as dust
settles vertically into the midplane of a rotating disk. For small grains, this
instability grows many orders of magnitude faster than the standard streaming
instability, with a growth rate that is independent of grain size. Growth
timescales for realistic dust-to-gas ratios are comparable to the disk orbital
period, and the characteristic wavelengths are more than an order of magnitude
larger than the streaming instability (allowing the instability to concentrate
larger masses). This suggests that in the process of settling, dust will band
into rings then filaments or clumps, potentially seeding dust traps,
high-metallicity regions that in turn seed the streaming instability, or even
overdensities that coagulate or directly collapse to planetesimals.
",Physics,Physics
"Replication issues in syntax-based aspect extraction for opinion mining   Reproducing experiments is an important instrument to validate previous work
and build upon existing approaches. It has been tackled numerous times in
different areas of science. In this paper, we introduce an empirical
replicability study of three well-known algorithms for syntactic centric
aspect-based opinion mining. We show that reproducing results continues to be a
difficult endeavor, mainly due to the lack of details regarding preprocessing
and parameter setting, as well as due to the absence of available
implementations that clarify these details. We consider these are important
threats to validity of the research on the field, specifically when compared to
other problems in NLP where public datasets and code availability are critical
validity components. We conclude by encouraging code-based research, which we
think has a key role in helping researchers to understand the meaning of the
state-of-the-art better and to generate continuous advances.
",Computer Science,Computer Science; Statistics
"A GNS construction of three-dimensional abelian Dijkgraaf-Witten theories   We give a detailed account of the so-called ""universal construction"" that
aims to extend invariants of closed manifolds, possibly with additional
structure, to topological field theories and show that it amounts to a
generalization of the GNS construction. We apply this construction to an
invariant defined in terms of the groupoid cardinality of groupoids of bundles
to recover Dijkgraaf-Witten theories, including the vector spaces obtained as a
linearization of spaces of principal bundles.
",Mathematics,Mathematics
"Using angular pair upweighting to improve 3D clustering measurements   Three dimensional galaxy clustering measurements provide a wealth of
cosmological information. However, obtaining spectra of galaxies is expensive,
and surveys often only measure redshifts for a subsample of a target galaxy
population. Provided that the spectroscopic data is representative, we argue
that angular pair upweighting should be used in these situations to improve the
3D clustering measurements. We present a toy model showing mathematically how
such a weighting can improve measurements, and provide a practical example of
its application using mocks created for the Baryon Oscillation Spectroscopic
Survey (BOSS). Our analysis of mocks suggests that, if an angular clustering
measurement is available over twice the area covered spectroscopically,
weighting gives a $\sim$10-20% reduction of the variance of the monopole
correlation function on the BAO scale.
",Physics,Physics
"Estimating occupation time functionals   We study the estimation of integral type functionals $\int_{0}^{t}f(X_{r})dr$
for a function $f$ and a $d$-dimensional càdlàg process $X$ with respect to
discrete observations by a Riemann-sum estimator. Based on novel semimartingale
approximations in the Fourier domain, central limit theorems are proved for
$L^{2}$-Sobolev functions $f$ with fractional smoothness and continuous Itô
semimartingales $X$. General $L^{2}(\mathbb{P})$-upper bounds on the error for
càdlàg processes are given under weak assumptions. These bounds combine and
generalize all previously obtained results in the literature and apply also to
non-Markovian processes. Several detailed examples are discussed. As
application the approximation of local times for fractional Brownian motion is
studied. The optimality of the $L^{2}(\mathbb{P})$-upper bounds is shown by
proving the corresponding lower bounds in case of Brownian motion.
",Mathematics; Statistics,Mathematics; Statistics
"Wind accretion onto compact objects   X-ray emission associated to accretion onto compact objects displays
important levels of photometric and spectroscopic time-variability. When the
accretor orbits a Supergiant star, it captures a fraction of the supersonic
radiatively-driven wind which forms shocks in its vicinity. The amplitude and
stability of this gravitational beaming of the flow conditions the mass
accretion rate responsible, in fine, for the X-ray luminosity of those
Supergiant X-ray Binaries. The capacity of this low angular momentum inflow to
form a disc-like structure susceptible to be the stage of well-known
instabilities remains at stake. Using state-of-the-art numerical setups, we
characterized the structure of a Bondi-Hoyle-Lyttleton flow onto a compact
object, from the shock down to the vicinity of the accretor, typically five
orders of magnitude smaller. The evolution of the mass accretion rate and of
the bow shock which forms around the accretor (transverse structure, opening
angle, stability, temperature profile) with the Mach number of the incoming
flow is described in detail. The robustness of those simulations based on the
High Performance Computing MPI-AMRVAC code is supported by the topology of the
inner sonic surface, in agreement with theoretical expectations. We developed a
synthetic model of mass transfer in Supergiant X-ray Binaries which couples the
launching of the wind accordingly to the stellar parameters, the orbital
evolution of the streamlines in a modified Roche potential and the accretion
process. We show that the shape of the permanent flow is entirely determined by
the mass ratio, the filling factor, the Eddington factor and the alpha-force
multiplier. Provided scales such as the orbital period are known, we can trace
back the observables to evaluate the mass accretion rates, the accretion
mechanism (stream or wind-dominated) and the shearing of the inflow.
",Physics,Physics
"An Empirical Analysis of Proximal Policy Optimization with Kronecker-factored Natural Gradients   In this technical report, we consider an approach that combines the PPO
objective and K-FAC natural gradient optimization, for which we call PPOKFAC.
We perform a range of empirical analysis on various aspects of the algorithm,
such as sample complexity, training speed, and sensitivity to batch size and
training epochs. We observe that PPOKFAC is able to outperform PPO in terms of
sample complexity and speed in a range of MuJoCo environments, while being
scalable in terms of batch size. In spite of this, it seems that adding more
epochs is not necessarily helpful for sample efficiency, and PPOKFAC seems to
be worse than its A2C counterpart, ACKTR.
",Statistics,Computer Science; Statistics
"Improved Power Decoding of One-Point Hermitian Codes   We propose a new partial decoding algorithm for one-point Hermitian codes
that can decode up to the same number of errors as the Guruswami--Sudan
decoder. Simulations suggest that it has a similar failure probability as the
latter one. The algorithm is based on a recent generalization of the power
decoding algorithm for Reed--Solomon codes and does not require an expensive
root-finding step. In addition, it promises improvements for decoding
interleaved Hermitian codes.
",Computer Science,Computer Science
"Extended Vertical Lists for Temporal Pattern Mining from Multivariate Time Series   Temporal Pattern Mining (TPM) is the problem of mining predictive complex
temporal patterns from multivariate time series in a supervised setting. We
develop a new method called the Fast Temporal Pattern Mining with Extended
Vertical Lists. This method utilizes an extension of the Apriori property which
requires a more complex pattern to appear within records only at places where
all of its subpatterns are detected as well. The approach is based on a novel
data structure called the Extended Vertical List that tracks positions of the
first state of the pattern inside records. Extensive computational results
indicate that the new method performs significantly faster than the previous
version of the algorithm for TMP. However, the speed-up comes at the expense of
memory usage.
",Statistics,Computer Science
"Flow equations for cold Bose gases   We derive flow equations for cold atomic gases with one macroscopically
populated energy level. The generator is chosen such that the ground state
decouples from all other states in the system as the renormalization group flow
progresses. We propose a self-consistent truncation scheme for the flow
equations at the level of three-body operators and show how they can be used to
calculate the ground state energy of a general $N$-body system. Moreover, we
provide a general method to estimate the truncation error in the calculated
energies. Finally, we test our scheme by benchmarking to the exactly solvable
Lieb-Liniger model and find good agreement for weak and moderate interaction
strengths.
",Physics,Physics
"Icing on the Cake: An Easy and Quick Post-Learnig Method You Can Try After Deep Learning   We found an easy and quick post-learning method named ""Icing on the Cake"" to
enhance a classification performance in deep learning. The method is that we
train only the final classifier again after an ordinary training is done.
",Statistics,Computer Science; Statistics
"Priv'IT: Private and Sample Efficient Identity Testing   We develop differentially private hypothesis testing methods for the small
sample regime. Given a sample $\cal D$ from a categorical distribution $p$ over
some domain $\Sigma$, an explicitly described distribution $q$ over $\Sigma$,
some privacy parameter $\varepsilon$, accuracy parameter $\alpha$, and
requirements $\beta_{\rm I}$ and $\beta_{\rm II}$ for the type I and type II
errors of our test, the goal is to distinguish between $p=q$ and
$d_{\rm{TV}}(p,q) \geq \alpha$.
We provide theoretical bounds for the sample size $|{\cal D}|$ so that our
method both satisfies $(\varepsilon,0)$-differential privacy, and guarantees
$\beta_{\rm I}$ and $\beta_{\rm II}$ type I and type II errors. We show that
differential privacy may come for free in some regimes of parameters, and we
always beat the sample complexity resulting from running the $\chi^2$-test with
noisy counts, or standard approaches such as repetition for endowing
non-private $\chi^2$-style statistics with differential privacy guarantees. We
experimentally compare the sample complexity of our method to that of recently
proposed methods for private hypothesis testing.
",Computer Science; Mathematics; Statistics,Computer Science; Statistics
"The sequence of open and closed prefixes of a Sturmian word   A finite word is closed if it contains a factor that occurs both as a prefix
and as a suffix but does not have internal occurrences, otherwise it is open.
We are interested in the {\it oc-sequence} of a word, which is the binary
sequence whose $n$-th element is $0$ if the prefix of length $n$ of the word is
open, or $1$ if it is closed. We exhibit results showing that this sequence is
deeply related to the combinatorial and periodic structure of a word. In the
case of Sturmian words, we show that these are uniquely determined (up to
renaming letters) by their oc-sequence. Moreover, we prove that the class of
finite Sturmian words is a maximal element with this property in the class of
binary factorial languages. We then discuss several aspects of Sturmian words
that can be expressed through this sequence. Finally, we provide a linear-time
algorithm that computes the oc-sequence of a finite word, and a linear-time
algorithm that reconstructs a finite Sturmian word from its oc-sequence.
",Computer Science; Mathematics,Computer Science
"Treewidth distance on phylogenetic trees   In this article we study the treewidth of the \emph{display graph}, an
auxiliary graph structure obtained from the fusion of phylogenetic (i.e.,
evolutionary) trees at their leaves. Earlier work has shown that the treewidth
of the display graph is bounded if the trees are in some formal sense
topologically similar. Here we further expand upon this relationship. We
analyse a number of reduction rules which are commonly used in the
phylogenetics literature to obtain fixed parameter tractable algorithms. In
some cases (the \emph{subtree} reduction) the reduction rules behave similarly
with respect to treewidth, while others (the \emph{cluster} reduction) behave
very differently, and the behaviour of the \emph{chain reduction} is
particularly intriguing because of its link with graph separators and forbidden
minors. We also show that the gap between treewidth and Tree Bisection and
Reconnect (TBR) distance can be infinitely large, and that unlike, for example,
planar graphs the treewidth of the display graph can be as much as linear in
its number of vertices. On a slightly different note we show that if a display
graph is formed from the fusion of a phylogenetic network and a tree, rather
than from two trees, the treewidth of the display graph is bounded whenever the
tree can be topologically embedded (""displayed"") within the network. This opens
the door to the formulation of the display problem in Monadic Second Order
Logic (MSOL). A number of other auxiliary results are given. We conclude with a
discussion and list a number of open problems.
",Computer Science,Computer Science; Mathematics
"Flashes of Hidden Worlds at Colliders   (This is a general physics level overview article about hidden sectors, and
how they motivate searches for long-lived particles. Intended for publication
in Physics Today.)
Searches for new physics at the Large Hadron Collider have so far come up
empty, but we just might not be looking in the right place. Spectacular bursts
of particles appearing seemingly out of nowhere could shed light on some of
nature's most profound mysteries.
",Physics,Physics
"Versality of the relative Fukaya category   Seidel introduced the notion of a Fukaya category `relative to an ample
divisor', explained that it is a deformation of the Fukaya category of the
affine variety that is the complement of the divisor, and showed how the
relevant deformation theory is controlled by the symplectic cohomology of the
complement. We elaborate on Seidel's definition of the relative Fukaya
category, and give a criterion under which the deformation is versal.
",Mathematics,Mathematics
"Future Energy Consumption Prediction Based on Grey Forecast Model   We use grey forecast model to predict the future energy consumption of four
states in the U.S, and make some improvments to the model.
",Statistics,Statistics
"Evaporation and scattering of momentum- and velocity-dependent dark matter in the Sun   Dark matter with momentum- or velocity-dependent interactions with nuclei has
shown significant promise for explaining the so-called Solar Abundance Problem,
a longstanding discrepancy between solar spectroscopy and helioseismology. The
best-fit models are all rather light, typically with masses in the range of 3-5
GeV. This is exactly the mass range where dark matter evaporation from the Sun
can be important, but to date no detailed calculation of the evaporation of
such models has been performed. Here we carry out this calculation, for the
first time including arbitrary velocity- and momentum-dependent interactions,
thermal effects, and a completely general treatment valid from the optically
thin limit all the way through to the optically thick regime. We find that
depending on the dark matter mass, interaction strength and type, the mass
below which evaporation is relevant can vary from 1 to 4 GeV. This has the
effect of weakening some of the better-fitting solutions to the Solar Abundance
Problem, but also improving a number of others. As a by-product, we also
provide an improved derivation of the capture rate that takes into account
thermal and optical depth effects, allowing the standard result to be smoothly
matched to the well-known saturation limit.
",Physics,Physics
"Scalable Twin Neural Networks for Classification of Unbalanced Data   Twin Support Vector Machines (TWSVMs) have emerged an efficient alternative
to Support Vector Machines (SVM) for learning from imbalanced datasets. The
TWSVM learns two non-parallel classifying hyperplanes by solving a couple of
smaller sized problems. However, it is unsuitable for large datasets, as it
involves matrix operations. In this paper, we discuss a Twin Neural Network
(Twin NN) architecture for learning from large unbalanced datasets. The Twin NN
also learns an optimal feature map, allowing for better discrimination between
classes. We also present an extension of this network architecture for
multiclass datasets. Results presented in the paper demonstrate that the Twin
NN generalizes well and scales well on large unbalanced datasets.
",Computer Science,Computer Science; Statistics
"LinNet: Probabilistic Lineup Evaluation Through Network Embedding   Which of your team's possible lineups has the best chances against each of
your opponents possible lineups? In order to answer this question we develop
LinNet. LinNet exploits the dynamics of a directed network that captures the
performance of lineups at their matchups. The nodes of this network represent
the different lineups, while an edge from node j to node i exists if lineup i
has outperformed lineup j. We further annotate each edge with the corresponding
performance margin (point margin per minute). We then utilize this structure to
learn a set of latent features for each node (i.e., lineup) using the node2vec
framework. Consequently, LinNet builds a model on this latent space for the
probability of lineup A beating lineup B. We evaluate LinNet using NBA lineup
data from the five seasons between 2007-08 and 2011-12. Our results indicate
that our method has an out-of-sample accuracy of 69%. In comparison, utilizing
the adjusted plus-minus of the players within a lineup for the same prediction
problem provides an accuracy of 56%. More importantly, the probabilities are
well-calibrated as shown by the probability validation curves. One of the
benefits of LinNet - apart from its accuracy - is that it is generic and can be
applied in different sports since the only input required is the lineups'
matchup performances, i.e., not sport-specific features are needed.
",Statistics,Computer Science
"Learning Deep CNN Denoiser Prior for Image Restoration   Model-based optimization methods and discriminative learning methods have
been the two dominant strategies for solving various inverse problems in
low-level vision. Typically, those two kinds of methods have their respective
merits and drawbacks, e.g., model-based optimization methods are flexible for
handling different inverse problems but are usually time-consuming with
sophisticated priors for the purpose of good performance; in the meanwhile,
discriminative learning methods have fast testing speed but their application
range is greatly restricted by the specialized task. Recent works have revealed
that, with the aid of variable splitting techniques, denoiser prior can be
plugged in as a modular part of model-based optimization methods to solve other
inverse problems (e.g., deblurring). Such an integration induces considerable
advantage when the denoiser is obtained via discriminative learning. However,
the study of integration with fast discriminative denoiser prior is still
lacking. To this end, this paper aims to train a set of fast and effective CNN
(convolutional neural network) denoisers and integrate them into model-based
optimization method to solve other inverse problems. Experimental results
demonstrate that the learned set of denoisers not only achieve promising
Gaussian denoising results but also can be used as prior to deliver good
performance for various low-level vision applications.
",Computer Science,Computer Science; Statistics
"Droplet states in quantum XXZ spin systems on general graphs   We study XXZ spin systems on general graphs. In particular, we describe the
formation of droplet states near the bottom of the spectrum in the Ising phase
of the model, where the Z-term dominates the XX-term. As key tools we use
particle number conservation of XXZ systems and symmetric products of graphs
with their associated adjacency matrices and Laplacians. Of particular interest
to us are strips and multi-dimensional Euclidean lattices, for which we discuss
the existence of spectral gaps above the droplet regime. We also prove a
Combes-Thomas bound which shows that the eigenstates in the droplet regime are
exponentially small perturbations of strict (classical) droplets.
",Mathematics,Physics
"Mining Public Opinion about Economic Issues: Twitter and the U.S. Presidential Election   Opinion polls have been the bridge between public opinion and politicians in
elections. However, developing surveys to disclose people's feedback with
respect to economic issues is limited, expensive, and time-consuming. In recent
years, social media such as Twitter has enabled people to share their opinions
regarding elections. Social media has provided a platform for collecting a
large amount of social media data. This paper proposes a computational public
opinion mining approach to explore the discussion of economic issues in social
media during an election. Current related studies use text mining methods
independently for election analysis and election prediction; this research
combines two text mining methods: sentiment analysis and topic modeling. The
proposed approach has effectively been deployed on millions of tweets to
analyze economic concerns of people during the 2012 US presidential election.
",Computer Science; Statistics,Computer Science
"Emergence of magnetic long-range order in kagome quantum antiferromagnets   The existence of a spin-liquid ground state of the $s=1/2$ Heisenberg kagome
antiferromagnet (KAFM) is well established. Meanwhile, also for the $s=1$
Heisenberg KAFM evidence for the absence of magnetic long-range order (LRO) was
found. Magnetic LRO in Heisenberg KAFMs can emerge by increasing the spin
quantum number $s$ to $s>1$ and for $s=1$ by an easy-plane anisotropy. In the
present paper we discuss the route to magnetic order in $s=1/2$ KAFMs by
including an isotropic interlayer coupling (ILC) $J_\perp$ as well as an
easy-plane anisotropy in the kagome layers by using the coupled-cluster method
to high orders of approximation. We consider ferro- as well as
antiferromagnetic $J_\perp$. To discuss the general question for the crossover
from a purely two-dimensional (2D) to a quasi-2D and finally to a
three-dimensional system we consider the simplest model of stacked (unshifted)
kagome layers. Although the ILC of real kagome compounds is often more
sophisticated, such a geometry of the ILC can be relevant for barlowite. We
find that the spin-liquid ground state present for the strictly 2D $s=1/2$
$XXZ$ KAFM survives a finite ILC, where the spin-liquid region shrinks
monotonously with increasing anisotropy. If the ILC becomes large enough (about
15\% of intralayer coupling for the isotropic Heisenberg case and about 4\% for
the $XY$ limit) magnetic LRO can be established, where the $q=0$ symmetry is
favorable if $J_\perp$ is of moderate strength. If the strength of the ILC
further increases, $\sqrt{3}\times \sqrt{3}$ LRO can become favorable against
$q=0$ LRO.
",Physics,Physics
"Binary orbits from combined astrometric and spectroscopic data   An efficient Bayesian technique for estimation problems in fundamental
stellar astronomy is tested on simulated data for a binary observed both
astrometrically and spectroscopically. Posterior distributions are computed for
the components' masses and for the binary's parallax. One thousand independent
repetitions of the simulation demonstrate that the 1- and 2-$\!\sigma$
credibility intervals for these fundamental quantities have close to the
correct coverage fractions. In addition, the simulations allow the
investigation of the statistical properties of a Bayesian goodness-of-fit
criterion and of the corresponding p-value. The criterion has closely similar
properties to the traditional chi^{2} test for minimum-chi^{2} solutions.
",Physics,Physics
"Anomalous current in diffusive ferromagnetic Josephson junctions   We demonstrate that in diffusive superconductor/ferromagnet/superconductor
(S/F/S) junctions a finite, {\it anomalous}, Josephson current can flow even at
zero phase difference between the S electrodes. The conditions for the
observation of this effect are non-coplanar magnetization distribution and a
broken magnetization inversion symmetry of the superconducting current. The
latter symmetry is intrinsic for the widely used quasiclassical approximation
and prevent previous works, based on this approximation, from obtaining the
Josephson anomalous current. We show that this symmetry can be removed by
introducing spin-dependent boundary conditions for the quasiclassical equations
at the superconducting/ferromagnet interfaces in diffusive systems. Using this
recipe we considered generic multilayer magnetic systems and determine the
ideal experimental conditions in order to maximize the anomalous current.
",Physics,Physics
"On the Performance of Network Parallel Training in Artificial Neural Networks   Artificial Neural Networks (ANNs) have received increasing attention in
recent years with applications that span a wide range of disciplines including
vital domains such as medicine, network security and autonomous transportation.
However, neural network architectures are becoming increasingly complex and
with an increasing need to obtain real-time results from such models, it has
become pivotal to use parallelization as a mechanism for speeding up network
training and deployment. In this work we propose an implementation of Network
Parallel Training through Cannon's Algorithm for matrix multiplication. We show
that increasing the number of processes speeds up training until the point
where process communication costs become prohibitive; this point varies by
network complexity. We also show through empirical efficiency calculations that
the speedup obtained is superlinear.
",Computer Science; Statistics,Computer Science; Statistics
"On the martingale property in the rough Bergomi model   We consider a class of fractional stochastic volatility models (including the
so-called rough Bergomi model), where the volatility is a superlinear function
of a fractional Gaussian process. We show that the stock price is a true
martingale if and only if the correlation $\rho$ between the driving Brownian
motions of the stock and the volatility is nonpositive. We also show that for
each $\rho<0$ and $m> \frac{1}{1-\rho^2}$, the $m$-th moment of the stock
price is infinite at each positive time.
",Quantitative Finance,Quantitative Finance
"Neural Machine Translation and Sequence-to-sequence Models: A Tutorial   This tutorial introduces a new and powerful set of techniques variously
called ""neural machine translation"" or ""neural sequence-to-sequence models"".
These techniques have been used in a number of tasks regarding the handling of
human language, and can be a powerful tool in the toolbox of anyone who wants
to model sequential data of some sort. The tutorial assumes that the reader
knows the basics of math and programming, but does not assume any particular
experience with neural networks or natural language processing. It attempts to
explain the intuition behind the various methods covered, then delves into them
with enough mathematical detail to understand them concretely, and culiminates
with a suggestion for an implementation exercise, where readers can test that
they understood the content in practice.
",Computer Science; Statistics,Computer Science
"Refining Trace Abstraction using Abstract Interpretation   The CEGAR loop in software model checking notoriously diverges when the
abstraction refinement procedure does not derive a loop invariant. An
abstraction refinement procedure based on an SMT solver is applied to a trace,
i.e., a restricted form of a program (without loops). In this paper, we present
a new abstraction refinement procedure that aims at circumventing this
restriction whenever possible. We apply abstract interpretation to a program
that we derive from the given trace. If the program contains a loop, we are
guaranteed to obtain a loop invariant. We call an SMT solver only in the case
where the abstract interpretation returns an indefinite answer. That is, the
idea is to use abstract interpretation and an SMT solver in tandem. An
experimental evaluation in the setting of trace abstraction indicates the
practical potential of this idea.
",Computer Science,Computer Science
"Effect of viscosity ratio on the self-sustained instabilities in planar immiscible jets   Previous studies have shown that intermediate surface tension has a
counterintuitive destabilizing effect on 2-phase planar jets. Here, the
transition process in confined 2D jets of two fluids with varying viscosity
ratio is investigated using DNS. Neutral curves for persistent oscillations are
found by recording the norm of the velocity residuals in DNS for over 1000
nondimensional time units, or until the signal has reached a constant level in
a logarithmic scale - either a converged steady state, or a ""statistically
steady"" oscillatory state. Oscillatory final states are found for all viscosity
ratios (0.1-10). For uniform viscosity (m=1), the first bifurcation is through
a surface tension-driven global instability. For low viscosity of the outer
fluid, there is a mode competition between a steady asymmetric Coanda-type
attachment mode and the surface tension-induced mode. At moderate surface
tension, the Coanda-type attachment dominates and eventually triggers
time-dependent convective bursts. At high surface tension, the surface
tension-dominated mode dominates. For high viscosity of the outer fluid,
persistent oscillations appear due to a strong convective instability. Finally,
the m=1 jet remains unstable far from the inlet when the shear profile is
nearly constant. Comparing this to a parallel Couette flow (without inflection
points), we show that in both flows, a hidden interfacial mode brought out by
surface tension becomes temporally and absolutely unstable in an intermediate
Weber and Reynolds regime. An energy analysis of the Couette setup shows that
surface tension, although dissipative, induces a velocity field near the
interface which extracts energy from the flow through a viscous mechanism. This
study highlights the rich dynamics of immiscible planar uniform-density jets,
where several self-sustained and convective mechanisms compete depending on the
exact parameters.
",Physics,Physics
"Shape analysis on Lie groups and homogeneous spaces   In this paper we are concerned with the approach to shape analysis based on
the so called Square Root Velocity Transform (SRVT). We propose a
generalisation of the SRVT from Euclidean spaces to shape spaces of curves on
Lie groups and on homogeneous manifolds. The main idea behind our approach is
to exploit the geometry of the natural Lie group actions on these spaces.
",Mathematics,Mathematics
"Highrisk Prediction from Electronic Medical Records via Deep Attention Networks   Predicting highrisk vascular diseases is a significant issue in the medical
domain. Most predicting methods predict the prognosis of patients from
pathological and radiological measurements, which are expensive and require
much time to be analyzed. Here we propose deep attention models that predict
the onset of the high risky vascular disease from symbolic medical histories
sequence of hypertension patients such as ICD-10 and pharmacy codes only,
Medical History-based Prediction using Attention Network (MeHPAN). We
demonstrate two types of attention models based on 1) bidirectional gated
recurrent unit (R-MeHPAN) and 2) 1D convolutional multilayer model (C-MeHPAN).
Two MeHPAN models are evaluated on approximately 50,000 hypertension patients
with respect to precision, recall, f1-measure and area under the curve (AUC).
Experimental results show that our MeHPAN methods outperform standard
classification models. Comparing two MeHPANs, R-MeHPAN provides more better
discriminative capability with respect to all metrics while C-MeHPAN presents
much shorter training time with competitive accuracy.
",Computer Science; Statistics,Computer Science; Statistics
"Inverse mean curvature flow in quaternionic hyperbolic space   In this paper we complete the study started in [Pi2] of evolution by inverse
mean curvature flow of star-shaped hypersurface in non-compact rank one
symmetric spaces. We consider the evolution by inverse mean curvature flow of a
closed, mean convex and star-shaped hypersurface in the quaternionic hyperbolic
space. We prove that the flow is defined for any positive time, the evolving
hypersurface stays star-shaped and mean convex. Moreover the induced metric
converges, after rescaling, to a conformal multiple of the standard
sub-Riemannian metric on the sphere defined on a codimension 3 distribution.
Finally we show that there exists a family of examples such that the qc-scalar
curvature of this sub-Riemannian limit is not constant.
",Mathematics,Physics
"Stability criteria for the 2D $α$-Euler equations   We derive analogues of the classical Rayleigh, Fjortoft and Arnold stability
and instability theorems in the context of the 2D $\alpha$-Euler equations.
",Physics,Mathematics
"Guessing Attacks on Distributed-Storage Systems   The secrecy of a distributed-storage system for passwords is studied. The
encoder, Alice, observes a length-n password and describes it using two hints,
which she stores in different locations. The legitimate receiver, Bob, observes
both hints. In one scenario the requirement is that the expected number of
guesses it takes Bob to guess the password approach one as n tends to infinity,
and in the other that the expected size of the shortest list that Bob must form
to guarantee that it contain the password approach one. The eavesdropper, Eve,
sees only one of the hints. Assuming that Alice cannot control which hints Eve
observes, the largest normalized (by n) exponent that can be guaranteed for the
expected number of guesses it takes Eve to guess the password is characterized
for each scenario. Key to the proof are new results on Arikan's guessing and
Bunte and Lapidoth's task-encoding problem; in particular, the paper
establishes a close relation between the two problems. A rate-distortion
version of the model is also discussed, as is a generalization that allows for
Alice to produce {\delta} (not necessarily two) hints, for Bob to observe {\nu}
(not necessarily two) of the hints, and for Eve to observe {\eta} (not
necessarily one) of the hints. The generalized model is robust against {\delta}
- {\nu} disk failures.
",Computer Science; Mathematics,Computer Science
"Regrasping by Fixtureless Fixturing   This paper presents a fixturing strategy for regrasping that does not require
a physical fixture. To regrasp an object in a gripper, a robot pushes the
object against external contact/s in the environment such that the external
contact keeps the object stationary while the fingers slide over the object. We
call this manipulation technique fixtureless fixturing. Exploiting the
mechanics of pushing, we characterize a convex polyhedral set of pushes that
results in fixtureless fixturing. These pushes are robust against uncertainty
in the object inertia, grasping force, and the friction at the contacts. We
propose a sampling-based planner that uses the sets of robust pushes to rapidly
build a tree of reachable grasps. A path in this tree is a pushing strategy,
possibly involving pushes from different sides, to regrasp the object. We
demonstrate the experimental validity and robustness of the proposed
manipulation technique with different regrasp examples on a manipulation
platform. Such a fast and flexible regrasp planner facilitates versatile and
flexible automation solutions.
",Computer Science,Computer Science
"The infrared to X-ray correlation spectra of unobscured type 1 active galactic nuclei   We use new X-ray data obtained with the Nuclear Spectroscopic Telescope Array
(NuSTAR), near-infrared (NIR) fluxes, and mid-infrared (MIR) spectra of a
sample of 24 unobscured type 1 active galactic nuclei (AGN) to study the
correlation between various hard X-ray bands between 3 and 80 keV and the
infrared (IR) emission. The IR to X-ray correlation spectrum (IRXCS) shows a
maximum at ~15-20 micron, coincident with the peak of the AGN contribution to
the MIR spectra of the majority of the sample. There is also a NIR correlation
peak at ~2 micron, which we associate with the NIR bump observed in some type 1
AGN at ~1-5 micron and is likely produced by nuclear hot dust emission. The
IRXCS shows practically the same behaviour in all the X-ray bands considered,
indicating a common origin for all of them. We finally evaluated correlations
between the X-ray luminosities and various MIR emission lines. All the lines
show a good correlation with the hard X-rays (rho>0.7), but we do not find the
expected correlation between their ionization potentials and the strength of
the IRXCS.
",Physics,Physics
"Probing the accretion disc structure by the twin kHz QPOs and spins of neutron stars in LMXBs   We analyze the relation between the emission radii of twin kilohertz
quasi-periodic oscillations (kHz QPOs) and the co-rotation radii of the 12
neutron star low mass X-ray binaries (NS-LMXBs) which are simultaneously
detected with the twin kHz QPOs and NS spins. We find that the average
co-rotation radius of these sources is r_co about 32 km, and all the emission
positions of twin kHz QPOs lie inside the corotation radii, indicating that the
twin kHz QPOs are formed in the spin-up process. It is noticed that the upper
frequency of twin kHz QPOs is higher than NS spin frequency by > 10%, which may
account for a critical velocity difference between the Keplerian motion of
accretion matter and NS spin that is corresponding to the production of twin
kHz QPOs. In addition, we also find that about 83% of twin kHz QPOs cluster
around the radius range of 15-20 km, which may be affected by the hard surface
or the local strong magnetic field of NS. As a special case, SAX J1808.4-3658
shows the larger emission radii of twin kHz QPOs of r about 21-24 km, which may
be due to its low accretion rate or small measured NS mass (< 1.4 solar mass).
",Physics,Physics
"On the n-th row of the graded Betti table of an n-dimensional toric variety   We prove an explicit formula for the first non-zero entry in the n-th row of
the graded Betti table of an n-dimensional projective toric variety associated
to a normal polytope with at least one interior lattice point. This applies to
Veronese embeddings of projective space where we prove a special case of a
conjecture of Ein and Lazarsfeld. We also prove an explicit formula for the
entire n-th row when the interior of the polytope is one-dimensional. All
results are valid over an arbitrary field k.
",Mathematics,Mathematics
"Privacy-Aware Guessing Efficiency   We investigate the problem of guessing a discrete random variable $Y$ under a
privacy constraint dictated by another correlated discrete random variable $X$,
where both guessing efficiency and privacy are assessed in terms of the
probability of correct guessing. We define $h(P_{XY}, \epsilon)$ as the maximum
probability of correctly guessing $Y$ given an auxiliary random variable $Z$,
where the maximization is taken over all $P_{Z|Y}$ ensuring that the
probability of correctly guessing $X$ given $Z$ does not exceed $\epsilon$. We
show that the map $\epsilon\mapsto h(P_{XY}, \epsilon)$ is strictly increasing,
concave, and piecewise linear, which allows us to derive a closed form
expression for $h(P_{XY}, \epsilon)$ when $X$ and $Y$ are connected via a
binary-input binary-output channel. For $(X^n, Y^n)$ being pairs of independent
and identically distributed binary random vectors, we similarly define
$\underline{h}_n(P_{X^nY^n}, \epsilon)$ under the assumption that $Z^n$ is also
a binary vector. Then we obtain a closed form expression for
$\underline{h}_n(P_{X^nY^n}, \epsilon)$ for sufficiently large, but nontrivial
values of $\epsilon$.
",Computer Science; Mathematics; Statistics,Computer Science
"Calculation of Effective Interaction Potential During Positron Channeling in Ionic Crystals   An analytical expression is received for the effective interaction potential
of a fast charged particle with the ionic crystal CsCl near the direction of
<100> axis as a function of the temperature of the medium. By numerical
analysis it is shown that the effective potential of axial channeling of
positrons along the axis <100> of negatively charged ions practically does not
depend on temperature of the media
",Physics,Physics
"End-to-end Recurrent Neural Network Models for Vietnamese Named Entity Recognition: Word-level vs. Character-level   This paper demonstrates end-to-end neural network architectures for
Vietnamese named entity recognition. Our best model is a combination of
bidirectional Long Short-Term Memory (Bi-LSTM), Convolutional Neural Network
(CNN), Conditional Random Field (CRF), using pre-trained word embeddings as
input, which achieves an F1 score of 88.59% on a standard test set. Our system
is able to achieve a comparable performance to the first-rank system of the
VLSP campaign without using any syntactic or hand-crafted features. We also
give an extensive empirical study on using common deep learning models for
Vietnamese NER, at both word and character level.
",Computer Science,Computer Science
"MSO+nabla is undecidable   This paper is about an extension of monadic second-order logic over infinite
trees, which adds a quantifier that says ""the set of branches \pi which satisfy
a formula \phi(\pi) has probability one"". This logic was introduced by
Michalewski and Mio; we call it MSO+nabla following Shelah and Lehmann. The
logic MSO+nabla subsumes many qualitative probabilistic formalisms, including
qualitative probabilistic CTL, probabilistic LTL, or parity tree automata with
probabilistic acceptance conditions. We consider the decision problem: decide
if a sentence of MSO+nabla is true in the infinite binary tree? For sentences
from the weak variant of this logic (set quantifiers range only over finite
sets) the problem was known to be decidable, but the question for the full
logic remained open. In this paper we show that the problem for the full logic
MSO+nabla is undecidable.
",Computer Science,Mathematics
"Strongly Hierarchical Factorization Machines and ANOVA Kernel Regression   High-order parametric models that include terms for feature interactions are
applied to various data mining tasks, where ground truth depends on
interactions of features. However, with sparse data, the high- dimensional
parameters for feature interactions often face three issues: expensive
computation, difficulty in parameter estimation and lack of structure. Previous
work has proposed approaches which can partially re- solve the three issues. In
particular, models with factorized parameters (e.g. Factorization Machines) and
sparse learning algorithms (e.g. FTRL-Proximal) can tackle the first two issues
but fail to address the third. Regarding to unstructured parameters,
constraints or complicated regularization terms are applied such that
hierarchical structures can be imposed. However, these methods make the
optimization problem more challenging. In this work, we propose Strongly
Hierarchical Factorization Machines and ANOVA kernel regression where all the
three issues can be addressed without making the optimization problem more
difficult. Experimental results show the proposed models significantly
outperform the state-of-the-art in two data mining tasks: cold-start user
response time prediction and stock volatility prediction.
",Computer Science,Statistics
"Quantification of market efficiency based on informational-entropy   Since the 1960s, the question whether markets are efficient or not is
controversially discussed. One reason for the difficulty to overcome the
controversy is the lack of a universal, but also precise, quantitative
definition of efficiency that is able to graduate between different states of
efficiency. The main purpose of this article is to fill this gap by developing
a measure for the efficiency of markets that fulfill all the stated
requirements. It is shown that the new definition of efficiency, based on
informational-entropy, is equivalent to the two most used definitions of
efficiency from Fama and Jensen. The new measure therefore enables steps to
settle the dispute over the state of efficiency in markets. Moreover, it is
shown that inefficiency in a market can either arise from the possibility to
use information to predict an event with higher than chance level, or can
emerge from wrong pricing/ quotes that do not reflect the right probabilities
of possible events. Finally, the calculation of efficiency is demonstrated on a
simple game (of coin tossing), to show how one could exactly quantify the
efficiency in any market-like system, if all probabilities are known.
",Quantitative Finance,Quantitative Finance
"Towards parallelizable sampling-based Nonlinear Model Predictive Control   This paper proposes a new sampling-based nonlinear model predictive control
(MPC) algorithm, with a bound on complexity quadratic in the prediction horizon
N and linear in the number of samples. The idea of the proposed algorithm is to
use the sequence of predicted inputs from the previous time step as a warm
start, and to iteratively update this sequence by changing its elements one by
one, starting from the last predicted input and ending with the first predicted
input. This strategy, which resembles the dynamic programming principle, allows
for parallelization up to a certain level and yields a suboptimal nonlinear MPC
algorithm with guaranteed recursive feasibility, stability and improved cost
function at every iteration, which is suitable for real-time implementation.
The complexity of the algorithm per each time step in the prediction horizon
depends only on the horizon, the number of samples and parallel threads, and it
is independent of the measured system state. Comparisons with the fmincon
nonlinear optimization solver on benchmark examples indicate that as the
simulation time progresses, the proposed algorithm converges rapidly to the
""optimal"" solution, even when using a small number of samples.
",Computer Science,Computer Science; Statistics
"Error Analysis of the Stochastic Linear Feedback Particle Filter   This paper is concerned with the convergence and long-term stability analysis
of the feedback particle filter (FPF) algorithm. The FPF is an interacting
system of $N$ particles where the interaction is designed such that the
empirical distribution of the particles approximates the posterior
distribution. It is known that in the mean-field limit ($N=\infty$), the
distribution of the particles is equal to the posterior distribution. However
little is known about the convergence to the mean-field limit. In this paper,
we consider the FPF algorithm for the linear Gaussian setting. In this setting,
the algorithm is similar to the ensemble Kalman-Bucy filter algorithm. Although
these algorithms have been numerically evaluated and widely used in
applications, their convergence and long-term stability analysis remains an
active area of research. In this paper, we show that, (i) the mean-field limit
is well-defined with a unique strong solution; (ii) the mean-field process is
stable with respect to the initial condition; (iii) we provide conditions such
that the finite-$N$ system is long term stable and we obtain some mean-squared
error estimates that are uniform in time.
",Computer Science,Computer Science; Mathematics
"Learning Rates for Kernel-Based Expectile Regression   Conditional expectiles are becoming an increasingly important tool in finance
as well as in other areas of applications. We analyse a support vector machine
type approach for estimating conditional expectiles and establish learning
rates that are minimax optimal modulo a logarithmic factor if Gaussian RBF
kernels are used and the desired expectile is smooth in a Besov sense. As a
special case, our learning rates improve the best known rates for kernel-based
least squares regression in this scenario. Key ingredients of our statistical
analysis are a general calibration inequality for the asymmetric least squares
loss, a corresponding variance bound as well as an improved entropy number
bound for Gaussian RBF kernels.
",Statistics,Statistics
"Blind Source Separation Using Mixtures of Alpha-Stable Distributions   We propose a new blind source separation algorithm based on mixtures of
alpha-stable distributions. Complex symmetric alpha-stable distributions have
been recently showed to better model audio signals in the time-frequency domain
than classical Gaussian distributions thanks to their larger dynamic range.
However, inference of these models is notoriously hard to perform because their
probability density functions do not have a closed-form expression in general.
Here, we introduce a novel method for estimating mixture of alpha-stable
distributions based on characteristic function matching. We apply this to the
blind estimation of binary masks in individual frequency bands from
multichannel convolutive audio mixes. We show that the proposed method yields
better separation performance than Gaussian-based binary-masking methods.
",Computer Science; Statistics,Computer Science; Statistics
"Semi-Supervised Learning for Detecting Human Trafficking   Human trafficking is one of the most atrocious crimes and among the
challenging problems facing law enforcement which demands attention of global
magnitude. In this study, we leverage textual data from the website ""Backpage""-
used for classified advertisement- to discern potential patterns of human
trafficking activities which manifest online and identify advertisements of
high interest to law enforcement. Due to the lack of ground truth, we rely on a
human analyst from law enforcement, for hand-labeling a small portion of the
crawled data. We extend the existing Laplacian SVM and present S3VM-R, by
adding a regularization term to exploit exogenous information embedded in our
feature space in favor of the task at hand. We train the proposed method using
labeled and unlabeled data and evaluate it on a fraction of the unlabeled data,
herein referred to as unseen data, with our expert's further verification.
Results from comparisons between our method and other semi-supervised and
supervised approaches on the labeled data demonstrate that our learner is
effective in identifying advertisements of high interest to law enforcement
",Computer Science,Computer Science
"Bounds on layer potentials with rough inputs for higher order elliptic equations   In this paper we establish square-function estimates on the double and single
layer potentials with rough inputs for divergence form elliptic operators, of
arbitrary even order 2m, with variable t-independent coefficients in the upper
half-space.
",Mathematics,Mathematics
"Seven Lessons from Manyfield Inflation in Random Potentials   We study inflation in models with many interacting fields subject to randomly
generated scalar potentials. We use methods from non-equilibrium random matrix
theory to construct the potentials and an adaption of the 'transport method' to
evolve the two-point correlators during inflation. This construction allows,
for the first time, for an explicit study of models with up to 100 interacting
fields supporting a period of 'approximately saddle-point' inflation. We
determine the statistical predictions for observables by generating over 30,000
models with 2-100 fields supporting at least 60 efolds of inflation. These
studies lead us to seven lessons: i) Manyfield inflation is not single-field
inflation, ii) The larger the number of fields, the simpler and sharper the
predictions, iii) Planck compatibility is not rare, but future experiments may
rule out this class of models, iv) The smoother the potentials, the sharper the
predictions, v) Hyperparameters can transition from stiff to sloppy, vi)
Despite tachyons, isocurvature can decay, vii) Eigenvalue repulsion drives the
predictions. We conclude that many of the 'generic predictions' of single-field
inflation can be emergent features of complex inflation models.
",Physics,Physics
"Long term availability of raw experimental data in experimental fracture mechanics   Experimental data availability is a cornerstone for reproducibility in
experimental fracture mechanics, which is crucial to the scientific method.
This short communication focuses on the accessibility and long term
availability of raw experimental data. The corresponding authors of the eleven
most cited papers, related to experimental fracture mechanics, for every year
from 2000 up to 2016, were kindly asked about the availability of the raw
experimental data associated with each publication. For the 187 e-mails sent:
22.46% resulted in outdated contact information, 57.75% of the authors did
received our request and did not reply, and 19.79 replied to our request. The
availability of data is generally low with only $11$ available data sets
(5.9%). The authors identified two main issues for the lacking availability of
raw experimental data. First, the ability to retrieve data is strongly attached
to the the possibility to contact the corresponding author. This study suggests
that institutional e-mail addresses are insufficient means for obtaining
experimental data sets. Second, lack of experimental data is also due that
submission and publication does not require to make the raw experimental data
available. The following solutions are proposed: (1) Requirement of unique
identifiers, like ORCID or ResearcherID, to detach the author(s) from their
institutional e-mail address, (2) Provide DOIs, like Zenodo or Dataverse, to
make raw experimental data citable, and (3) grant providing organizations
should ensure that experimental data by public funded projects is available to
the public.
",Statistics,Computer Science; Physics
"The comprehension construction   In this paper we construct an analogue of Lurie's ""unstraightening""
construction that we refer to as the ""comprehension construction"". Its input is
a cocartesian fibration $p \colon E \to B$ between $\infty$-categories together
with a third $\infty$-category $A$. The comprehension construction then defines
a map from the quasi-category of functors from $A$ to $B$ to the large
quasi-category of cocartesian fibrations over $A$ that acts on $f \colon A \to
B$ by forming the pullback of $p$ along $f$. To illustrate the versatility of
this construction, we define the covariant and contravariant Yoneda embeddings
as special cases of the comprehension functor. We then prove that the hom-wise
action of the comprehension functor coincides with an ""external action"" of the
hom-spaces of $B$ on the fibres of $p$ and use this to prove that the Yoneda
embedding is fully faithful, providing an explicit equivalence between a
quasi-category and the homotopy coherent nerve of a Kan-complex enriched
category.
",Mathematics,Mathematics
"Calibrated Filtered Reduced Order Modeling   We propose a calibrated filtered reduced order model (CF-ROM) framework for
the numerical simulation of general nonlinear PDEs that are amenable to reduced
order modeling. The novel CF-ROM framework consists of two steps: (i) In the
first step, we use explicit ROM spatial filtering of the nonlinear PDE to
construct a filtered ROM. This filtered ROM is low-dimensional, but is not
closed (because of the nonlinearity in the given PDE). (ii) In the second step,
we use a calibration procedure to close the filtered ROM, i.e., to model the
interaction between the resolved and unresolved modes. To this end, we use a
linear or quadratic ansatz to model this interaction and close the filtered
ROM. To find the new coefficients in the closed filtered ROM, we solve an
optimization problem that minimizes the difference between the full order model
data and our ansatz. Although we use a fluid dynamics setting to illustrate how
to construct and use the CF-ROM framework, we emphasize that it is built on
general ideas of spatial filtering and optimization and is independent of
(restrictive) phenomenological arguments. Thus, the CF-ROM framework can be
applied to a wide variety of PDEs.
",Physics; Mathematics,Computer Science; Statistics
"A Fourier-Chebyshev Spectral Method for Cavitation Computation in Nonlinear Elasticity   A Fourier-Chebyshev spectral method is proposed in this paper for solving the
cavitation problem in nonlinear elasticity. The interpolation error for the
cavitation solution is analyzed, the elastic energy error estimate for the
discrete cavitation solution is obtained, and the convergence of the method is
proved. An algorithm combined a gradient type method with a damped quasi-Newton
method is applied to solve the discretized nonlinear equilibrium equations.
Numerical experiments show that the Fourier-Chebyshev spectral method is
efficient and capable of producing accurate numerical cavitation solutions.
",Mathematics,Mathematics
"Fabrication tolerant chalcogenide mid-infrared multimode interference coupler design with application for Bracewell nulling interferometry   Understanding exoplanet formation and finding potentially habitable
exoplanets is vital to an enhanced understanding of the universe. The use of
nulling interferometry to strongly attenuate the central starlight provides the
opportunity to see objects closer to the star than ever before. Given that
exoplanets are usually warm, the 4 microns Mid-Infrared region is advantageous
for such observations. The key performance parameters for a nulling
interferometer are the extinction ratio it can attain and how well that is
maintained across the operational bandwidth. Both parameters depend on the
design and fabrication accuracy of the subcomponents and their wavelength
dependence. Via detailed simulation it is shown in this paper that a planar
chalcogenide photonic chip, consisting of three highly fabrication tolerant
multimode interference couplers, can exceed an extinction ratio of 60 dB in
double nulling operation and up to 40 dB for a single nulling operation across
a wavelength window of 3.9 to 4.2 microns. This provides a beam combiner with
sufficient performance, in theory, to image exoplanets.
",Physics,Physics
"A model for Faraday pilot waves over variable topography   Couder and Fort discovered that droplets walking on a vibrating bath possess
certain features previously thought to be exclusive to quantum systems. These
millimetric droplets synchronize with their Faraday wavefield, creating a
macroscopic pilot-wave system. In this paper we exploit the fact that the waves
generated are nearly monochromatic and propose a hydrodynamic model capable of
quantitatively capturing the interaction between bouncing drops and a variable
topography. We show that our reduced model is able to reproduce some important
experiments involving the drop-topography interaction, such as non-specular
reflection and single-slit diffraction.
",Physics,Physics
"Distributed Learning for Cooperative Inference   We study the problem of cooperative inference where a group of agents
interact over a network and seek to estimate a joint parameter that best
explains a set of observations. Agents do not know the network topology or the
observations of other agents. We explore a variational interpretation of the
Bayesian posterior density, and its relation to the stochastic mirror descent
algorithm, to propose a new distributed learning algorithm. We show that, under
appropriate assumptions, the beliefs generated by the proposed algorithm
concentrate around the true parameter exponentially fast. We provide explicit
non-asymptotic bounds for the convergence rate. Moreover, we develop explicit
and computationally efficient algorithms for observation models belonging to
exponential families.
",Computer Science; Mathematics; Statistics,Computer Science; Statistics
"Assortative Mixing Equilibria in Social Network Games   It is known that individuals in social networks tend to exhibit homophily
(a.k.a. assortative mixing) in their social ties, which implies that they
prefer bonding with others of their own kind. But what are the reasons for this
phenomenon? Is it that such relations are more convenient and easier to
maintain? Or are there also some more tangible benefits to be gained from this
collective behaviour?
The current work takes a game-theoretic perspective on this phenomenon, and
studies the conditions under which different assortative mixing strategies lead
to equilibrium in an evolving social network. We focus on a biased preferential
attachment model where the strategy of each group (e.g., political or social
minority) determines the level of bias of its members toward other group
members and non-members. Our first result is that if the utility function that
the group attempts to maximize is the degree centrality of the group,
interpreted as the sum of degrees of the group members in the network, then the
only strategy achieving Nash equilibrium is a perfect homophily, which implies
that cooperation with other groups is harmful to this utility function. A
second, and perhaps more surprising, result is that if a reward for inter-group
cooperation is added to the utility function (e.g., externally enforced by an
authority as a regulation), then there are only two possible equilibria,
namely, perfect homophily or perfect heterophily, and it is possible to
characterize their feasibility spaces. Interestingly, these results hold
regardless of the minority-majority ratio in the population.
We believe that these results, as well as the game-theoretic perspective
presented herein, may contribute to a better understanding of the forces that
shape the groups and communities of our society.
",Computer Science; Physics,Computer Science
"Rethinking Reprojection: Closing the Loop for Pose-aware ShapeReconstruction from a Single Image   An emerging problem in computer vision is the reconstruction of 3D shape and
pose of an object from a single image. Hitherto, the problem has been addressed
through the application of canonical deep learning methods to regress from the
image directly to the 3D shape and pose labels. These approaches, however, are
problematic from two perspectives. First, they are minimizing the error between
3D shapes and pose labels - with little thought about the nature of this label
error when reprojecting the shape back onto the image. Second, they rely on the
onerous and ill-posed task of hand labeling natural images with respect to 3D
shape and pose. In this paper we define the new task of pose-aware shape
reconstruction from a single image, and we advocate that cheaper 2D annotations
of objects silhouettes in natural images can be utilized. We design
architectures of pose-aware shape reconstruction which re-project the predicted
shape back on to the image using the predicted pose. Our evaluation on several
object categories demonstrates the superiority of our method for predicting
pose-aware 3D shapes from natural images.
",Computer Science,Computer Science
"Proceedings of the IJCAI 2017 Workshop on Learning in the Presence of Class Imbalance and Concept Drift (LPCICD'17)   With the wide application of machine learning algorithms to the real world,
class imbalance and concept drift have become crucial learning issues. Class
imbalance happens when the data categories are not equally represented, i.e.,
at least one category is minority compared to other categories. It can cause
learning bias towards the majority class and poor generalization. Concept drift
is a change in the underlying distribution of the problem, and is a significant
issue specially when learning from data streams. It requires learners to be
adaptive to dynamic changes.
Class imbalance and concept drift can significantly hinder predictive
performance, and the problem becomes particularly challenging when they occur
simultaneously. This challenge arises from the fact that one problem can affect
the treatment of the other. For example, drift detection algorithms based on
the traditional classification error may be sensitive to the imbalanced degree
and become less effective; and class imbalance techniques need to be adaptive
to changing imbalance rates, otherwise the class receiving the preferential
treatment may not be the correct minority class at the current moment.
Therefore, the mutual effect of class imbalance and concept drift should be
considered during algorithm design.
The aim of this workshop is to bring together researchers from the areas of
class imbalance learning and concept drift in order to encourage discussions
and new collaborations on solving the combined issue of class imbalance and
concept drift. It provides a forum for international researchers and
practitioners to share and discuss their original work on addressing new
challenges and research issues in class imbalance learning, concept drift, and
the combined issues of class imbalance and concept drift. The proceedings
include 8 papers on these topics.
",Computer Science,Computer Science; Statistics
"WebPol: Fine-grained Information Flow Policies for Web Browsers   In the standard web browser programming model, third-party scripts included
in an application execute with the same privilege as the application's own
code. This leaves the application's confidential data vulnerable to theft and
leakage by malicious code and inadvertent bugs in the third-party scripts.
Security mechanisms in modern browsers (the same-origin policy, cross-origin
resource sharing and content security policies) are too coarse to suit this
programming model. All these mechanisms (and their extensions) describe whether
or not a script can access certain data, whereas the meaningful requirement is
to allow untrusted scripts access to confidential data that they need and to
prevent the scripts from leaking data on the side. Motivated by this gap, we
propose WebPol, a policy mechanism that allows a website developer to include
fine-grained policies on confidential application data in the familiar syntax
of the JavaScript programming language. The policies can be associated with any
webpage element, and specify what aspects of the element can be accessed by
which third-party domains. A script can access data that the policy allows it
to, but it cannot pass the data (or data derived from it) to other scripts or
remote hosts in contravention of the policy. To specify the policies, we expose
a small set of new native APIs in JavaScript. Our policies can be enforced
using any of the numerous existing proposals for information flow tracking in
web browsers. We have integrated our policies into one such proposal that we
use to evaluate performance overheads and to test our examples.
",Computer Science,Computer Science
"Precision matrix expansion - efficient use of numerical simulations in estimating errors on cosmological parameters   Computing the inverse covariance matrix (or precision matrix) of large data
vectors is crucial in weak lensing (and multi-probe) analyses of the large
scale structure of the universe. Analytically computed covariances are
noise-free and hence straightforward to invert, however the model
approximations might be insufficient for the statistical precision of future
cosmological data. Estimating covariances from numerical simulations improves
on these approximations, but the sample covariance estimator is inherently
noisy, which introduces uncertainties in the error bars on cosmological
parameters and also additional scatter in their best fit values. For future
surveys, reducing both effects to an acceptable level requires an unfeasibly
large number of simulations.
In this paper we describe a way to expand the true precision matrix around a
covariance model and show how to estimate the leading order terms of this
expansion from simulations. This is especially powerful if the covariance
matrix is the sum of two contributions, $\smash{\mathbf{C} =
\mathbf{A}+\mathbf{B}}$, where $\smash{\mathbf{A}}$ is well understood
analytically and can be turned off in simulations (e.g. shape-noise for cosmic
shear) to yield a direct estimate of $\smash{\mathbf{B}}$. We test our method
in mock experiments resembling tomographic weak lensing data vectors from the
Dark Energy Survey (DES) and the Large Synoptic Survey Telecope (LSST). For DES
we find that $400$ N-body simulations are sufficient to achive negligible
statistical uncertainties on parameter constraints. For LSST this is achieved
with $2400$ simulations. The standard covariance estimator would require
>$10^5$ simulations to reach a similar precision. We extend our analysis to a
DES multi-probe case finding a similar performance.
",Physics,Physics
"Perception-based energy functions in seam-cutting   Image stitching is challenging in consumer-level photography, due to
alignment difficulties in unconstrained shooting environment. Recent studies
show that seam-cutting approaches can effectively relieve artifacts generated
by local misalignment. Normally, seam-cutting is described in terms of energy
minimization, however, few of existing methods consider human perception in
their energy functions, which sometimes causes that a seam with minimum energy
is not most invisible in the overlapping region. In this paper, we propose a
novel perception-based energy function in the seam-cutting framework, which
considers the nonlinearity and the nonuniformity of human perception in energy
minimization. Our perception-based approach adopts a sigmoid metric to
characterize the perception of color discrimination, and a saliency weight to
simulate that human eyes incline to pay more attention to salient objects. In
addition, our seam-cutting composition can be easily implemented into other
stitching pipelines. Experiments show that our method outperforms the
seam-cutting method of the normal energy function, and a user study
demonstrates that our composed results are more consistent with human
perception.
",Computer Science,Computer Science
"Interpretations of family size distributions: The Datura example   Young asteroid families are unique sources of information about fragmentation
physics and the structure of their parent bodies, since their physical
properties have not changed much since their birth. Families have different
properties such as age, size, taxonomy, collision severity and others, and
understanding the effect of those properties on our observations of the
size-frequency distribution (SFD) of family fragments can give us important
insights into the hypervelocity collision processes at scales we cannot achieve
in our laboratories. Here we take as an example the very young Datura family,
with a small 8-km parent body, and compare its size distribution to other
families, with both large and small parent bodies, and created by both
catastrophic and cratering formation events. We conclude that most likely
explanation for the shallower size distribution compared to larger families is
a more pronounced observational bias because of its small size. Its size
distribution is perfectly normal when its parent body size is taken into
account. We also discuss some other possibilities. In addition, we study
another common feature: an offset or ""bump"" in the distribution occurring for a
few of the larger elements. We hypothesize that it can be explained by a newly
described regime of cratering, ""spall cratering"", which controls the majority
of impact craters on the surface of small asteroids like Datura.
",Physics,Physics
"Semi-classical limit of the Levy-Lieb functional in Density Functional Theory   In a recent work, Bindini and De Pascale have introduced a regularization of
$N$-particle symmetric probabilities which preserves their one-particle
marginals. In this short note, we extend their construction to mixed quantum
fermionic states. This enables us to prove the convergence of the Levy-Lieb
functional in Density Functional Theory , to the corresponding multi-marginal
optimal transport in the semi-classical limit. Our result holds for mixed
states of any particle number $N$, with or without spin.
",Physics; Mathematics,Physics
"On the accuracy and usefulness of analytic energy models for contemporary multicore processors   This paper presents refinements to the execution-cache-memory performance
model and a previously published power model for multicore processors. The
combination of both enables a very accurate prediction of performance and
energy consumption of contemporary multicore processors as a function of
relevant parameters such as number of active cores as well as core and Uncore
frequencies. Model validation is performed on the Sandy Bridge-EP and
Broadwell-EP microarchitectures. Production-related variations in chip quality
are demonstrated through a statistical analysis of the fit parameters obtained
on one hundred Broadwell-EP CPUs of the same model. Insights from the models
are used to explain the performance- and energy-related behavior of the
processors for scalable as well as saturating (i.e., memory-bound) codes. In
the process we demonstrate the models' capability to identify optimal operating
points with respect to highest performance, lowest energy-to-solution, and
lowest energy-delay product and identify a set of best practices for
energy-efficient execution.
",Computer Science,Computer Science
"Nonlocal Nonlinear Schrödinger Equations and Their Soliton Solutions   We study standard and nonlocal nonlinear Schrödinger (NLS) equations
obtained from the coupled NLS system of equations (Ablowitz-Kaup-Newell-Segur
(AKNS) equations) by using standard and nonlocal reductions respectively. By
using the Hirota bilinear method we first find soliton solutions of the coupled
NLS system of equations then using the reduction formulas we find the soliton
solutions of the standard and nonlocal NLS equations. We give examples for
particular values of the parameters and plot the function $|q(t,x)|^2$ for the
standard and nonlocal NLS equations.
",Physics,Mathematics
"Modified mean curvature flow of entire locally Lipschitz radial graphs in hyperbolic space   In a previous joint work of Xiao and the second author, the modified mean
curvature flow (MMCF) in hyperbolic space $\mathbb{H}^{n+1}$: $$\frac{\partial
\mathbf{F}}{\partial t} = (H-\sigma)\,\vnu\,,\quad \quad \sigma\in (-n,n)$$ was
first introduced and the flow starting from an entire Lipschitz continuous
radial graph with uniform local ball condition on the asymptotic boundary was
shown to exist for all time and converge to a complete hypersurface of constant
mean curvature with prescribed asymptotic boundary at infinity. In this paper,
we remove the uniform local ball condition on the asymptotic boundary of the
initial hypersurface, and prove that the MMCF starting from an entire locally
Lipschitz continuous radial graph exists and stays radially graphic for all
time.
",Mathematics,Mathematics
"Learning Neural Models for End-to-End Clustering   We propose a novel end-to-end neural network architecture that, once trained,
directly outputs a probabilistic clustering of a batch of input examples in one
pass. It estimates a distribution over the number of clusters $k$, and for each
$1 \leq k \leq k_\mathrm{max}$, a distribution over the individual cluster
assignment for each data point. The network is trained in advance in a
supervised fashion on separate data to learn grouping by any perceptual
similarity criterion based on pairwise labels (same/different group). It can
then be applied to different data containing different groups. We demonstrate
promising performance on high-dimensional data like images (COIL-100) and
speech (TIMIT). We call this ``learning to cluster'' and show its conceptual
difference to deep metric learning, semi-supervise clustering and other related
approaches while having the advantage of performing learnable clustering fully
end-to-end.
",Statistics,Computer Science; Statistics
"Discrete time Pontryagin maximum principle for optimal control problems under state-action-frequency constraints   We establish a Pontryagin maximum principle for discrete time optimal control
problems under the following three types of constraints: a) constraints on the
states pointwise in time, b) constraints on the control actions pointwise in
time, and c) constraints on the frequency spectrum of the optimal control
trajectories. While the first two types of constraints are already included in
the existing versions of the Pontryagin maximum principle, it turns out that
the third type of constraints cannot be recast in any of the standard forms of
the existing results for the original control system. We provide two different
proofs of our Pontryagin maximum principle in this article, and include several
special cases fine-tuned to control-affine nonlinear and linear system models.
In particular, for minimization of quadratic cost functions and linear time
invariant control systems, we provide tight conditions under which the optimal
controls under frequency constraints are either normal or abnormal.
",Computer Science; Mathematics,Mathematics
"The Momentum Distribution of Liquid $^4$He   We report high-resolution neutron Compton scattering measurements of liquid
$^4$He under saturated vapor pressure. There is excellent agreement between the
observed scattering and ab initio predictions of its lineshape. Quantum Monte
Carlo calculations predict that the Bose condensate fraction is zero in the
normal fluid, builds up rapidly just below the superfluid transition
temperature, and reaches a value of approximately $7.5\%$ below 1 K. We also
used model fit functions to obtain from the scattering data empirical estimates
for the average atomic kinetic energy and Bose condensate fraction. These
quantities are also in excellent agreement with ab initio calculations. The
convergence between the scattering data and Quantum Monte Carlo calculations is
strong evidence for a Bose broken symmetry in superfluid $^4$He.
",Physics,Physics
"Semi-equivelar maps on the torus are Archimedean   If the face-cycles at all the vertices in a map on a surface are of same type
then the map is called semi-equivelar. There are eleven types of Archimedean
tilings on the plane. All the Archimedean tilings are semi-equivelar maps. If a
map $X$ on the torus is a quotient of an Archimedean tiling on the plane then
the map $X$ is semi-equivelar. We show that each semi-equivelar map on the
torus is a quotient of an Archimedean tiling on the plane.
Vertex-transitive maps are semi-equivelar maps. We know that four types of
semi-equivelar maps on the torus are always vertex-transitive and there are
examples of other seven types of semi-equivelar maps which are not
vertex-transitive. We show that the number of ${\rm Aut}(Y)$-orbits of vertices
for any semi-equivelar map $Y$ on the torus is at most six. In fact, the number
of orbits is at most three except one type of semi-equivelar maps. Our bounds
on the number of orbits are sharp.
",Mathematics,Mathematics
"Electrically driven quantum light emission in electromechanically-tuneable photonic crystal cavities   A single quantum dot deterministically coupled to a photonic crystal
environment constitutes an indispensable elementary unit to both generate and
manipulate single-photons in next-generation quantum photonic circuits. To
date, the scaling of the number of these quantum nodes on a fully-integrated
chip has been prevented by the use of optical pumping strategies that require a
bulky off-chip laser along with the lack of methods to control the energies of
nano-cavities and emitters. Here, we concurrently overcome these limitations by
demonstrating electrical injection of single excitonic lines within a
nano-electro-mechanically tuneable photonic crystal cavity. When an
electrically-driven dot line is brought into resonance with a photonic crystal
mode, its emission rate is enhanced. Anti-bunching experiments reveal the
quantum nature of these on-demand sources emitting in the telecom range. These
results represent an important step forward in the realization of integrated
quantum optics experiments featuring multiple electrically-triggered
Purcell-enhanced single-photon sources embedded in a reconfigurable
semiconductor architecture.
",Physics,Physics
"Analisis of the power flow in Low Voltage DC grids   Power flow in a low voltage direct current grid (LVDC) is a non-linear
problem just as its counterpart ac. This paper demonstrates that, unlike in ac
grids, convergence and uniqueness of the solution can be guaranteed in this
type of grids. The result is not a linearization nor an approximation, but an
analysis of the set of non-linear algebraic equations, which is valid for any
LVDC grid regardless its size, topology or load condition. Computer simulation
corroborate the theoretical analysis.
",Mathematics,Mathematics
"Clustering Patients with Tensor Decomposition   In this paper we present a method for the unsupervised clustering of
high-dimensional binary data, with a special focus on electronic healthcare
records. We present a robust and efficient heuristic to face this problem using
tensor decomposition. We present the reasons why this approach is preferable
for tasks such as clustering patient records, to more commonly used
distance-based methods. We run the algorithm on two datasets of healthcare
records, obtaining clinically meaningful results.
",Computer Science; Statistics,Computer Science; Statistics
"On well-posedness of a velocity-vorticity formulation of the Navier-Stokes equations with no-slip boundary conditions   We study well-posedness of a velocity-vorticity formulation of the
Navier--Stokes equations, supplemented with no-slip velocity boundary
conditions, a no-penetration vorticity boundary condition, along with a natural
vorticity boundary condition depending on a pressure functional. In the
stationary case we prove existence and uniqueness of a suitable weak solution
to the system under a small data condition. The topic of the paper is driven by
recent developments of vorticity based numerical methods for the Navier--Stokes
equations.
",Mathematics,Mathematics
"Millisecond Pulsars as Standards: Timing, positioning and communication   Millisecond pulsars (MSPs) have a great potential to set standards in
timekeeping, positioning and metadata communication.
",Physics,Computer Science
"Learning to Price with Reference Effects   As a firm varies the price of a product, consumers exhibit reference effects,
making purchase decisions based not only on the prevailing price but also the
product's price history. We consider the problem of learning such behavioral
patterns as a monopolist releases, markets, and prices products. This context
calls for pricing decisions that intelligently trade off between maximizing
revenue generated by a current product and probing to gain information for
future benefit. Due to dependence on price history, realized demand can reflect
delayed consequences of earlier pricing decisions. As such, inference entails
attribution of outcomes to prior decisions and effective exploration requires
planning price sequences that yield informative future outcomes. Despite the
considerable complexity of this problem, we offer a tractable systematic
approach. In particular, we frame the problem as one of reinforcement learning
and leverage Thompson sampling. We also establish a regret bound that provides
graceful guarantees on how performance improves as data is gathered and how
this depends on the complexity of the demand model. We illustrate merits of the
approach through simulations.
",Computer Science,Computer Science; Statistics
"DF-SLAM: A Deep-Learning Enhanced Visual SLAM System based on Deep Local Features   As the foundation of driverless vehicle and intelligent robots, Simultaneous
Localization and Mapping(SLAM) has attracted much attention these days.
However, non-geometric modules of traditional SLAM algorithms are limited by
data association tasks and have become a bottleneck preventing the development
of SLAM. To deal with such problems, many researchers seek to Deep Learning for
help. But most of these studies are limited to virtual datasets or specific
environments, and even sacrifice efficiency for accuracy. Thus, they are not
practical enough.
We propose DF-SLAM system that uses deep local feature descriptors obtained
by the neural network as a substitute for traditional hand-made features.
Experimental results demonstrate its improvements in efficiency and stability.
DF-SLAM outperforms popular traditional SLAM systems in various scenes,
including challenging scenes with intense illumination changes. Its versatility
and mobility fit well into the need for exploring new environments. Since we
adopt a shallow network to extract local descriptors and remain others the same
as original SLAM systems, our DF-SLAM can still run in real-time on GPU.
",Computer Science,Computer Science
"Highly Nonlinear and Low Confinement Loss Photonic Crystal Fiber Using GaP Slot Core   This paper presents a triangular lattice photonic crystal fiber with very
high nonlinear coefficient. Finite element method (FEM) is used to scrutinize
different optical properties of proposed highly nonlinear photonic crystal
fiber (HNL-PCF). The HNL-PCF exhibits a high nonlinearity up to $10\times10^{4}
W^{-1}km^{-1}$ over the wavelength of 1500 nm to 1700 nm. Moreover, proposed
HNL-PCF shows a very low confinement loss of $10^{-3} dB/km$ at 1550 nm
wavelength. Furthermore, chromatic dispersion, dispersion slope, effective area
etc. are also analyzed thoroughly. The proposed fiber will be a suitable
candidate for broadband dispersion compensation, sensor devices and
supercontinuum generation.
",Physics,Physics
"Effect of Isopropanol on Gold Assisted Chemical Etching of Silicon Microstructures   Wet etching is an essential and complex step in semiconductor device
processing. Metal-Assisted Chemical Etching (MacEtch) is fundamentally a wet
but anisotropic etching method. In the MacEtch technique, there are still a
number of unresolved challenges preventing the optimal fabrication of
high-aspect-ratio semiconductor micro- and nanostructures, such as undesired
etching, uncontrolled catalyst movement, non-uniformity and micro-porosity in
the metal-free areas. Here, an optimized MacEtch process using with a
nanostructured Au catalyst is proposed for fabrication of Si high aspect ratio
microstructures. The addition of isopropanol as surfactant in the HF-H2O2 water
solution improves the uniformity and the control of the H2 gas release. An
additional KOH etching removes eventually the unwanted nanowires left by the
MacEtch through the nanoporous catalyst film. We demonstrate the benefits of
the isopropanol addition for reducing the etching rate and the nanoporosity of
etched structures with a monothonical decrease as a function of the isopropanol
concentration.
",Physics,Physics
"Road to safe autonomy with data and formal reasoning   We present an overview of recently developed data-driven tools for safety
analysis of autonomous vehicles and advanced driver assist systems. The core
algorithms combine model-based, hybrid system reachability analysis with
sensitivity analysis of components with unknown or inaccessible models. We
illustrate the applicability of this approach with a new case study of
emergency braking systems in scenarios with two or three vehicles. This problem
is representative of the most common type of rear-end crashes, which is
relevant for safety analysis of automatic emergency braking (AEB) and forward
collision avoidance systems. We show that our verification tool can effectively
prove the safety of certain scenarios (specified by several parameters like
braking profiles, initial velocities, uncertainties in position and reaction
times), and also compute the severity of accidents for unsafe scenarios.
Through hundreds of verification experiments, we quantified the safety envelope
of the system across relevant parameters. These results show that the approach
is promising for design, debugging and certification. We also show how the
reachability analysis can be combined with statistical information about the
parameters, to assess the risk level of the control system, which in turn is
essential, for example, for determining Automotive Safety Integrity Levels
(ASIL) for the ISO26262 standard.
",Computer Science,Computer Science
"Risk ratios for contagious outcomes   The risk ratio is a popular tool for summarizing the relationship between a
binary covariate and outcome, even when outcomes may be dependent.
Investigations of infectious disease outcomes in cohort studies of individuals
embedded within clusters -- households, villages, or small groups -- often
report risk ratios. Epidemiologists have warned that risk ratios may be
misleading when outcomes are contagious, but the nature and severity of this
error is not well understood. In this study, we assess the epidemiologic
meaning of the risk ratio when outcomes are contagious. We first give a
structural definition of infectious disease transmission within clusters, based
on the canonical susceptible-infective epidemic model. From this standard
characterization, we define the individual-level ratio of instantaneous risks
(hazard ratio) as the inferential target, and evaluate the properties of the
risk ratio as an estimate of this quantity. We exhibit analytically and by
simulation the circumstances under which the risk ratio implies an effect whose
direction is opposite that of the true individual-level hazard ratio. In
particular, the risk ratio can be greater than one even when the covariate of
interest reduces both individual-level susceptibility to infection, and
transmissibility once infected. We explain these findings in the epidemiologic
language of confounding and relate the direction bias to Simpson's paradox.
",Statistics,Quantitative Biology
"Inductive Representation Learning on Large Graphs   Low-dimensional embeddings of nodes in large graphs have proved extremely
useful in a variety of prediction tasks, from content recommendation to
identifying protein functions. However, most existing approaches require that
all nodes in the graph are present during training of the embeddings; these
previous approaches are inherently transductive and do not naturally generalize
to unseen nodes. Here we present GraphSAGE, a general, inductive framework that
leverages node feature information (e.g., text attributes) to efficiently
generate node embeddings for previously unseen data. Instead of training
individual embeddings for each node, we learn a function that generates
embeddings by sampling and aggregating features from a node's local
neighborhood. Our algorithm outperforms strong baselines on three inductive
node-classification benchmarks: we classify the category of unseen nodes in
evolving information graphs based on citation and Reddit post data, and we show
that our algorithm generalizes to completely unseen graphs using a multi-graph
dataset of protein-protein interactions.
",Computer Science; Statistics,Computer Science; Statistics
"A Data-Driven Supply-Side Approach for Measuring Cross-Border Internet Purchases   The digital economy is a highly relevant item on the European Union's policy
agenda. Cross-border internet purchases are part of the digital economy, but
their total value can currently not be accurately measured or estimated.
Traditional approaches based on consumer surveys or business surveys are shown
to be inadequate for this purpose, due to language bias and sampling issues,
respectively. We address both problems by proposing a novel approach based on
supply-side data, namely tax returns. The proposed data-driven record-linkage
techniques and machine learning algorithms utilize two additional open data
sources: European business registers and internet data. Our main finding is
that the value of total cross-border internet purchases within the European
Union by Dutch consumers was over EUR 1.3 billion in 2016. This is more than 6
times as high as current estimates. Our finding motivates the implementation of
the proposed methodology in other EU member states. Ultimately, it could lead
to more accurate estimates of cross-border internet purchases within the entire
European Union.
",Statistics,Computer Science
"Discrete Dynamic Causal Modeling and Its Relationship with Directed Information   This paper explores the discrete Dynamic Causal Modeling (DDCM) and its
relationship with Directed Information (DI). We prove the conditional
equivalence between DDCM and DI in characterizing the causal relationship
between two brain regions. The theoretical results are demonstrated using fMRI
data obtained under both resting state and stimulus based state. Our numerical
analysis is consistent with that reported in previous study.
",Statistics,Statistics
"Laplacian Prior Variational Automatic Relevance Determination for Transmission Tomography   In the classic sparsity-driven problems, the fundamental L-1 penalty method
has been shown to have good performance in reconstructing signals for a wide
range of problems. However this performance relies on a good choice of penalty
weight which is often found from empirical experiments. We propose an algorithm
called the Laplacian variational automatic relevance determination (Lap-VARD)
that takes this penalty weight as a parameter of a prior Laplace distribution.
Optimization of this parameter using an automatic relevance determination
framework results in a balance between the sparsity and accuracy of signal
reconstruction. Our algorithm is implemented in a transmission tomography model
with sparsity constraint in wavelet domain.
",Statistics,Computer Science; Statistics
"EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples   Recent studies have highlighted the vulnerability of deep neural networks
(DNNs) to adversarial examples - a visually indistinguishable adversarial image
can easily be crafted to cause a well-trained model to misclassify. Existing
methods for crafting adversarial examples are based on $L_2$ and $L_\infty$
distortion metrics. However, despite the fact that $L_1$ distortion accounts
for the total variation and encourages sparsity in the perturbation, little has
been developed for crafting $L_1$-based adversarial examples. In this paper, we
formulate the process of attacking DNNs via adversarial examples as an
elastic-net regularized optimization problem. Our elastic-net attacks to DNNs
(EAD) feature $L_1$-oriented adversarial examples and include the
state-of-the-art $L_2$ attack as a special case. Experimental results on MNIST,
CIFAR10 and ImageNet show that EAD can yield a distinct set of adversarial
examples with small $L_1$ distortion and attains similar attack performance to
the state-of-the-art methods in different attack scenarios. More importantly,
EAD leads to improved attack transferability and complements adversarial
training for DNNs, suggesting novel insights on leveraging $L_1$ distortion in
adversarial machine learning and security implications of DNNs.
",Computer Science; Statistics,Computer Science; Statistics
"Robust and Flexible Estimation of Stochastic Mediation Effects: A Proposed Method and Example in a Randomized Trial Setting   Causal mediation analysis can improve understanding of the mechanisms
underlying epidemiologic associations. However, the utility of natural direct
and indirect effect estimation has been limited by the assumption of no
confounder of the mediator-outcome relationship that is affected by prior
exposure---an assumption frequently violated in practice. We build on recent
work that identified alternative estimands that do not require this assumption
and propose a flexible and double robust semiparametric targeted minimum
loss-based estimator for data-dependent stochastic direct and indirect effects.
The proposed method treats the intermediate confounder affected by prior
exposure as a time-varying confounder and intervenes stochastically on the
mediator using a distribution which conditions on baseline covariates and
marginalizes over the intermediate confounder. In addition, we assume the
stochastic intervention is given, conditional on observed data, which results
in a simpler estimator and weaker identification assumptions. We demonstrate
the estimator's finite sample and robustness properties in a simple simulation
study. We apply the method to an example from the Moving to Opportunity
experiment. In this application, randomization to receive a housing voucher is
the treatment/instrument that influenced moving to a low-poverty neighborhood,
which is the intermediate confounder. We estimate the data-dependent stochastic
direct effect of randomization to the voucher group on adolescent marijuana use
not mediated by change in school district and the stochastic indirect effect
mediated by change in school district. We find no evidence of mediation. Our
estimator is easy to implement in standard statistical software, and we provide
annotated R code to further lower implementation barriers.
",Statistics,Statistics
"First-principles insights into ultrashort laser spectroscopy of molecular nitrogen   In this research, we employ accurate time-dependent density functional
calculations for ultrashort laser spectroscopy of nitrogen molecule. Laser
pulses with different frequencies, intensities, and durations are applied to
the molecule and the resulting photoelectron spectra are analyzed. It is argued
that relative orientation of the molecule in the laser pulse significantly
influence the orbital character of the emitted photoelectrons. Moreover, the
duration of the laser pulse is also found to be very effective in controlling
the orbital resolution and intensity of photoelectrons. Angular resolved
distribution of photoelectrons are computed at different pulse frequencies and
recording times. By exponential growth of the laser pulse intensity, the
theoretical threshold of two photons absorption in nitrogen molecule is
determined.
",Physics,Physics
"Exploring one particle orbitals in large Many-Body Localized systems   Strong disorder in interacting quantum systems can give rise to the
phenomenon of Many-Body Localization (MBL), which defies thermalization due to
the formation of an extensive number of quasi local integrals of motion. The
one particle operator content of these integrals of motion is related to the
one particle orbitals of the one particle density matrix and shows a strong
signature across the MBL transition as recently pointed out by Bera et al.
[Phys. Rev. Lett. 115, 046603 (2015); Ann. Phys. 529, 1600356 (2017)]. We study
the properties of the one particle orbitals of many-body eigenstates of an MBL
system in one dimension. Using shift-and-invert MPS (SIMPS), a matrix product
state method to target highly excited many-body eigenstates introduced in
[Phys. Rev. Lett. 118, 017201 (2017)], we are able to obtain accurate results
for large systems of sizes up to L = 64. We find that the one particle orbitals
drawn from eigenstates at different energy densities have high overlap and
their occupations are correlated with the energy of the eigenstates. Moreover,
the standard deviation of the inverse participation ratio of these orbitals is
maximal at the nose of the mobility edge. Also, the one particle orbitals decay
exponentially in real space, with a correlation length that increases at low
disorder. In addition, we find a 1/f distribution of the coupling constants of
a certain range of the number operators of the OPOs, which is related to their
exponential decay.
",Physics,Physics
"Learning to Compose Task-Specific Tree Structures   For years, recursive neural networks (RvNNs) have been shown to be suitable
for representing text into fixed-length vectors and achieved good performance
on several natural language processing tasks. However, the main drawback of
RvNNs is that they require structured input, which makes data preparation and
model implementation hard. In this paper, we propose Gumbel Tree-LSTM, a novel
tree-structured long short-term memory architecture that learns how to compose
task-specific tree structures only from plain text data efficiently. Our model
uses Straight-Through Gumbel-Softmax estimator to decide the parent node among
candidates dynamically and to calculate gradients of the discrete decision. We
evaluate the proposed model on natural language inference and sentiment
analysis, and show that our model outperforms or is at least comparable to
previous models. We also find that our model converges significantly faster
than other models.
",Computer Science,Computer Science
"Graphene quantum dots prevent alpha-synucleinopathy in Parkinson's disease   While the emerging evidence indicates that the pathogenesis of Parkinson's
disease (PD) is strongly correlated to the accumulation of alpha-synuclein
({\alpha}-syn) aggregates, there has been no clinical success in
anti-aggregation agents for the disease to date. Here we show that graphene
quantum dots (GQDs) exhibit anti-amyloid activity via direct interaction with
{\alpha}-syn. Employing biophysical, biochemical, and cell-based assays as well
as molecular dynamics (MD) simulation, we find that GQDs have notable potency
in not only inhibiting fibrillization of {\alpha}-syn but also disaggregating
mature fibrils in a time-dependent manner. Remarkably, GQDs rescue neuronal
death and synaptic loss, reduce Lewy body (LB)/Lewy neurite (LN) formation,
ameliorate mitochondrial dysfunctions, and prevent neuron-to-neuron
transmission of {\alpha}-syn pathology induced by {\alpha}-syn preformed
fibrils (PFFs) in neurons. In addition, in vivo administration of GQDs protects
against {\alpha}-syn PFFs-induced loss of dopamine neurons, LB/LN pathology,
and behavioural deficits through the penetration of the blood-brain barrier
(BBB). The finding that GQDs function as an anti-aggregation agent provides a
promising novel therapeutic target for the treatment of PD and related
{\alpha}-synucleinopathies.
",Physics,Quantitative Biology
"Inter-site pair superconductivity: origins and recent validation experiments   The challenge of understanding high-temperature superconductivity has led to
a plethora of ideas, but 30 years after its discovery in cuprates, very few
have achieved convincing experimental validation. While Hubbard and t-J models
were given a lot of attention, a number of recent experiments appear to give
decisive support to the model of real-space inter-site pairing and percolative
superconductivity in cuprates. Systematic measurements of the doping dependence
of the superfluid density show a linear dependence on superfluid density -
rather than doping - over the entire phase diagram, in accordance with the
model's predictions. The doping-dependence of the anomalous lattice dynamics of
in-plane Cu-O mode vibrations observed by inelastic neutron scattering, gives
remarkable reciprocal space signature of the inter-site pairing interaction
whose doping dependence closely follows the predicted pair density.
Symmetry-specific time-domain spectroscopy shows carrier localization, polaron
formation, pairing and superconductivity to be distinct processes occurring on
distinct timescales throughout the entire superconducting phase diagram. The
three diverse experimental results confirm non-trivial predictions made more
than a decade ago by the inter-site pairing model in the cuprates, remarkably
also confirming some of the fundamental notions mentioned in the seminal paper
on the discovery of high-temperature superconductivity in cuprates.
",Physics,Physics
"Cell-to-cell variation sets a tissue-rheology-dependent bound on collective gradient sensing   When a single cell senses a chemical gradient and chemotaxes, stochastic
receptor-ligand binding can be a fundamental limit to the cell's accuracy. For
clusters of cells responding to gradients, however, there is a critical
difference: even genetically identical cells have differing responses to
chemical signals. With theory and simulation, we show collective chemotaxis is
limited by cell-to-cell variation in signaling. We find that when different
cells cooperate the resulting bias can be much larger than the effects of
ligand-receptor binding. Specifically, when a strongly-responding cell is at
one end of a cell cluster, cluster motion is biased toward that cell. These
errors are mitigated if clusters average measurements over times long enough
for cells to rearrange. In consequence, fluid clusters are better able to sense
gradients: we derive a link between cluster accuracy, cell-to-cell variation,
and the cluster rheology. Because of this connection, increasing the noisiness
of individual cell motion can actually increase the collective accuracy of a
cluster by improving fluidity.
",Physics,Quantitative Biology
"Machine Learning Approach to RF Transmitter Identification   With the development and widespread use of wireless devices in recent years
(mobile phones, Internet of Things, Wi-Fi), the electromagnetic spectrum has
become extremely crowded. In order to counter security threats posed by rogue
or unknown transmitters, it is important to identify RF transmitters not by the
data content of the transmissions but based on the intrinsic physical
characteristics of the transmitters. RF waveforms represent a particular
challenge because of the extremely high data rates involved and the potentially
large number of transmitters present in a given location. These factors outline
the need for rapid fingerprinting and identification methods that go beyond the
traditional hand-engineered approaches. In this study, we investigate the use
of machine learning (ML) strategies to the classification and identification
problems, and the use of wavelets to reduce the amount of data required. Four
different ML strategies are evaluated: deep neural nets (DNN), convolutional
neural nets (CNN), support vector machines (SVM), and multi-stage training
(MST) using accelerated Levenberg-Marquardt (A-LM) updates. The A-LM MST method
preconditioned by wavelets was by far the most accurate, achieving 100%
classification accuracy of transmitters, as tested using data originating from
12 different transmitters. We discuss strategies for extension of MST to a much
larger number of transmitters.
",Computer Science; Statistics,Computer Science; Statistics
"Statistics of turbulence in the energy-containing range of Taylor-Couette compared to canonical wall-bounded flows   Considering structure functions of the streamwise velocity component in a
framework akin to the extended self-similarity hypothesis (ESS), de Silva
\textit{et al.} (\textit{J. Fluid Mech.}, vol. 823,2017, pp. 498-510) observed
that remarkably the \textit{large-scale} (energy-containing range) statistics
in canonical wall bounded flows exhibit universal behaviour. In the present
study, we extend this universality, which was seen to encompass also flows at
moderate Reynolds number, to Taylor-Couette flow. In doing so, we find that
also the transversal structure function of the spanwise velocity component
exhibits the same universal behaviour across all flow types considered. We
further demonstrate that these observations are consistent with predictions
developed based on an attached-eddy hypothesis. These considerations also yield
a possible explanation for the efficacy of the ESS framework by showing that it
relaxes the self-similarity assumption for the attached eddy contributions. By
taking the effect of streamwise alignment into account, the attached eddy model
predicts different behaviour for structure functions in the streamwise and in
the spanwise directions and that this effect cancels in the ESS-framework ---
both consistent with the data. Moreover, it is demonstrated here that also the
additive constants, which were previously believed to be flow dependent, are
indeed universal at least in turbulent boundary layers and pipe flow where
high-Reynolds number data are currently available.
",Physics,Physics
"Attention-based Natural Language Person Retrieval   Following the recent progress in image classification and captioning using
deep learning, we develop a novel natural language person retrieval system
based on an attention mechanism. More specifically, given the description of a
person, the goal is to localize the person in an image. To this end, we first
construct a benchmark dataset for natural language person retrieval. To do so,
we generate bounding boxes for persons in a public image dataset from the
segmentation masks, which are then annotated with descriptions and attributes
using the Amazon Mechanical Turk. We then adopt a region proposal network in
Faster R-CNN as a candidate region generator. The cropped images based on the
region proposals as well as the whole images with attention weights are fed
into Convolutional Neural Networks for visual feature extraction, while the
natural language expression and attributes are input to Bidirectional Long
Short- Term Memory (BLSTM) models for text feature extraction. The visual and
text features are integrated to score region proposals, and the one with the
highest score is retrieved as the output of our system. The experimental
results show significant improvement over the state-of-the-art method for
generic object retrieval and this line of research promises to benefit search
in surveillance video footage.
",Computer Science,Computer Science
"When a triangle is isosceles?   In 1840 Jacob Steiner on Christian Rudolf's request proved that a triangle
with two equal bisectors is isosceles. But what about changing the bisectors to
cevians? Cevian is any line segment in a triangle with one endpoint on a vertex
of the triangle and other endpoint on the opposite side. Not for any pairs of
equal cevians the triangle is isosceles. Theorem. If for a triangle ABC there
are equal cevians issuing from A and B, which intersect on the bisector or on
the median of the angle C, then AC=BC (so the triangle ABC is isosceles).
Proposition. Let ABC be an isosceles triangle. Define circle C to be the circle
symmetric relative to AB to the circumscribed circle of the triangle ABC. Then
the locus of intersection points of pairs of equal cevians is the union of the
base AB, the triangle's axis of symmetry, and the circle C.
",Mathematics,Mathematics
"How to model fake news   Over the past three years it has become evident that fake news is a danger to
democracy. However, until now there has been no clear understanding of how to
define fake news, much less how to model it. This paper addresses both these
issues. A definition of fake news is given, and two approaches for the
modelling of fake news and its impact in elections and referendums are
introduced. The first approach, based on the idea of a representative voter, is
shown to be suitable to obtain a qualitative understanding of phenomena
associated with fake news at a macroscopic level. The second approach, based on
the idea of an election microstructure, describes the collective behaviour of
the electorate by modelling the preferences of individual voters. It is shown
through a simulation study that the mere knowledge that pieces of fake news may
be in circulation goes a long way towards mitigating the impact of fake news.
",Computer Science; Quantitative Finance,Computer Science
"Deep Reinforcement Learning for General Video Game AI   The General Video Game AI (GVGAI) competition and its associated software
framework provides a way of benchmarking AI algorithms on a large number of
games written in a domain-specific description language. While the competition
has seen plenty of interest, it has so far focused on online planning,
providing a forward model that allows the use of algorithms such as Monte Carlo
Tree Search.
In this paper, we describe how we interface GVGAI to the OpenAI Gym
environment, a widely used way of connecting agents to reinforcement learning
problems. Using this interface, we characterize how widely used implementations
of several deep reinforcement learning algorithms fare on a number of GVGAI
games. We further analyze the results to provide a first indication of the
relative difficulty of these games relative to each other, and relative to
those in the Arcade Learning Environment under similar conditions.
",Statistics,Computer Science
"Dynamics of domain walls in weak ferromagnets   It is shown that the total set of equations, which determines the dynamics of
the domain bounds (DB) in a weak ferromagnet, has the same type of specific
solution as the well-known Walker's solution for ferromagnets. We calculated
the functional dependence of the velocity of the DB on the magnetic field,
which is described by the obtained solution. This function has a maximum at a
finite field and a section of the negative differential mobility of the DB.
According to the calculation, the maximum velocity $ c \approx 2 \times 10^6$
cm/sec in YFeO$_3$ is reached at $H_m \approx 4 \times 10^3$ Oe.
",Physics,Physics
"In-gap bound states induced by a single nonmagnetic impurity in sign-preserving s-wave superconductors with incipient bands   We have investigated the in-gap bound states (IGBS) induced by a single
nonmagnetic impurity in multiband superconductors with incipient bands.
Contrary to the naive expectation, we found that even if the superconducting
(SC) order parameter is sign-preserving s-wave on the Fermi surfaces, the
incipient bands may still affect the appearance and locations of the IGBS,
although the gap between the incipient bands and the Fermi level is much larger
than the SC gap. Therefore in scanning tunneling microscopy experiments, the
IGBS induced by a single nonmagnetic impurity are not the definitive evidences
for the sign-changing order parameter on the Fermi surfaces. Our findings have
special implications for the experimental determination of the pairing symmetry
in the FeSe-based superconductors.
",Physics,Physics
"On the K-theory of C*-algebras for substitution tilings (a pedestrian version)   Under suitable conditions, a substitution tiling gives rise to a Smale space,
from which three equivalence relations can be constructed, namely the stable,
unstable, and asymptotic equivalence relations. We denote with $S$, $U$, and
$A$ their corresponding $C^*$-algebras in the sense of Renault. In this article
we show that the $K$-theories of $S$ and $U$ can be computed from the
cohomology and homology of a single cochain complex with connecting maps for
tilings of the line and of the plane. Moreover, we provide formulas to compute
the $K$-theory for these three $C^*$-algebras. Furthermore, we show that the
$K$-theory groups for tilings of dimension 1 are always torsion free. For
tilings of dimension 2, only $K_0(U)$ and $K_1(S)$ can contain torsion.
",Mathematics,Mathematics
"Magnetization spin dynamics in a (LuBi)3Fe5O12 (BLIG) epitaxial film   Bismuth substituted lutetium iron garnet (BLIG) films exhibit larger Faraday
rotation, and have a higher Curie temperature than yttrium iron garnet. We have
observed magnetic stripe domains and measured domain widths of 1.4 {\mu}{\mu}m
using Fourier domain polarization microscopy, Faraday rotation experiments
yield a coercive field of 5 Oe. These characterizations form the basis of
micromagnetic simulations that allow us to estimate and compare spin wave
excitations in BLIG films. We observed that these films support thermal magnons
with a precessional frequency of 7 GHz with a line width of 400 MHz. Further,
we studied the dependence of precessional frequency on the externally applied
magnetic field. Brillouin light scattering experiments and precession
frequencies predicted by simulations show similar trend with increasing field.
",Physics,Physics
"Angle-resolved photoemission spectroscopy with quantum gas microscopes   Quantum gas microscopes are a promising tool to study interacting quantum
many-body systems and bridge the gap between theoretical models and real
materials. So far they were limited to measurements of instantaneous
correlation functions of the form $\langle \hat{O}(t) \rangle$, even though
extensions to frequency-resolved response functions $\langle \hat{O}(t)
\hat{O}(0) \rangle$ would provide important information about the elementary
excitations in a many-body system. For example, single particle spectral
functions, which are usually measured using photoemission experiments in
electron systems, contain direct information about fractionalization and the
quasiparticle excitation spectrum. Here, we propose a measurement scheme to
experimentally access the momentum and energy resolved spectral function in a
quantum gas microscope with currently available techniques. As an example for
possible applications, we numerically calculate the spectrum of a single hole
excitation in one-dimensional $t-J$ models with isotropic and anisotropic
antiferromagnetic couplings. A sharp asymmetry in the distribution of spectral
weight appears when a hole is created in an isotropic Heisenberg spin chain.
This effect slowly vanishes for anisotropic spin interactions and disappears
completely in the case of pure Ising interactions. The asymmetry strongly
depends on the total magnetization of the spin chain, which can be tuned in
experiments with quantum gas microscopes. An intuitive picture for the observed
behavior is provided by a slave-fermion mean field theory. The key properties
of the spectra are visible at currently accessible temperatures.
",Physics,Physics
"Role of the orbital degree of freedom in iron-based superconductors   Almost a decade has passed since the serendipitous discovery of the
iron-based high temperature superconductors (FeSCs) in 2008. The question of
how much similarity the FeSCs have with the copper oxide high temperature
superconductors emerged since the initial discovery of long-range
antiferromagnetism in the FeSCs in proximity to superconductivity. Despite the
great resemblance in their phase diagrams, there exist important disparities
between FeSCs and cuprates that need to be considered in order to paint a full
picture of these two families of high temperature superconductors. One of the
key differences lies in the multi-orbital multi-band nature of FeSCs, in
contrast to the effective single-band model for cuprates. Due to the complexity
of multi-orbital band structures, the orbital degree of freedom is often
neglected in formulating the theoretical models for FeSCs. On the experimental
side, systematic studies of the orbital related phenomena in FeSCs have been
largely lacking. In this review, we summarize angle-resolved photoemission
spectroscopy (ARPES) measurements across various FeSC families in literature,
focusing on the systematic trend of orbital dependent electron correlations and
the role of different Fe 3d orbitals in driving the nematic transition, the
spin-density-wave transition, and implications for superconductivity.
",Physics,Physics
"Recursive Multikernel Filters Exploiting Nonlinear Temporal Structure   In kernel methods, temporal information on the data is commonly included by
using time-delayed embeddings as inputs. Recently, an alternative formulation
was proposed by defining a gamma-filter explicitly in a reproducing kernel
Hilbert space, giving rise to a complex model where multiple kernels operate on
different temporal combinations of the input signal. In the original
formulation, the kernels are then simply combined to obtain a single kernel
matrix (for instance by averaging), which provides computational benefits but
discards important information on the temporal structure of the signal.
Inspired by works on multiple kernel learning, we overcome this drawback by
considering the different kernels separately. We propose an efficient strategy
to adaptively combine and select these kernels during the training phase. The
resulting batch and online algorithms automatically learn to process highly
nonlinear temporal information extracted from the input signal, which is
implicitly encoded in the kernel values. We evaluate our proposal on several
artificial and real tasks, showing that it can outperform classical approaches
both in batch and online settings.
",Computer Science; Statistics,Computer Science; Statistics
"Deep Learning from Shallow Dives: Sonar Image Generation and Training for Underwater Object Detection   Among underwater perceptual sensors, imaging sonar has been highlighted for
its perceptual robustness underwater. The major challenge of imaging sonar,
however, arises from the difficulty in defining visual features despite limited
resolution and high noise levels. Recent developments in deep learning provide
a powerful solution for computer-vision researches using optical images.
Unfortunately, deep learning-based approaches are not well established for
imaging sonars, mainly due to the scant data in the training phase. Unlike the
abundant publically available terrestrial images, obtaining underwater images
is often costly, and securing enough underwater images for training is not
straightforward. To tackle this issue, this paper presents a solution to this
field's lack of data by introducing a novel end-to-end image-synthesizing
method in the training image preparation phase. The proposed method present
image synthesizing scheme to the images captured by an underwater simulator.
Our synthetic images are based on the sonar imaging models and noisy
characteristics to represent the real data obtained from the sea. We validate
the proposed scheme by training using a simulator and by testing the simulated
images with real underwater sonar images obtained from a water tank and the
sea.
",Computer Science,Computer Science
"Rigidity of branching microstructures in shape memory alloys   We analyze generic sequences for which the geometrically linear energy
\[E_\eta(u,\chi):= \eta^{-\frac{2}{3}}\int_{B_{0}(1)} \left| e(u)-
\sum_{i=1}^3 \chi_ie_i\right|^2 d x+\eta^\frac{1}{3} \sum_{i=1}^3
|D\chi_i|(B_{0}(1))\] remains bounded in the limit $\eta \to 0$. Here $ e(u)
:=1/2(Du + Du^T)$ is the (linearized) strain of the displacement $u$, the
strains $e_i$ correspond to the martensite strains of a shape memory alloy
undergoing cubic-to-tetragonal transformations and $\chi_i:B_{0}(1) \to
\{0,1\}$ is the partition into phases. In this regime it is known that in
addition to simple laminates also branched structures are possible, which if
austenite was present would enable the alloy to form habit planes.
In an ansatz-free manner we prove that the alignment of macroscopic
interfaces between martensite twins is as predicted by well-known rank-one
conditions. Our proof proceeds via the non-convex, non-discrete-valued
differential inclusion \[e(u) \in \bigcup_{1\leq i\neq j\leq 3}
\operatorname{conv} \{e_i,e_j\}\] satisfied by the weak limits of bounded
energy sequences and of which we classify all solutions. In particular, there
exist no convex integration solutions of the inclusion with complicated
geometric structures.
",Mathematics,Physics
"Period polynomials, derivatives of $L$-functions, and zeros of polynomials   Period polynomials have long been fruitful tools for the study of values of
$L$-functions in the context of major outstanding conjectures. In this paper,
we survey some facets of this study from the perspective of Eichler cohomology.
We discuss ways to incorporate non-cuspidal modular forms and values of
derivatives of $L$-functions into the same framework. We further review
investigations of the location of zeros of the period polynomial as well as of
its analogue for $L$-derivatives.
",Mathematics,Mathematics
"Online Learning for Changing Environments using Coin Betting   A key challenge in online learning is that classical algorithms can be slow
to adapt to changing environments. Recent studies have proposed ""meta""
algorithms that convert any online learning algorithm to one that is adaptive
to changing environments, where the adaptivity is analyzed in a quantity called
the strongly-adaptive regret. This paper describes a new meta algorithm that
has a strongly-adaptive regret bound that is a factor of $\sqrt{\log(T)}$
better than other algorithms with the same time complexity, where $T$ is the
time horizon. We also extend our algorithm to achieve a first-order (i.e.,
dependent on the observed losses) strongly-adaptive regret bound for the first
time, to our knowledge. At its heart is a new parameter-free algorithm for the
learning with expert advice (LEA) problem in which experts sometimes do not
output advice for consecutive time steps (i.e., \emph{sleeping} experts). This
algorithm is derived by a reduction from optimal algorithms for the so-called
coin betting problem. Empirical results show that our algorithm outperforms
state-of-the-art methods in both learning with expert advice and metric
learning scenarios.
",Computer Science; Statistics,Computer Science; Statistics
"Real embedding and equivariant eta forms   In 1993, Bismut and Zhang establish a mod Z embedding formula of
Atiyah-Patodi-Singer reduced eta invariants. In this paper, we explain the
hidden mod Z term as a spectral flow and extend this embedding formula to the
equivariant family case. In this case, the spectral flow is generalized to the
equivariant chern character of some equivariant Dai-Zhang higher spectral flow.
",Mathematics,Mathematics
"Global weak solutions in a three-dimensional Keller-Segel-Navier-Stokes system with nonlinear diffusion   The coupled quasilinear Keller-Segel-Navier-Stokes system is considered under
Neumann boundary conditions for $n$ and $c$ and no-slip boundary conditions for
$u$ in three-dimensional bounded domains $\Omega\subseteq \mathbb{R}^3$ with
smooth boundary, where $m>0,\kappa\in \mathbb{R}$ are given constants, $\phi\in
W^{1,\infty}(\Omega)$. If $ m> 2$, then for all reasonably regular initial
data, a corresponding initial-boundary value problem for $(KSNF)$ possesses a
globally defined weak solution.
",Mathematics,Mathematics
"Chiral and Topological Orbital Magnetism of Spin Textures   Using a semiclassical Green's function formalism, we discover the emergence
of chiral and topological orbital magnetism in two-dimensional chiral spin
textures by explicitly finding the corrections to the orbital magnetization,
proportional to the powers of the gradients of the texture. We show that in the
absence of spin-orbit coupling, the resulting orbital moment can be understood
as the electronic response to the emergent magnetic field associated with the
real-space Berry curvature. By referring to the Rashba model, we demonstrate
that by tuning the parameters of surface systems the engineering of emergent
orbital magnetism in spin textures can pave the way to novel concepts in
orbitronics.
",Physics,Physics
"Personalized Driver Stress Detection with Multi-task Neural Networks using Physiological Signals   Stress can be seen as a physiological response to everyday emotional, mental
and physical challenges. A long-term exposure to stressful situations can have
negative health consequences, such as increased risk of cardiovascular diseases
and immune system disorder. Therefore, a timely stress detection can lead to
systems for better management and prevention in future circumstances. In this
paper, we suggest a multi-task learning based neural network approach (with
hard parameter sharing of mutual representation and task-specific layers) for
personalized stress recognition using skin conductance and heart rate from
wearable devices. The proposed method is tested on multi-modal physiological
responses collected during real-world and simulator driving tasks.
",Computer Science,Computer Science
"The co-evolution of emotional well-being with weak and strong friendship ties   Social ties are strongly related to well-being. But what characterizes this
relationship? This study investigates social mechanisms explaining how social
ties affect well-being through social integration and social influence, and how
well-being affects social ties through social selection. We hypothesize that
highly integrated individuals - those with more extensive and dense friendship
networks - report higher emotional well-being than others. Moreover, emotional
well-being should be influenced by the well-being of close friends. Finally,
well-being should affect friendship selection when individuals prefer others
with higher levels of well-being, and others whose well-being is similar to
theirs. We test our hypotheses using longitudinal social network and well-being
data of 117 individuals living in a graduate housing community. The application
of a novel extension of Stochastic Actor-Oriented Models for ordered networks
(ordered SAOMs) allows us to detail and test our hypotheses for weak- and
strong-tied friendship networks simultaneously. Results do not support our
social integration and social influence hypotheses but provide evidence for
selection: individuals with higher emotional well-being tend to have more
strong-tied friends, and there are homophily processes regarding emotional
well-being in strong-tied networks. Our study highlights the two-directional
relationship between social ties and well-being, and demonstrates the
importance of considering different tie strengths for various social processes.
",Computer Science; Statistics,Computer Science; Physics
"Exothermicity is not a necessary condition for enhanced diffusion of enzymes   Recent experiments have revealed that the diffusivity of exothermic and fast
enzymes is enhanced when they are catalytically active, and different physical
mechanisms have been explored and quantified to account for this observation.
We perform measurements on the endothermic and relatively slow enzyme aldolase,
which also shows substrate-induced enhanced diffusion. We propose a new
physical paradigm, which reveals that the diffusion coefficient of a model
enzyme hydrodynamically coupled to its environment increases significantly when
undergoing changes in conformational fluctuations in a substrate-dependent
manner, and is independent of the overall turnover rate of the underlying
enzymatic reaction. Our results show that substrate-induced enhanced diffusion
of enzyme molecules can be explained within an equilibrium picture, and that
the exothermicity of the catalyzed reaction is not a necessary condition for
the observation of this phenomenon.
",Physics,Physics
"AdaGAN: Boosting Generative Models   Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) are an
effective method for training generative models of complex data such as natural
images. However, they are notoriously hard to train and can suffer from the
problem of missing modes where the model is not able to produce examples in
certain regions of the space. We propose an iterative procedure, called AdaGAN,
where at every step we add a new component into a mixture model by running a
GAN algorithm on a reweighted sample. This is inspired by boosting algorithms,
where many potentially weak individual predictors are greedily aggregated to
form a strong composite predictor. We prove that such an incremental procedure
leads to convergence to the true distribution in a finite number of steps if
each step is optimal, and convergence at an exponential rate otherwise. We also
illustrate experimentally that this procedure addresses the problem of missing
modes.
",Computer Science; Statistics,Computer Science; Statistics
"Interaction energy between vortices of vector fields on Riemannian surfaces   We study a variational Ginzburg-Landau type model depending on a small
parameter $\epsilon>0$ for (tangent) vector fields on a $2$-dimensional
Riemannian surface. As $\epsilon\to 0$, the vector fields tend to be of unit
length and will have singular points of a (non-zero) index, called vortices.
Our main result determines the interaction energy between these vortices as a
$\Gamma$-limit (at the second order) as $\epsilon\to 0$.
",Mathematics,Mathematics
"Uncertainty principle and geometry of the infinite Grassmann manifold   We study the pairs of projections $$ P_If=\chi_If ,\ \ Q_Jf= \left(\chi_J
\hat{f}\right)\check{\ } , \ \ f\in L^2(\mathbb{R}^n), $$ where $I, J\subset
\mathbb{R}^n$ are sets of finite Lebesgue measure, $\chi_I, \chi_J$ denote the
corresponding characteristic functions and $\hat{\ } , \check{\ }$ denote the
Fourier-Plancherel transformation $L^2(\mathbb{R}^n)\to L^2(\mathbb{R}^n)$ and
its inverse. These pairs of projections have been widely studied by several
authors in connection with the mathematical formulation of Heisenberg's
uncertainty principle. Our study is done from a differential geometric point of
view. We apply known results on the Finsler geometry of the Grassmann manifold
${\cal P}({\cal H})$ of a Hilbert space ${\cal H}$ to establish that there
exists a unique minimal geodesic of ${\cal P}({\cal H})$, which is a curve of
the form $$ \delta(t)=e^{itX_{I,J}}P_Ie^{-itX_{I,J}} $$ which joins $P_I$ and
$Q_J$ and has length $\pi/2$. As a consequence we obtain that if $H$ is the
logarithm of the Fourier-Plancherel map, then $$ \|[H,P_I]\|\ge \pi/2. $$ The
spectrum of $X_{I,J}$ is denumerable and symmetric with respect to the origin,
it has a smallest positive eigenvalue $\gamma(X_{I,J})$ which satisfies $$
\cos(\gamma(X_{I,J}))=\|P_IQ_J\|. $$
",Mathematics,Mathematics
"Comparing anticyclotomic Selmer groups of positive coranks for congruent modular forms   We study the variation of Iwasawa invariants of the anticyclotomic Selmer
groups of congruent modular forms under the Heegner hypothesis. In particular,
we show that even if the Selmer groups we study may have positive coranks, the
mu-invariant vanishes for one modular form if and only if it vanishes for the
other, and that their lambda-invariants are related by an explicit formula.
This generalizes results of Greenberg-Vatsal for the cyclotomic extension, as
well as results of Pollack-Weston and Castella-Kim-Longo for the anticyclotomic
extension when the Selmer groups in question are cotorsion.
",Mathematics,Mathematics
"Arbitrary order 2D virtual elements for polygonal meshes: Part II, inelastic problem   The present paper is the second part of a twofold work, whose first part is
reported in [3], concerning a newly developed Virtual Element Method (VEM) for
2D continuum problems. The first part of the work proposed a study for linear
elastic problem. The aim of this part is to explore the features of the VEM
formulation when material nonlinearity is considered, showing that the accuracy
and easiness of implementation discovered in the analysis inherent to the first
part of the work are still retained. Three different nonlinear constitutive
laws are considered in the VEM formulation. In particular, the generalized
viscoplastic model, the classical Mises plasticity with isotropic/kinematic
hardening and a shape memory alloy (SMA) constitutive law are implemented. The
versatility with respect to all the considered nonlinear material constitutive
laws is demonstrated through several numerical examples, also remarking that
the proposed 2D VEM formulation can be straightforwardly implemented as in a
standard nonlinear structural finite element method (FEM) framework.
",Mathematics,Mathematics
"Martin David Kruskal: a biographical memoir   Martin David Kruskal was one of the most versatile theoretical physicists of
his generation and is distinguished for his enduring work in several different
areas, most notably plasma physics, a memorable detour into relativity, and his
pioneering work in nonlinear waves. In the latter, together with Norman
Zabusky, he invented the concept of the soliton and, with others, developed its
application to classes of partial differential equations of physical
significance.
",Physics,Physics
"Skin Lesion Classification Using Deep Multi-scale Convolutional Neural Networks   We present a deep learning approach to the ISIC 2017 Skin Lesion
Classification Challenge using a multi-scale convolutional neural network. Our
approach utilizes an Inception-v3 network pre-trained on the ImageNet dataset,
which is fine-tuned for skin lesion classification using two different scales
of input images.
",Computer Science,Computer Science; Statistics
"Coalescence of Two Impurities in a Trapped One-dimensional Bose Gas   We study the ground state of a one-dimensional (1D) trapped Bose gas with two
mobile impurity particles. To investigate this set-up, we develop a variational
procedure in which the coordinates of the impurity particles are slow-like
variables. We validate our method using the exact results obtained for small
systems. Then, we discuss energies and pair densities for systems that contain
of the order of one hundred atoms. We show that bosonic non-interacting
impurities cluster. To explain this clustering, we calculate and discuss
induced impurity-impurity potentials in a harmonic trap. Further, we compute
the force between static impurities in a ring ({\it {à} la} the Casimir
force), and contrast the two effective potentials: the one obtained from the
mean-field approximation, and the one due to the one-phonon exchange. Our
formalism and findings are important for understanding (beyond the polaron
model) the physics of modern 1D cold-atom systems with more than one impurity.
",Physics,Physics
"Generating Synthetic Data for Real World Detection of DoS Attacks in the IoT   Denial of service attacks are especially pertinent to the internet of things
as devices have less computing power, memory and security mechanisms to defend
against them. The task of mitigating these attacks must therefore be redirected
from the device onto a network monitor. Network intrusion detection systems can
be used as an effective and efficient technique in internet of things systems
to offload computation from the devices and detect denial of service attacks
before they can cause harm. However the solution of implementing a network
intrusion detection system for internet of things networks is not without
challenges due to the variability of these systems and specifically the
difficulty in collecting data. We propose a model-hybrid approach to model the
scale of the internet of things system and effectively train network intrusion
detection systems. Through bespoke datasets generated by the model, the IDS is
able to predict a wide spectrum of real-world attacks, and as demonstrated by
an experiment construct more predictive datasets at a fraction of the time of
other more standard techniques.
",Computer Science,Computer Science
"Analytic continuation with Padé decomposition   The ill-posed analytic continuation problem for Green's functions or
self-energies can be done using the Padé rational polynomial approximation.
However, to extract accurate results from this approximation, high precision
input data of the Matsubara Green's function are needed. The calculation of the
Matsubara Green's function generally involves a Matsubara frequency summation
which cannot be evaluated analytically. Numerical summation is requisite but it
converges slowly with the increase of the Matsubara frequency. Here we show
that this slow convergence problem can be significantly improved by utilizing
the Padé decomposition approach to replace the Matsubara frequency summation
by a Padé frequency summation, and high precision input data can be obtained
to successfully perform the Padé analytic continuation.
",Physics,Mathematics
"On Generalized Gibbs Ensembles with an infinite set of conserved charges   We revisit the question of whether and how the steady states arising after
non-equilibrium time evolution in integrable models (and in particular in the
XXZ spin chain) can be described by the so-called Generalized Gibbs Ensemble
(GGE). It is known that the micro-canonical ensemble built on a complete set of
charges correctly describes the long-time limit of local observables, and
recently a canonical ensemble was built by Ilievski et. al. using particle
occupation number operators. Here we provide an alternative construction by
considering truncated GGE's (tGGE's) that only include a finite number of well
localized conserved operators. It is shown that the tGGE's can approximate the
steady states with arbitrary precision, i.e. all physical observables are
exactly reproduced in the infinite truncation limit. In addition, we show that
a complete canonical ensemble can in fact be built in terms of a new (discrete)
set of charges built as linear combinations of the standard ones.
Our general arguments are applied to concrete quench situations in the XXZ
chain, where the initial states are simple two-site or four-site product
states. Depending on the quench we find that numerical results for the local
correlators can be obtained with remarkable precision using truncated GGE's
with only 10-100 charges.
",Physics,Physics
"Robust MPC for tracking of nonholonomic robots with additive disturbances   In this paper, two robust model predictive control (MPC) schemes are proposed
for tracking control of nonholonomic systems with bounded disturbances:
tube-MPC and nominal robust MPC (NRMPC). In tube-MPC, the control signal
consists of a control action and a nonlinear feedback law based on the
deviation of the actual states from the states of a nominal system. It renders
the actual trajectory within a tube centered along the optimal trajectory of
the nominal system. Recursive feasibility and input-to-state stability are
established and the constraints are ensured by tightening the input domain and
the terminal region. While in NRMPC, an optimal control sequence is obtained by
solving an optimization problem based on the current state, and the first
portion of this sequence is applied to the real system in an open-loop manner
during each sampling period. The state of nominal system model is updated by
the actual state at each step, which provides additional a feedback. By
introducing a robust state constraint and tightening the terminal region,
recursive feasibility and input-to-state stability are guaranteed. Simulation
results demonstrate the effectiveness of both strategies proposed.
",Computer Science,Computer Science
"Candidate exoplanet host HD131399A: a nascent Am star   Direct imaging suggests that there is a Jovian exoplanet around the primary
A-star in the triple-star system HD131399. We investigate a high-quality
spectrum of the primary component HD131399A obtained with FEROS on the ESO/MPG
2.2m telescope, aiming to characterise the star's atmospheric and fundamental
parameters, and to determine elemental abundances at high precision and
accuracy. The aim is to constrain the chemical composition of the birth cloud
of the system and therefore the bulk composition of the putative planet. A
hybrid non-local thermal equilibrium (non-LTE) model atmosphere technique is
adopted for the quantitative spectral analysis. Comparison with the most recent
stellar evolution models yields the fundamental parameters. The atmospheric and
fundamental stellar parameters of HD131399A are constrained to Teff=9200+-100
K, log g=4.37+-0.10, M=1.95+0.08-0.06 Msun, R=1.51+0.13-0.10 Rsun, and log
L/Lsun=1.17+-0.07, locating the star on the zero-age main sequence. Non-LTE
effects on the derived metal abundances are often smaller than 0.1dex, but can
reach up to ~0.8dex for individual lines. The observed lighter elements up to
calcium are overall consistent with present-day cosmic abundances, with a C/O
ratio of 0.45$\pm$0.07 by number, while the heavier elements show mild
overabundances. We conclude that the birth cloud of the system had a standard
chemical composition, but we witness the onset of the Am phenomenon in the
slowly rotating star. We furthermore show that non-LTE analyses have the
potential to solve the remaining discrepancies between observed abundances and
predictions by diffusion models for Am stars. Moreover, the present case allows
mass loss, not turbulent mixing, to be identified as the main transport process
competing with diffusion in very young Am stars.
",Physics,Physics
"Progressive Neural Architecture Search   We propose a new method for learning the structure of convolutional neural
networks (CNNs) that is more efficient than recent state-of-the-art methods
based on reinforcement learning and evolutionary algorithms. Our approach uses
a sequential model-based optimization (SMBO) strategy, in which we search for
structures in order of increasing complexity, while simultaneously learning a
surrogate model to guide the search through structure space. Direct comparison
under the same search space shows that our method is up to 5 times more
efficient than the RL method of Zoph et al. (2018) in terms of number of models
evaluated, and 8 times faster in terms of total compute. The structures we
discover in this way achieve state of the art classification accuracies on
CIFAR-10 and ImageNet.
",Computer Science; Statistics,Computer Science; Statistics
"Optimal Weighting for Exam Composition   A problem faced by many instructors is that of designing exams that
accurately assess the abilities of the students. Typically these exams are
prepared several days in advance, and generic question scores are used based on
rough approximation of the question difficulty and length. For example, for a
recent class taught by the author, there were 30 multiple choice questions
worth 3 points, 15 true/false with explanation questions worth 4 points, and 5
analytical exercises worth 10 points. We describe a novel framework where
algorithms from machine learning are used to modify the exam question weights
in order to optimize the exam scores, using the overall class grade as a proxy
for a student's true ability. We show that significant error reduction can be
obtained by our approach over standard weighting schemes, and we make several
new observations regarding the properties of the ""good"" and ""bad"" exam
questions that can have impact on the design of improved future evaluation
methods.
",Statistics,Computer Science; Statistics
"Enemy At the Gateways: A Game Theoretic Approach to Proxy Distribution   A core technique used by popular proxy-based circumvention systems like Tor,
Psiphon, and Lantern is to secretly share the IP addresses of circumvention
proxies with the censored clients for them to be able to use such systems. For
instance, such secretly shared proxies are known as bridges in Tor. However, a
key challenge to this mechanism is the insider attack problem: censoring agents
can impersonate as benign censored clients in order to obtain (and then block)
such secretly shared circumvention proxies.
In this paper, we perform a fundamental study on the problem of insider
attack on proxy-based circumvention systems. We model the proxy distribution
problem using game theory, based on which we derive the optimal strategies of
the parties involved, i.e., the censors and circumvention system operators.
That is, we derive the optimal proxy distribution mechanism of a
circumvention system like Tor, against the censorship adversary who also takes
his optimal censorship strategies.
This is unlike previous works that design ad hoc mechanisms for proxy
distribution, against non-optimal censors.
We perform extensive simulations to evaluate our optimal proxy assignment
algorithm under various adversarial and network settings. Comparing with the
state-of-the-art prior work, we show that our optimal proxy assignment
algorithm has superior performance, i.e., better resistance to censorship even
against the strongest censorship adversary who takes her optimal actions. We
conclude with lessons and recommendation for the design of proxy-based
circumvention systems.
",Computer Science,Computer Science
"Generalized Sheet Transition Conditions (GSTCs) for a Metascreen -- A Fishnet Metasurface   We used a multiple-scale homogenization method to derive generalized sheet
transition conditions (GSTCs) for electromagnetic fields at the surface of a
metascreen---a metasurface with a ""fishnet"" structure. These surfaces are
characterized by periodically-spaced arbitrary-shaped apertures in an otherwise
relatively impenetrable surface. The parameters in these GSTCs are interpreted
as effective surface susceptibilities and surface porosities, which are related
to the geometry of the apertures that constitute the metascreen. Finally, we
emphasize the subtle but important difference between the GSTCs required for
metascreens and those required for metafilms (a metasurface with a ""cermet""
structure, i.e., an array of isolated (non-touching) scatterers).
",Physics,Physics
"Boltzmann Encoded Adversarial Machines   Restricted Boltzmann Machines (RBMs) are a class of generative neural network
that are typically trained to maximize a log-likelihood objective function. We
argue that likelihood-based training strategies may fail because the objective
does not sufficiently penalize models that place a high probability in regions
where the training data distribution has low probability. To overcome this
problem, we introduce Boltzmann Encoded Adversarial Machines (BEAMs). A BEAM is
an RBM trained against an adversary that uses the hidden layer activations of
the RBM to discriminate between the training data and the probability
distribution generated by the model. We present experiments demonstrating that
BEAMs outperform RBMs and GANs on multiple benchmarks.
",Statistics,Computer Science; Statistics
"See the Near Future: A Short-Term Predictive Methodology to Traffic Load in ITS   The Intelligent Transportation System (ITS) targets to a coordinated traffic
system by applying the advanced wireless communication technologies for road
traffic scheduling. Towards an accurate road traffic control, the short-term
traffic forecasting to predict the road traffic at the particular site in a
short period is often useful and important. In existing works, Seasonal
Autoregressive Integrated Moving Average (SARIMA) model is a popular approach.
The scheme however encounters two challenges: 1) the analysis on related data
is insufficient whereas some important features of data may be neglected; and
2) with data presenting different features, it is unlikely to have one
predictive model that can fit all situations. To tackle above issues, in this
work, we develop a hybrid model to improve accuracy of SARIMA. In specific, we
first explore the autocorrelation and distribution features existed in traffic
flow to revise structure of the time series model. Based on the Gaussian
distribution of traffic flow, a hybrid model with a Bayesian learning algorithm
is developed which can effectively expand the application scenarios of SARIMA.
We show the efficiency and accuracy of our proposal using both analysis and
experimental studies. Using the real-world trace data, we show that the
proposed predicting approach can achieve satisfactory performance in practice.
",Computer Science; Statistics,Computer Science; Statistics
"Improved Kernels and Algorithms for Claw and Diamond Free Edge Deletion Based on Refined Observations   In the {claw, diamond}-free edge deletion problem, we are given a graph $G$
and an integer $k>0$, the question is whether there are at most $k$ edges whose
deletion results in a graph without claws and diamonds as induced graphs. Based
on some refined observations, we propose a kernel of $O(k^3)$ vertices and
$O(k^4)$ edges, significantly improving the previous kernel of $O(k^{12})$
vertices and $O(k^{24})$ edges. In addition, we derive an $O^*(3.792^k)$-time
algorithm for the {claw, diamond}-free edge deletion problem.
",Computer Science,Computer Science
"The singular locus of hypersurface sections containing a closed subscheme over finite fields   We prove that there exist hypersurfaces that contain a given closed subscheme
$Z$ of the projective space over a finite field and intersect a given smooth
scheme $X$ off of $Z$ smoothly, if the intersection $V = Z \cap X$ is smooth.
Furthermore, we can give a bound on the dimension of the singular locus of the
hypersurface section and prescribe finitely many local conditions on the
hypersurface. This is an analogue of a Bertini theorem of Bloch over finite
fields and is proved using Poonen's closed point sieve. We also show a similar
theorem for the case where $V$ is not smooth.
",Mathematics,Mathematics
"Geometric GAN   Generative Adversarial Nets (GANs) represent an important milestone for
effective generative models, which has inspired numerous variants seemingly
different from each other. One of the main contributions of this paper is to
reveal a unified geometric structure in GAN and its variants. Specifically, we
show that the adversarial generative model training can be decomposed into
three geometric steps: separating hyperplane search, discriminator parameter
update away from the separating hyperplane, and the generator update along the
normal vector direction of the separating hyperplane. This geometric intuition
reveals the limitations of the existing approaches and leads us to propose a
new formulation called geometric GAN using SVM separating hyperplane that
maximizes the margin. Our theoretical analysis shows that the geometric GAN
converges to a Nash equilibrium between the discriminator and generator. In
addition, extensive numerical results show that the superior performance of
geometric GAN.
",Computer Science; Physics; Statistics,Computer Science; Statistics
"Accelerating Kernel Classifiers Through Borders Mapping   Support vector machines (SVM) and other kernel techniques represent a family
of powerful statistical classification methods with high accuracy and broad
applicability. Because they use all or a significant portion of the training
data, however, they can be slow, especially for large problems. Piecewise
linear classifiers are similarly versatile, yet have the additional advantages
of simplicity, ease of interpretation and, if the number of component linear
classifiers is not too large, speed. Here we show how a simple, piecewise
linear classifier can be trained from a kernel-based classifier in order to
improve the classification speed. The method works by finding the root of the
difference in conditional probabilities between pairs of opposite classes to
build up a representation of the decision boundary. When tested on 17 different
datasets, it succeeded in improving the classification speed of a SVM for 9 of
them by factors as high as 88 times or more. The method is best suited to
problems with continuum features data and smooth probability functions. Because
the component linear classifiers are built up individually from an existing
classifier, rather than through a simultaneous optimization procedure, the
classifier is also fast to train.
",Computer Science; Statistics,Computer Science; Statistics
"Inertia-Constrained Pixel-by-Pixel Nonnegative Matrix Factorisation: a Hyperspectral Unmixing Method Dealing with Intra-class Variability   Blind source separation is a common processing tool to analyse the
constitution of pixels of hyperspectral images. Such methods usually suppose
that pure pixel spectra (endmembers) are the same in all the image for each
class of materials. In the framework of remote sensing, such an assumption is
no more valid in the presence of intra-class variabilities due to illumination
conditions, weathering, slight variations of the pure materials, etc... In this
paper, we first describe the results of investigations highlighting intra-class
variability measured in real images. Considering these results, a new
formulation of the linear mixing model is presented leading to two new methods.
Unconstrained Pixel-by-pixel NMF (UP-NMF) is a new blind source separation
method based on the assumption of a linear mixing model, which can deal with
intra-class variability. To overcome UP-NMF limitations an extended method is
proposed, named Inertia-constrained Pixel-by-pixel NMF (IP-NMF). For each
sensed spectrum, these extended versions of NMF extract a corresponding set of
source spectra. A constraint is set to limit the spreading of each source's
estimates in IP-NMF. The methods are tested on a semi-synthetic data set built
with spectra extracted from a real hyperspectral image and then numerically
mixed. We thus demonstrate the interest of our methods for realistic source
variabilities. Finally, IP-NMF is tested on a real data set and it is shown to
yield better performance than state of the art methods.
",Computer Science; Physics; Statistics,Computer Science
"When Can Neural Networks Learn Connected Decision Regions?   Previous work has questioned the conditions under which the decision regions
of a neural network are connected and further showed the implications of the
corresponding theory to the problem of adversarial manipulation of classifiers.
It has been proven that for a class of activation functions including leaky
ReLU, neural networks having a pyramidal structure, that is no layer has more
hidden units than the input dimension, produce necessarily connected decision
regions. In this paper, we advance this important result by further developing
the sufficient and necessary conditions under which the decision regions of a
neural network are connected. We then apply our framework to overcome the
limits of existing work and further study the capacity to learn connected
regions of neural networks for a much wider class of activation functions
including those widely used, namely ReLU, sigmoid, tanh, softlus, and
exponential linear function.
",Computer Science; Statistics,Computer Science; Statistics
"AspEm: Embedding Learning by Aspects in Heterogeneous Information Networks   Heterogeneous information networks (HINs) are ubiquitous in real-world
applications. Due to the heterogeneity in HINs, the typed edges may not fully
align with each other. In order to capture the semantic subtlety, we propose
the concept of aspects with each aspect being a unit representing one
underlying semantic facet. Meanwhile, network embedding has emerged as a
powerful method for learning network representation, where the learned
embedding can be used as features in various downstream applications.
Therefore, we are motivated to propose a novel embedding learning
framework---AspEm---to preserve the semantic information in HINs based on
multiple aspects. Instead of preserving information of the network in one
semantic space, AspEm encapsulates information regarding each aspect
individually. In order to select aspects for embedding purpose, we further
devise a solution for AspEm based on dataset-wide statistics. To corroborate
the efficacy of AspEm, we conducted experiments on two real-words datasets with
two types of applications---classification and link prediction. Experiment
results demonstrate that AspEm can outperform baseline network embedding
learning methods by considering multiple aspects, where the aspects can be
selected from the given HIN in an unsupervised manner.
",Computer Science,Computer Science; Statistics
"Scalable Inference for Space-Time Gaussian Cox Processes   The log-Gaussian Cox process is a flexible and popular class of point pattern
models for capturing spatial and space-time dependence for point patterns.
Model fitting requires approximation of stochastic integrals which is
implemented through discretization over the domain of interest. With fine scale
discretization, inference based on Markov chain Monte Carlo is computationally
burdensome because of the cost of matrix decompositions and storage, such as
the Cholesky, for high dimensional covariance matrices associated with latent
Gaussian variables. This article addresses these computational bottlenecks by
combining two recent developments: (i) a data augmentation strategy that has
been proposed for space-time Gaussian Cox processes that is based on exact
Bayesian inference and does not require fine grid approximations for infinite
dimensional integrals, and (ii) a recently developed family of
sparsity-inducing Gaussian processes, called nearest-neighbor Gaussian
processes, to avoid expensive matrix computations. Our inference is delivered
within the fully model-based Bayesian paradigm and does not sacrifice the
richness of traditional log-Gaussian Cox processes. We apply our method to
crime event data in San Francisco and investigate the recovery of the intensity
surface.
",Statistics,Statistics
"ALMA Observations of Gas-Rich Galaxies in z~1.6 Galaxy Clusters: Evidence for Higher Gas Fractions in High-Density Environments   We present ALMA CO (2-1) detections in 11 gas-rich cluster galaxies at z~1.6,
constituting the largest sample of molecular gas measurements in z>1.5 clusters
to date. The observations span three galaxy clusters, derived from the Spitzer
Adaptation of the Red-sequence Cluster Survey. We augment the >5sigma
detections of the CO (2-1) fluxes with multi-band photometry, yielding stellar
masses and infrared-derived star formation rates, to place some of the first
constraints on molecular gas properties in z~1.6 cluster environments. We
measure sizable gas reservoirs of 0.5-2x10^11 solar masses in these objects,
with high gas fractions and long depletion timescales, averaging 62% and 1.4
Gyr, respectively. We compare our cluster galaxies to the scaling relations of
the coeval field, in the context of how gas fractions and depletion timescales
vary with respect to the star-forming main sequence. We find that our cluster
galaxies lie systematically off the field scaling relations at z=1.6 toward
enhanced gas fractions, at a level of ~4sigma, but have consistent depletion
timescales. Exploiting CO detections in lower-redshift clusters from the
literature, we investigate the evolution of the gas fraction in cluster
galaxies, finding it to mimic the strong rise with redshift in the field. We
emphasize the utility of detecting abundant gas-rich galaxies in high-redshift
clusters, deeming them as crucial laboratories for future statistical studies.
",Physics,Physics
"Ca II K 1-A Emission Index Composites   We describe here a procedure to combine measurements in the 393.37 nm Ca II K
spectral line taken at different observatories. Measurements from the National
Solar Observatory (NSO) Integrated Sunlight Spectrometer (ISS) on the Synoptic
Optical Long-term Investigations of the Sun (SOLIS) telescope, the NSO/Sac Peak
Ca II K-Line Monitoring Program, and Ca II K filtergrams from Kodaikanal Solar
Observatory (KKL) are merged together to create a pair of composites of the Ca
II K 1-A emission index. These composites are publicly available from the SOLIS
website at this http URL.
",Physics,Physics
"Flow Fields: Dense Correspondence Fields for Highly Accurate Large Displacement Optical Flow Estimation   Modern large displacement optical flow algorithms usually use an
initialization by either sparse descriptor matching techniques or dense
approximate nearest neighbor fields. While the latter have the advantage of
being dense, they have the major disadvantage of being very outlier-prone as
they are not designed to find the optical flow, but the visually most similar
correspondence. In this article we present a dense correspondence field
approach that is much less outlier-prone and thus much better suited for
optical flow estimation than approximate nearest neighbor fields. Our approach
does not require explicit regularization, smoothing (like median filtering) or
a new data term. Instead we solely rely on patch matching techniques and a
novel multi-scale matching strategy. We also present enhancements for outlier
filtering. We show that our approach is better suited for large displacement
optical flow estimation than modern descriptor matching techniques. We do so by
initializing EpicFlow with our approach instead of their originally used
state-of-the-art descriptor matching technique. We significantly outperform the
original EpicFlow on MPI-Sintel, KITTI 2012, KITTI 2015 and Middlebury. In this
extended article of our former conference publication we further improve our
approach in matching accuracy as well as runtime and present more experiments
and insights.
",Computer Science,Computer Science
"Using data science as a community advocacy tool to promote equity in urban renewal programs: An analysis of Atlanta's Anti-Displacement Tax Fund   Cities across the United States are undergoing great transformation and urban
growth. Data and data analysis has become an essential element of urban
planning as cities use data to plan land use and development. One great
challenge is to use the tools of data science to promote equity along with
growth. The city of Atlanta is an example site of large-scale urban renewal
that aims to engage in development without displacement. On the Westside of
downtown Atlanta, the construction of the new Mercedes-Benz Stadium and the
conversion of an underutilized rail-line into a multi-use trail may result in
increased property values. In response to community residents' concerns and a
commitment to development without displacement, the city and philanthropic
partners announced an Anti-Displacement Tax Fund to subsidize future property
tax increases of owner occupants for the next twenty years. To achieve greater
transparency, accountability, and impact, residents expressed a desire for a
tool that would help them determine eligibility and quantify this commitment.
In support of this goal, we use machine learning techniques to analyze
historical tax assessment and predict future tax assessments. We then apply
eligibility estimates to our predictions to estimate the total cost for the
first seven years of the program. These forecasts are also incorporated into an
interactive tool for community residents to determine their eligibility for the
fund and the expected increase in their home value over the next seven years.
",Computer Science,Computer Science
"Improved Convergence Rates for Distributed Resource Allocation   In this paper, we develop a class of decentralized algorithms for solving a
convex resource allocation problem in a network of $n$ agents, where the agent
objectives are decoupled while the resource constraints are coupled. The agents
communicate over a connected undirected graph, and they want to collaboratively
determine a solution to the overall network problem, while each agent only
communicates with its neighbors. We first study the connection between the
decentralized resource allocation problem and the decentralized consensus
optimization problem. Then, using a class of algorithms for solving consensus
optimization problems, we propose a novel class of decentralized schemes for
solving resource allocation problems in a distributed manner. Specifically, we
first propose an algorithm for solving the resource allocation problem with an
$o(1/k)$ convergence rate guarantee when the agents' objective functions are
generally convex (could be nondifferentiable) and per agent local convex
constraints are allowed; We then propose a gradient-based algorithm for solving
the resource allocation problem when per agent local constraints are absent and
show that such scheme can achieve geometric rate when the objective functions
are strongly convex and have Lipschitz continuous gradients. We have also
provided scalability/network dependency analysis. Based on these two
algorithms, we have further proposed a gradient projection-based algorithm
which can handle smooth objective and simple constraints more efficiently.
Numerical experiments demonstrates the viability and performance of all the
proposed algorithms.
",Computer Science; Mathematics,Computer Science
"A novel procedure for the identification of chaos in complex biological systems   We demonstrate the presence of chaos in stochastic simulations that are
widely used to study biodiversity in nature. The investigation deals with a set
of three distinct species that evolve according to the standard rules of
mobility, reproduction and predation, with predation following the cyclic rules
of the popular rock, paper and scissors game. The study uncovers the
possibility to distinguish between time evolutions that start from slightly
different initial states, guided by the Hamming distance which heuristically
unveils the chaotic behavior. The finding opens up a quantitative approach that
relates the correlation length to the average density of maxima of a typical
species, and an ensemble of stochastic simulations is implemented to support
the procedure. The main result of the work shows how a single and simple
experimental realization that counts the density of maxima associated with the
chaotic evolution of the species serves to infer its correlation length. We use
the result to investigate others distinct complex systems, one dealing with a
set of differential equations that can be used to model a diversity of natural
and artificial chaotic systems, and another one, focusing on the ocean water
level.
",Physics,Physics
"How hard is it to cross the room? -- Training (Recurrent) Neural Networks to steer a UAV   This work explores the feasibility of steering a drone with a (recurrent)
neural network, based on input from a forward looking camera, in the context of
a high-level navigation task. We set up a generic framework for training a
network to perform navigation tasks based on imitation learning. It can be
applied to both aerial and land vehicles. As a proof of concept we apply it to
a UAV (Unmanned Aerial Vehicle) in a simulated environment, learning to cross a
room containing a number of obstacles. So far only feedforward neural networks
(FNNs) have been used to train UAV control. To cope with more complex tasks, we
propose the use of recurrent neural networks (RNN) instead and successfully
train an LSTM (Long-Short Term Memory) network for controlling UAVs. Vision
based control is a sequential prediction problem, known for its highly
correlated input data. The correlation makes training a network hard,
especially an RNN. To overcome this issue, we investigate an alternative
sampling method during training, namely window-wise truncated backpropagation
through time (WW-TBPTT). Further, end-to-end training requires a lot of data
which often is not available. Therefore, we compare the performance of
retraining only the Fully Connected (FC) and LSTM control layers with networks
which are trained end-to-end. Performing the relatively simple task of crossing
a room already reveals important guidelines and good practices for training
neural control networks. Different visualizations help to explain the behavior
learned.
",Computer Science,Computer Science
"Strong Functional Representation Lemma and Applications to Coding Theorems   This paper shows that for any random variables $X$ and $Y$, it is possible to
represent $Y$ as a function of $(X,Z)$ such that $Z$ is independent of $X$ and
$I(X;Z|Y)\le\log(I(X;Y)+1)+4$ bits. We use this strong functional
representation lemma (SFRL) to establish a bound on the rate needed for
one-shot exact channel simulation for general (discrete or continuous) random
variables, strengthening the results by Harsha et al. and Braverman and Garg,
and to establish new and simple achievability results for one-shot
variable-length lossy source coding, multiple description coding and Gray-Wyner
system. We also show that the SFRL can be used to reduce the channel with state
noncausally known at the encoder to a point-to-point channel, which provides a
simple achievability proof of the Gelfand-Pinsker theorem.
",Computer Science; Mathematics,Computer Science
"Near-Perfect Conversion of a Propagating Plane Wave into a Surface Wave Using Metasurfaces   In this paper, theoretical and numerical studies of perfect/nearly-perfect
conversion of a plane wave into a surface wave are presented. The problem of
determining the electromagnetic properties of an inhomogeneous lossless
boundary which would fully transform an incident plane wave into a surface wave
propagating along the boundary is considered. An approximate field solution
which produces a slowly growing surface wave and satisfies the energy
conservation law is discussed and numerically demonstrated. The results of the
study are of great importance for the future development of such devices as
perfect leaky-wave antennas and can potentially lead to many novel
applications.
",Physics,Physics
"Radiomics strategies for risk assessment of tumour failure in head-and-neck cancer   Quantitative extraction of high-dimensional mineable data from medical images
is a process known as radiomics. Radiomics is foreseen as an essential
prognostic tool for cancer risk assessment and the quantification of
intratumoural heterogeneity. In this work, 1615 radiomic features (quantifying
tumour image intensity, shape, texture) extracted from pre-treatment FDG-PET
and CT images of 300 patients from four different cohorts were analyzed for the
risk assessment of locoregional recurrences (LR) and distant metastases (DM) in
head-and-neck cancer. Prediction models combining radiomic and clinical
variables were constructed via random forests and imbalance-adjustment
strategies using two of the four cohorts. Independent validation of the
prediction and prognostic performance of the models was carried out on the
other two cohorts (LR: AUC = 0.69 and CI = 0.67; DM: AUC = 0.86 and CI = 0.88).
Furthermore, the results obtained via Kaplan-Meier analysis demonstrated the
potential of radiomics for assessing the risk of specific tumour outcomes using
multiple stratification groups. This could have important clinical impact,
notably by allowing for a better personalization of chemo-radiation treatments
for head-and-neck cancer patients from different risk groups.
",Computer Science,Quantitative Biology
"Synergies between Asteroseismology and Exoplanetary Science   Over the past decade asteroseismology has become a powerful method to
systematically characterize host stars and dynamical architectures of exoplanet
systems. In this contribution I review current key synergies between
asteroseismology and exoplanetary science such as the precise determination of
planet radii and ages, the measurement of orbital eccentricities, stellar
obliquities and their impact on hot Jupiter formation theories, and the
importance of asteroseismology on spectroscopic analyses of exoplanet hosts. I
also give an outlook on future synergies such as the characterization of
sub-Neptune-size planets orbiting solar-type stars, the study of planet
populations orbiting evolved stars, and the determination of ages of
intermediate-mass stars hosting directly imaged planets.
",Physics,Physics
"Expropriations, Property Confiscations and New Offshore Entities: Evidence from the Panama Papers   Using the Panama Papers, we show that the beginning of media reporting on
expropriations and property confiscations in a country increases the
probability that offshore entities are incorporated by agents from the same
country in the same month. This result is robust to the use of country-year
fixed effects and the exclusion of tax havens. Further analysis shows that the
effect is driven by countries with non-corrupt and effective governments, which
supports the notion that offshore entities are incorporated when reasonably
well-intended and well-functioning governments become more serious about
fighting organized crime by confiscating proceeds of crime.
",Quantitative Finance,Computer Science
"Efficient anchor loss suppression in coupled near-field optomechanical resonators   Elastic dissipation through radiation towards the substrate is a major loss
channel in micro- and nanomechanical resonators. Engineering the coupling of
these resonators with optical cavities further complicates and constrains the
design of low-loss optomechanical devices. In this work we rely on the coherent
cancellation of mechanical radiation to demonstrate material and surface
absorption limited silicon near-field optomechanical resonators oscillating at
tens of MHz. The effectiveness of our dissipation suppression scheme is
investigated at room and cryogenic temperatures. While at room temperature we
can reach a maximum quality factor of 7.61k ($fQ$-product of the order of
$10^{11}$~Hz), at 22~K the quality factor increases to 37k, resulting in a
$fQ$-product of $2\times10^{12}$~Hz.
",Physics,Physics
"LAMOST Spectroscopic Survey of the Galactic Anticentre (LSS-GAC): the second release of value-added catalogues   We present the second release of value-added catalogues of the LAMOST
Spectroscopic Survey of the Galactic Anticentre (LSS-GAC DR2). The catalogues
present values of radial velocity $V_{\rm r}$, atmospheric parameters ---
effective temperature $T_{\rm eff}$, surface gravity log$g$, metallicity
[Fe/H], $\alpha$-element to iron (metal) abundance ratio [$\alpha$/Fe]
([$\alpha$/M]), elemental abundances [C/H] and [N/H], and absolute magnitudes
${\rm M}_V$ and ${\rm M}_{K_{\rm s}}$ deduced from 1.8 million spectra of 1.4
million unique stars targeted by the LSS-GAC since September 2011 until June
2014. The catalogues also give values of interstellar reddening, distance and
orbital parameters determined with a variety of techniques, as well as proper
motions and multi-band photometry from the far-UV to the mid-IR collected from
the literature and various surveys. Accuracies of radial velocities reach
5kms$^{-1}$ for late-type stars, and those of distance estimates range between
10 -- 30 per cent, depending on the spectral signal-to-noise ratios. Precisions
of [Fe/H], [C/H] and [N/H] estimates reach 0.1dex, and those of [$\alpha$/Fe]
and [$\alpha$/M] reach 0.05dex. The large number of stars, the contiguous sky
coverage, the simple yet non-trivial target selection function and the robust
estimates of stellar radial velocities and atmospheric parameters, distances
and elemental abundances, make the catalogues a valuable data set to study the
structure and evolution of the Galaxy, especially the solar-neighbourhood and
the outer disk.
",Physics,Physics
"A New Framework for Synthetic Aperture Sonar Micronavigation   Synthetic aperture imaging systems achieve constant azimuth resolution by
coherently summating the observations acquired along the aperture path. At this
aim, their locations have to be known with subwavelength accuracy. In
underwater Synthetic Aperture Sonar (SAS), the nature of propagation and
navigation in water makes the retrieval of this information challenging.
Inertial sensors have to be employed in combination with signal processing
techniques, which are usually referred to as micronavigation. In this paper we
propose a novel micronavigation approach based on the minimization of an error
function between two contiguous pings having some mutual information. This
error is obtained by comparing the vector space intersections between the pings
orthogonal projectors. The effectiveness and generality of the proposed
approach is demonstrated by means of simulations and by means of an experiment
performed in a controlled environment.
",Computer Science,Physics
"Flux-Rope Twist in Eruptive Flares and CMEs: due to Zipper and Main-Phase Reconnection   The nature of three-dimensional reconnection when a twisted flux tube erupts
during an eruptive flare or coronal mass ejection is considered. The
reconnection has two phases: first of all, 3D ""zipper reconnection"" propagates
along the initial coronal arcade, parallel to the polarity inversion line
(PIL), then subsequent quasi-2D ""main phase reconnection"" in the low corona
around a flux rope during its eruption produces coronal loops and chromospheric
ribbons that propagate away from the PIL in a direction normal to it.
One scenario starts with a sheared arcade: the zipper reconnection creates a
twisted flux rope of roughly one turn ($2\pi$ radians of twist), and then main
phase reconnection builds up the bulk of the erupting flux rope with a
relatively uniform twist of a few turns. A second scenario starts with a
pre-existing flux rope under the arcade. Here the zipper phase can create a
core with many turns that depend on the ratio of the magnetic fluxes in the
newly formed flare ribbons and the new flux rope. Main phase reconnection then
adds a layer of roughly uniform twist to the twisted central core. Both phases
and scenarios are modeled in a simple way that assumes the initial magnetic
flux is fragmented along the PIL. The model uses conservation of magnetic
helicity and flux, together with equipartition of magnetic helicity, to deduce
the twist of the erupting flux rope in terms the geometry of the initial
configuration.
Interplanetary observations show some flux ropes have a fairly uniform twist,
which could be produced when the zipper phase and any pre-existing flux rope
possess small or moderate twist (up to one or two turns). Other interplanetary
flux ropes have highly twisted cores (up to five turns), which could be
produced when there is a pre-existing flux rope and an active zipper phase that
creates substantial extra twist.
",Physics,Physics
"Pile-up Reduction, Bayesian Decomposition and Applications of Silicon Drift Detectors at LCLS   Silicon drift detectors (SDDs) revolutionized spectroscopy in fields as
diverse as geology and dentistry. For a subset of experiments at ultra-fast,
x-ray free-electron lasers (FELs), SDDs can make substantial contributions.
Often the unknown spectrum is interesting, carrying science data, or the
background measurement is useful to identify unexpected signals. Many
measurements involve only several discrete photon energies known a priori. We
designed a pulse function (a combination of gradual step and exponential decay
function) and demonstrated that for individual pulses the signal amplitude,
peaking time, and pulse amplitude are interrelated and the signal amplitude and
peaking time are obtained for each pulse by fitting. Avoiding pulse shaping
reduced peaking times to tens of nanoseconds, resulting in reduced pulse
pile-up and allowing decomposition of remaining pulse pile-up at photon
separation times down to 100~ns while yielding time-of-arrival information with
precision of 10~nanoseconds. At pulsed sources or high photon rates, photon
pile-up still occurs. We showed that the area of one photon peaks is not
suitable for estimating high photon rates while pile-up spectrum fitting is
relatively simple and preferable to pile-up spectrum deconvolution. We
developed a photon pile-up model for constant intensity sources, extended it to
variable intensity sources (typical for FELs) and used it to fit a complex
pile-up spectrum, demonstrating its accuracy. Based on the pile-up model, we
developed a Bayesian pile-up decomposition method that allows decomposing
pile-up of single events with up to 6 photons from 6 monochromatic lines with
99% accuracy. The usefulness of SDDs will continue into the x-ray FEL era of
science. Their successors, the ePixS hybrid pixel detectors, already offer
hundreds of pixels, each with similar performance to an SDD, in a compact,
robust and affordable package.
",Physics,Physics
"Low-shot learning with large-scale diffusion   This paper considers the problem of inferring image labels from images when
only a few annotated examples are available at training time. This setup is
often referred to as low-shot learning, where a standard approach is to
re-train the last few layers of a convolutional neural network learned on
separate classes for which training examples are abundant. We consider a
semi-supervised setting based on a large collection of images to support label
propagation. This is possible by leveraging the recent advances on large-scale
similarity graph construction.
We show that despite its conceptual simplicity, scaling label propagation up
to hundred millions of images leads to state of the art accuracy in the
low-shot learning regime.
",Computer Science; Statistics,Computer Science; Statistics
"Ensemble Methods for Personalized E-Commerce Search Challenge at CIKM Cup 2016   Personalized search has been a hot research topic for many years and has been
widely used in e-commerce. This paper describes our solution to tackle the
challenge of personalized e-commerce search at CIKM Cup 2016. The goal of this
competition is to predict search relevance and re-rank the result items in SERP
according to the personalized search, browsing and purchasing preferences.
Based on a detailed analysis of the provided data, we extract three different
types of features, i.e., statistic features, query-item features and session
features. Different models are used on these features, including logistic
regression, gradient boosted decision trees, rank svm and a novel deep match
model. With the blending of multiple models, a stacking ensemble model is built
to integrate the output of individual models and produce a more accurate
prediction result. Based on these efforts, our solution won the champion of the
competition on all the evaluation metrics.
",Computer Science,Computer Science
"Leaf Space Isometries of Singular Riemannian Foliations and Their Spectral Properties   In this paper, the authors consider leaf spaces of singular Riemannian
foliations $\mathcal{F}$ on compact manifolds $M$ and the associated
$\mathcal{F}$-basic spectrum on $M$, $spec_B(M, \mathcal{F}),$ counted with
multiplicities. Recently, a notion of smooth isometry $\varphi:
M_1/\mathcal{F}_1\rightarrow M_2/\mathcal{F}_2$ between the leaf spaces of such
singular Riemannian foliations $(M_1,\mathcal{F}_1)$ and $(M_2,\mathcal{F}_2)$
has appeared in the literature. In this paper, the authors provide an example
to show that the existence a smooth isometry of leaf spaces as above is not
sufficient to guarantee the equality of $spec_B(M_1,\mathcal{F}_1)$ and
$spec_B(M_2,\mathcal{F}_2).$ The authors then prove that if some additional
conditions involving the geometry of the leaves are satisfied, then the
equality of $spec_B(M_1,\mathcal{F}_1)$ and $spec_B(M_2,\mathcal{F}_2)$ is
guaranteed. Consequences and applications to orbifold spectral theory,
isometric group actions, and their reductions are also explored.
",Mathematics,Mathematics
"Phylogenetic networks that are their own fold-ups   Phylogenetic networks are becoming of increasing interest to evolutionary
biologists due to their ability to capture complex non-treelike evolutionary
processes. From a combinatorial point of view, such networks are certain types
of rooted directed acyclic graphs whose leaves are labelled by, for example,
species. A number of mathematically interesting classes of phylogenetic
networks are known. These include the biologically relevant class of stable
phylogenetic networks whose members are defined via certain fold-up and un-fold
operations that link them with concepts arising within the theory of, for
example, graph fibrations. Despite this exciting link, the structural
complexity of stable phylogenetic networks is still relatively poorly
understood. Employing the popular tree-based, reticulation-visible, and
tree-child properties which allow one to gauge this complexity in one way or
another, we provide novel characterizations for when a stable phylogenetic
network satisfies either one of these three properties.
",Quantitative Biology,Physics
"Dirichlet's theorem and Jacobsthal's function   If $a$ and $d$ are relatively prime, we refer to the set of integers
congruent to $a$ mod $d$ as an `eligible' arithmetic progression. A theorem of
Dirichlet says that every eligible arithmetic progression contains infinitely
many primes; the theorem follows from the assertion that every eligible
arithmetic progression contains at least one prime. The Jacobsthal function
$g(n)$ is defined as the smallest positive integer such that every sequence of
$g(n)$ consecutive integers contains an integer relatively prime to $n$. In
this paper, we show by a combinatorial argument that every eligible arithmetic
progression with $d\le76$ contains at least one prime, and we show that certain
plausible bounds on the Jacobsthal function of primorials would imply that
every eligible arithmetic progression contains at least one prime. That is,
certain plausible bounds on the Jacobsthal function would lead to an elementary
proof of Dirichlet's theorem.
",Mathematics,Mathematics
"Compactness of the resolvent for the Witten Laplacian   In this paper we consider the Witten Laplacian on 0-forms and give sufficient
conditions under which the Witten Laplacian admits a compact resolvent. These
conditions are imposed on the potential itself, involving the control of high
order derivatives by lower ones, as well as the control of the positive
eigenvalues of the Hessian matrix. This compactness criterion for resolvent is
inspired by the one for the Fokker-Planck operator. Our method relies on the
nilpotent group techniques developed by Helffer-Nourrigat [Hypoellipticité
maximale pour des opérateurs polynômes de champs de vecteurs, 1985].
",Mathematics,Mathematics
"Electric Vehicle Charging Station Placement Method for Urban Areas   For accommodating more electric vehicles (EVs) to battle against fossil fuel
emission, the problem of charging station placement is inevitable and could be
costly if done improperly. Some researches consider a general setup, using
conditions such as driving ranges for planning. However, most of the EV growths
in the next decades will happen in the urban area, where driving ranges is not
the biggest concern. For such a need, we consider several practical aspects of
urban systems, such as voltage regulation cost and protection device upgrade
resulting from the large integration of EVs. Notably, our diversified objective
can reveal the trade-off between different factors in different cities
worldwide. To understand the global optimum of large-scale analysis, we add
constraint one-by-one to see how to preserve the problem convexity. Our
sensitivity analysis before and after convexification shows that our approach
is not only universally applicable but also has a small approximation error for
prioritizing the most urgent constraint in a specific setup. Finally, numerical
results demonstrate the trade-off, the relationship between different factors
and the global objective, and the small approximation error. A unique
observation in this study shows the importance of incorporating the protection
device upgrade in urban system planning on charging stations.
",Computer Science,Computer Science
"Formation of coalition structures as a non-cooperative game   Traditionally social sciences are interested in structuring people in
multiple groups based on their individual preferences. This pa- per suggests an
approach to this problem in the framework of a non- cooperative game theory.
Definition of a suggested finite game includes a family of nested simultaneous
non-cooperative finite games with intra- and inter-coalition externalities. In
this family, games differ by the size of maximum coalition, partitions and by
coalition structure formation rules. A result of every game consists of
partition of players into coalitions and a payoff? profiles for every player.
Every game in the family has an equilibrium in mixed strategies with possibly
more than one coalition. The results of the game differ from those
conventionally discussed in cooperative game theory, e.g. the Shapley value,
strong Nash, coalition-proof equilibrium, core, kernel, nucleolus. We discuss
the following applications of the new game: cooperation as an allocation in one
coalition, Bayesian games, stochastic games and construction of a
non-cooperative criterion of coalition structure stability for studying focal
points.
",Computer Science; Mathematics,Computer Science; Physics
"The QLBS Q-Learner Goes NuQLear: Fitted Q Iteration, Inverse RL, and Option Portfolios   The QLBS model is a discrete-time option hedging and pricing model that is
based on Dynamic Programming (DP) and Reinforcement Learning (RL). It combines
the famous Q-Learning method for RL with the Black-Scholes (-Merton) model's
idea of reducing the problem of option pricing and hedging to the problem of
optimal rebalancing of a dynamic replicating portfolio for the option, which is
made of a stock and cash. Here we expand on several NuQLear (Numerical
Q-Learning) topics with the QLBS model. First, we investigate the performance
of Fitted Q Iteration for a RL (data-driven) solution to the model, and
benchmark it versus a DP (model-based) solution, as well as versus the BSM
model. Second, we develop an Inverse Reinforcement Learning (IRL) setting for
the model, where we only observe prices and actions (re-hedges) taken by a
trader, but not rewards. Third, we outline how the QLBS model can be used for
pricing portfolios of options, rather than a single option in isolation, thus
providing its own, data-driven and model independent solution to the (in)famous
volatility smile problem of the Black-Scholes model.
",Quantitative Finance,Quantitative Finance
"Monte Carlo modified profile likelihood in models for clustered data   The main focus of the analysts who deal with clustered data is usually not on
the clustering variables, and hence the group-specific parameters are treated
as nuisance. If a fixed effects formulation is preferred and the total number
of clusters is large relative to the single-group sizes, classical frequentist
techniques relying on the profile likelihood are often misleading. The use of
alternative tools, such as modifications to the profile likelihood or
integrated likelihoods, for making accurate inference on a parameter of
interest can be complicated by the presence of nonstandard modelling and/or
sampling assumptions. We show here how to employ Monte Carlo simulation in
order to approximate the modified profile likelihood in some of these
unconventional frameworks. The proposed solution is widely applicable and is
shown to retain the usual properties of the modified profile likelihood. The
approach is examined in two instances particularly relevant in applications,
i.e. missing-data models and survival models with unspecified censoring
distribution. The effectiveness of the proposed solution is validated via
simulation studies and two clinical trial applications.
",Statistics,Statistics
"Comparison of methods for early-readmission prediction in a high-dimensional heterogeneous covariates and time-to-event outcome framework   Background: Choosing the most performing method in terms of outcome
prediction or variables selection is a recurring problem in prognosis studies,
leading to many publications on methods comparison. But some aspects have
received little attention. First, most comparison studies treat prediction
performance and variable selection aspects separately. Second, methods are
either compared within a binary outcome setting (based on an arbitrarily chosen
delay) or within a survival setting, but not both. In this paper, we propose a
comparison methodology to weight up those different settings both in terms of
prediction and variables selection, while incorporating advanced machine
learning strategies. Methods: Using a high-dimensional case study on a
sickle-cell disease (SCD) cohort, we compare 8 statistical methods. In the
binary outcome setting, we consider logistic regression (LR), support vector
machine (SVM), random forest (RF), gradient boosting (GB) and neural network
(NN); while on the survival analysis setting, we consider the Cox Proportional
Hazards (PH), the CURE and the C-mix models. We then compare performances of
all methods both in terms of risk prediction and variable selection, with a
focus on the use of Elastic-Net regularization technique. Results: Among all
assessed statistical methods assessed, the C-mix model yields the better
performances in both the two considered settings, as well as interesting
interpretation aspects. There is some consistency in selected covariates across
methods within a setting, but not much across the two settings. Conclusions: It
appears that learning withing the survival setting first, and then going back
to a binary prediction using the survival estimates significantly enhance
binary predictions.
",Statistics,Statistics
"Asteroid 2017 FZ2 et al.: signs of recent mass-shedding from YORP?   The first direct detection of the asteroidal YORP effect, a phenomenon that
changes the spin states of small bodies due to thermal reemission of sunlight
from their surfaces, was obtained for (54509) YORP 2000 PH5. Such an alteration
can slowly increase the rotation rate of asteroids, driving them to reach their
fission limit and causing their disruption. This process can produce binaries
and unbound asteroid pairs. Secondary fission opens the door to the eventual
formation of transient but genetically-related groupings. Here, we show that
the small near-Earth asteroid (NEA) 2017 FZ2 was a co-orbital of our planet of
the quasi-satellite type prior to their close encounter on 2017 March 23.
Because of this flyby with the Earth, 2017 FZ2 has become a non-resonant NEA.
Our N-body simulations indicate that this object may have experienced
quasi-satellite engagements with our planet in the past and it may return as a
co-orbital in the future. We identify a number of NEAs that follow similar
paths, the largest named being YORP, which is also an Earth's co-orbital. An
apparent excess of NEAs moving in these peculiar orbits is studied within the
framework of two orbit population models. A possibility that emerges from this
analysis is that such an excess, if real, could be the result of mass shedding
from YORP itself or a putative larger object that produced YORP. Future
spectroscopic observations of 2017 FZ2 during its next visit in 2018 (and of
related objects when feasible) may be able to confirm or reject this
interpretation.
",Physics,Physics
"A Generative Model for Exploring Structure Regularities in Attributed Networks   Many real-world networks known as attributed networks contain two types of
information: topology information and node attributes. It is a challenging task
on how to use these two types of information to explore structural
regularities. In this paper, by characterizing potential relationship between
link communities and node attributes, a principled statistical model named
PSB_PG that generates link topology and node attributes is proposed. This model
for generating links is based on the stochastic blockmodels following a Poisson
distribution. Therefore, it is capable of detecting a wide range of network
structures including community structures, bipartite structures and other
mixture structures. The model for generating node attributes assumes that node
attributes are high dimensional and sparse and also follow a Poisson
distribution. This makes the model be uniform and the model parameters can be
directly estimated by expectation-maximization (EM) algorithm. Experimental
results on artificial networks and real networks containing various structures
have shown that the proposed model PSB_PG is not only competitive with the
state-of-the-art models, but also provides good semantic interpretation for
each community via the learned relationship between the community and its
related attributes.
",Computer Science,Computer Science
"Conversion of Mersenne Twister to double-precision floating-point numbers   The 32-bit Mersenne Twister generator MT19937 is a widely used random number
generator. To generate numbers with more than 32 bits in bit length, and
particularly when converting into 53-bit double-precision floating-point
numbers in $[0,1)$ in the IEEE 754 format, the typical implementation
concatenates two successive 32-bit integers and divides them by a power of $2$.
In this case, the 32-bit MT19937 is optimized in terms of its equidistribution
properties (the so-called dimension of equidistribution with $v$-bit accuracy)
under the assumption that one will mainly be using 32-bit output values, and
hence the concatenation sometimes degrades the dimension of equidistribution
compared with the simple use of 32-bit outputs. In this paper, we analyze such
phenomena by investigating hidden $\mathbb{F}_2$-linear relations among the
bits of high-dimensional outputs. Accordingly, we report that MT19937 with a
specific lag set fails several statistical tests, such as the overlapping
collision test, matrix rank test, and Hamming independence test.
",Computer Science; Statistics,Computer Science
"Hyperplane arrangements associated to symplectic quotient singularities   We study the hyperplane arrangements associated, via the minimal model
programme, to symplectic quotient singularities. We show that this hyperplane
arrangement equals the arrangement of CM-hyperplanes coming from the
representation theory of restricted rational Cherednik algebras. We explain
some of the interesting consequences of this identification for the
representation theory of restricted rational Cherednik algebras. We also show
that the Calogero-Moser space is smooth if and only if the Calogero-Moser
families are trivial. We describe the arrangements of CM-hyperplanes associated
to several exceptional complex reflection groups, some of which are free.
",Mathematics,Mathematics
"Online control of the false discovery rate with decaying memory   In the online multiple testing problem, p-values corresponding to different
null hypotheses are observed one by one, and the decision of whether or not to
reject the current hypothesis must be made immediately, after which the next
p-value is observed. Alpha-investing algorithms to control the false discovery
rate (FDR), formulated by Foster and Stine, have been generalized and applied
to many settings, including quality-preserving databases in science and
multiple A/B or multi-armed bandit tests for internet commerce. This paper
improves the class of generalized alpha-investing algorithms (GAI) in four
ways: (a) we show how to uniformly improve the power of the entire class of
monotone GAI procedures by awarding more alpha-wealth for each rejection,
giving a win-win resolution to a recent dilemma raised by Javanmard and
Montanari, (b) we demonstrate how to incorporate prior weights to indicate
domain knowledge of which hypotheses are likely to be non-null, (c) we allow
for differing penalties for false discoveries to indicate that some hypotheses
may be more important than others, (d) we define a new quantity called the
decaying memory false discovery rate (mem-FDR) that may be more meaningful for
truly temporal applications, and which alleviates problems that we describe and
refer to as ""piggybacking"" and ""alpha-death"". Our GAI++ algorithms incorporate
all four generalizations simultaneously, and reduce to more powerful variants
of earlier algorithms when the weights and decay are all set to unity. Finally,
we also describe a simple method to derive new online FDR rules based on an
estimated false discovery proportion.
",Computer Science; Mathematics; Statistics,Computer Science; Statistics
"A Maximum Matching Algorithm for Basis Selection in Spectral Learning   We present a solution to scale spectral algorithms for learning sequence
functions. We are interested in the case where these functions are sparse (that
is, for most sequences they return 0). Spectral algorithms reduce the learning
problem to the task of computing an SVD decomposition over a special type of
matrix called the Hankel matrix. This matrix is designed to capture the
relevant statistics of the training sequences. What is crucial is that to
capture long range dependencies we must consider very large Hankel matrices.
Thus the computation of the SVD becomes a critical bottleneck. Our solution
finds a subset of rows and columns of the Hankel that realizes a compact and
informative Hankel submatrix. The novelty lies in the way that this subset is
selected: we exploit a maximal bipartite matching combinatorial algorithm to
look for a sub-block with full structural rank, and show how computation of
this sub-block can be further improved by exploiting the specific structure of
Hankel matrices.
",Computer Science; Statistics,Computer Science; Statistics
"Tilings with noncongruent triangles   We solve a problem of R. Nandakumar by proving that there is no tiling of the
plane with pairwise noncongruent triangles of equal area and equal perimeter.
We also show that no convex polygon with more than three sides can be tiled
with finitely many triangles such that no pair of them share a full side.
",Computer Science; Mathematics,Mathematics
"A bound on partitioning clusters   Let $X$ be a finite collection of sets (or ""clusters""). We consider the
problem of counting the number of ways a cluster $A \in X$ can be partitioned
into two disjoint clusters $A_1, A_2 \in X$, thus $A = A_1 \uplus A_2$ is the
disjoint union of $A_1$ and $A_2$; this problem arises in the run time analysis
of the ASTRAL algorithm in phylogenetic reconstruction. We obtain the bound $$
| \{ (A_1,A_2,A) \in X \times X \times X: A = A_1 \uplus A_2 \} | \leq
|X|^{3/p} $$ where $|X|$ denotes the cardinality of $X$, and $p := \log_3
\frac{27}{4} = 1.73814\dots$, so that $\frac{3}{p} = 1.72598\dots$.
Furthermore, the exponent $p$ cannot be replaced by any larger quantity. This
improves upon the trivial bound of $|X|^2$. The argument relies on establishing
a one-dimensional convolution inequality that can be established by elementary
calculus combined with some numerical verification.
In a similar vein, we show that for any subset $A$ of a discrete cube
$\{0,1\}^n$, the additive energy of $A$ (the number of quadruples
$(a_1,a_2,a_3,a_4)$ in $A^4$ with $a_1+a_2=a_3+a_4$) is at most $|A|^{\log_2
6}$, and that this exponent is best possible.
",Mathematics,Mathematics
"Algebraic Bethe ansatz for the trigonometric sl(2) Gaudin model with triangular boundary   In the derivation of the generating function of the Gaudin Hamiltonians with
boundary terms, we follow the same approach used previously in the rational
case, which in turn was based on Sklyanin's method in the periodic case. Our
derivation is centered on the quasi-classical expansion of the linear
combination of the transfer matrix of the XXZ Heisenberg spin chain and the
central element, the so-called Sklyanin determinant. The corresponding Gaudin
Hamiltonians with boundary terms are obtained as the residues of the generating
function. By defining the appropriate Bethe vectors which yield strikingly
simple off-shell action of the generating function, we fully implement the
algebraic Bethe ansatz, obtaining the spectrum of the generating function and
the corresponding Bethe equations.
",Physics,Mathematics
"Similarity-based Multi-label Learning   Multi-label classification is an important learning problem with many
applications. In this work, we propose a principled similarity-based approach
for multi-label learning called SML. We also introduce a similarity-based
approach for predicting the label set size. The experimental results
demonstrate the effectiveness of SML for multi-label classification where it is
shown to compare favorably with a wide variety of existing algorithms across a
range of evaluation criterion.
",Computer Science; Statistics,Computer Science; Statistics
"Machine learning for graph-based representations of three-dimensional discrete fracture networks   Structural and topological information play a key role in modeling flow and
transport through fractured rock in the subsurface. Discrete fracture network
(DFN) computational suites such as dfnWorks are designed to simulate flow and
transport in such porous media. Flow and transport calculations reveal that a
small backbone of fractures exists, where most flow and transport occurs.
Restricting the flowing fracture network to this backbone provides a
significant reduction in the network's effective size. However, the particle
tracking simulations needed to determine the reduction are computationally
intensive. Such methods may be impractical for large systems or for robust
uncertainty quantification of fracture networks, where thousands of forward
simulations are needed to bound system behavior.
In this paper, we develop an alternative network reduction approach to
characterizing transport in DFNs, by combining graph theoretical and machine
learning methods. We consider a graph representation where nodes signify
fractures and edges denote their intersections. Using random forest and support
vector machines, we rapidly identify a subnetwork that captures the flow
patterns of the full DFN, based primarily on node centrality features in the
graph. Our supervised learning techniques train on particle-tracking backbone
paths found by dfnWorks, but run in negligible time compared to those
simulations. We find that our predictions can reduce the network to
approximately 20% of its original size, while still generating breakthrough
curves consistent with those of the original network.
",Computer Science; Physics; Statistics,Computer Science; Statistics
"Phase reduction and synchronization of a network of coupled dynamical elements exhibiting collective oscillations   A general phase reduction method for a network of coupled dynamical elements
exhibiting collective oscillations, which is applicable to arbitrary networks
of heterogeneous dynamical elements, is developed. A set of coupled adjoint
equations for phase sensitivity functions, which characterize phase response of
the collective oscillation to small perturbations applied to individual
elements, is derived. Using the phase sensitivity functions, collective
oscillation of the network under weak perturbation can be described
approximately by a one-dimensional phase equation. As an example, mutual
synchronization between a pair of collectively oscillating networks of
excitable and oscillatory FitzHugh-Nagumo elements with random coupling is
studied.
",Physics,Physics
"Pachinko Prediction: A Bayesian method for event prediction from social media data   The combination of large open data sources with machine learning approaches
presents a potentially powerful way to predict events such as protest or social
unrest. However, accounting for uncertainty in such models, particularly when
using diverse, unstructured datasets such as social media, is essential to
guarantee the appropriate use of such methods. Here we develop a Bayesian
method for predicting social unrest events in Australia using social media
data. This method uses machine learning methods to classify individual postings
to social media as being relevant, and an empirical Bayesian approach to
calculate posterior event probabilities. We use the method to predict events in
Australian cities over a period in 2017/18.
",Computer Science,Statistics
"Numerical study of the Kadomtsev--Petviashvili equation and dispersive shock waves   A detailed numerical study of the long time behaviour of dispersive shock
waves in solutions to the Kadomtsev-Petviashvili (KP) I equation is presented.
It is shown that modulated lump solutions emerge from the dispersive shock
waves. For the description of dispersive shock waves, Whitham modulation
equations for KP are obtained. It is shown that the modulation equations near
the soliton line are hyperbolic for the KPII equation while they are elliptic
for the KPI equation leading to a focusing effect and the formation of lumps.
Such a behaviour is similar to the appearance of breathers for the focusing
nonlinear Schrodinger equation in the semiclassical limit.
",Physics; Mathematics,Physics
"Sensitivity of Love and quasi-Rayleigh waves to model parameters   We examine the sensitivity of the Love and the quasi-Rayleigh waves to model
parameters. Both waves are guided waves that propagate in the same model of an
elastic layer above an elastic halfspace. We study their dispersion curves
without any simplifying assumptions, beyond the standard approach of elasticity
theory in isotropic media. We examine the sensitivity of both waves to
elasticity parameters, frequency and layer thickness, for varying frequency and
different modes. In the case of Love waves, we derive and plot the absolute
value of a dimensionless sensitivity coefficient in terms of partial
derivatives, and perform an analysis to find the optimum frequency for
determining the layer thickness. For a coherency of the background information,
we briefly review the Love-wave dispersion relation and provide details of the
less common derivation of the quasi-Rayleigh relation in an appendix. We
compare that derivation to past results in the literature, finding certain
discrepancies among them.
",Physics,Physics
"A topological lower bound for the energy of a unit vector field on a closed Euclidean hypersurface   For a unit vector field on a closed immersed Euclidean hypersurface
$M^{2n+1}$, $n\geq 1$, we exhibit a nontrivial lower bound for its energy which
depends on the degree of the Gauss map of the immersion. When the hypersurface
is the unit sphere $\mathbb{S}^{2n+1}$, immersed with degree one, this lower
bound corresponds to a well established value from the literature. We introduce
a list of functionals $\mathcal{B}_k$ on a compact Riemannian manifold $M^{m}$,
$1\leq k\leq m$, and show that, when the underlying manifold is a closed
hypersurface, these functionals possess similar properties regarding the degree
of the immersion. In addition, we prove that Hopf flows minimize
$\mathcal{B}_n$ on $\mathbb{S}^{2n+1}$.
",Mathematics,Mathematics
"Bayesian Learning of Consumer Preferences for Residential Demand Response   In coming years residential consumers will face real-time electricity tariffs
with energy prices varying day to day, and effective energy saving will require
automation - a recommender system, which learns consumer's preferences from her
actions. A consumer chooses a scenario of home appliance use to balance her
comfort level and the energy bill. We propose a Bayesian learning algorithm to
estimate the comfort level function from the history of appliance use. In
numeric experiments with datasets generated from a simulation model of a
consumer interacting with small home appliances the algorithm outperforms
popular regression analysis tools. Our approach can be extended to control an
air heating and conditioning system, which is responsible for up to half of a
household's energy bill.
",Computer Science; Statistics,Computer Science; Statistics
"Privacy-Preserving Economic Dispatch in Competitive Electricity Market   With the emerging of smart grid techniques, cyber attackers may be able to
gain access to critical energy infrastructure data and strategic market
participants may be able to identify offer prices of their rivals. This paper
discusses a privacy-preserving economic dispatch approach in competitive
electricity market, in which individual generation companies (GENCOs) and load
serving entities (LSEs) can mask their actual bidding information and physical
data by multiplying with random numbers before submitting to Independent System
Operators (ISOs) and Regional Transmission Owners (RTOs). This would avoid
potential information leakage of critical energy infrastructure and financial
data of market participants. The optimal solution to the original ED problem,
including optimal dispatches of generators and loads and locational marginal
prices (LMPs), can be retrieved from the optimal solution of the proposed
privacy-preserving ED approach. Numerical case studies show the effectiveness
of the proposed approach for protecting private information of individual
market participants while guaranteeing the same optimal ED solution.
Computation and communication costs of the proposed privacy-preserving ED
approach and the original ED are also compared in case studies.
",Mathematics,Computer Science
"An influence-based fast preceding questionnaire model for elderly assessments   To improve the efficiency of elderly assessments, an influence-based fast
preceding questionnaire model (FPQM) is proposed. Compared with traditional
assessments, the FPQM optimizes questionnaires by reordering their attributes.
The values of low-ranking attributes can be predicted by the values of the
high-ranking attributes. Therefore, the number of attributes can be reduced
without redesigning the questionnaires. A new function for calculating the
influence of the attributes is proposed based on probability theory. Reordering
and reducing algorithms are given based on the attributes' influences. The
model is verified through a practical application. The practice in an
elderly-care company shows that the FPQM can reduce the number of attributes by
90.56% with a prediction accuracy of 98.39%. Compared with other methods, such
as the Expert Knowledge, Rough Set and C4.5 methods, the FPQM achieves the best
performance. In addition, the FPQM can also be applied to other questionnaires.
",Computer Science,Computer Science; Statistics
"Homotopy Parametric Simplex Method for Sparse Learning   High dimensional sparse learning has imposed a great computational challenge
to large scale data analysis. In this paper, we are interested in a broad class
of sparse learning approaches formulated as linear programs parametrized by a
{\em regularization factor}, and solve them by the parametric simplex method
(PSM). Our parametric simplex method offers significant advantages over other
competing methods: (1) PSM naturally obtains the complete solution path for all
values of the regularization parameter; (2) PSM provides a high precision dual
certificate stopping criterion; (3) PSM yields sparse solutions through very
few iterations, and the solution sparsity significantly reduces the
computational cost per iteration. Particularly, we demonstrate the superiority
of PSM over various sparse learning approaches, including Dantzig selector for
sparse linear regression, LAD-Lasso for sparse robust linear regression, CLIME
for sparse precision matrix estimation, sparse differential network estimation,
and sparse Linear Programming Discriminant (LPD) analysis. We then provide
sufficient conditions under which PSM always outputs sparse solutions such that
its computational performance can be significantly boosted. Thorough numerical
experiments are provided to demonstrate the outstanding performance of the PSM
method.
",Computer Science; Mathematics; Statistics,Computer Science; Statistics
"Volumetric Super-Resolution of Multispectral Data   Most multispectral remote sensors (e.g. QuickBird, IKONOS, and Landsat 7
ETM+) provide low-spatial high-spectral resolution multispectral (MS) or
high-spatial low-spectral resolution panchromatic (PAN) images, separately. In
order to reconstruct a high-spatial/high-spectral resolution multispectral
image volume, either the information in MS and PAN images are fused (i.e.
pansharpening) or super-resolution reconstruction (SRR) is used with only MS
images captured on different dates. Existing methods do not utilize temporal
information of MS and high spatial resolution of PAN images together to improve
the resolution. In this paper, we propose a multiframe SRR algorithm using
pansharpened MS images, taking advantage of both temporal and spatial
information available in multispectral imagery, in order to exceed spatial
resolution of given PAN images. We first apply pansharpening to a set of
multispectral images and their corresponding PAN images captured on different
dates. Then, we use the pansharpened multispectral images as input to the
proposed wavelet-based multiframe SRR method to yield full volumetric SRR. The
proposed SRR method is obtained by deriving the subband relations between
multitemporal MS volumes. We demonstrate the results on Landsat 7 ETM+ images
comparing our method to conventional techniques.
",Computer Science,Computer Science
"Propagation in media as a probe for topological properties   The central goal of this thesis is to develop methods to experimentally study
topological phases. We do so by applying the powerful toolbox of quantum
simulation techniques with cold atoms in optical lattices. To this day, a
complete classification of topological phases remains elusive. In this context,
experimental studies are key, both for studying the interplay between topology
and complex effects and for identifying new forms of topological order. It is
therefore crucial to find complementary means to measure topological properties
in order to reach a fundamental understanding of topological phases. In one
dimensional chiral systems, we suggest a new way to construct and identify
topologically protected bound states, which are the smoking gun of these
materials. In two dimensional Hofstadter strips (i.e: systems which are very
short along one dimension), we suggest a new way to measure the topological
invariant directly from the atomic dynamics.
",Physics,Physics
"$\mathcal{P}$-schemes and Deterministic Polynomial Factoring over Finite Fields   We introduce a family of mathematical objects called $\mathcal{P}$-schemes,
where $\mathcal{P}$ is a poset of subgroups of a finite group $G$. A
$\mathcal{P}$-scheme is a collection of partitions of the right coset spaces
$H\backslash G$, indexed by $H\in\mathcal{P}$, that satisfies a list of axioms.
These objects generalize the classical notion of association schemes as well as
the notion of $m$-schemes (Ivanyos et al. 2009).
Based on $\mathcal{P}$-schemes, we develop a unifying framework for the
problem of deterministic factoring of univariate polynomials over finite fields
under the generalized Riemann hypothesis (GRH).
",Computer Science; Mathematics,Mathematics
"Breakthrough revisited: investigating the requirements for growth of dust beyond the bouncing barrier   For grain growth to proceed effectively and lead to planet formation a number
of barriers to growth must be overcome. One such barrier, relevant for compact
grains in the inner regions of the disc, is the `bouncing barrier' in which
large grains ($\sim$ mm size) tend to bounce off each other rather than
sticking. However, by maintaining a population of small grains it has been
suggested that cm-size particles may grow rapidly by sweeping up these small
grains. We present the first numerically resolved investigation into the
conditions under which grains may be lucky enough to grow beyond the bouncing
barrier by a series of rare collisions leading to growth (so-called
`breakthrough'). Our models support previous results, and show that in simple
models breakthrough requires the mass ratio at which high velocity collisions
transition to growth instead of causing fragmentation to be low, $\phi \lesssim
50$. However, in models that take into account the dependence of the
fragmentation threshold on mass-ratio, we find breakthrough occurs more
readily, even if mass transfer is relatively inefficient. This suggests that
bouncing may only slow down growth, rather than preventing growth beyond a
threshold barrier. However, even when growth beyond the bouncing barrier is
possible, radial drift will usually prevent growth to arbitrarily large sizes.
",Physics,Physics
"Bayesian uncertainty quantification in linear models for diffusion MRI   Diffusion MRI (dMRI) is a valuable tool in the assessment of tissue
microstructure. By fitting a model to the dMRI signal it is possible to derive
various quantitative features. Several of the most popular dMRI signal models
are expansions in an appropriately chosen basis, where the coefficients are
determined using some variation of least-squares. However, such approaches lack
any notion of uncertainty, which could be valuable in e.g. group analyses. In
this work, we use a probabilistic interpretation of linear least-squares
methods to recast popular dMRI models as Bayesian ones. This makes it possible
to quantify the uncertainty of any derived quantity. In particular, for
quantities that are affine functions of the coefficients, the posterior
distribution can be expressed in closed-form. We simulated measurements from
single- and double-tensor models where the correct values of several quantities
are known, to validate that the theoretically derived quantiles agree with
those observed empirically. We included results from residual bootstrap for
comparison and found good agreement. The validation employed several different
models: Diffusion Tensor Imaging (DTI), Mean Apparent Propagator MRI (MAP-MRI)
and Constrained Spherical Deconvolution (CSD). We also used in vivo data to
visualize maps of quantitative features and corresponding uncertainties, and to
show how our approach can be used in a group analysis to downweight subjects
with high uncertainty. In summary, we convert successful linear models for dMRI
signal estimation to probabilistic models, capable of accurate uncertainty
quantification.
",Physics; Statistics,Statistics
"Relativistic Spacecraft Propelled by Directed Energy   Achieving relativistic flight to enable extrasolar exploration is one of the
dreams of humanity and the long term goal of our NASA Starlight program. We
derive a fully relativistic solution for the motion of a spacecraft propelled
by radiation pressure from a directed energy system. Depending on the system
parameters, low mass spacecraft can achieve relativistic speeds; thereby
enabling interstellar exploration. The diffraction of the directed energy
system plays an important role and limits the maximum speed of the spacecraft.
We consider 'photon recycling' as a possible method to achieving higher speeds.
We also discuss recent claims that our previous work on this topic is incorrect
and show that these claims arise from an improper treatment of causality.
",Physics,Physics
"Adaptive Similar Triangles Method: a Stable Alternative to Sinkhorn's Algorithm for Regularized Optimal Transport   In this paper, we are motivated by two important applications:
entropy-regularized optimal transport problem and road or IP traffic demand
matrix estimation by entropy model. Both of them include solving a special type
of optimization problem with linear equality constraints and objective given as
a sum of an entropy regularizer and a linear function. It is known that the
state-of-the-art solvers for this problem, which are based on Sinkhorn's method
(also known as RSA or balancing method), can fail to work, when the
entropy-regularization parameter is small. We consider the above optimization
problem as a particular instance of a general strongly convex optimization
problem with linear constraints. We propose a new algorithm to solve this
general class of problems. Our approach is based on the transition to the dual
problem. First, we introduce a new accelerated gradient method with adaptive
choice of gradient's Lipschitz constant. Then, we apply this method to the dual
problem and show, how to reconstruct an approximate solution to the primal
problem with provable convergence rate. We prove the rate $O(1/k^2)$, $k$ being
the iteration counter, both for the absolute value of the primal objective
residual and constraints infeasibility. Our method has similar to Sinkhorn's
method complexity of each iteration, but is faster and more stable numerically,
when the regularization parameter is small. We illustrate the advantage of our
method by numerical experiments for the two mentioned applications. We show
that there exists a threshold, such that, when the regularization parameter is
smaller than this threshold, our method outperforms the Sinkhorn's method in
terms of computation time.
",Mathematics,Computer Science; Mathematics; Statistics
"Automatic Detection of Cyberbullying in Social Media Text   While social media offer great communication opportunities, they also
increase the vulnerability of young people to threatening situations online.
Recent studies report that cyberbullying constitutes a growing problem among
youngsters. Successful prevention depends on the adequate detection of
potentially harmful messages and the information overload on the Web requires
intelligent systems to identify potential risks automatically. The focus of
this paper is on automatic cyberbullying detection in social media text by
modelling posts written by bullies, victims, and bystanders of online bullying.
We describe the collection and fine-grained annotation of a training corpus for
English and Dutch and perform a series of binary classification experiments to
determine the feasibility of automatic cyberbullying detection. We make use of
linear support vector machines exploiting a rich feature set and investigate
which information sources contribute the most for this particular task.
Experiments on a holdout test set reveal promising results for the detection of
cyberbullying-related posts. After optimisation of the hyperparameters, the
classifier yields an F1-score of 64% and 61% for English and Dutch
respectively, and considerably outperforms baseline systems based on keywords
and word unigrams.
",Computer Science,Computer Science
"Personalized and Private Peer-to-Peer Machine Learning   The rise of connected personal devices together with privacy concerns call
for machine learning algorithms capable of leveraging the data of a large
number of agents to learn personalized models under strong privacy
requirements. In this paper, we introduce an efficient algorithm to address the
above problem in a fully decentralized (peer-to-peer) and asynchronous fashion,
with provable convergence rate. We show how to make the algorithm
differentially private to protect against the disclosure of information about
the personal datasets, and formally analyze the trade-off between utility and
privacy. Our experiments show that our approach dramatically outperforms
previous work in the non-private case, and that under privacy constraints, we
can significantly improve over models learned in isolation.
",Computer Science; Statistics,Computer Science; Statistics
"On Efficiently Detecting Overlapping Communities over Distributed Dynamic Graphs   Modern networks are of huge sizes as well as high dynamics, which challenges
the efficiency of community detection algorithms. In this paper, we study the
problem of overlapping community detection on distributed and dynamic graphs.
Given a distributed, undirected and unweighted graph, the goal is to detect
overlapping communities incrementally as the graph is dynamically changing. We
propose an efficient algorithm, called \textit{randomized Speaker-Listener
Label Propagation Algorithm} (rSLPA), based on the \textit{Speaker-Listener
Label Propagation Algorithm} (SLPA) by relaxing the probability distribution of
label propagation. Besides detecting high-quality communities, rSLPA can
incrementally update the detected communities after a batch of edge insertion
and deletion operations. To the best of our knowledge, rSLPA is the first
algorithm that can incrementally capture the same communities as those obtained
by applying the detection algorithm from the scratch on the updated graph.
Extensive experiments are conducted on both synthetic and real-world datasets,
and the results show that our algorithm can achieve high accuracy and
efficiency at the same time.
",Computer Science,Computer Science
"Excited states of defect lines in silicon: A first-principles study based on hydrogen cluster analogues   Excited states of a single donor in bulk silicon have previously been studied
extensively based on effective mass theory. However, a proper theoretical
description of the excited states of a donor cluster is still scarce. Here we
study the excitations of lines of defects within a single-valley spherical band
approximation, thus mapping the problem to a scaled hydrogen atom array. A
series of detailed full configuration-interaction and time-dependent hybrid
density-functional theory calculations have been performed to understand linear
clusters of up to 10 donors. Our studies illustrate the generic features of
their excited states, addressing the competition between formation of
inter-donor ionic states and intra-donor atomic excited states. At short
inter-donor distances, excited states of donor molecules are dominant, at
intermediate distances ionic states play an important role, and at long
distances the intra-donor excitations are predominant as expected. The
calculations presented here emphasise the importance of correlations between
donor electrons, and are thus complementary to other recent approaches that
include effective mass anisotropy and multi-valley effects. The exchange
splittings between relevant excited states have also been estimated for a donor
pair and for a three-donor arrays; the splittings are much larger than those in
the ground state in the range of donor separations between 10 and 20 nm. This
establishes a solid theoretical basis for the use of excited-state exchange
interactions for controllable quantum gate operations in silicon.
",Physics,Physics
"Validation of small Kepler transiting planet candidates in or near the habitable zone   A main goal of NASA's Kepler Mission is to establish the frequency of
potentially habitable Earth-size planets (eta Earth). Relatively few such
candidates identified by the mission can be confirmed to be rocky via dynamical
measurement of their mass. Here we report an effort to validate 18 of them
statistically using the BLENDER technique, by showing that the likelihood they
are true planets is far greater than that of a false positive. Our analysis
incorporates follow-up observations including high-resolution optical and
near-infrared spectroscopy, high-resolution imaging, and information from the
analysis of the flux centroids of the Kepler observations themselves. While
many of these candidates have been previously validated by others, the
confidence levels reported typically ignore the possibility that the planet may
transit a different star than the target along the same line of sight. If that
were the case, a planet that appears small enough to be rocky may actually be
considerably larger and therefore less interesting from the point of view of
habitability. We take this into consideration here, and are able to validate 15
of our candidates at a 99.73% (3 sigma) significance level or higher, and the
other three at slightly lower confidence. We characterize the GKM host stars
using available ground-based observations and provide updated parameters for
the planets, with sizes between 0.8 and 2.9 Earth radii. Seven of them
(KOI-0438.02, 0463.01, 2418.01, 2626.01, 3282.01, 4036.01, and 5856.01) have a
better than 50% chance of being smaller than 2 Earth radii and being in the
habitable zone of their host stars.
",Physics,Physics
"Revisiting the quest for a universal log-law and the role of pressure gradient in ""canonical"" wall-bounded turbulent flows   The trinity of so-called ""canonical"" wall-bounded turbulent flows, comprising
the zero pressure gradient turbulent boundary layer, abbreviated ZPG TBL,
turbulent pipe flow and channel/duct flows has continued to receive intense
attention as new and more reliable experimental data have become available.
Nevertheless, the debate on whether the logarithmic part of the mean velocity
profile, in particular the Kármán constant $\kappa$, is identical for these
three canonical flows or flow-dependent is still ongoing. In this paper, which
expands upon Monkewitz and Nagib (24th ICTAM Conf., Montreal, 2016), the
asymptotic matching requirement of equal $\kappa$ in the log-law and in the
expression for the centerline/free-stream velocity is reiterated and shown to
preclude a single universal log-law in the three canonical flows or at least
make it very unlikely. The current re-analysis of high quality mean velocity
profiles in ZPG TBL's, the Princeton ""Superpipe"" and in channels and ducts
leads to a coherent description of (almost) all seemingly contradictory data
interpretations in terms of TWO logarithmic regions in pipes and channels: A
universal interior, near-wall logarithmic region with the same parameters as in
the ZPG TBL, in particular $\kappa_{\mathrm{wall}} \cong 0.384$, but only
extending from around $150$ to around $10^3$ wall units, and shrinking with
increasing pressure gradient, followed by an exterior logarithmic region with a
flow specific $\kappa$ matching the logarithmic slope of the respective
free-stream or centerline velocity. The log-law parameters of the exterior
logarithmic region in channels and pipes are shown to depend monotonically on
the pressure gradient.
",Physics,Physics
"Deformations of coisotropic submanifolds in Jacobi manifolds   In this thesis, we study the deformation problem of coisotropic submanifolds
in Jacobi manifolds. In particular we attach two algebraic invariants to any
coisotropic submanifold $S$ in a Jacobi manifold, namely the
$L_\infty[1]$-algebra and the BFV-complex of $S$. Our construction generalizes
and unifies analogous constructions in symplectic, Poisson, and locally
conformal symplectic geometry. As a new special case we also attach an
$L_\infty[1]$-algebra and a BFV-complex to any coisotropic submanifold in a
contact manifold. The $L_\infty[1]$-algebra of $S$ controls the formal
coisotropic deformation problem of $S$, even under Hamiltonian equivalence. The
BFV-complex of $S$ controls the non-formal coisotropic deformation problem of
$S$, even under both Hamiltonian and Jacobi equivalence. In view of these
results, we exhibit, in the contact setting, two examples of coisotropic
submanifolds whose coisotropic deformation problem is obstructed.
",Mathematics,Mathematics
"San Pedro Meeting on Wide Field Variability Surveys: Some Concluding Comments   This is a written version of the closing talk at the 22nd Los Alamos Stellar
pulsation conference on wide field variability surveys. It comments on some of
the issues which arise from the meeting. These include the need for attention
to photometric standardization (especially in the infrared) and the somewhat
controversial problem of statistical bias in the use of parallaxes (and other
methods of distance determination). Some major advances in the use of pulsating
variables to study Galactic structure are mentioned. The paper includes a
clarification of apparently conflicting results from classical Cepheids and RR
Lyrae stars in the inner Galaxy and bulge. The importance of understanding
non-periodic phenomena in variable stars,particularly AGB variables and RCB
stars is stressed, especially for its relevance to mass-loss, in which
pulsation may only play a minor role.
",Physics,Physics
"On Vague Computers   Vagueness is something everyone is familiar with. In fact, most people think
that vagueness is closely related to language and exists only there. However,
vagueness is a property of the physical world. Quantum computers harness
superposition and entanglement to perform their computational tasks. Both
superposition and entanglement are vague processes. Thus quantum computers,
which process exact data without ""exploiting"" vagueness, are actually vague
computers.
",Computer Science,Computer Science
"Global aspects of polarization optics and coset space geometry   We use group theoretic ideas and coset space methods to deal with problems in
polarization optics of a global nature. These include the possibility of a
globally smooth phase convention for electric fields for all points on the
Poincaré sphere, and a similar possibility of real or complex bases of
transverse electric vectors for all possible propagation directions. It is
shown that these methods help in understanding some known results in an
effective manner, and in answering new questions as well. We find that apart
from the groups $SU(2)$ and $SO(3)$ which occur naturally in these problems,
the group $SU(3)$ also plays an important role.
",Physics,Physics
"Safety-Aware Apprenticeship Learning   Apprenticeship learning (AL) is a kind of Learning from Demonstration
techniques where the reward function of a Markov Decision Process (MDP) is
unknown to the learning agent and the agent has to derive a good policy by
observing an expert's demonstrations. In this paper, we study the problem of
how to make AL algorithms inherently safe while still meeting its learning
objective. We consider a setting where the unknown reward function is assumed
to be a linear combination of a set of state features, and the safety property
is specified in Probabilistic Computation Tree Logic (PCTL). By embedding
probabilistic model checking inside AL, we propose a novel
counterexample-guided approach that can ensure safety while retaining
performance of the learnt policy. We demonstrate the effectiveness of our
approach on several challenging AL scenarios where safety is essential.
",Computer Science,Computer Science; Statistics
"Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with JPEG Compression   Deep neural networks (DNNs) have achieved great success in solving a variety
of machine learning (ML) problems, especially in the domain of image
recognition. However, recent research showed that DNNs can be highly vulnerable
to adversarially generated instances, which look seemingly normal to human
observers, but completely confuse DNNs. These adversarial samples are crafted
by adding small perturbations to normal, benign images. Such perturbations,
while imperceptible to the human eye, are picked up by DNNs and cause them to
misclassify the manipulated instances with high confidence. In this work, we
explore and demonstrate how systematic JPEG compression can work as an
effective pre-processing step in the classification pipeline to counter
adversarial attacks and dramatically reduce their effects (e.g., Fast Gradient
Sign Method, DeepFool). An important component of JPEG compression is its
ability to remove high frequency signal components, inside square blocks of an
image. Such an operation is equivalent to selective blurring of the image,
helping remove additive perturbations. Further, we propose an ensemble-based
technique that can be constructed quickly from a given well-performing DNN, and
empirically show how such an ensemble that leverages JPEG compression can
protect a model from multiple types of adversarial attacks, without requiring
knowledge about the model.
",Computer Science,Computer Science; Statistics
"The Capacity of Some Classes of Polyhedra   K. Borsuk in 1979, in the Topological Conference in Moscow, introduced the
concept of the capacity of a compactum. In this paper, we compute the capacity
of the product of two spheres of the same or different dimensions and the
capacity of lense spaces. Also, we present an upper bound for the capacity of a
$\mathbb{Z}_n$-complex, i.e., a connected finite 2-dimensional CW-complex with
finite cyclic fundamental group $\mathbb{Z}_n$.
",Mathematics,Mathematics
"Collective decision for open set recognition   In open set recognition (OSR), almost all existing methods are designed
specially for recognizing individual instances, even these instances are
collectively coming in batch. Recognizers in decision either reject or
categorize them to some known class using empirically-set threshold. Thus the
threshold plays a key role, however, the selection for it usually depends on
the knowledge of known classes, inevitably incurring risks due to lacking
available information from unknown classes. On the other hand, a more realistic
OSR system should NOT just rest on a reject decision but should go further,
especially for discovering the hidden unknown classes among the reject
instances, whereas existing OSR methods do not pay special attention. In this
paper, we introduce a novel collective/batch decision strategy with an aim to
extend existing OSR for new class discovery while considering correlations
among the testing instances. Specifically, a collective decision-based OSR
framework (CD-OSR) is proposed by slightly modifying the Hierarchical Dirichlet
process (HDP). Thanks to the HDP, our CD-OSR does not need to define the
specific threshold and can automatically reserve space for unknown classes in
testing, naturally resulting in a new class discovery function. Finally,
extensive experiments on benchmark datasets indicate the validity of CD-OSR.
",Statistics,Computer Science; Statistics
"Local equilibrium in the Bak-Sneppen model   The Bak Sneppen (BS) model is a very simple model that exhibits all the
richness of self-organized criticality theory. At the thermodynamic limit, the
BS model converges to a situation where all particles have a fitness that is
uniformly distributed between a critical value $p_c$ and 1. The $p_c$ value is
unknown, as are the variables that influence and determine this value. Here, we
study the Bak Sneppen model in the case in which the lowest fitness particle
interacts with an arbitrary even number of $m$ nearest neighbors. We show that
$p_{c,m}$ verifies a simple local equilibrium relationship. Based on this
relationship, we can determine bounds for $p_{c,m}$.
",Physics,Physics
"On the Scientific Value of Large-scale Testbeds for Wireless Multi-hop Networks   Large-scale wireless testbeds have been setup in the last years with the goal
to study wireless multi-hop networks in more realistic environments. Since the
setup and operation of such a testbed is expensive in terms of money, time, and
labor, the crucial question rises whether this effort is justified with the
scientific results the testbed generates.
In this paper, we give an answer to this question based on our experience
with the DES-Testbed, a large-scale wireless sensor network and wireless mesh
network testbed. The DES-Testbed has been operated for almost 5 years. Our
analysis comprises more than 1000 experiments that have been run on the testbed
in the years 2010 and 2011. We discuss the scientific value in respect to the
effort of experimentation.
",Computer Science,Computer Science
"Infinite Mixture of Inverted Dirichlet Distributions   In this work, we develop a novel Bayesian estimation method for the Dirichlet
process (DP) mixture of the inverted Dirichlet distributions, which has been
shown to be very flexible for modeling vectors with positive elements. The
recently proposed extended variational inference (EVI) framework is adopted to
derive an analytically tractable solution. The convergency of the proposed
algorithm is theoretically guaranteed by introducing single lower bound
approximation to the original objective function in the VI framework. In
principle, the proposed model can be viewed as an infinite inverted Dirichelt
mixture model (InIDMM) that allows the automatic determination of the number of
mixture components from data. Therefore, the problem of pre-determining the
optimal number of mixing components has been overcome. Moreover, the problems
of over-fitting and under-fitting are avoided by the Bayesian estimation
approach. Comparing with several recently proposed DP-related methods, the good
performance and effectiveness of the proposed method have been demonstrated
with both synthesized data and real data evaluations.
",Statistics,Statistics
"Efficient conversion from rotating matrix to rotation axis and angle by extending Rodrigues' formula   In computational 3D geometric problems involving rotations, it is often that
people have to convert back and forth between a rotational matrix and a
rotation described by an axis and a corresponding angle. For this purpose,
Rodrigues' rotation formula is a very popular expression to use because of its
simplicity and efficiency. Nevertheless, while converting a rotation matrix to
an axis of rotation and the rotation angle, there exists ambiguity. Further
judgement or even manual interference may be necessary in some situations. An
extension of the Rodrigues' formula helps to find the sine and cosine values of
the rotation angle with respect to a given rotation axis is found and this
simple extension may help to accelerate many applications.
",Computer Science,Computer Science
"A Phase Variable Approach for Improved Volitional and Rhythmic Control of a Powered Knee-Ankle Prosthesis   Although there has been recent progress in control of multi-joint prosthetic
legs for periodic tasks such as walking, volitional control of these systems
for non-periodic maneuvers is still an open problem. In this paper, we develop
a new controller that is capable of both periodic walking and common volitional
leg motions based on a piecewise holonomic phase variable through a finite
state machine. The phase variable is constructed by measuring the thigh angle,
and the transitions in the finite state machine are formulated through sensing
foot contact along with attributes of a nominal reference gait trajectory. The
controller was implemented on a powered knee-ankle prosthesis and tested with a
transfemoral amputee subject, who successfully performed a wide range of
periodic and non-periodic tasks, including low- and high-speed walking, quick
start and stop, backward walking, walking over obstacles, and kicking a soccer
ball. Use of the powered leg resulted in significant reductions in amputee
compensations including vaulting and hip circumduction when compared to use of
the take-home passive leg. The proposed approach is expected to provide better
understanding of volitional motions and lead to more reliable control of
multi-joint prostheses for a wider range of tasks.
",Computer Science,Computer Science
"The sharp for the Chang model is small   Woodin has shown that if there is a measurable Woodin cardinal then there is,
in an appropriate sense, a sharp for the Chang model. We produce, in a weaker
sense, a sharp for the Chang model using only the existence of a cardinal
$\kappa$ having an extender of length $\kappa^{+\omega_1}$.
",Mathematics,Mathematics
"Response to ""Counterexample to global convergence of DSOS and SDSOS hierarchies""   In a recent note [8], the author provides a counterexample to the global
convergence of what his work refers to as ""the DSOS and SDSOS hierarchies"" for
polynomial optimization problems (POPs) and purports that this refutes claims
in our extended abstract [4] and slides in [3]. The goal of this paper is to
clarify that neither [4], nor [3], and certainly not our full paper [5], ever
defined DSOS or SDSOS hierarchies as it is done in [8]. It goes without saying
that no claims about convergence properties of the hierarchies in [8] were ever
made as a consequence. What was stated in [4,3] was completely different: we
stated that there exist hierarchies based on DSOS and SDSOS optimization that
converge. This is indeed true as we discuss in this response. We also emphasize
that we were well aware that some (S)DSOS hierarchies do not converge even if
their natural SOS counterparts do. This is readily implied by an example in our
prior work [5], which makes the counterexample in [8] superfluous. Finally, we
provide concrete counterarguments to claims made in [8] that aim to challenge
the scalability improvements obtained by DSOS and SDSOS optimization as
compared to sum of squares (SOS) optimization.
[3] A. A. Ahmadi and A. Majumdar. DSOS and SDSOS: More tractable alternatives
to SOS. Slides at the meeting on Geometry and Algebra of Linear Matrix
Inequalities, CIRM, Marseille, 2013. [4] A. A. Ahmadi and A. Majumdar. DSOS and
SDSOS optimization: LP and SOCP-based alternatives to sum of squares
optimization. In proceedings of the 48th annual IEEE Conference on Information
Sciences and Systems, 2014. [5] A. A. Ahmadi and A. Majumdar. DSOS and SDSOS
optimization: more tractable alternatives to sum of squares and semidefinite
optimization. arXiv:1706.02586, 2017. [8] C. Josz. Counterexample to global
convergence of DSOS and SDSOS hierarchies. arXiv:1707.02964, 2017.
",Computer Science; Statistics,Computer Science; Mathematics
"Towards a population synthesis model of self-gravitating disc fragmentation and tidal downsizing II: The effect of fragment-fragment interactions   It is likely that most protostellar systems undergo a brief phase where the
protostellar disc is self-gravitating. If these discs are prone to
fragmentation, then they are able to rapidly form objects that are initially of
several Jupiter masses and larger. The fate of these disc fragments (and the
fate of planetary bodies formed afterwards via core accretion) depends
sensitively not only on the fragment's interaction with the disc, but with its
neighbouring fragments.
We return to and revise our population synthesis model of self-gravitating
disc fragmentation and tidal downsizing. Amongst other improvements, the model
now directly incorporates fragment-fragment interactions while the disc is
still present. We find that fragment-fragment scattering dominates the orbital
evolution, even when we enforce rapid migration and inefficient gap formation.
Compared to our previous model, we see a small increase in the number of
terrestrial-type objects being formed, although their survival under tidal
evolution is at best unclear. We also see evidence for disrupted fragments with
evolved grain populations - this is circumstantial evidence for the formation
of planetesimal belts, a phenomenon not seen in runs where fragment-fragment
interactions are ignored.
In spite of intense dynamical evolution, our population is dominated by
massive giant planets and brown dwarfs at large semimajor axis, which direct
imaging surveys should, but only rarely, detect. Finally, disc fragmentation is
shown to be an efficient manufacturer of free floating planetary mass objects,
and the typical multiplicity of systems formed via gravitational instability
will be low.
",Physics,Physics
"On Popov's formula involving the Von Mangoldt function   We offer a generalization of a formula of Popov involving the Von Mangoldt
function. Some commentary on its relation to other results in analytic number
theory is mentioned as well as an analogue involving the m$\ddot{o}$bius
function.
",Mathematics,Mathematics
"Reduced-Order Modeling through Machine Learning Approaches for Brittle Fracture Applications   In this paper, five different approaches for reduced-order modeling of
brittle fracture in geomaterials, specifically concrete, are presented and
compared. Four of the five methods rely on machine learning (ML) algorithms to
approximate important aspects of the brittle fracture problem. In addition to
the ML algorithms, each method incorporates different physics-based assumptions
in order to reduce the computational complexity while maintaining the physics
as much as possible. This work specifically focuses on using the ML approaches
to model a 2D concrete sample under low strain rate pure tensile loading
conditions with 20 preexisting cracks present. A high-fidelity finite
element-discrete element model is used to both produce a training dataset of
150 simulations and an additional 35 simulations for validation. Results from
the ML approaches are directly compared against the results from the
high-fidelity model. Strengths and weaknesses of each approach are discussed
and the most important conclusion is that a combination of physics-informed and
data-driven features are necessary for emulating the physics of crack
propagation, interaction and coalescence. All of the models presented here have
runtimes that are orders of magnitude faster than the original high-fidelity
model and pave the path for developing accurate reduced order models that could
be used to inform larger length-scale models with important sub-scale physics
that often cannot be accounted for due to computational cost.
",Statistics,Computer Science; Physics
"Transfer Learning by Asymmetric Image Weighting for Segmentation across Scanners   Supervised learning has been very successful for automatic segmentation of
images from a single scanner. However, several papers report deteriorated
performances when using classifiers trained on images from one scanner to
segment images from other scanners. We propose a transfer learning classifier
that adapts to differences between training and test images. This method uses a
weighted ensemble of classifiers trained on individual images. The weight of
each classifier is determined by the similarity between its training image and
the test image.
We examine three unsupervised similarity measures, which can be used in
scenarios where no labeled data from a newly introduced scanner or scanning
protocol is available. The measures are based on a divergence, a bag distance,
and on estimating the labels with a clustering procedure. These measures are
asymmetric. We study whether the asymmetry can improve classification. Out of
the three similarity measures, the bag similarity measure is the most robust
across different studies and achieves excellent results on four brain tissue
segmentation datasets and three white matter lesion segmentation datasets,
acquired at different centers and with different scanners and scanning
protocols. We show that the asymmetry can indeed be informative, and that
computing the similarity from the test image to the training images is more
appropriate than the opposite direction.
",Computer Science; Statistics,Computer Science; Statistics
"Nonlinear stability for the Maxwell--Born--Infeld system on a Schwarzschild background   In this paper we prove small data global existence for solutions to the
Maxwell--Born--Infeld (MBI) system on a fixed Schwarzschild background. This
system has appeared in the context of string theory and can be seen as a
nonlinear model problem for the stability of the background metric itself, due
to its tensorial and quasilinear nature. The MBI system models nonlinear
electromagnetism and does not display birefringence. The key element in our
proof lies in the observation that there exists a first-order differential
transformation which brings solutions of the spin $\pm 1$ Teukolsky equations,
satisfied by the extreme components of the field, into solutions of a ""good""
equation (the Fackerell--Ipser Equation). This strategy was established in [F.
Pasqualotto, The spin $\pm 1$ Teukolsky equations and the Maxwell system on
Schwarzschild, Preprint 2016, arXiv:1612.07244] for the linear Maxwell field on
Schwarzschild. We show that analogous Fackerell--Ipser equations hold for the
MBI system on a fixed Schwarzschild background, which are however nonlinearly
coupled. To essentially decouple these right hand sides, we setup a bootstrap
argument. We use the $r^p$ method of Dafermos and Rodnianski in [M. Dafermos
and I. Rodnianski, A new physical-space approach to decay for the wave equation
with applications to black hole spacetimes, in XVIth International Congress on
Mathematical Physics, Pavel Exner ed., Prague 2009 pp. 421-433, 2009,
arXiv:0910.4957] in order to deduce decay of some null components, and we infer
decay for the remaining quantities by integrating the MBI system as transport
equations.
",Mathematics,Physics
"Addressing Class Imbalance in Classification Problems of Noisy Signals by using Fourier Transform Surrogates   Randomizing the Fourier-transform (FT) phases of temporal-spatial data
generates surrogates that approximate examples from the data-generating
distribution. We propose such FT surrogates as a novel tool to augment and
analyze training of neural networks and explore the approach in the example of
sleep-stage classification. By computing FT surrogates of raw EEG, EOG, and EMG
signals of under-represented sleep stages, we balanced the CAPSLPDB sleep
database. We then trained and tested a convolutional neural network for sleep
stage classification, and found that our surrogate-based augmentation improved
the mean F1-score by 7%. As another application of FT surrogates, we formulated
an approach to compute saliency maps for individual sleep epochs. The
visualization is based on the response of inferred class probabilities under
replacement of short data segments by partial surrogates. To quantify how well
the distributions of the surrogates and the original data match, we evaluated a
trained classifier on surrogates of correctly classified examples, and
summarized these conditional predictions in a confusion matrix. We show how
such conditional confusion matrices can qualitatively explain the performance
of surrogates in class balancing. The FT-surrogate augmentation approach may
improve classification on noisy signals if carefully adapted to the data
distribution under analysis.
",Statistics; Quantitative Biology,Computer Science; Statistics
"Testing convexity of a discrete distribution   Based on the convex least-squares estimator, we propose two different
procedures for testing convexity of a probability mass function supported on N
with an unknown finite support. The procedures are shown to be asymptotically
calibrated.
",Mathematics; Statistics,Mathematics; Statistics
"Optimizing Long Short-Term Memory Recurrent Neural Networks Using Ant Colony Optimization to Predict Turbine Engine Vibration   This article expands on research that has been done to develop a recurrent
neural network (RNN) capable of predicting aircraft engine vibrations using
long short-term memory (LSTM) neurons. LSTM RNNs can provide a more
generalizable and robust method for prediction over analytical calculations of
engine vibration, as analytical calculations must be solved iteratively based
on specific empirical engine parameters, making this approach ungeneralizable
across multiple engines. In initial work, multiple LSTM RNN architectures were
proposed, evaluated and compared. This research improves the performance of the
most effective LSTM network design proposed in the previous work by using a
promising neuroevolution method based on ant colony optimization (ACO) to
develop and enhance the LSTM cell structure of the network. A parallelized
version of the ACO neuroevolution algorithm has been developed and the evolved
LSTM RNNs were compared to the previously used fixed topology. The evolved
networks were trained on a large database of flight data records obtained from
an airline containing flights that suffered from excessive vibration. Results
were obtained using MPI (Message Passing Interface) on a high performance
computing (HPC) cluster, evolving 1000 different LSTM cell structures using 168
cores over 4 days. The new evolved LSTM cells showed an improvement of 1.35%,
reducing prediction error from 5.51% to 4.17% when predicting excessive engine
vibrations 10 seconds in the future, while at the same time dramatically
reducing the number of weights from 21,170 to 11,810.
",Computer Science,Computer Science; Statistics
"Quantum machine learning: a classical perspective   Recently, increased computational power and data availability, as well as
algorithmic advances, have led machine learning techniques to impressive
results in regression, classification, data-generation and reinforcement
learning tasks. Despite these successes, the proximity to the physical limits
of chip fabrication alongside the increasing size of datasets are motivating a
growing number of researchers to explore the possibility of harnessing the
power of quantum computation to speed-up classical machine learning algorithms.
Here we review the literature in quantum machine learning and discuss
perspectives for a mixed readership of classical machine learning and quantum
computation experts. Particular emphasis will be placed on clarifying the
limitations of quantum algorithms, how they compare with their best classical
counterparts and why quantum resources are expected to provide advantages for
learning problems. Learning in the presence of noise and certain
computationally hard problems in machine learning are identified as promising
directions for the field. Practical questions, like how to upload classical
data into quantum form, will also be addressed.
",Computer Science; Statistics,Computer Science; Statistics
"Formal Synthesis of Control Strategies for Positive Monotone Systems   We design controllers from formal specifications for positive discrete-time
monotone systems that are subject to bounded disturbances. Such systems are
widely used to model the dynamics of transportation and biological networks.
The specifications are described using signal temporal logic (STL), which can
express a broad range of temporal properties. We formulate the problem as a
mixed-integer linear program (MILP) and show that under the assumptions made in
this paper, which are not restrictive for traffic applications, the existence
of open-loop control policies is sufficient and almost necessary to ensure the
satisfaction of STL formulas. We establish a relation between satisfaction of
STL formulas in infinite time and set-invariance theories and provide an
efficient method to compute robust control invariant sets in high dimensions.
We also develop a robust model predictive framework to plan controls optimally
while ensuring the satisfaction of the specification. Illustrative examples and
a traffic management case study are included.
",Computer Science; Mathematics,Computer Science
"Solving Graph Isomorphism Problem for a Special case   Graph isomorphism is an important computer science problem. The problem for
the general case is unknown to be in polynomial time. The base algorithm for
the general case works in quasi-polynomial time. The solutions in polynomial
time for some special type of classes are known. In this work, we have worked
with a special type of graphs. We have proposed a method to represent these
graphs and finding isomorphism between these graphs. The method uses a modified
version of the degree list of a graph and neighbourhood degree list. These
special type of graphs have a property that neighbourhood degree list of any
two immediate neighbours is different for every vertex.The representation
becomes invariant to the order in which the node was selected for giving the
representation making the isomorphism problem trivial for this case. The
algorithm works in $O(n^4)$ time, where n is the number of vertices present in
the graph. The proposed algorithm runs faster than quasi-polynomial time for
the graphs used in the study.
",Computer Science,Computer Science
"Benefits from Superposed Hawkes Processes   The superposition of temporal point processes has been studied for many
years, although the usefulness of such models for practical applications has
not be fully developed. We investigate superposed Hawkes process as an
important class of such models, with properties studied in the framework of
least squares estimation. The superposition of Hawkes processes is demonstrated
to be beneficial for tightening the upper bound of excess risk under certain
conditions, and we show the feasibility of the benefit in typical situations.
The usefulness of superposed Hawkes processes is verified on synthetic data,
and its potential to solve the cold-start problem of recommendation systems is
demonstrated on real-world data.
",Statistics,Statistics
"Einstein's 1935 papers: EPR=ER?   In May of 1935, Einstein published with two co-authors the famous EPR-paper
about entangled particles, which questioned the completeness of Quantum
Mechanics by means of a gedankenexperiment. Only one month later, he published
a work that seems unconnected to the EPR-paper at first, the so called
Einstein-Rosen-paper, that presented a solution of the field equations for
particles in the framework of general relativity. Both papers ask for the
conception of completeness in a theory and, from a modern perspective, it is
easy to believe that there is a connection between these topics. We question
whether Einstein might have considered that a correlation between nonlocal
features of Quantum Mechanics and the Einstein-Rosen bridge can be used to
explain entanglement. We analyse this question by discussing the used
conceptions of ""completeness,"" ""atomistic structure of matter,"" and ""quantum
phenomena."" We discuss the historical embedding of the two works and the
context to modern research. Recent approaches are presented that formulate an
EPR=ER principle and claim an equivalence of the basic principles of these two
papers.
",Physics,Physics
"Fractal dimension and lower bounds for geometric problems   We study the complexity of geometric problems on spaces of low fractal
dimension. It was recently shown by [Sidiropoulos & Sridhar, SoCG 2017] that
several problems admit improved solutions when the input is a pointset in
Euclidean space with fractal dimension smaller than the ambient dimension. In
this paper we prove nearly-matching lower bounds, thus establishing
nearly-optimal bounds for various problems as a function of the fractal
dimension.
More specifically, we show that for any set of $n$ points in $d$-dimensional
Euclidean space, of fractal dimension $\delta\in (1,d)$, for any $\epsilon >0$
and $c\geq 1$, any $c$-spanner must have treewidth at least $\Omega \left(
\frac{n^{1-1/(\delta - \epsilon)}}{c^{d-1}} \right)$, matching the previous
upper bound. The construction used to prove this lower bound on the treewidth
of spanners can also be used to derive lower bounds on the running time of
algorithms for various problems, assuming the Exponential Time Hypothesis. We
provide two prototypical results of this type. For any $\delta \in (1,d)$ and
any $\epsilon >0$ we show that:
1) $d$-dimensional Euclidean TSP on $n$ points with fractal dimension at most
$\delta$ cannot be solved in time $2^{O\left(n^{1-1/(\delta - \epsilon)}
\right)}$. The best-known upper bound is $2^{O(n^{1-1/\delta} \log n)}$.
2) The problem of finding $k$-pairwise non-intersecting $d$-dimensional unit
balls/axis parallel unit cubes with centers having fractal dimension at most
$\delta$ cannot be solved in time $f(k)n^{O \left(k^{1-1/(\delta -
\epsilon)}\right)}$ for any computable function $f$. The best-known upper bound
is $n^{O(k^{1-1/\delta} \log n)}$.
The above results nearly match previously known upper bounds from
[Sidiropoulos & Sridhar, SoCG 2017], and generalize analogous lower bounds for
the case of ambient dimension due to [Marx & Sidiropoulos, SoCG 2014].
",Computer Science,Computer Science
"Improved approximation algorithm for the Dense-3-Subhypergraph Problem   The study of Dense-$3$-Subhypergraph problem was initiated in Chlamt{á}c
et al. [Approx'16]. The input is a universe $U$ and collection ${\cal S}$ of
subsets of $U$, each of size $3$, and a number $k$. The goal is to choose a set
$W$ of $k$ elements from the universe, and maximize the number of sets, $S\in
{\cal S}$ so that $S\subseteq W$. The members in $U$ are called {\em vertices}
and the sets of ${\cal S}$ are called the {\em hyperedges}. This is the
simplest extension into hyperedges of the case of sets of size $2$ which is the
well known Dense $k$-subgraph problem.
The best known ratio for the Dense-$3$-Subhypergraph is $O(n^{0.69783..})$ by
Chlamt{á}c et al. We improve this ratio to $n^{0.61802..}$. More
importantly, we give a new algorithm that approximates Dense-$3$-Subhypergraph
within a ratio of $\tilde O(n/k)$, which improves the ratio of $O(n^2/k^2)$ of
Chlamt{á}c et al.
We prove that under the {\em log density conjecture} (see Bhaskara et al.
[STOC'10]) the ratio cannot be better than $\Omega(\sqrt{n})$ and demonstrate
some cases in which this optimum can be attained.
",Computer Science,Computer Science
"Almost complex structures on connected sums of complex projective spaces   We show that the m-fold connected sum $m\#\mathbb{C}\mathbb{P}^{2n}$ admits
an almost complex structure if and only if m is odd.
",Mathematics,Mathematics
"Kustaanheimo-Stiefel transformation with an arbitrary defining vector   Kustaanheimo-Stiefel (KS) transformation depends on the choice of some
preferred direction in the Cartesian 3D space. This choice, seldom explicitly
mentioned, amounts typically to the direction of the first or the third
coordinate axis in celestial mechanics and atomic physics, respectively. The
present work develops a canonical KS transformation with an arbitrary preferred
direction, indicated by what we call a defining vector. Using a mix of vector
and quaternion algebra, we formulate the transformation in a reference frame
independent manner. The link between the oscillator and Keplerian first
integrals is given. As an example of the present formulation, the Keplerian
motion in a rotating frame is re-investigated.
",Mathematics,Physics
"Large Scale Graph Learning from Smooth Signals   Graphs are a prevalent tool in data science, as they model the inherent
structure of the data. They have been used successfully in unsupervised and
semi-supervised learning. Typically they are constructed either by connecting
nearest samples, or by learning them from data, solving an optimization
problem. While graph learning does achieve a better quality, it also comes with
a higher computational cost. In particular, the current state-of-the-art model
cost is $\mathcal{O}(n^2)$ for $n$ samples. In this paper, we show how to scale
it, obtaining an approximation with leading cost of $\mathcal{O}(n\log(n))$,
with quality that approaches the exact graph learning model. Our algorithm uses
known approximate nearest neighbor techniques to reduce the number of
variables, and automatically selects the correct parameters of the model,
requiring a single intuitive input: the desired edge density.
",Computer Science; Statistics,Computer Science; Statistics
"Sports stars: analyzing the performance of astronomers at visualization-based discovery   In this data-rich era of astronomy, there is a growing reliance on automated
techniques to discover new knowledge. The role of the astronomer may change
from being a discoverer to being a confirmer. But what do astronomers actually
look at when they distinguish between ""sources"" and ""noise?"" What are the
differences between novice and expert astronomers when it comes to visual-based
discovery? Can we identify elite talent or coach astronomers to maximize their
potential for discovery? By looking to the field of sports performance
analysis, we consider an established, domain-wide approach, where the expertise
of the viewer (i.e. a member of the coaching team) plays a crucial role in
identifying and determining the subtle features of gameplay that provide a
winning advantage. As an initial case study, we investigate whether the
SportsCode performance analysis software can be used to understand and document
how an experienced HI astronomer makes discoveries in spectral data cubes. We
find that the process of timeline-based coding can be applied to spectral cube
data by mapping spectral channels to frames within a movie. SportsCode provides
a range of easy to use methods for annotation, including feature-based codes
and labels, text annotations associated with codes, and image-based drawing.
The outputs, including instance movies that are uniquely associated with coded
events, provide the basis for a training program or team-based analysis that
could be used in unison with discipline specific analysis software. In this
coordinated approach to visualization and analysis, SportsCode can act as a
visual notebook, recording the insight and decisions in partnership with
established analysis methods. Alternatively, in situ annotation and coding of
features would be a valuable addition to existing and future visualisation and
analysis packages.
",Physics,Computer Science
"Uniqueness of planar vortex patch in incompressible steady flow   We investigate a steady planar flow of an ideal fluid in a bounded simple
connected domain and focus on the vortex patch problem with prescribed
vorticity strength. There are two methods to deal with the existence of
solutions for this problem: the vorticity method and the stream function
method. A long standing open problem is whether these two entirely different
methods result in the same solution. In this paper, we will give a positive
answer to this problem by studying the local uniqueness of the solutions.
Another result obtained in this paper is that if the domain is convex, then the
vortex patch problem has a unique solution.
",Mathematics,Mathematics
"Exact partial information decompositions for Gaussian systems based on dependency constraints   The Partial Information Decomposition (PID) [arXiv:1004.2515] provides a
theoretical framework to characterize and quantify the structure of
multivariate information sharing. A new method (Idep) has recently been
proposed for computing a two-predictor PID over discrete spaces.
[arXiv:1709.06653] A lattice of maximum entropy probability models is
constructed based on marginal dependency constraints, and the unique
information that a particular predictor has about the target is defined as the
minimum increase in joint predictor-target mutual information when that
particular predictor-target marginal dependency is constrained. Here, we apply
the Idep approach to Gaussian systems, for which the marginally constrained
maximum entropy models are Gaussian graphical models. Closed form solutions for
the Idep PID are derived for both univariate and multivariate Gaussian systems.
Numerical and graphical illustrations are provided, together with practical and
theoretical comparisons of the Idep PID with the minimum mutual information PID
(Immi). [arXiv:1411.2832] In particular, it is proved that the Immi method
generally produces larger estimates of redundancy and synergy than does the
Idep method. In discussion of the practical examples, the PIDs are complemented
by the use of deviance tests for the comparison of Gaussian graphical models.
",Statistics; Quantitative Biology,Computer Science; Statistics
"On convergence for graphexes   We study four different notions of convergence for graphexes, recently
introduced by Borgs, Chayes, Cohn and Holden, and by Veitch and Roy. We give
some properties of them and some relations between them. We also extend results
by Veitch and Roy on convergence of empirical graphons.
",Mathematics,Mathematics
"Local and global existence of solutions to a strongly damped wave equation of the $p$-Laplacian type   This article focuses on a quasilinear wave equation of $p$-Laplacian type: $$
u_{tt} - \Delta_p u - \Delta u_t=0$$ in a bounded domain
$\Omega\subset\mathbb{R}^3$ with a sufficiently smooth boundary
$\Gamma=\partial\Omega$ subject to a generalized Robin boundary condition
featuring boundary damping and a nonlinear source term. The operator
$\Delta_p$, $2 < p < 3$, denotes the classical $p$-Laplacian. The nonlinear
boundary term $f (u)$ is a source feedback that is allowed to have a
supercritical exponent, in the sense that the associated Nemytskii operator is
not locally Lipschitz from $W^{1,p}(\Omega)$ into $L^2(\Gamma)$. Under suitable
assumptions on the parameters we provide a rigorous proof of existence of a
local weak solution which can be extended globally in time provided the source
term satisfies an appropriate growth condition.
",Mathematics,Mathematics
"Spectral Graph Convolutions for Population-based Disease Prediction   Exploiting the wealth of imaging and non-imaging information for disease
prediction tasks requires models capable of representing, at the same time,
individual features as well as data associations between subjects from
potentially large populations. Graphs provide a natural framework for such
tasks, yet previous graph-based approaches focus on pairwise similarities
without modelling the subjects' individual characteristics and features. On the
other hand, relying solely on subject-specific imaging feature vectors fails to
model the interaction and similarity between subjects, which can reduce
performance. In this paper, we introduce the novel concept of Graph
Convolutional Networks (GCN) for brain analysis in populations, combining
imaging and non-imaging data. We represent populations as a sparse graph where
its vertices are associated with image-based feature vectors and the edges
encode phenotypic information. This structure was used to train a GCN model on
partially labelled graphs, aiming to infer the classes of unlabelled nodes from
the node features and pairwise associations between subjects. We demonstrate
the potential of the method on the challenging ADNI and ABIDE databases, as a
proof of concept of the benefit from integrating contextual information in
classification tasks. This has a clear impact on the quality of the
predictions, leading to 69.5% accuracy for ABIDE (outperforming the current
state of the art of 66.8%) and 77% for ADNI for prediction of MCI conversion,
significantly outperforming standard linear classifiers where only individual
features are considered.
",Computer Science; Statistics,Computer Science; Statistics
"Second order structural phase transitions, free energy curvature, and temperature-dependent anharmonic phonons in the self-consistent harmonic approximation: theory and stochastic implementation   The self-consistent harmonic approximation is an effective harmonic theory to
calculate the free energy of systems with strongly anharmonic atomic
vibrations, and its stochastic implementation has proved to be an efficient
method to study, from first-principles, the anharmonic properties of solids.
The free energy as a function of average atomic positions (centroids) can be
used to study quantum or thermal lattice instability. In particular the
centroids are order parameters in second-order structural phase transitions
such as, e.g., charge-density-waves or ferroelectric instabilities. According
to Landau's theory, the knowledge of the second derivative of the free energy
(i.e. the curvature) with respect to the centroids in a high-symmetry
configuration allows the identification of the phase-transition and of the
instability modes. In this work we derive the exact analytic formula for the
second derivative of the free energy in the self-consistent harmonic
approximation for a generic atomic configuration. The analytic derivative is
expressed in terms of the atomic displacements and forces in a form that can be
evaluated by a stochastic technique using importance sampling. Our approach is
particularly suitable for applications based on first-principles
density-functional-theory calculations, where the forces on atoms can be
obtained with a negligible computational effort compared to total energy
determination. Finally we propose a dynamical extension of the theory to
calculate spectral properties of strongly anharmonic phonons, as probed by
inelastic scattering processes. We illustrate our method with a numerical
application on a toy model that mimics the ferroelectric transition in
rock-salt crystals such as SnTe or GeTe.
",Physics,Physics
"Diffraction-limited plenoptic imaging with correlated light   Traditional optical imaging faces an unavoidable trade-off between resolution
and depth of field (DOF). To increase resolution, high numerical apertures (NA)
are needed, but the associated large angular uncertainty results in a limited
range of depths that can be put in sharp focus. Plenoptic imaging was
introduced a few years ago to remedy this trade off. To this aim, plenoptic
imaging reconstructs the path of light rays from the lens to the sensor.
However, the improvement offered by standard plenoptic imaging is practical and
not fundamental: the increased DOF leads to a proportional reduction of the
resolution well above the diffraction limit imposed by the lens NA. In this
paper, we demonstrate that correlation measurements enable pushing plenoptic
imaging to its fundamental limits of both resolution and DOF. Namely, we
demonstrate to maintain the imaging resolution at the diffraction limit while
increasing the depth of field by a factor of 7. Our results represent the
theoretical and experimental basis for the effective development of the
promising applications of plenoptic imaging.
",Physics,Physics
"DeepFense: Online Accelerated Defense Against Adversarial Deep Learning   Recent advances in adversarial Deep Learning (DL) have opened up a largely
unexplored surface for malicious attacks jeopardizing the integrity of
autonomous DL systems. With the wide-spread usage of DL in critical and
time-sensitive applications, including unmanned vehicles, drones, and video
surveillance systems, online detection of malicious inputs is of utmost
importance. We propose DeepFense, the first end-to-end automated framework that
simultaneously enables efficient and safe execution of DL models. DeepFense
formalizes the goal of thwarting adversarial attacks as an optimization problem
that minimizes the rarely observed regions in the latent feature space spanned
by a DL network. To solve the aforementioned minimization problem, a set of
complementary but disjoint modular redundancies are trained to validate the
legitimacy of the input samples in parallel with the victim DL model. DeepFense
leverages hardware/software/algorithm co-design and customized acceleration to
achieve just-in-time performance in resource-constrained settings. The proposed
countermeasure is unsupervised, meaning that no adversarial sample is leveraged
to train modular redundancies. We further provide an accompanying API to reduce
the non-recurring engineering cost and ensure automated adaptation to various
platforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders
of magnitude performance improvement while enabling online adversarial sample
detection.
",Computer Science; Statistics,Computer Science
"Resilient Non-Submodular Maximization over Matroid Constraints   The control and sensing of large-scale systems results in combinatorial
problems not only for sensor and actuator placement but also for scheduling or
observability/controllability. Such combinatorial constraints in system design
and implementation can be captured using a structure known as matroids. In
particular, the algebraic structure of matroids can be exploited to develop
scalable algorithms for sensor and actuator selection, along with quantifiable
approximation bounds. However, in large-scale systems, sensors and actuators
may fail or may be (cyber-)attacked. The objective of this paper is to focus on
resilient matroid-constrained problems arising in control and sensing but in
the presence of sensor and actuator failures. In general, resilient
matroid-constrained problems are computationally hard. Contrary to the
non-resilient case (with no failures), even though they often involve objective
functions that are monotone or submodular, no scalable approximation algorithms
are known for their solution. In this paper, we provide the first algorithm,
that also has the following properties: First, it achieves system-wide
resiliency, i.e., the algorithm is valid for any number of denial-of-service
attacks or failures. Second, it is scalable, as our algorithm terminates with
the same running time as state-of-the-art algorithms for (non-resilient)
matroid-constrained optimization. Third, it provides provable approximation
bounds on the system performance, since for monotone objective functions our
algorithm guarantees a solution close to the optimal. We quantify our
algorithm's approximation performance using a notion of curvature for monotone
(not necessarily submodular) set functions. Finally, we support our theoretical
analyses with numerical experiments, by considering a control-aware sensor
selection scenario, namely, sensing-constrained robot navigation.
",Computer Science; Statistics,Computer Science
"Design, Engineering and Optimization of a Grid-Tie Multicell Inverter for Energy Storage Applications   Multilevel converters have found many applications within renewable energy
systems thanks to their unique capability of generating multiple voltage
levels. However, these converters need multiple DC sources and the voltage
balancing over capacitors for these systems is cumbersome. In this work, a new
grid-tie multicell inverter with high level of safety has been designed,
engineered and optimized for integrating energy storage devices to the electric
grid. The multilevel converter proposed in this work is capable of maintaining
the flying capacitors voltage in the desired value. The solar cells are the
primary energy sources for proposed inverter where the maximum power density is
obtained. Finally, the performance of the inverter and its control method
simulated using PSCAD/EMTDC software package and good agreement achieved with
experimental data.
",Physics,Computer Science
"The Price of Diversity in Assignment Problems   We introduce and analyze an extension to the matching problem on a weighted
bipartite graph: Assignment with Type Constraints. The two parts of the graph
are partitioned into subsets called types and blocks; we seek a matching with
the largest sum of weights under the constraint that there is a pre-specified
cap on the number of vertices matched in every type-block pair. Our primary
motivation stems from the public housing program of Singapore, accounting for
over 70% of its residential real estate. To promote ethnic diversity within its
housing projects, Singapore imposes ethnicity quotas: each new housing
development comprises blocks of flats and each ethnicity-based group in the
population must not own more than a certain percentage of flats in a block.
Other domains using similar hard capacity constraints include matching
prospective students to schools or medical residents to hospitals. Limiting
agents' choices for ensuring diversity in this manner naturally entails some
welfare loss. One of our goals is to study the trade-off between diversity and
social welfare in such settings. We first show that, while the classic
assignment program is polynomial-time computable, adding diversity constraints
makes it computationally intractable; however, we identify a
$\tfrac{1}{2}$-approximation algorithm, as well as reasonable assumptions on
the weights that permit poly-time algorithms. Next, we provide two upper bounds
on the price of diversity -- a measure of the loss in welfare incurred by
imposing diversity constraints -- as functions of natural problem parameters.
We conclude the paper with simulations based on publicly available data from
two diversity-constrained allocation problems -- Singapore Public Housing and
Chicago School Choice -- which shed light on how the constrained maximization
as well as lottery-based variants perform in practice.
",Computer Science,Computer Science; Statistics
"\textit{Ab Initio} Study of the Magnetic Behavior of Metal Hydrides: A Comparison with the Slater-Pauling Curve   We investigated the magnetic behavior of metal hydrides FeH$_{x}$, CoH$_{x}$
and NiH$_{x}$ for several concentrations of hydrogen ($x$) by using Density
Functional Theory calculations. Several structural phases of the metallic host:
bcc ($\alpha$), fcc ($\gamma$), hcp ($\varepsilon$), dhcp ($\varepsilon'$),
tetragonal structure for FeH$_{x}$ and $\varepsilon$-$\gamma$ phases for
CoH$_{x}$, were studied. We found that for CoH$_{x}$ and NiH$_{x}$ the magnetic
moment ($m$) decreases regardless the concentration $x$. However, for FeH$_{x}$
systems, $m$ increases or decreases depending on the variation in $x$. In order
to find a general trend for these changes of $m$ in magnetic metal hydrides, we
compare our results with the Slater-Pauling curve for ferromagnetic metallic
binary alloys. It is found that the $m$ of metal hydrides made of Fe, Co and Ni
fits the shape of the Slater-Pauling curve as a function of $x$. Our results
indicate that there are two main effects that determine the $m$ value due to
hydrogenation: an increase of volume causes $m$ to increase, and the addition
of an extra electron to the metal always causes it to decrease. We discuss
these behaviors in detail.
",Physics,Physics
"Floquet prethermalization in the resonantly driven Hubbard model   We demonstrate the existence of long-lived prethermalized states in the Mott
insulating Hubbard model driven by periodic electric fields. These states,
which also exist in the resonantly driven case with a large density of
photo-induced doublons and holons, are characterized by a nonzero current and
an effective temperature of the doublons and holons which depends sensitively
on the driving condition. Focusing on the specific case of resonantly driven
models whose effective time-independent Hamiltonian in the high-frequency
driving limit corresponds to noninteracting fermions, we show that the time
evolution of the double occupation can be reproduced by the effective
Hamiltonian, and that the prethermalization plateaus at finite driving
frequency are controlled by the next-to-leading order correction in the
high-frequency expansion of the effective Hamiltonian. We propose a numerical
procedure to determine an effective Hubbard interaction that mimics the
correlation effects induced by these higher order terms.
",Physics,Physics
"Synthesizing Neural Network Controllers with Probabilistic Model based Reinforcement Learning   We present an algorithm for rapidly learning controllers for robotics
systems. The algorithm follows the model-based reinforcement learning paradigm,
and improves upon existing algorithms; namely Probabilistic learning in Control
(PILCO) and a sample-based version of PILCO with neural network dynamics
(Deep-PILCO). We propose training a neural network dynamics model using
variational dropout with truncated Log-Normal noise. This allows us to obtain a
dynamics model with calibrated uncertainty, which can be used to simulate
controller executions via rollouts. We also describe set of techniques,
inspired by viewing PILCO as a recurrent neural network model, that are crucial
to improve the convergence of the method. We test our method on a variety of
benchmark tasks, demonstrating data-efficiency that is competitive with PILCO,
while being able to optimize complex neural network controllers. Finally, we
assess the performance of the algorithm for learning motor controllers for a
six legged autonomous underwater vehicle. This demonstrates the potential of
the algorithm for scaling up the dimensionality and dataset sizes, in more
complex control tasks.
",Computer Science,Computer Science; Statistics
"Bridging Semantic Gaps between Natural Languages and APIs with Word Embedding   Developers increasingly rely on text matching tools to analyze the relation
between natural language words and APIs. However, semantic gaps, namely textual
mismatches between words and APIs, negatively affect these tools. Previous
studies have transformed words or APIs into low-dimensional vectors for
matching; however, inaccurate results were obtained due to the failure of
modeling words and APIs simultaneously. To resolve this problem, two main
challenges are to be addressed: the acquisition of massive words and APIs for
mining and the alignment of words and APIs for modeling. Therefore, this study
proposes Word2API to effectively estimate relatedness of words and APIs.
Word2API collects millions of commonly used words and APIs from code
repositories to address the acquisition challenge. Then, a shuffling strategy
is used to transform related words and APIs into tuples to address the
alignment challenge. Using these tuples, Word2API models words and APIs
simultaneously. Word2API outperforms baselines by 10%-49.6% of relatedness
estimation in terms of precision and NDCG. Word2API is also effective on
solving typical software tasks, e.g., query expansion and API documents
linking. A simple system with Word2API-expanded queries recommends up to 21.4%
more related APIs for developers. Meanwhile, Word2API improves comparison
algorithms by 7.9%-17.4% in linking questions in Question&Answer communities to
API documents.
",Computer Science,Computer Science
"Scattering dominated high-temperature phase of 1T-TiSe2: an optical conductivity study   The controversy regarding the precise nature of the high-temperature phase of
1T-TiSe2 lasts for decades. It has intensified in recent times when new
evidence for the excitonic origin of the low-temperature charge-density wave
state started to unveil. Here we address the problem of the high-temperature
phase through precise measurements and detailed analysis of the optical
response of 1T-TiSe2 single crystals. The separate responses of electron and
hole subsystems are identified and followed in temperature. We show that
neither semiconductor nor semimetal pictures can be applied in their generic
forms as the scattering for both types of carriers is in the vicinity of the
Ioffe-Regel limit with decay rates being comparable to or larger than the
offsets of band extrema. The nonmetallic temperature dependence of transport
properties comes from the anomalous temperature dependence of scattering rates.
Near the transition temperature the heavy electrons and the light holes
contribute equally to the conductivity. This surprising coincidence is regarded
as the consequence of dominant intervalley scattering that precedes the
transition. The low-frequency peak in the optical spectra is identified and
attributed to the critical softening of the L-point collective mode.
",Physics,Physics
"Backward-emitted sub-Doppler fluorescence from an optically thick atomic vapor   Literature mentions only incidentally a sub-Doppler contribution in the
excitation spectrum of the backward fluorescence of a dense vapor. This
contribution is here investigated on Cs vapor, both on the first resonance line
(894 nm) and on the weaker second resonance line (459 nm). We show that in a
strongly absorbing medium, the quenching of excited atoms moving towards a
window irradiated under near normal incidence reduces the fluorescence on the
red side of the excitation spectrum. Atoms moving slowly towards the window
produce a sub- Doppler velocity-selective contribution, whose visibility is
here improved by applying a frequency-modulation technique. This sub-Doppler
feature, induced by a surface quenching combined with a short absorption length
for the incident irradiation, exhibits close analogies with the narrow spectra
appearing with thin vapor cells. We also show that a normal incidence
irradiation is essential for the sub-Doppler feature to be observed, while it
should be independent of the detection geometry
",Physics,Physics
"Planning with Verbal Communication for Human-Robot Collaboration   Human collaborators coordinate effectively their actions through both verbal
and non-verbal communication. We believe that the the same should hold for
human-robot teams. We propose a formalism that enables a robot to decide
optimally between doing a task and issuing an utterance. We focus on two types
of utterances: verbal commands, where the robot expresses how it wants its
human teammate to behave, and state-conveying actions, where the robot explains
why it is behaving this way. Human subject experiments show that enabling the
robot to issue verbal commands is the most effective form of communicating
objectives, while retaining user trust in the robot. Communicating why
information should be done judiciously, since many participants questioned the
truthfulness of the robot statements.
",Computer Science,Computer Science
"Superconductivity in ultra-thin carbon nanotubes and carbyne-nanotube composites: an ab-initio approach   The superconductivity of the 4-angstrom single-walled carbon nanotubes
(SWCNTs) was discovered more than a decade ago, and marked the breakthrough of
finding superconductivity in pure elemental undoped carbon compounds. The van
Hove singularities in the electronic density of states at the Fermi level in
combination with a large Debye temperature of the SWCNTs are expected to cause
an impressively large superconducting gap. We have developed an innovative
computational algorithm specially tailored for the investigation of
superconductivity in ultrathin SWCNTs. We predict the superconducting
transition temperature of various thin carbon nanotubes resulting from
electron-phonon coupling by an ab-initio method, taking into account the effect
of radial pressure, symmetry, chirality (N,M) and bond lengths. By optimizing
the geometry of the carbon nanotubes, a maximum Tc of 60K is found. We also use
our method to calculate the Tc of a linear carbon chain embedded in the center
of (5,0) SWCNTs. The strong curvature in the (5,0) carbon nanotubes in the
presence of the inner carbon chain provides an alternative path to increase the
Tc of this carbon composite by a factor of 2.2 with respect to the empty (5,0)
SWCNTs.
",Physics,Physics
"Actively Learning what makes a Discrete Sequence Valid   Deep learning techniques have been hugely successful for traditional
supervised and unsupervised machine learning problems. In large part, these
techniques solve continuous optimization problems. Recently however, discrete
generative deep learning models have been successfully used to efficiently
search high-dimensional discrete spaces. These methods work by representing
discrete objects as sequences, for which powerful sequence-based deep models
can be employed. Unfortunately, these techniques are significantly hindered by
the fact that these generative models often produce invalid sequences. As a
step towards solving this problem, we propose to learn a deep recurrent
validator model. Given a partial sequence, our model learns the probability of
that sequence occurring as the beginning of a full valid sequence. Thus this
identifies valid versus invalid sequences and crucially it also provides
insight about how individual sequence elements influence the validity of
discrete objects. To learn this model we propose an approach inspired by
seminal work in Bayesian active learning. On a synthetic dataset, we
demonstrate the ability of our model to distinguish valid and invalid
sequences. We believe this is a key step toward learning generative models that
faithfully produce valid discrete objects.
",Computer Science; Statistics,Computer Science; Statistics
"On stably trivial spin torsors over low-dimensional schemes   The paper discusses stably trivial torsors for spin and orthogonal groups
over smooth affine schemes over infinite perfect fields of characteristic
unequal to 2. We give a complete description of all the invariants relevant for
the classification of such objects over schemes of dimension at most $3$, along
with many examples. The results are based on the
$\mathbb{A}^1$-representability theorem for torsors and transfer of known
computations of $\mathbb{A}^1$-homotopy sheaves along the sporadic isomorphisms
to spin groups.
",Mathematics,Mathematics
"Towards a Deep Improviser: a prototype deep learning post-tonal free music generator   Two modest-sized symbolic corpora of post-tonal and post-metric keyboard
music have been constructed, one algorithmic, the other improvised. Deep
learning models of each have been trained and largely optimised. Our purpose is
to obtain a model with sufficient generalisation capacity that in response to a
small quantity of separate fresh input seed material, it can generate outputs
that are distinctive, rather than recreative of the learned corpora or the seed
material. This objective has been first assessed statistically, and as judged
by k-sample Anderson-Darling and Cramer tests, has been achieved. Music has
been generated using the approach, and informal judgements place it roughly on
a par with algorithmic and composed music in related forms. Future work will
aim to enhance the model such that it can be evaluated in relation to
expression, meaning and utility in real-time performance.
",Computer Science,Computer Science; Statistics
"Levels of distribution for sieve problems in prehomogeneous vector spaces   In a companion paper, we developed an efficient algebraic method for
computing the Fourier transforms of certain functions defined on prehomogeneous
vector spaces over finite fields, and we carried out these computations in a
variety of cases.
Here we develop a method, based on Fourier analysis and algebraic geometry,
which exploits these Fourier transform formulas to yield level of distribution
results, in the sense of analytic number theory. Such results are of the shape
typically required for a variety of sieve methods. As an example of such an
application we prove that there are $\gg$ X/log(X) quartic fields whose
discriminant is squarefree, bounded above by X, and has at most eight prime
factors.
",Mathematics,Mathematics
"Equilibria, information and frustration in heterogeneous network games with conflicting preferences   Interactions between people are the basis on which the structure of our
society arises as a complex system and, at the same time, are the starting
point of any physical description of it. In the last few years, much
theoretical research has addressed this issue by combining the physics of
complex networks with a description of interactions in terms of evolutionary
game theory. We here take this research a step further by introducing a most
salient societal factor such as the individuals' preferences, a characteristic
that is key to understand much of the social phenomenology these days. We
consider a heterogeneous, agent-based model in which agents interact
strategically with their neighbors but their preferences and payoffs for the
possible actions differ. We study how such a heterogeneous network behaves
under evolutionary dynamics and different strategic interactions, namely
coordination games and best shot games. With this model we study the emergence
of the equilibria predicted analytically in random graphs under best response
dynamics, and we extend this test to unexplored contexts like proportional
imitation and scale free networks. We show that some theoretically predicted
equilibria do not arise in simulations with incomplete Information, and we
demonstrate the importance of the graph topology and the payoff function
parameters for some games. Finally, we discuss our results with available
experimental evidence on coordination games, showing that our model agrees
better with the experiment that standard economic theories, and draw hints as
to how to maximize social efficiency in situations of conflicting preferences.
",Computer Science; Physics,Computer Science; Physics
"Multivariate stable distributions and their applications for modelling cryptocurrency-returns   In this paper we extend the known methodology for fitting stable
distributions to the multivariate case and apply the suggested method to the
modelling of daily cryptocurrency-return data. The investigated time period is
cut into 10 non-overlapping sections, thus the changes can also be observed. We
apply bootstrap tests for checking the models and compare our approach to the
more traditional extreme-value and copula models.
",Quantitative Finance,Statistics
"Simultaneous Confidence Band for Partially Linear Panel Data Models with Fixed Effects   In this paper, we construct the simultaneous confidence band (SCB) for the
nonparametric component in partially linear panel data models with fixed
effects. We remove the fixed effects, and further obtain the estimators of
parametric and nonparametric components, which do not depend on the fixed
effects. We establish the asymptotic distribution of their maximum absolute
deviation between the estimated nonparametric component and the true
nonparametric component under some suitable conditions, and hence the result
can be used to construct the simultaneous confidence band of the nonparametric
component. Based on the asymptotic distribution, it becomes difficult for the
construction of the simultaneous confidence band. The reason is that the
asymptotic distribution involves the estimators of the asymptotic bias and
conditional variance, and the choice of the bandwidth for estimating the second
derivative of nonparametric function. Clearly, these will cause computational
burden and accumulative errors. To overcome these problems, we propose a
Bootstrap method to construct simultaneous confidence band. Simulation studies
indicate that the proposed Bootstrap method exhibits better performance under
the limited samples.
",Statistics,Mathematics; Statistics
"Statistical Speech Enhancement Based on Probabilistic Integration of Variational Autoencoder and Non-Negative Matrix Factorization   This paper presents a statistical method of single-channel speech enhancement
that uses a variational autoencoder (VAE) as a prior distribution on clean
speech. A standard approach to speech enhancement is to train a deep neural
network (DNN) to take noisy speech as input and output clean speech. Although
this supervised approach requires a very large amount of pair data for
training, it is not robust against unknown environments. Another approach is to
use non-negative matrix factorization (NMF) based on basis spectra trained on
clean speech in advance and those adapted to noise on the fly. This
semi-supervised approach, however, causes considerable signal distortion in
enhanced speech due to the unrealistic assumption that speech spectrograms are
linear combinations of the basis spectra. Replacing the poor linear generative
model of clean speech in NMF with a VAE---a powerful nonlinear deep generative
model---trained on clean speech, we formulate a unified probabilistic
generative model of noisy speech. Given noisy speech as observed data, we can
sample clean speech from its posterior distribution. The proposed method
outperformed the conventional DNN-based method in unseen noisy environments.
",Computer Science; Statistics,Computer Science
"SOLAR: Deep Structured Latent Representations for Model-Based Reinforcement Learning   Model-based reinforcement learning (RL) methods can be broadly categorized as
global model methods, which depend on learning models that provide sensible
predictions in a wide range of states, or local model methods, which
iteratively refit simple models that are used for policy improvement. While
predicting future states that will result from the current actions is
difficult, local model methods only attempt to understand system dynamics in
the neighborhood of the current policy, making it possible to produce local
improvements without ever learning to predict accurately far into the future.
The main idea in this paper is that we can learn representations that make it
easy to retrospectively infer simple dynamics given the data from the current
policy, thus enabling local models to be used for policy learning in complex
systems. To that end, we focus on learning representations with probabilistic
graphical model (PGM) structure, which allows us to devise an efficient local
model method that infers dynamics from real-world rollouts with the PGM as a
global prior. We compare our method to other model-based and model-free RL
methods on a suite of robotics tasks, including manipulation tasks on a real
Sawyer robotic arm directly from camera images. Videos of our results are
available at this https URL
",Computer Science; Statistics,Computer Science; Statistics
"It Takes Two to Tango: Towards Theory of AI's Mind   Theory of Mind is the ability to attribute mental states (beliefs, intents,
knowledge, perspectives, etc.) to others and recognize that these mental states
may differ from one's own. Theory of Mind is critical to effective
communication and to teams demonstrating higher collective performance. To
effectively leverage the progress in Artificial Intelligence (AI) to make our
lives more productive, it is important for humans and AI to work well together
in a team. Traditionally, there has been much emphasis on research to make AI
more accurate, and (to a lesser extent) on having it better understand human
intentions, tendencies, beliefs, and contexts. The latter involves making AI
more human-like and having it develop a theory of our minds. In this work, we
argue that for human-AI teams to be effective, humans must also develop a
theory of AI's mind (ToAIM) - get to know its strengths, weaknesses, beliefs,
and quirks. We instantiate these ideas within the domain of Visual Question
Answering (VQA). We find that using just a few examples (50), lay people can be
trained to better predict responses and oncoming failures of a complex VQA
model. We further evaluate the role existing explanation (or interpretability)
modalities play in helping humans build ToAIM. Explainable AI has received
considerable scientific and popular attention in recent times. Surprisingly, we
find that having access to the model's internal states - its confidence in its
top-k predictions, explicit or implicit attention maps which highlight regions
in the image (and words in the question) the model is looking at (and listening
to) while answering a question about an image - do not help people better
predict its behavior.
",Computer Science,Computer Science
"New low-mass eclipsing binary systems in Praesepe discovered by K2   We present the discovery of four low-mass ($M<0.6$ $M_\odot$) eclipsing
binary (EB) systems in the sub-Gyr old Praesepe open cluster using Kepler/K2
time-series photometry and Keck/HIRES spectroscopy. We present a new Gaussian
process eclipsing binary model, GP-EBOP, as well as a method of simultaneously
determining effective temperatures and distances for EBs. Three of the reported
systems (AD 3814, AD 2615 and AD 1508) are detached and double-lined, and
precise solutions are presented for the first two. We determine masses and
radii to 1-3% precision for AD 3814 and to 5-6% for AD 2615. Together with
effective temperatures determined to $\sim$50 K precision, we test the PARSEC
v1.2 and BHAC15 stellar evolution models. Our EB parameters are more consistent
with the PARSEC models, primarily because the BHAC15 temperature scale is
hotter than our data over the mid M-dwarf mass range probed. Both ADs 3814 and
2615, which have orbital periods of 6.0 and 11.6 days, are circularized but not
synchronized. This suggests that either synchronization proceeds more slowly in
fully convective stars than the theory of equilibrium tides predicts or
magnetic braking is currently playing a more important role than tidal forces
in the spin evolution of these binaries. The fourth system (AD 3116) comprises
a brown dwarf transiting a mid M-dwarf, which is the first such system
discovered in a sub-Gyr open cluster. Finally, these new discoveries increase
the number of characterized EBs in sub-Gyr open clusters by 20% (40%) below
$M<1.5$ $M_{\odot}$ ($M<0.6$ $M_{\odot}$).
",Physics,Physics
"Preserving Differential Privacy in Convolutional Deep Belief Networks   The remarkable development of deep learning in medicine and healthcare domain
presents obvious privacy issues, when deep neural networks are built on users'
personal and highly sensitive data, e.g., clinical records, user profiles,
biomedical images, etc. However, only a few scientific studies on preserving
privacy in deep learning have been conducted. In this paper, we focus on
developing a private convolutional deep belief network (pCDBN), which
essentially is a convolutional deep belief network (CDBN) under differential
privacy. Our main idea of enforcing epsilon-differential privacy is to leverage
the functional mechanism to perturb the energy-based objective functions of
traditional CDBNs, rather than their results. One key contribution of this work
is that we propose the use of Chebyshev expansion to derive the approximate
polynomial representation of objective functions. Our theoretical analysis
shows that we can further derive the sensitivity and error bounds of the
approximate polynomial representation. As a result, preserving differential
privacy in CDBNs is feasible. We applied our model in a health social network,
i.e., YesiWell data, and in a handwriting digit dataset, i.e., MNIST data, for
human behavior prediction, human behavior classification, and handwriting digit
recognition tasks. Theoretical analysis and rigorous experimental evaluations
show that the pCDBN is highly effective. It significantly outperforms existing
solutions.
",Computer Science; Statistics,Computer Science; Statistics
"Decay Estimates and Strichartz Estimates of Fourth-order Schrödinger Operator   We study time decay estimates of the fourth-order Schrödinger operator
$H=(-\Delta)^{2}+V(x)$ in $\mathbb{R}^{d}$ for $d=3$ and $d\geq5$. We analyze
the low energy and high energy behaviour of resolvent $R(H; z)$, and then
derive the Jensen-Kato dispersion decay estimate and local decay estimate for
$e^{-itH}P_{ac}$ under suitable spectrum assumptions of $H$. Based on
Jensen-Kato decay estimate and local decay estimate, we obtain the
$L^1\rightarrow L^{\infty}$ estimate of $e^{-itH}P_{ac}$ in $3$-dimension by
Ginibre argument, and also establish the endpoint global Strichartz estimates
of $e^{-itH}P_{ac}$ for $d\geq5$. Furthermore, using the local decay estimate
and the Georgescu-Larenas-Soffer conjugate operator method, we prove the
Jensen-Kato type decay estimates for some functions of $H$.
",Mathematics,Mathematics
"Adaptive Risk Bounds in Univariate Total Variation Denoising and Trend Filtering   We study trend filtering, a relatively recent method for univariate
nonparametric regression. For a given positive integer $r$, the $r$-th order
trend filtering estimator is defined as the minimizer of the sum of squared
errors when we constrain (or penalize) the sum of the absolute $r$-th order
discrete derivatives of the fitted function at the design points. For $r=1$,
the estimator reduces to total variation regularization which has received much
attention in the statistics and image processing literature. In this paper, we
study the performance of the trend filtering estimator for every positive
integer $r$, both in the constrained and penalized forms. Our main results show
that in the strong sparsity setting when the underlying function is a
(discrete) spline with few ""knots"", the risk (under the global squared error
loss) of the trend filtering estimator (with an appropriate choice of the
tuning parameter) achieves the parametric $n^{-1}$ rate, up to a logarithmic
(multiplicative) factor. Our results therefore provide support for the use of
trend filtering, for every $r$, in the strong sparsity setting.
",Mathematics; Statistics,Mathematics; Statistics
"On periodic solutions of nonlinear wave equations, including Einstein equations with a negative cosmological constant   We construct periodic solutions of nonlinear wave equations using analytic
continuation. The construction applies in particular to Einstein equations,
leading to infinite-dimensional families of time-periodic solutions of the
vacuum, or of the Einstein-Maxwell-dilaton-scalar
fields-Yang-Mills-Higgs-Chern-Simons-$f(R)$ equations, with a negative
cosmological constant.
",Mathematics,Physics
"Blind Gain and Phase Calibration via Sparse Spectral Methods   Blind gain and phase calibration (BGPC) is a bilinear inverse problem
involving the determination of unknown gains and phases of the sensing system,
and the unknown signal, jointly. BGPC arises in numerous applications, e.g.,
blind albedo estimation in inverse rendering, synthetic aperture radar
autofocus, and sensor array auto-calibration. In some cases, sparse structure
in the unknown signal alleviates the ill-posedness of BGPC. Recently there has
been renewed interest in solutions to BGPC with careful analysis of error
bounds. In this paper, we formulate BGPC as an eigenvalue/eigenvector problem,
and propose to solve it via power iteration, or in the sparsity or joint
sparsity case, via truncated power iteration. Under certain assumptions, the
unknown gains, phases, and the unknown signal can be recovered simultaneously.
Numerical experiments show that power iteration algorithms work not only in the
regime predicted by our main results, but also in regimes where theoretical
analysis is limited. We also show that our power iteration algorithms for BGPC
compare favorably with competing algorithms in adversarial conditions, e.g.,
with noisy measurement or with a bad initial estimate.
",Computer Science,Computer Science
"On the Humphreys conjecture on support varieties of tilting modules   Let $G$ be a simply-connected semisimple algebraic group over an
algebraically closed field of characteristic $p$, assumed to be larger than the
Coxeter number. The ""support variety"" of a $G$-module $M$ is a certain closed
subvariety of the nilpotent cone of $G$, defined in terms of cohomology for the
first Frobenius kernel $G_1$. In the 1990s, Humphreys proposed a conjectural
description of the support varieties of tilting modules; this conjecture has
been proved for $G = \mathrm{SL}_n$ in earlier work of the second author.
In this paper, we show that for any $G$, the support variety of a tilting
module always contains the variety predicted by Humphreys, and that they
coincide (i.e., the Humphreys conjecture is true) when $p$ is sufficiently
large. We also prove variants of these statements involving ""relative support
varieties.""
",Mathematics,Mathematics
"Unified theory for finite Markov chains   We provide a unified framework to compute the stationary distribution of any
finite irreducible Markov chain or equivalently of any irreducible random walk
on a finite semigroup $S$. Our methods use geometric finite semigroup theory
via the Karnofsky-Rhodes and the McCammond expansions of finite semigroups with
specified generators; this does not involve any linear algebra. The original
Tsetlin library is obtained by applying the expansions to $P(n)$, the set of
all subsets of an $n$ element set. Our set-up generalizes previous
groundbreaking work involving left-regular bands (or $\mathscr{R}$-trivial
bands) by Brown and Diaconis, extensions to $\mathscr{R}$-trivial semigroups by
Ayyer, Steinberg, Thiéry and the second author, and important recent work by
Chung and Graham. The Karnofsky-Rhodes expansion of the right Cayley graph of
$S$ in terms of generators yields again a right Cayley graph. The McCammond
expansion provides normal forms for elements in the expanded $S$. Using our
previous results with Silva based on work by Berstel, Perrin, Reutenauer, we
construct (infinite) semaphore codes on which we can define Markov chains.
These semaphore codes can be lumped using geometric semigroup theory. Using
normal forms and associated Kleene expressions, they yield formulas for the
stationary distribution of the finite Markov chain of the expanded $S$ and the
original $S$. Analyzing the normal forms also provides an estimate on the
mixing time.
",Mathematics,Mathematics
"Modeling and Reasoning About Wireless Networks: A Graph-based Calculus Approach   We propose a graph-based process calculus for modeling and reasoning about
wireless networks with local broadcasts. Graphs are used at syntactical level
to describe the topological structures of networks. This calculus is equipped
with a reduction semantics and a labelled transition semantics. The former is
used to define weak barbed congruence. The latter is used to define a
parameterized weak bisimulation emphasizing locations and local broadcasts. We
prove that weak bisimilarity implies weak barbed congruence. The potential
applications are illustrated by some examples and two case studies.
",Computer Science,Computer Science
"Transmission spectra and valley processing of graphene and carbon nanotube superlattices with inter-valley coupling   We numerically investigate the electronic transport properties of graphene
nanoribbons and carbon nanotubes with inter-valley coupling, e.g., in \sqrt{3}N
\times \sqrt{3}N and 3N \times 3N superlattices. By taking the \sqrt{3} \times
\sqrt{3} graphene superlattice as an example, we show that tailoring the bulk
graphene superlattice results in rich structural configurations of nanoribbons
and nanotubes. After studying the electronic characteristics of the
corresponding armchair and zigzag nanoribbon geometries, we find that the
linear bands of carbon nanotubes can lead to the Klein tunnelling-like
phenomenon, i.e., electrons propagate along tubes without backscattering even
in the presence of a barrier. Due to the coupling between K and K' valleys of
pristine graphene by \sqrt{3} \times \sqrt{3} supercells,we propose a
valley-field-effect transistor based on the armchair carbon nanotube, where the
valley polarization of the current can be tuned by applying a gate voltage or
varying the length of the armchair carbon nanotubes.
",Physics,Physics
"On the Privacy of the Opal Data Release: A Response   This document is a response to a report from the University of Melbourne on
the privacy of the Opal dataset release. The Opal dataset was released by
Data61 (CSIRO) in conjunction with the Transport for New South Wales (TfNSW).
The data consists of two separate weeks of ""tap-on/tap-off"" data of individuals
who used any of the four different modes of public transport from TfNSW: buses,
light rail, train and ferries. These taps are recorded through the smart
ticketing system, known as Opal, available in the state of New South Wales,
Australia.
",Computer Science,Computer Science
"A remark on the disorienting of species due to the fluctuating environment   In this article we study the stabilizing of a primitive pattern of behaviour
for the two-species community with chemotaxis due to the short-wavelength
external signal. We use a system of Patlak-Keller-Segel type as a model of the
community. It is well-known that such systems can produce complex unsteady
patterns of behaviour which are usually explained mathematically by
bifurcations of some basic solutions that describe simpler patterns. As far as
we aware, all such bifurcations in the models of the Patlak-Keller-Segel type
had been found for homogeneous (i.e. translationally invariant) systems where
the basic solutions are equilibria with homogeneous distributions of all
species. The model considered in the present paper does not possess the
translational invariance: one of species (the predators) is assumed to be
capable of moving in response to a signal produced externally in addition to
the signal emitted by another species (the prey). For instance, the external
signal may arise from the inhomogeneity of the distribution of an environmental
characteristic such as temperature, salinity, terrain relief, etc. Our goal is
to examine the effect of short-wavelength inhomogeneity. To do this, we employ
a certain homogenization procedure. We separate the short-wavelength and smooth
components of the system response and derive a slow system governing the latter
one. Analysing the slow system and comparing it with the case of homogeneous
environment shows that, generically, a short-wavelength inhomogeneity results
in an exponential decrease in the motility of the predators. The loss of
motility prevents, to a great extent, the occurrence of complex unsteady
patterns and dramatically stabilizes the primitive basic solution. In some
sense, the necessity of dealing with intensive small-scale changes of the
environment makes the system unable to respond to other challenges.
",Quantitative Biology,Quantitative Biology
"ADN: An Information-Centric Networking Architecture for the Internet of Things   Forwarding data by name has been assumed to be a necessary aspect of an
information-centric redesign of the current Internet architecture that makes
content access, dissemination, and storage more efficient. The Named Data
Networking (NDN) and Content-Centric Networking (CCNx) architectures are the
leading examples of such an approach. However, forwarding data by name incurs
storage and communication complexities that are orders of magnitude larger than
solutions based on forwarding data using addresses. Furthermore, the specific
algorithms used in NDN and CCNx have been shown to have a number of
limitations. The Addressable Data Networking (ADN) architecture is introduced
as an alternative to NDN and CCNx. ADN is particularly attractive for
large-scale deployments of the Internet of Things (IoT), because it requires
far less storage and processing in relaying nodes than NDN. ADN allows things
and data to be denoted by names, just like NDN and CCNx do. However, instead of
replacing the waist of the Internet with named-data forwarding, ADN uses an
address-based forwarding plane and introduces an information plane that
seamlessly maps names to addresses without the involvement of end-user
applications. Simulation results illustrate the order of magnitude savings in
complexity that can be attained with ADN compared to NDN.
",Computer Science,Computer Science
"Problems on Matchings and Independent Sets of a Graph   Let $G$ be a finite simple graph. For $X \subset V(G)$, the difference of
$X$, $d(X) := |X| - |N (X)|$ where $N(X)$ is the neighborhood of $X$ and $\max
\, \{d(X):X\subset V(G)\}$ is called the critical difference of $G$. $X$ is
called a critical set if $d(X)$ equals the critical difference and ker$(G)$ is
the intersection of all critical sets. It is known that ker$(G)$ is an
independent (vertex) set of $G$. diadem$(G)$ is the union of all critical
independent sets. An independent set $S$ is an inclusion minimal set with $d(S)
> 0$ if no proper subset of $S$ has positive difference.
A graph $G$ is called König-Egerváry if the sum of its independence
number ($\alpha (G)$) and matching number ($\mu (G)$) equals $|V(G)|$. It is
known that bipartite graphs are König-Egerváry.
In this paper, we study independent sets with positive difference for which
every proper subset has a smaller difference and prove a result conjectured by
Levit and Mandrescu in 2013. The conjecture states that for any graph, the
number of inclusion minimal sets $S$ with $d(S) > 0$ is at least the critical
difference of the graph. We also give a short proof of the inequality
$|$ker$(G)| + |$diadem$(G)| \le 2\alpha (G)$ (proved by Short in 2016).
A characterization of unicyclic non-König-Egerváry graphs is also
presented and a conjecture which states that for such a graph $G$, the critical
difference equals $\alpha (G) - \mu (G)$, is proved.
We also make an observation about ker$G)$ using Edmonds-Gallai Structure
Theorem as a concluding remark.
",Computer Science; Mathematics,Mathematics
"DeepSD: Generating High Resolution Climate Change Projections through Single Image Super-Resolution   The impacts of climate change are felt by most critical systems, such as
infrastructure, ecological systems, and power-plants. However, contemporary
Earth System Models (ESM) are run at spatial resolutions too coarse for
assessing effects this localized. Local scale projections can be obtained using
statistical downscaling, a technique which uses historical climate observations
to learn a low-resolution to high-resolution mapping. Depending on statistical
modeling choices, downscaled projections have been shown to vary significantly
terms of accuracy and reliability. The spatio-temporal nature of the climate
system motivates the adaptation of super-resolution image processing techniques
to statistical downscaling. In our work, we present DeepSD, a generalized
stacked super resolution convolutional neural network (SRCNN) framework for
statistical downscaling of climate variables. DeepSD augments SRCNN with
multi-scale input channels to maximize predictability in statistical
downscaling. We provide a comparison with Bias Correction Spatial
Disaggregation as well as three Automated-Statistical Downscaling approaches in
downscaling daily precipitation from 1 degree (~100km) to 1/8 degrees (~12.5km)
over the Continental United States. Furthermore, a framework using the NASA
Earth Exchange (NEX) platform is discussed for downscaling more than 20 ESM
models with multiple emission scenarios.
",Computer Science,Computer Science; Physics
"On the stability and applications of distance-based flexible formations   This paper investigates the stability of distance-based \textit{flexible}
undirected formations in the plane. Without rigidity, there exists a set of
connected shapes for given distance constraints, which is called the ambit. We
show that a flexible formation can lose its flexibility, or equivalently may
reduce the degrees of freedom of its ambit, if a small disturbance is
introduced in the range sensor of the agents. The stability of the disturbed
equilibrium can be characterized by analyzing the eigenvalues of the linearized
augmented error system. Unlike infinitesimally rigid formations, the disturbed
desired equilibrium can be turned unstable regardless of how small the
disturbance is. We finally present two examples of how to exploit these
disturbances as design parameters. The first example shows how to combine rigid
and flexible formations such that some of the agents can move freely in the
desired and locally stable ambit. The second example shows how to achieve a
specific shape with fewer edges than the necessary for the standard controller
in rigid formations.
",Computer Science,Computer Science
"An Equivalence of Fully Connected Layer and Convolutional Layer   This article demonstrates that convolutional operation can be converted to
matrix multiplication, which has the same calculation way with fully connected
layer. The article is helpful for the beginners of the neural network to
understand how fully connected layer and the convolutional layer work in the
backend. To be concise and to make the article more readable, we only consider
the linear case. It can be extended to the non-linear case easily through
plugging in a non-linear encapsulation to the values like this $\sigma(x)$
denoted as $x^{\prime}$.
",Computer Science; Statistics,Computer Science
"Cooperation and Environment Characterize the Low-Lying Optical Spectrum of Liquid Water   The optical spectrum of liquid water is analyzed by subsystem time-dependent
density functional theory. We provide simple explanations for several important
(and so far elusive) features. Due to the disordered environment surrounding
each water molecule, the joint density of states of the liquid is much broader
than that of the vapor. This results in a red shifted Urbach tail. Confinement
effects provided by the first solvation shell are responsible for the blue
shift of the first absorption peak compared to the vapor. In addition, we also
characterize many-body excitonic effects. These dramatically affect the
spectral weights at low frequencies, contributing to the refractive index by a
small but significant amount.
",Physics,Physics
"Extended quantum field theory, index theory and the parity anomaly   We use techniques from functorial quantum field theory to provide a geometric
description of the parity anomaly in fermionic systems coupled to background
gauge and gravitational fields on odd-dimensional spacetimes. We give an
explicit construction of a geometric cobordism bicategory which incorporates
general background fields in a stack, and together with the theory of symmetric
monoidal bicategories we use it to provide the concrete forms of invertible
extended quantum field theories which capture anomalies in both the path
integral and Hamiltonian frameworks. Specialising this situation by using the
extension of the Atiyah-Patodi-Singer index theorem to manifolds with corners
due to Loya and Melrose, we obtain a new Hamiltonian perspective on the parity
anomaly. We compute explicitly the 2-cocycle of the projective representation
of the gauge symmetry on the quantum state space, which is defined in a
parity-symmetric way by suitably augmenting the standard chiral fermionic Fock
spaces with Lagrangian subspaces of zero modes of the Dirac Hamiltonian that
naturally appear in the index theorem. We describe the significance of our
constructions for the bulk-boundary correspondence in a large class of
time-reversal invariant gauge-gravity symmetry-protected topological phases of
quantum matter with gapless charged boundary fermions, including the standard
topological insulator in 3+1 dimensions.
",Physics; Mathematics,Physics
"Filamentary superconductivity in semiconducting policrystalline ZrSe2 compound with Zr vacancies   ZrSe2 is a band semiconductor studied long time ago. It has interesting
electronic properties, and because its layers structure can be intercalated
with different atoms to change some of the physical properties. In this
investigation we found that Zr deficiencies alter the semiconducting behavior
and the compound can be turned into a superconductor. In this paper we report
our studies related to this discovery. The decreasing of the number of Zr atoms
in small proportion according to the formula ZrxSe2, where x is varied from
about 8.1 to 8.6 K, changing the semiconducting behavior to a superconductor
with transition temperatures ranging between 7.8 to 8.5 K, it depending of the
deficiencies. Outside of those ranges the compound behaves as semiconducting
with the properties already known. In our experiments we found that this new
superconductor has only a very small fraction of superconducting material
determined by magnetic measurements with applied magnetic field of 10 Oe. Our
conclusions is that superconductivity is filamentary. However, in one studied
sample the fraction was about 10.2 %, whereas in others is only about 1 % or
less. We determined the superconducting characteristics; the critical fields
that indicate a type two superonductor with Ginzburg-Landau ? parameter of the
order about 2.7. The synthesis procedure is quite normal fol- lowing the
conventional solid state reaction. In this paper are included, the electronic
characteristics, transition temperature, and evolution with temperature of the
critical fields.
",Physics,Physics
"Theoretical limitations of Encoder-Decoder GAN architectures   Encoder-decoder GANs architectures (e.g., BiGAN and ALI) seek to add an
inference mechanism to the GANs setup, consisting of a small encoder deep net
that maps data-points to their succinct encodings. The intuition is that being
forced to train an encoder alongside the usual generator forces the system to
learn meaningful mappings from the code to the data-point and vice-versa, which
should improve the learning of the target distribution and ameliorate
mode-collapse. It should also yield meaningful codes that are useful as
features for downstream tasks. The current paper shows rigorously that even on
real-life distributions of images, the encode-decoder GAN training objectives
(a) cannot prevent mode collapse; i.e. the objective can be near-optimal even
when the generated distribution has low and finite support (b) cannot prevent
learning meaningless codes for data -- essentially white noise. Thus if
encoder-decoder GANs do indeed work then it must be due to reasons as yet not
understood, since the training objective can be low even for meaningless
solutions.
",Computer Science; Statistics,Computer Science; Statistics
"Stochastic Calculus with respect to Gaussian Processes: Part I   Stochastic integration \textit{wrt} Gaussian processes has raised strong
interest in recent years, motivated in particular by its applications in
Internet traffic modeling, biomedicine and finance. The aim of this work is to
define and develop a White Noise Theory-based anticipative stochastic calculus
with respect to all Gaussian processes that have an integral representation
over a real (maybe infinite) interval. Very rich, this class of Gaussian
processes contains, among many others, Volterra processes (and thus fractional
Brownian motion) as well as processes the regularity of which varies along the
time (such as multifractional Brownian motion).A systematic comparison of the
stochastic calculus (including It{ô} formula) we provide here, to the ones
given by Malliavin calculus in
\cite{nualart,MV05,NuTa06,KRT07,KrRu10,LN12,SoVi14,LN12}, and by It{ô}
stochastic calculus is also made. Not only our stochastic calculus fully
generalizes and extends the ones originally proposed in \cite{MV05} and in
\cite{NuTa06} for Gaussian processes, but also the ones proposed in
\cite{ell,bosw,ben1} for fractional Brownian motion (\textit{resp.} in
\cite{JLJLV1,JL13,LLVH} for multifractional Brownian motion).
",Mathematics,Computer Science; Mathematics
"Von Neumann Regular Cellular Automata   For any group $G$ and any set $A$, a cellular automaton (CA) is a
transformation of the configuration space $A^G$ defined via a finite memory set
and a local function. Let $\text{CA}(G;A)$ be the monoid of all CA over $A^G$.
In this paper, we investigate a generalisation of the inverse of a CA from the
semigroup-theoretic perspective. An element $\tau \in \text{CA}(G;A)$ is von
Neumann regular (or simply regular) if there exists $\sigma \in \text{CA}(G;A)$
such that $\tau \circ \sigma \circ \tau = \tau$ and $\sigma \circ \tau \circ
\sigma = \sigma$, where $\circ$ is the composition of functions. Such an
element $\sigma$ is called a generalised inverse of $\tau$. The monoid
$\text{CA}(G;A)$ itself is regular if all its elements are regular. We
establish that $\text{CA}(G;A)$ is regular if and only if $\vert G \vert = 1$
or $\vert A \vert = 1$, and we characterise all regular elements in
$\text{CA}(G;A)$ when $G$ and $A$ are both finite. Furthermore, we study
regular linear CA when $A= V$ is a vector space over a field $\mathbb{F}$; in
particular, we show that every regular linear CA is invertible when $G$ is
torsion-free elementary amenable (e.g. when $G=\mathbb{Z}^d, \ d \in
\mathbb{N}$) and $V=\mathbb{F}$, and that every linear CA is regular when $V$
is finite-dimensional and $G$ is locally finite with $\text{Char}(\mathbb{F})
\nmid o(g)$ for all $g \in G$.
",Computer Science; Mathematics,Computer Science
"Average sampling and average splines on combinatorial graphs   In the setting of a weighted combinatorial finite or infinite countable graph
$G$ we introduce functional Paley-Wiener spaces $PW_{\omega}(L),\>\omega>0,$
defined in terms of the spectral resolution of the combinatorial Laplace
operator $L$ in the space $L_{2}(G)$. It is shown that functions in certain
$PW_{\omega}(L),\>\omega>0,$ are uniquely defined by their averages over some
families of ""small"" subgraphs which form a cover of $G$. Reconstruction methods
for reconstruction of an $f\in PW_{\omega}(L)$ from appropriate set of its
averages are introduced. One method is using language of Hilbert frames.
Another one is using average variational interpolating splines which are
constructed in the setting of combinatorial graphs.
",Computer Science,Mathematics
"Path integral molecular dynamics with surface hopping for thermal equilibrium sampling of nonadiabatic systems   In this work, a novel ring polymer representation for multi-level quantum
system is proposed for thermal average calculations. The proposed presentation
keeps the discreteness of the electronic states: besides position and momentum,
each bead in the ring polymer is also characterized by a surface index
indicating the electronic energy surface. A path integral molecular dynamics
with surface hopping (PIMD-SH) dynamics is also developed to sample the
equilibrium distribution of ring polymer configurational space. The PIMD-SH
sampling method is validated theoretically and by numerical examples.
",Physics; Mathematics,Physics
"On the normal centrosymmetric Nonnegative inverse eigenvalue problem   We give sufficient conditions of the nonnegative inverse eigenvalue problem
(NIEP) for normal centrosymmetric matrices. These sufficient conditions are
analogous to the sufficient conditions of the NIEP for normal matrices given by
Xu [16] and Julio, Manzaneda and Soto [2].
",Mathematics,Mathematics
"A Nonlinear Kernel Support Matrix Machine for Matrix Learning   In many problems of supervised tensor learning (STL), real world data such as
face images or MRI scans are naturally represented as matrices, which are also
called as second order tensors. Most existing classifiers based on tensor
representation, such as support tensor machine (STM) need to solve iteratively
which occupy much time and may suffer from local minima. In this paper, we
present a kernel support matrix machine (KSMM) to perform supervised learning
when data are represented as matrices. KSMM is a general framework for the
construction of matrix-based hyperplane to exploit structural information. We
analyze a unifying optimization problem for which we propose an asymptotically
convergent algorithm. Theoretical analysis for the generalization bounds is
derived based on Rademacher complexity with respect to a probability
distribution. We demonstrate the merits of the proposed method by exhaustive
experiments on both simulation study and a number of real-word datasets from a
variety of application domains.
",Computer Science; Statistics,Computer Science; Statistics
"Combined MEG and fMRI Exponential Random Graph Modeling for inferring functional Brain Connectivity   Estimated connectomes by the means of neuroimaging techniques have enriched
our knowledge of the organizational properties of the brain leading to the
development of network-based clinical diagnostics. Unfortunately, to date, many
of those network-based clinical diagnostics tools, based on the mere
description of isolated instances of observed connectomes are noisy estimates
of the true connectivity network. Modeling brain connectivity networks is
therefore important to better explain the functional organization of the brain
and allow inference of specific brain properties. In this report, we present
pilot results on the modeling of combined MEG and fMRI neuroimaging data
acquired during an n-back memory task experiment. We adopted a pooled
Exponential Random Graph Model (ERGM) as a network statistical model to capture
the underlying process in functional brain networks of 9 subjects MEG and fMRI
data out of 32 during a 0-back vs 2-back memory task experiment. Our results
suggested strong evidence that all the functional connectomes of the 9 subjects
have small world properties. A group level comparison using comparing the
conditions pairwise showed no significant difference in the functional
connectomes across the subjects. Our pooled ERGMs successfully reproduced
important brain properties such as functional segregation and functional
integration. However, the ERGMs reproducing the functional segregation of the
brain networks discriminated between the 0-back and 2-back conditions while the
models reproducing both properties failed to successfully discriminate between
both conditions. Our results are promising and would improve in robustness with
a larger sample size. Nevertheless, our pilot results tend to support previous
findings that functional segregation and integration are sufficient to
statistically reproduce the main properties of brain network.
",Quantitative Biology,Statistics
"A numerical scheme for an improved Green-Naghdi model in the Camassa-Holm regime for the propagation of internal waves   In this paper we introduce a new reformulation of the Green-Naghdi model in
the Camassa-Holm regime for the propagation of internal waves over a flat
topography derived by Duchêne, Israwi and Talhouk. These new Green-Naghdi
systems are adapted to improve the frequency dispersion of the original model,
they share the same order of precision as the standard one but have an
appropriate structure which makes them much more suitable for the numerical
resolution. We develop a second order splitting scheme where the hyperbolic
part of the system is treated with a high-order finite volume scheme and the
dispersive part is treated with a finite difference approach. Numerical
simulations are then performed to validate the model and the numerical methods.
",Mathematics,Physics
"Cosmological Simulations in Exascale Era   The architecture of Exascale computing facilities, which involves millions of
heterogeneous processing units, will deeply impact on scientific applications.
Future astrophysical HPC applications must be designed to make such computing
systems exploitable. The ExaNeSt H2020 EU-funded project aims to design and
develop an exascale ready prototype based on low-energy-consumption ARM64 cores
and FPGA accelerators. We participate to the design of the platform and to the
validation of the prototype with cosmological N-body and hydrodynamical codes
suited to perform large-scale, high-resolution numerical simulations of cosmic
structures formation and evolution. We discuss our activities on astrophysical
applications to take advantage of the underlying architecture.
",Computer Science; Physics,Physics
"New Derivatives for the Functions with the Fractal Tartan Support   In this manuscript, we generalize F-calculus to apply it on fractal Tartan
spaces. The generalized standard F-calculus is used to obtain the integral and
derivative of the functions on the fractal Tartan with different dimensions.
The generalized fractional derivatives have local properties that make it more
useful in modelling physical problems. The illustrative examples are used to
present the details.
",Mathematics,Mathematics
"A null test of General Relativity: New limits on Local Position Invariance and the variation of fundamental constants   We compare the long-term fractional frequency variation of four hydrogen
masers that are part of an ensemble of clocks comprising the National Institute
of Standards and Technology,(NIST), Boulder, timescale with the fractional
frequencies of primary frequency standards operated by leading metrology
laboratories in the United States, France, Germany, Italy and the United
Kingdom for a period extending more than 14 years. The measure of the assumed
variation of non-gravitational interaction,(LPI parameter, $\beta$)---within
the atoms of H and Cs---over time as the earth orbits the sun, has been
constrained to $\beta=(2.2 \pm 2.5)\times 10^{-7}$, a factor of two improvement
over previous estimates. Using our results together with the previous best
estimates of $\beta$ based on Rb vs. Cs, and Rb vs. H comparisons, we impose
the most stringent limits to date on the dimensionless coupling constants that
relate the variation of fundamental constants such as the fine-structure
constant and the scaled quark mass with strong(QCD) interaction to the
variation in the local gravitational potential. For any metric theory of
gravity $\beta=0$.
",Physics,Physics
"OpenML Benchmarking Suites and the OpenML100   We advocate the use of curated, comprehensive benchmark suites of machine
learning datasets, backed by standardized OpenML-based interfaces and
complementary software toolkits written in Python, Java and R. Major
distinguishing features of OpenML benchmark suites are (a) ease of use through
standardized data formats, APIs, and existing client libraries; (b)
machine-readable meta-information regarding the contents of the suite; and (c)
online sharing of results, enabling large scale comparisons. As a first such
suite, we propose the OpenML100, a machine learning benchmark suite of
100~classification datasets carefully curated from the thousands of datasets
available on OpenML.org.
",Computer Science; Statistics,Computer Science; Statistics
"Analyzing Cloud Optical Properties Using Sky Cameras   Clouds play a significant role in the fluctuation of solar radiation received
by the earth's surface. It is important to study the various cloud properties,
as it impacts the total solar irradiance falling on the earth's surface. One of
such important optical properties of the cloud is the Cloud Optical Thickness
(COT). It is defined with the amount of light that can pass through the clouds.
The COT values are generally obtained from satellite images. However, satellite
images have a low temporal- and spatial- resolutions; and are not suitable for
study in applications as solar energy generation and forecasting. Therefore,
ground-based sky cameras are now getting popular in such fields. In this paper,
we analyze the cloud optical thickness value, from the ground-based sky
cameras, and provide future research directions.
",Physics,Physics
"An adaptive Newton algorithm for optimal control problems with application to optimal electrode design   In this work we present an adaptive Newton-type method to solve nonlinear
constrained optimization problems in which the constraint is a system of
partial differential equations discretized by the finite element method. The
adaptive strategy is based on a goal-oriented a posteriori error estimation for
the discretization and for the iteration error. The iteration error stems from
an inexact solution of the nonlinear system of first order optimality
conditions by the Newton-type method. This strategy allows to balance the two
errors and to derive effective stopping criteria for the Newton-iterations. The
algorithm proceeds with the search of the optimal point on coarse grids which
are refined only if the discretization error becomes dominant. Using computable
error indicators the mesh is refined locally leading to a highly efficient
solution process. The performance of the algorithm is shown with several
examples and in particular with an application in the neurosciences: the
optimal electrode design for the study of neuronal networks.
",Mathematics,Computer Science; Physics
"Delving into adversarial attacks on deep policies   Adversarial examples have been shown to exist for a variety of deep learning
architectures. Deep reinforcement learning has shown promising results on
training agent policies directly on raw inputs such as image pixels. In this
paper we present a novel study into adversarial attacks on deep reinforcement
learning polices. We compare the effectiveness of the attacks using adversarial
examples vs. random noise. We present a novel method for reducing the number of
times adversarial examples need to be injected for a successful attack, based
on the value function. We further explore how re-training on random noise and
FGSM perturbations affects the resilience against adversarial examples.
",Computer Science; Statistics,Computer Science; Statistics
"Fast construction of efficient composite likelihood equations   Growth in both size and complexity of modern data challenges the
applicability of traditional likelihood-based inference. Composite likelihood
(CL) methods address the difficulties related to model selection and
computational intractability of the full likelihood by combining a number of
low-dimensional likelihood objects into a single objective function used for
inference. This paper introduces a procedure to combine partial likelihood
objects from a large set of feasible candidates and simultaneously carry out
parameter estimation. The new method constructs estimating equations balancing
statistical efficiency and computing cost by minimizing an approximate distance
from the full likelihood score subject to a L1-norm penalty representing the
available computing resources. This results in truncated CL equations
containing only the most informative partial likelihood score terms. An
asymptotic theory within a framework where both sample size and data dimension
grow is developed and finite-sample properties are illustrated through
numerical examples.
",Mathematics; Statistics,Mathematics; Statistics
"Lower Bounds for Maximum Gap in (Inverse) Cyclotomic Polynomials   The maximum gap $g(f)$ of a polynomial $f$ is the maximum of the differences
(gaps) between two consecutive exponents that appear in $f$. Let $\Phi_{n}$ and
$\Psi_{n}$ denote the $n$-th cyclotomic and $n$-th inverse cyclotomic
polynomial, respectively. In this paper, we give several lower bounds for
$g(\Phi_{n})$ and $g(\Psi_{n})$, where $n$ is the product of odd primes. We
observe that they are very often exact. We also give an exact expression for
$g(\Psi_{n})$ under a certain condition. Finally we conjecture an exact
expression for $g(\Phi_{n})$ under a certain condition.
",Mathematics,Mathematics
"A Robust Utility Learning Framework via Inverse Optimization   In many smart infrastructure applications flexibility in achieving
sustainability goals can be gained by engaging end-users. However, these users
often have heterogeneous preferences that are unknown to the decision-maker
tasked with improving operational efficiency. Modeling user interaction as a
continuous game between non-cooperative players, we propose a robust parametric
utility learning framework that employs constrained feasible generalized least
squares estimation with heteroskedastic inference. To improve forecasting
performance, we extend the robust utility learning scheme by employing
bootstrapping with bagging, bumping, and gradient boosting ensemble methods.
Moreover, we estimate the noise covariance which provides approximated
correlations between players which we leverage to develop a novel correlated
utility learning framework. We apply the proposed methods both to a toy example
arising from Bertrand-Nash competition between two firms as well as to data
from a social game experiment designed to encourage energy efficient behavior
amongst smart building occupants. Using occupant voting data for shared
resources such as lighting, we simulate the game defined by the estimated
utility functions to demonstrate the performance of the proposed methods.
",Computer Science; Mathematics,Computer Science; Statistics
"On Quitting: Performance and Practice in Online Game Play   We study the relationship between performance and practice by analyzing the
activity of many players of a casual online game. We find significant
heterogeneity in the improvement of player performance, given by score, and
address this by dividing players into similar skill levels and segmenting each
player's activity into sessions, i.e., sequence of game rounds without an
extended break. After disaggregating data, we find that performance improves
with practice across all skill levels. More interestingly, players are more
likely to end their session after an especially large improvement, leading to a
peak score in their very last game of a session. In addition, success is
strongly correlated with a lower quitting rate when the score drops, and only
weakly correlated with skill, in line with psychological findings about the
value of persistence and ""grit"": successful players are those who persist in
their practice despite lower scores. Finally, we train an epsilon-machine, a
type of hidden Markov model, and find a plausible mechanism of game play that
can predict player performance and quitting the game. Our work raises the
possibility of real-time assessment and behavior prediction that can be used to
optimize human performance.
",Computer Science,Computer Science
"Stability analysis of a system coupled to a heat equation   As a first approach to the study of systems coupling finite and infinite
dimensional natures, this article addresses the stability of a system of
ordinary differential equations coupled with a classic heat equation using a
Lyapunov functional technique. Inspired from recent developments in the area of
time delay systems, a new methodology to study the stability of such a class of
distributed parameter systems is presented here. The idea is to use a
polynomial approximation of the infinite dimensional state of the heat equation
in order to build an enriched energy functional. A well known efficient
integral inequality (Bessel inequality) will allow to obtain stability
conditions expressed in terms of linear matrix inequalities. We will eventually
test our approach on academic examples in order to illustrate the efficiency of
our theoretical results.
",Mathematics,Mathematics
"Spectral properties and breathing dynamics of a few-body Bose-Bose mixture in a 1D harmonic trap   We investigate a few-body mixture of two bosonic components, each consisting
of two particles confined in a quasi one-dimensional harmonic trap. By means of
exact diagonalization with a correlated basis approach we obtain the low-energy
spectrum and eigenstates for the whole range of repulsive intra- and
inter-component interaction strengths. We analyse the eigenvalues as a function
of the inter-component coupling, covering hereby all the limiting regimes, and
characterize the behaviour in-between these regimes by exploiting the
symmetries of the Hamiltonian. Provided with this knowledge we study the
breathing dynamics in the linear-response regime by slightly quenching the trap
frequency symmetrically for both components. Depending on the choice of
interactions strengths, we identify 1 to 3 monopole modes besides the breathing
mode of the center of mass coordinate. For the uncoupled mixture each monopole
mode corresponds to the breathing oscillation of a specific relative
coordinate. Increasing the inter-component coupling first leads to multi-mode
oscillations in each relative coordinate, which turn into single-mode
oscillations of the same frequency in the composite-fermionization regime.
",Physics,Physics
"Traveling dark-bright solitons in a reduced spin-orbit coupled system: application to Bose-Einstein condensates   In the present work, we explore the potential of spin-orbit (SO) coupled
Bose-Einstein condensates to support multi-component solitonic states in the
form of dark-bright (DB) solitons. In the case where Raman linear coupling
between components is absent, we use a multiscale expansion method to reduce
the model to the integrable Mel'nikov system. The soliton solutions of the
latter allow us to reconstruct approximate traveling DB solitons for the
reduced SO coupled system. For small values of the formal perturbation
parameter, the resulting waveforms propagate undistorted, while for large
values thereof, they shed some dispersive radiation, and subsequently distill
into a robust propagating structure. After quantifying the relevant radiation
effect, we also study the dynamics of DB solitons in a parabolic trap,
exploring how their oscillation frequency varies as a function of the bright
component mass and the Raman laser wavenumber.
",Physics,Physics
"Geometric tracking control of thrust vectoring UAVs   In this paper a geometric approach to the trajectory tracking control of
Unmanned Aerial Vehicles with thrust vectoring capabilities is proposed. The
control design is suitable for aerial systems that allow to effectively
decouple position and orientation tracking tasks. The control problem is
developed within the framework of geometric control theory on the group of
rigid displacements SE(3), yielding a control law that is independent of any
parametrization of the configuration space. The proposed design works seamlessy
when the thrust vectoring capability is limited, by prioritizing position over
orientation tracking. A characterization of the region of attraction and of the
convergence properties is explicitly derived. Finally, a numerical example is
presented to test the proposed control law. The generality of the control
scheme can be exploited for a broad class of aerial vehicles.
",Computer Science; Mathematics,Computer Science
"The Moon Illusion explained by the Projective Consciousness Model   The Moon often appears larger near the perceptual horizon and smaller high in
the sky though the visual angle subtended is invariant. We show how this
illusion results from the optimization of a projective geometrical frame for
conscious perception through free energy minimization, as articulated in the
Projective Consciousness Model. The model accounts for all documented
modulations of the illusion without anomalies (e.g., the size-distance
paradox), surpasses other theories in explanatory power, makes sense of inter-
and intra-subjective variability vis-a-vis the illusion, and yields new
quantitative and qualitative predictions.
",Quantitative Biology,Physics
"Disorder robustness and protection of Majorana bound states in ferromagnetic chains on conventional superconductors   Majorana bound states (MBS) are well-established in the clean limit in chains
of ferromagnetically aligned impurities deposited on conventional
superconductors with finite spin-orbit coupling. Here we show that these MBS
are very robust against disorder. By performing self-consistent calculations we
find that the MBS are protected as long as the surrounding superconductor show
no large signs of inhomogeneity. We find that longer chains offer more
stability against disorder for the MBS, albeit the minigap decreases, as do
increasing strengths of spin-orbit coupling and superconductivity.
",Physics,Physics
"A System of Three Super Earths Transiting the Late K-Dwarf GJ 9827 at Thirty Parsecs   We report the discovery of three small transiting planets orbiting GJ 9827, a
bright (K = 7.2) nearby late K-type dwarf star. GJ 9827 hosts a $1.62\pm0.11$
$R_{\rm \oplus}$ super Earth on a 1.2 day period, a $1.269^{+0.087}_{-0.089}$
$R_{\rm \oplus}$ super Earth on a 3.6 day period, and a $2.07\pm0.14$ $R_{\rm
\oplus}$ super Earth on a 6.2 day period. The radii of the planets transiting
GJ 9827 span the transition between predominantly rocky and gaseous planets,
and GJ 9827 b and c fall in or close to the known gap in the radius
distribution of small planets between these populations. At a distance of 30
parsecs, GJ 9827 is the closest exoplanet host discovered by K2 to date, making
these planets well-suited for atmospheric studies with the upcoming James Webb
Space Telescope. The GJ 9827 system provides a valuable opportunity to
characterize interior structure and atmospheric properties of coeval planets
spanning the rocky to gaseous transition.
",Physics,Physics
"DyNet: The Dynamic Neural Network Toolkit   We describe DyNet, a toolkit for implementing neural network models based on
dynamic declaration of network structure. In the static declaration strategy
that is used in toolkits like Theano, CNTK, and TensorFlow, the user first
defines a computation graph (a symbolic representation of the computation), and
then examples are fed into an engine that executes this computation and
computes its derivatives. In DyNet's dynamic declaration strategy, computation
graph construction is mostly transparent, being implicitly constructed by
executing procedural code that computes the network outputs, and the user is
free to use different network structures for each input. Dynamic declaration
thus facilitates the implementation of more complicated network architectures,
and DyNet is specifically designed to allow users to implement their models in
a way that is idiomatic in their preferred programming language (C++ or
Python). One challenge with dynamic declaration is that because the symbolic
computation graph is defined anew for every training example, its construction
must have low overhead. To achieve this, DyNet has an optimized C++ backend and
lightweight graph representation. Experiments show that DyNet's speeds are
faster than or comparable with static declaration toolkits, and significantly
faster than Chainer, another dynamic declaration toolkit. DyNet is released
open-source under the Apache 2.0 license and available at
this http URL.
",Computer Science; Statistics,Computer Science
"What Happens - After the First Race? Enhancing the Predictive Power of Happens - Before Based Dynamic Race Detection   Dynamic race detection is the problem of determining if an observed program
execution reveals the presence of a data race in a program. The classical
approach to solving this problem is to detect if there is a pair of conflicting
memory accesses that are unordered by Lamport's happens-before (HB) relation.
HB based race detection is known to not report false positives, i.e., it is
sound. However, the soundness guarantee of HB only promises that the first pair
of unordered, conflicting events is a schedulable data race. That is, there can
be pairs of HB-unordered conflicting data accesses that are not schedulable
races because there is no reordering of the events of the execution, where the
events in race can be executed immediately after each other. We introduce a new
partial order, called schedulable happens-before (SHB) that exactly
characterizes the pairs of schedulable data races --- every pair of conflicting
data accesses that are identified by SHB can be scheduled, and every HB-race
that can be scheduled is identified by SHB. Thus, the SHB partial order is
truly sound. We present a linear time, vector clock algorithm to detect
schedulable races using SHB. Our experiments demonstrate the value of our
algorithm for dynamic race detection --- SHB incurs only little performance
overhead and can scale to executions from real-world software applications
without compromising soundness.
",Computer Science,Computer Science
"Understanding System Characteristics of Online Erasure Coding on Scalable, Distributed and Large-Scale SSD Array Systems   Large-scale systems with arrays of solid state disks (SSDs) have become
increasingly common in many computing segments. To make such systems resilient,
we can adopt erasure coding such as Reed-Solomon (RS) code as an alternative to
replication because erasure coding can offer a significantly lower storage cost
than replication. To understand the impact of using erasure coding on system
performance and other system aspects such as CPU utilization and network
traffic, we build a storage cluster consisting of approximately one hundred
processor cores with more than fifty high-performance SSDs, and evaluate the
cluster with a popular open-source distributed parallel file system, Ceph. Then
we analyze behaviors of systems adopting erasure coding from the following five
viewpoints, compared with those of systems using replication: (1) storage
system I/O performance; (2) computing and software overheads; (3) I/O
amplification; (4) network traffic among storage nodes; (5) the impact of
physical data layout on performance of RS-coded SSD arrays. For all these
analyses, we examine two representative RS configurations, which are used by
Google and Facebook file systems, and compare them with triple replication that
a typical parallel file system employs as a default fault tolerance mechanism.
Lastly, we collect 54 block-level traces from the cluster and make them
available for other researchers.
",Computer Science,Computer Science
"Unsupervised and Semi-supervised Anomaly Detection with LSTM Neural Networks   We investigate anomaly detection in an unsupervised framework and introduce
Long Short Term Memory (LSTM) neural network based algorithms. In particular,
given variable length data sequences, we first pass these sequences through our
LSTM based structure and obtain fixed length sequences. We then find a decision
function for our anomaly detectors based on the One Class Support Vector
Machines (OC-SVM) and Support Vector Data Description (SVDD) algorithms. As the
first time in the literature, we jointly train and optimize the parameters of
the LSTM architecture and the OC-SVM (or SVDD) algorithm using highly effective
gradient and quadratic programming based training methods. To apply the
gradient based training method, we modify the original objective criteria of
the OC-SVM and SVDD algorithms, where we prove the convergence of the modified
objective criteria to the original criteria. We also provide extensions of our
unsupervised formulation to the semi-supervised and fully supervised
frameworks. Thus, we obtain anomaly detection algorithms that can process
variable length data sequences while providing high performance, especially for
time series data. Our approach is generic so that we also apply this approach
to the Gated Recurrent Unit (GRU) architecture by directly replacing our LSTM
based structure with the GRU based structure. In our experiments, we illustrate
significant performance gains achieved by our algorithms with respect to the
conventional methods.
",Computer Science; Statistics,Computer Science; Statistics
"A split step Fourier/discontinuous Galerkin scheme for the Kadomtsev--Petviashvili equation   In this paper we propose a method to solve the Kadomtsev--Petviashvili
equation based on splitting the linear part of the equation from the nonlinear
part. The linear part is treated using FFTs, while the nonlinear part is
approximated using a semi-Lagrangian discontinuous Galerkin approach of
arbitrary order.
We demonstrate the efficiency and accuracy of the numerical method by
providing a range of numerical simulations. In particular, we find that our
approach can outperform the numerical methods considered in the literature by
up to a factor of five. Although we focus on the Kadomtsev--Petviashvili
equation in this paper, the proposed numerical scheme can be extended to a
range of related models as well.
",Physics; Mathematics,Mathematics
"The Forgettable-Watcher Model for Video Question Answering   A number of visual question answering approaches have been proposed recently,
aiming at understanding the visual scenes by answering the natural language
questions. While the image question answering has drawn significant attention,
video question answering is largely unexplored.
Video-QA is different from Image-QA since the information and the events are
scattered among multiple frames. In order to better utilize the temporal
structure of the videos and the phrasal structures of the answers, we propose
two mechanisms: the re-watching and the re-reading mechanisms and combine them
into the forgettable-watcher model. Then we propose a TGIF-QA dataset for video
question answering with the help of automatic question generation. Finally, we
evaluate the models on our dataset. The experimental results show the
effectiveness of our proposed models.
",Computer Science,Computer Science
"A Frame Tracking Model for Memory-Enhanced Dialogue Systems   Recently, resources and tasks were proposed to go beyond state tracking in
dialogue systems. An example is the frame tracking task, which requires
recording multiple frames, one for each user goal set during the dialogue. This
allows a user, for instance, to compare items corresponding to different goals.
This paper proposes a model which takes as input the list of frames created so
far during the dialogue, the current user utterance as well as the dialogue
acts, slot types, and slot values associated with this utterance. The model
then outputs the frame being referenced by each triple of dialogue act, slot
type, and slot value. We show that on the recently published Frames dataset,
this model significantly outperforms a previously proposed rule-based baseline.
In addition, we propose an extensive analysis of the frame tracking task by
dividing it into sub-tasks and assessing their difficulty with respect to our
model.
",Computer Science,Computer Science
"A tale of seven narrow spikes and a long trough: constraining the timing of the percolation of HII bubbles at the tail-end of reionization with ULAS J1120+0641   High-signal to noise observations of the Ly$\alpha$ forest transmissivity in
the z = 7.085 QSO ULAS J1120+0641 show seven narrow transmission spikes
followed by a long 240 cMpc/h trough. Here we use radiative transfer
simulations of cosmic reionization previously calibrated to match a wider range
of Ly$\alpha$ forest data to show that the occurrence of seven transmission
spikes in the narrow redshift range z = 5.85 - 6.1 is very sensitive to the
exact timing of reionization. Occurrence of the spikes requires the most under
dense regions of the IGM to be already fully ionised. The rapid onset of a long
trough at z = 6.12 requires a strong decrease of the photo-ionisation rate at
z$\sim$6.1 in this line-of-sight, consistent with the end of percolation at
this redshift. The narrow range of reionisation histories that we previously
found to be consistent with a wider range of Ly$\alpha$ forest data have a
reasonable probability of showing seven spikes and the mock absorption spectra
provide an excellent match to the spikes and the trough in the observed
spectrum of ULAS J1120+0641. Despite the large overall opacity of Ly$\alpha$ at
z > 5.8, larger samples of high signal-to-noise observations of rare
transmission spikes should therefore provide important further insights into
the exact timing of the percolation of HII bubbles at the tail-end of
reionization
",Physics,Physics
"Learning to Represent Edits   We introduce the problem of learning distributed representations of edits. By
combining a ""neural editor"" with an ""edit encoder"", our models learn to
represent the salient information of an edit and can be used to apply edits to
new inputs. We experiment on natural language and source code edit data. Our
evaluation yields promising results that suggest that our neural network models
learn to capture the structure and semantics of edits. We hope that this
interesting task and data source will inspire other researchers to work further
on this problem.
",Computer Science,Computer Science
"Sampling Errors in Nested Sampling Parameter Estimation   Sampling errors in nested sampling parameter estimation differ from those in
Bayesian evidence calculation, but have been little studied in the literature.
This paper provides the first explanation of the two main sources of sampling
errors in nested sampling parameter estimation, and presents a new diagrammatic
representation for the process. We find no current method can accurately
measure the parameter estimation errors of a single nested sampling run, and
propose a method for doing so using a new algorithm for dividing nested
sampling runs. We empirically verify our conclusions and the accuracy of our
new method.
",Physics; Statistics,Statistics
"Anderson localization in the Non-Hermitian Aubry-André-Harper model with physical gain and loss   We investigate the Anderson localization in non-Hermitian
Aubry-André-Harper (AAH) models with imaginary potentials added to lattice
sites to represent the physical gain and loss during the interacting processes
between the system and environment. By checking the mean inverse participation
ratio (MIPR) of the system, we find that different configurations of physical
gain and loss have very different impacts on the localization phase transition
in the system. In the case with balanced physical gain and loss added in an
alternate way to the lattice sites, the critical region (in the case with
p-wave superconducting pairing) and the critical value (both in the situations
with and without p-wave pairing) for the Anderson localization phase transition
will be significantly reduced, which implies an enhancement of the localization
process. However, if the system is divided into two parts with one of them
coupled to physical gain and the other coupled to the corresponding physical
loss, the transition process will be impacted only in a very mild way. Besides,
we also discuss the situations with imbalanced physical gain and loss and find
that the existence of random imaginary potentials in the system will also
affect the localization process while constant imaginary potentials will not.
",Physics,Physics
"Geometric Enclosing Networks   Training model to generate data has increasingly attracted research attention
and become important in modern world applications. We propose in this paper a
new geometry-based optimization approach to address this problem. Orthogonal to
current state-of-the-art density-based approaches, most notably VAE and GAN, we
present a fresh new idea that borrows the principle of minimal enclosing ball
to train a generator G\left(\bz\right) in such a way that both training and
generated data, after being mapped to the feature space, are enclosed in the
same sphere. We develop theory to guarantee that the mapping is bijective so
that its inverse from feature space to data space results in expressive
nonlinear contours to describe the data manifold, hence ensuring data generated
are also lying on the data manifold learned from training data. Our model
enjoys a nice geometric interpretation, hence termed Geometric Enclosing
Networks (GEN), and possesses some key advantages over its rivals, namely
simple and easy-to-control optimization formulation, avoidance of mode
collapsing and efficiently learn data manifold representation in a completely
unsupervised manner. We conducted extensive experiments on synthesis and
real-world datasets to illustrate the behaviors, strength and weakness of our
proposed GEN, in particular its ability to handle multi-modal data and quality
of generated data.
",Computer Science; Statistics,Computer Science; Statistics
"Portfolio Optimization in Fractional and Rough Heston Models   We consider a fractional version of the Heston volatility model which is
inspired by [16]. Within this model we treat portfolio optimization problems
for power utility functions. Using a suitable representation of the fractional
part, followed by a reasonable approximation we show that it is possible to
cast the problem into the classical stochastic control framework. This approach
is generic for fractional processes. We derive explicit solutions and obtain as
a by-product the Laplace transform of the integrated volatility. In order to
get rid of some undesirable features we introduce a new model for the rough
path scenario which is based on the Marchaud fractional derivative. We provide
a numerical study to underline our results.
",Quantitative Finance,Mathematics
"Understanding Black-box Predictions via Influence Functions   How can we explain the predictions of a black-box model? In this paper, we
use influence functions -- a classic technique from robust statistics -- to
trace a model's prediction through the learning algorithm and back to its
training data, thereby identifying training points most responsible for a given
prediction. To scale up influence functions to modern machine learning
settings, we develop a simple, efficient implementation that requires only
oracle access to gradients and Hessian-vector products. We show that even on
non-convex and non-differentiable models where the theory breaks down,
approximations to influence functions can still provide valuable information.
On linear models and convolutional neural networks, we demonstrate that
influence functions are useful for multiple purposes: understanding model
behavior, debugging models, detecting dataset errors, and even creating
visually-indistinguishable training-set attacks.
",Computer Science; Statistics,Computer Science; Statistics
"Simulating Linear Logic in 1-Only Linear Logic   Linear Logic was introduced by Girard as a resource-sensitive refinement of
classical logic. It turned out that full propositional Linear Logic is
undecidable (Lincoln, Mitchell, Scedrov, and Shankar) and, hence, it is more
expressive than (modalized) classical or intuitionistic logic. In this paper we
focus on the study of the simplest fragments of Linear Logic, such as the
one-literal and constant-only fragments (the latter contains no literals at
all). Here we demonstrate that all these extremely simple fragments of Linear
Logic (one-literal, $\bot$-only, and even unit-only) are exactly of the same
expressive power as the corresponding full versions. We present also a complete
computational interpretation (in terms of acyclic programs with stack) for
bottom-free Intuitionistic Linear Logic. Based on this interpretation, we prove
the fairness of our encodings and establish the foregoing complexity results.
",Computer Science,Computer Science
"Remarks on defective Fano manifolds   This note continues our previous work on special secant defective
(specifically, conic connected and local quadratic entry locus) and dual
defective manifolds. These are now well understood, except for the prime Fano
ones. Here we add a few remarks on this case, completing the results in our
papers \cite{LQEL I}, \cite{LQEL II}, \cite{CC}, \cite{HC} and \cite{DD}; see
also the recent book \cite{Russo}
",Mathematics,Mathematics
"Spin-resolved electronic structure of ferroelectric α-GeTe and multiferroic Ge1-xMnxTe   Germanium telluride features special spin-electric effects originating from
spin-orbit coupling and symmetry breaking by the ferroelectric lattice
polarization, which opens up many prospectives for electrically tunable and
switchable spin electronic devices. By Mn doping of the {\alpha}-GeTe host
lattice, the system becomes a multiferroic semiconductor possessing
magnetoelectric properties in which the electric polarization, magnetization
and spin texture are coupled to each other. Employing spin- and angle-resolved
photoemission spectroscopy in bulk- and surface-sensitive energy ranges and by
varying dipole transition matrix elements, we disentangle the bulk, surface and
surface-resonance states of the electronic structure and determine the spin
textures for selected parameters. From our results, we derive a comprehensive
model of the {\alpha}-GeTe surface electronic structure which fits experimental
data and first principle theoretical predictions and we discuss the
unconventional evolution of the Rashba-type spin splitting upon manipulation by
external B- and E-fields.
",Physics,Physics
"An initial-boundary value problem of the general three-component nonlinear Schrodinger equation with a 4x4 Lax pair on a finite interval   We investigate the initial-boundary value problem for the general
three-component nonlinear Schrodinger (gtc-NLS) equation with a 4x4 Lax pair on
a finite interval by extending the Fokas unified approach. The solutions of the
gtc-NLS equation can be expressed in terms of the solutions of a 4x4 matrix
Riemann-Hilbert (RH) problem formulated in the complex k-plane. Moreover, the
relevant jump matrices of the RH problem can be explicitly found via the three
spectral functions arising from the initial data, the Dirichlet-Neumann
boundary data. The global relation is also established to deduce two distinct
but equivalent types of representations (i.e., one by using the large k of
asymptotics of the eigenfunctions and another one in terms of the
Gelfand-Levitan-Marchenko (GLM) method) for the Dirichlet and Neumann boundary
value problems. Moreover, the relevant formulae for boundary value problems on
the finite interval can reduce to ones on the half-line as the length of the
interval approaches to infinity. Finally, we also give the linearizable
boundary conditions for the GLM representation.
",Physics; Mathematics,Mathematics
"Hazard Analysis and Risk Assessment for an Automated Unmanned Protective Vehicle   For future application of automated vehicles in public traffic, ensuring
functional safety is essential. In this context, a hazard analysis and risk
assessment is an important input for designing functionally vehicle automation
systems. In this contribution, we present a detailed hazard analysis and risk
assessment (HARA) according to the ISO 26262 standard for a specific Level 4
application, namely an unmanned protective vehicle operated without human
supervision for motorway hard shoulder roadworks.
",Computer Science,Computer Science
"Comment on the Equality Condition for the I-MMSE Proof of Entropy Power Inequality   The paper establishes the equality condition in the I-MMSE proof of the
entropy power inequality (EPI). This is done by establishing an exact
expression for the deficit between the two sides of the EPI. Interestingly, a
necessary condition for the equality is established by making a connection to
the famous Cauchy functional equation.
",Computer Science,Mathematics
"Interferometric confirmation of ""water fountain"" candidates   Water fountain stars (WFs) are evolved objects with water masers tracing
high-velocity jets (up to several hundreds of km s$^{-1}$). They could
represent one of the first manifestations of collimated mass-loss in evolved
objects and thus, be a key to understanding the shaping mechanisms of planetary
nebulae. Only 13 objects had been confirmed so far as WFs with interferometer
observations. We present new observations with the Australia Telescope Compact
Array and archival observations with the Very Large Array of four objects that
are considered to be WF candidates, mainly based on single-dish observations.
We confirm IRAS 17291-2147 and IRAS 18596+0315 (OH 37.1-0.8) as bona fide
members of the WF class, with high-velocity water maser emission consistent
with tracing bipolar jets. We argue that IRAS 15544-5332 has been wrongly
considered as a WF in previous works, since we see no evidence in our data nor
in the literature that this object harbours high-velocity water maser emission.
In the case of IRAS 19067+0811, we did not detect any water maser emission, so
its confirmation as a WF is still pending. With the result of this work, there
are 15 objects that can be considered confirmed WFs. We speculate that there is
no significant physical difference between WFs and obscured post-AGB stars in
general. The absence of high-velocity water maser emission in some obscured
post-AGB stars could be attributed to a variability or orientation effect.
",Physics,Physics
"Surface energy of strained amorphous solids   Surface stress and surface energy are fundamental quantities which
characterize the interface between two materials. Although these quantities are
identical for interfaces involving only fluids, the Shuttleworth effect
demonstrates that this is not the case for most interfaces involving solids,
since their surface energies change with strain. Crystalline materials are
known to have strain dependent surface energies, but in amorphous materials,
such as polymeric glasses and elastomers, the strain dependence is debated due
to a dearth of direct measurements. Here, we utilize contact angle measurements
on strained glassy and elastomeric solids to address this matter. We show
conclusively that interfaces involving polymeric glasses exhibit strain
dependent surface energies, and give strong evidence for the absence of such a
dependence for incompressible elastomers. The results provide fundamental
insight into our understanding of the interfaces of amorphous solids and their
interaction with contacting liquids.
",Physics,Physics
"Singlet ground state in the spin-$1/2$ weakly coupled dimer compound NH$_4$[(V$_2$O$_3$)$_2$(4,4$^\prime$-$bpy$)$_2$(H$_2$PO$_4$)(PO$_4$)$_2$]$\cdot$0.5H$_2$O   We present the synthesis and a detailed investigation of structural and
magnetic properties of polycrystalline
NH$_4$[(V$_2$O$_3$)$_2$(4,4$^\prime$-$bpy$)$_2$(H$_2$PO$_4$)(PO$_4$)$_2$]$\cdot$0.5H$_2$O
by means of x-ray diffraction, magnetic susceptibility, electron spin
resonance, and $^{31}$P nuclear magnetic resonance measurements. Temperature
dependent magnetic susceptibility could be described well using a weakly
coupled spin-$1/2$ dimer model with an excitation gap $\Delta/k_{\rm B}\simeq
26.1$ K between the singlet ground state and triplet excited states and a weak
inter-dimer exchange coupling $J^\prime/k_{\rm B} \simeq 4.6$ K. A gapped chain
model also describes the data well with a gap of about 20 K. The ESR intensity
as a function of temperature traces the bulk susceptibility nicely. The
isotropic Land$\acute{\rm e}$ $g$-factor is estimated to be about $g \simeq
1.97$, at room temperature. We are able to resolve the $^{31}$P NMR signal as
coming from two inequivalent P-sites in the crystal structure. The hyperfine
coupling constant between $^{31}$P nucleus and V$^{4+}$ spins is calculated to
be $A_{\rm hf}(1) \simeq 2963$ Oe/$\mu_{\rm B}$ and $A_{\rm hf}(2) \simeq 1466$
Oe/$\mu_{\rm B}$ for the P(1) and P(2) sites, respectively. Our NMR shift and
spin-lattice relaxation rate for both the $^{31}$P sites show an activated
behaviour at low temperatures, further confirming the singlet ground state. The
estimated value of the spin gap from the NMR data measured in an applied field
of $H = 9.394$ T is consistent with the gap obtained from the magnetic
susceptibility analysis using the dimer model. Because of a relatively small
spin gap,
NH$_4$[(V$_2$O$_3$)$_2$(4,4$^\prime$-$bpy$)$_2$(H$_2$PO$_4$)(PO$_4$)$_2$]$\cdot$0.5H$_2$O
is a promising compound for further experimental studies under high magnetic
fields.
",Physics,Physics
"Zeros of real random polynomials spanned by OPUC   Let \( \{\varphi_i\}_{i=0}^\infty \) be a sequence of orthonormal polynomials
on the unit circle with respect to a probability measure \( \mu \). We study
zero distribution of random linear combinations of the form \[
P_n(z)=\sum_{i=0}^{n-1}\eta_i\varphi_i(z), \] where \( \eta_0,\dots,\eta_{n-1}
\) are i.i.d. standard Gaussian variables. We use the Christoffel-Darboux
formula to simplify the density functions provided by Vanderbei for the
expected number real and complex of zeros of \( P_n \). From these expressions,
under the assumption that \( \mu \) is in the Nevai class, we deduce the
limiting value of these density functions away from the unit circle. Under the
mere assumption that \( \mu \) is doubling on subarcs of \( \T \) centered at
\( 1 \) and \( -1 \), we show that the expected number of real zeros of \( P_n
\) is at most \[ (2/\pi) \log n +O(1), \] and that the asymptotic equality
holds when the corresponding recurrence coefficients decay no slower than \(
n^{-(3+\epsilon)/2} \), \( \epsilon>0 \). We conclude with providing results
that estimate the expected number of complex zeros of \( P_n \) in shrinking
neighborhoods of compact subsets of \( \T \).
",Mathematics,Mathematics; Statistics
"A Hand Combining Two Simple Grippers to Pick up and Arrange Objects for Assembly   This paper proposes a novel robotic hand design for assembly tasks. The idea
is to combine two simple grippers -- an inner gripper which is used for precise
alignment, and an outer gripper which is used for stable holding. Conventional
robotic hands require complicated compliant mechanisms or complicated control
strategy and force sensing to conduct assemble tasks, which makes them costly
and difficult to pick and arrange small objects like screws or washers.
Compared to the conventional hands, the proposed design provides a low-cost
solution for aligning, picking up, and arranging various objects by taking
advantages of the geometric constraints of the positioning fingers and gravity.
It is able to deal with small screws and washers, and eliminate the position
errors of cylindrical objects or objects with cylindrical holes. In the
experiments, both real-world tasks and quantitative analysis are performed to
validate the aligning, picking, and arrangements abilities of the design.
",Computer Science,Computer Science
"Large time behavior of solution to nonlinear Dirac equation in $1+1$ dimensions   This paper studies the large time behavior of solution for a class of
nonlinear massless Dirac equations in $R^{1+1}$. It is shown that the solution
will tend to travelling wave solution when time tends to infinity.
",Mathematics,Mathematics
"Positive semi-definite embedding for dimensionality reduction and out-of-sample extensions   In machine learning or statistics, it is often desirable to reduce the
dimensionality of high dimensional data. We propose to obtain the low
dimensional embedding coordinates as the eigenvectors of a positive
semi-definite kernel matrix. This kernel matrix is the solution of a
semi-definite program promoting a low rank solution and defined with the help
of a diffusion kernel. Besides, we also discuss an infinite dimensional
analogue of the same semi-definite program. From a practical perspective, a
main feature of our approach is the existence of a non-linear out-of-sample
extension formula of the embedding coordinates that we call a projected
Nyström approximation. This extension formula yields an extension of the
kernel matrix to a data-dependent Mercer kernel function. Although the
semi-definite program may be solved directly, we propose another strategy based
on a rank constrained formulation solved thanks to a projected power method
algorithm followed by a singular value decomposition. This strategy allows for
a reduced computational time.
",Computer Science; Statistics,Computer Science; Statistics
"Drawing Planar Graphs with Few Geometric Primitives   We define the \emph{visual complexity} of a plane graph drawing to be the
number of basic geometric objects needed to represent all its edges. In
particular, one object may represent multiple edges (e.g., one needs only one
line segment to draw a path with an arbitrary number of edges). Let $n$ denote
the number of vertices of a graph. We show that trees can be drawn with $3n/4$
straight-line segments on a polynomial grid, and with $n/2$ straight-line
segments on a quasi-polynomial grid. Further, we present an algorithm for
drawing planar 3-trees with $(8n-17)/3$ segments on an $O(n)\times O(n^2)$
grid. This algorithm can also be used with a small modification to draw maximal
outerplanar graphs with $3n/2$ edges on an $O(n)\times O(n^2)$ grid. We also
study the problem of drawing maximal planar graphs with circular arcs and
provide an algorithm to draw such graphs using only $(5n - 11)/3$ arcs. This is
significantly smaller than the lower bound of $2n$ for line segments for a
nontrivial graph class.
",Computer Science,Computer Science
"The Promise and Peril of Human Evaluation for Model Interpretability   Transparency, user trust, and human comprehension are popular ethical
motivations for interpretable machine learning. In support of these goals,
researchers evaluate model explanation performance using humans and real world
applications. This alone presents a challenge in many areas of artificial
intelligence. In this position paper, we propose a distinction between
descriptive and persuasive explanations. We discuss reasoning suggesting that
functional interpretability may be correlated with cognitive function and user
preferences. If this is indeed the case, evaluation and optimization using
functional metrics could perpetuate implicit cognitive bias in explanations
that threaten transparency. Finally, we propose two potential research
directions to disambiguate cognitive function and explanation models, retaining
control over the tradeoff between accuracy and interpretability.
",Computer Science; Statistics,Computer Science; Statistics
"Dynamic density structure factor of a unitary Fermi gas at finite temperature   We present a theoretical investigation of the dynamic density structure
factor of a strongly interacting Fermi gas near a Feshbach resonance at finite
temperature. The study is based on a gauge invariant linear response theory.
The theory is consistent with a diagrammatic approach for the equilibrium state
taking into account the pair fluctuation effects and respects some important
restrictions like the $f$-sum rule. Our numerical results show that the dynamic
density structure factor at large incoming momentum and at half recoil
frequency has a qualitatively similar behavior as the order parameter, which
can signify the appearance of the condensate. This qualitatively agrees with
the recent Bragg spectroscopy experiment results. We also present the results
at small incoming momentum.
",Physics,Physics
"Alliance formation with exclusion in the spatial public goods game   Detecting defection and alarming partners about the possible danger could be
essential to avoid being exploited. This act, however, may require a huge
individual effort from those who take this job, hence such a strategy seems to
be unfavorable. But structured populations can provide an opportunity where a
largely unselfish excluder strategy can form an effective alliance with other
cooperative strategies, hence they can sweep out defection. Interestingly, this
alliance is functioning even at the extremely high cost of exclusion where the
sole application of an exclusion strategy would be harmful otherwise. These
results may explain why the emergence of extreme selfless behavior is not
necessarily against individual selection but could be the result of an
evolutionary process.
",Computer Science; Physics,Quantitative Biology
"Automatic Detection of Knee Joints and Quantification of Knee Osteoarthritis Severity using Convolutional Neural Networks   This paper introduces a new approach to automatically quantify the severity
of knee OA using X-ray images. Automatically quantifying knee OA severity
involves two steps: first, automatically localizing the knee joints; next,
classifying the localized knee joint images. We introduce a new approach to
automatically detect the knee joints using a fully convolutional neural network
(FCN). We train convolutional neural networks (CNN) from scratch to
automatically quantify the knee OA severity optimizing a weighted ratio of two
loss functions: categorical cross-entropy and mean-squared loss. This joint
training further improves the overall quantification of knee OA severity, with
the added benefit of naturally producing simultaneous multi-class
classification and regression outputs. Two public datasets are used to evaluate
our approach, the Osteoarthritis Initiative (OAI) and the Multicenter
Osteoarthritis Study (MOST), with extremely promising results that outperform
existing approaches.
",Computer Science,Computer Science; Statistics
"Direct Optimization through $\arg \max$ for Discrete Variational Auto-Encoder   Reparameterization of variational auto-encoders with continuous latent spaces
is an effective method for reducing the variance of their gradient estimates.
However, using the same approach when latent variables are discrete is
problematic, due to the resulting non-differentiable objective. In this work,
we present a direct optimization method that propagates gradients through a
non-differentiable $\arg \max$ prediction operation. We apply this method to
discrete variational auto-encoders, by modeling a discrete random variable by
the $\arg \max$ function of the Gumbel-Max perturbation model.
",Statistics,Computer Science; Statistics
"Multi-robot motion-formation distributed control with sensor self-calibration: experimental validation   In this paper, we present the design and implementation of a robust motion
formation distributed control algorithm for a team of mobile robots. The
primary task for the team is to form a geometric shape, which can be freely
translated and rotated at the same time. This approach makes the robots to
behave as a cohesive whole, which can be useful in tasks such as collaborative
transportation. The robustness of the algorithm relies on the fact that each
robot employs only local measurements from a laser sensor which does not need
to be off-line calibrated. Furthermore, robots do not need to exchange any
information with each other. Being free of sensor calibration and not requiring
a communication channel helps the scaling of the overall system to a large
number of robots. In addition, since the robots do not need any off-board
localization system, but require only relative positions with respect to their
neighbors, it can be aimed to have a full autonomous team that operates in
environments where such localization systems are not available. The
computational cost of the algorithm is inexpensive and the resources from a
standard microcontroller will suffice. This fact makes the usage of our
approach appealing as a support for other more demanding algorithms, e.g.,
processing images from onboard cameras. We validate the performance of the
algorithm with a team of four mobile robots equipped with low-cost commercially
available laser scanners.
",Computer Science,Computer Science
"On Optimal Generalizability in Parametric Learning   We consider the parametric learning problem, where the objective of the
learner is determined by a parametric loss function. Employing empirical risk
minimization with possibly regularization, the inferred parameter vector will
be biased toward the training samples. Such bias is measured by the cross
validation procedure in practice where the data set is partitioned into a
training set used for training and a validation set, which is not used in
training and is left to measure the out-of-sample performance. A classical
cross validation strategy is the leave-one-out cross validation (LOOCV) where
one sample is left out for validation and training is done on the rest of the
samples that are presented to the learner, and this process is repeated on all
of the samples. LOOCV is rarely used in practice due to the high computational
complexity. In this paper, we first develop a computationally efficient
approximate LOOCV (ALOOCV) and provide theoretical guarantees for its
performance. Then we use ALOOCV to provide an optimization algorithm for
finding the regularizer in the empirical risk minimization framework. In our
numerical experiments, we illustrate the accuracy and efficiency of ALOOCV as
well as our proposed framework for the optimization of the regularizer.
",Computer Science; Statistics,Computer Science; Statistics
"Laser annealing heals radiation damage in avalanche photodiodes   Avalanche photodiodes (APDs) are a practical option for space-based quantum
communications requiring single-photon detection. However, radiation damage to
APDs significantly increases their dark count rates and reduces their useful
lifetimes in orbit. We show that high-power laser annealing of irradiated APDs
of three different models (Excelitas C30902SH, Excelitas SLiK, and Laser
Components SAP500S2) heals the radiation damage and substantially restores low
dark count rates. Of nine samples, the maximum dark count rate reduction factor
varies between 5.3 and 758 when operating at minus 80 degrees Celsius. The
illumination power to reach these reduction factors ranges from 0.8 to 1.6 W.
Other photon detection characteristics, such as photon detection efficiency,
timing jitter, and afterpulsing probability, remain mostly unaffected. These
results herald a promising method to extend the lifetime of a quantum satellite
equipped with APDs.
",Physics,Physics
"ParaGraphE: A Library for Parallel Knowledge Graph Embedding   Knowledge graph embedding aims at translating the knowledge graph into
numerical representations by transforming the entities and relations into
continuous low-dimensional vectors. Recently, many methods [1, 5, 3, 2, 6] have
been proposed to deal with this problem, but existing single-thread
implementations of them are time-consuming for large-scale knowledge graphs.
Here, we design a unified parallel framework to parallelize these methods,
which achieves a significant time reduction without influencing the accuracy.
We name our framework as ParaGraphE, which provides a library for parallel
knowledge graph embedding. The source code can be downloaded from
this https URL .
",Computer Science,Computer Science
"Strong and Weak Equilibria for Time-Inconsistent Stochastic Control in Continuous Time   A new definition of continuous-time equilibrium controls is introduced. As
opposed to the standard definition, which involves a derivative-type operation,
the new definition parallels how a discrete-time equilibrium is defined, and
allows for unambiguous economic interpretation. The terms ""strong equilibria""
and ""weak equilibria"" are coined for controls under the new and the standard
definitions, respectively. When the state process is a time-homogeneous
continuous-time Markov chain, a careful asymptotic analysis gives complete
characterizations of weak and strong equilibria. Thanks to Kakutani-Fan's
fixed-point theorem, general existence of weak and strong equilibria is also
established, under additional compactness assumption. Our theoretic results are
applied to a two-state model under non-exponential discounting. In particular,
we demonstrate explicitly that there can be incentive to deviate from a weak
equilibrium, which justifies the need for strong equilibria. Our analysis also
provides new results for the existence and characterization of discrete-time
equilibria under infinite horizon.
",Quantitative Finance,Mathematics
"Mixed-Effect Modeling for Longitudinal Prediction of Cancer Tumor   In this paper, a mixed-effect modeling scheme is proposed to construct a
predictor for different features of cancer tumor. For this purpose, a set of
features is extracted from two groups of patients with the same type of cancer
but with two medical outcome: 1) survived and 2) passed away. The goal is to
build different models for the two groups, where in each group,
patient-specified behavior of individuals can be characterized. These models
are then used as predictors to forecast future state of patients with a given
history or initial state. To this end, a leave-on-out cross validation method
is used to measure the prediction accuracy of each patient-specified model.
Experiments show that compared to fixed-effect modeling (regression),
mixed-effect modeling has a superior performance on some of the extracted
features and similar or worse performance on the others.
",Statistics,Statistics
"Comparative study of Discrete Wavelet Transforms and Wavelet Tensor Train decomposition to feature extraction of FTIR data of medicinal plants   Fourier-transform infra-red (FTIR) spectra of samples from 7 plant species
were used to explore the influence of preprocessing and feature extraction on
efficiency of machine learning algorithms. Wavelet Tensor Train (WTT) and
Discrete Wavelet Transforms (DWT) were compared as feature extraction
techniques for FTIR data of medicinal plants. Various combinations of signal
processing steps showed different behavior when applied to classification and
clustering tasks. Best results for WTT and DWT found through grid search were
similar, significantly improving quality of clustering as well as
classification accuracy for tuned logistic regression in comparison to original
spectra. Unlike DWT, WTT has only one parameter to be tuned (rank), making it a
more versatile and easier to use as a data processing tool in various signal
processing applications.
",Computer Science; Statistics,Statistics
"A Large Term Rewrite System Modelling a Pioneering Cryptographic Algorithm   We present a term rewrite system that formally models the Message
Authenticator Algorithm (MAA), which was one of the first cryptographic
functions for computing a Message Authentication Code and was adopted, between
1987 and 2001, in international standards (ISO 8730 and ISO 8731-2) to ensure
the authenticity and integrity of banking transactions. Our term rewrite system
is large (13 sorts, 18 constructors, 644 non-constructors, and 684 rewrite
rules), confluent, and terminating. Implementations in thirteen different
languages have been automatically derived from this model and used to validate
200 official test vectors for the MAA.
",Computer Science,Computer Science
"Quantum Annealing Applied to De-Conflicting Optimal Trajectories for Air Traffic Management   We present the mapping of a class of simplified air traffic management (ATM)
problems (strategic conflict resolution) to quadratic unconstrained boolean
optimization (QUBO) problems. The mapping is performed through an original
representation of the conflict-resolution problem in terms of a conflict graph,
where nodes of the graph represent flights and edges represent a potential
conflict between flights. The representation allows a natural decomposition of
a real world instance related to wind- optimal trajectories over the Atlantic
ocean into smaller subproblems, that can be discretized and are amenable to be
programmed in quantum annealers. In the study, we tested the new programming
techniques and we benchmark the hardness of the instances using both classical
solvers and the D-Wave 2X and D-Wave 2000Q quantum chip. The preliminary
results show that for reasonable modeling choices the most challenging
subproblems which are programmable in the current devices are solved to
optimality with 99% of probability within a second of annealing time.
",Computer Science,Computer Science
"Stronger selection can slow down evolution driven by recombination on a smooth fitness landscape   Stronger selection implies faster evolution---that is, the greater the force,
the faster the change. This apparently self-evident proposition, however, is
derived under the assumption that genetic variation within a population is
primarily supplied by mutation (i.e.\ mutation-driven evolution). Here, we show
that this proposition does not actually hold for recombination-driven
evolution, i.e.\ evolution in which genetic variation is primarily created by
recombination rather than mutation. By numerically investigating population
genetics models of recombination, migration and selection, we demonstrate that
stronger selection can slow down evolution on a perfectly smooth fitness
landscape. Through simple analytical calculation, this apparently
counter-intuitive result is shown to stem from two opposing effects of natural
selection on the rate of evolution. On the one hand, natural selection tends to
increase the rate of evolution by increasing the fixation probability of fitter
genotypes. On the other hand, natural selection tends to decrease the rate of
evolution by decreasing the chance of recombination between immigrants and
resident individuals. As a consequence of these opposing effects, there is a
finite selection pressure maximizing the rate of evolution. Hence, stronger
selection can imply slower evolution if genetic variation is primarily supplied
by recombination.
",Physics,Quantitative Biology
"Two-Dimensional Systolic Complexes Satisfy Property A   We show that 2-dimensional systolic complexes are quasi-isometric to quadric
complexes with flat intervals. We use this fact along with the weight function
of Brodzki, Campbell, Guentner, Niblo and Wright to prove that 2-dimensional
systolic complexes satisfy Property A.
",Mathematics,Mathematics
"CT Image Reconstruction in a Low Dimensional Manifold   Regularization methods are commonly used in X-ray CT image reconstruction.
Different regularization methods reflect the characterization of different
prior knowledge of images. In a recent work, a new regularization method called
a low-dimensional manifold model (LDMM) is investigated to characterize the
low-dimensional patch manifold structure of natural images, where the manifold
dimensionality characterizes structural information of an image. In this paper,
we propose a CT image reconstruction method based on the prior knowledge of the
low-dimensional manifold of CT image. Using the clinical raw projection data
from GE clinic, we conduct comparisons for the CT image reconstruction among
the proposed method, the simultaneous algebraic reconstruction technique (SART)
with the total variation (TV) regularization, and the filtered back projection
(FBP) method. Results show that the proposed method can successfully recover
structural details of an imaging object, and achieve higher spatial and
contrast resolution of the reconstructed image than counterparts of FBP and
SART with TV.
",Computer Science; Physics,Computer Science
"Binary Matrix Factorization via Dictionary Learning   Matrix factorization is a key tool in data analysis; its applications include
recommender systems, correlation analysis, signal processing, among others.
Binary matrices are a particular case which has received significant attention
for over thirty years, especially within the field of data mining. Dictionary
learning refers to a family of methods for learning overcomplete basis (also
called frames) in order to efficiently encode samples of a given type; this
area, now also about twenty years old, was mostly developed within the signal
processing field. In this work we propose two binary matrix factorization
methods based on a binary adaptation of the dictionary learning paradigm to
binary matrices. The proposed algorithms focus on speed and scalability; they
work with binary factors combined with bit-wise operations and a few auxiliary
integer ones. Furthermore, the methods are readily applicable to online binary
matrix factorization. Another important issue in matrix factorization is the
choice of rank for the factors; we address this model selection problem with an
efficient method based on the Minimum Description Length principle. Our
preliminary results show that the proposed methods are effective at producing
interpretable factorizations of various data types of different nature.
",Statistics,Computer Science; Statistics
"Computing maximum cliques in $B_2$-EPG graphs   EPG graphs, introduced by Golumbic et al. in 2009, are edge-intersection
graphs of paths on an orthogonal grid. The class $B_k$-EPG is the subclass of
EPG graphs where the path on the grid associated to each vertex has at most $k$
bends. Epstein et al. showed in 2013 that computing a maximum clique in
$B_1$-EPG graphs is polynomial. As remarked in [Heldt et al., 2014], when the
number of bends is at least $4$, the class contains $2$-interval graphs for
which computing a maximum clique is an NP-hard problem. The complexity status
of the Maximum Clique problem remains open for $B_2$ and $B_3$-EPG graphs. In
this paper, we show that we can compute a maximum clique in polynomial time in
$B_2$-EPG graphs given a representation of the graph.
Moreover, we show that a simple counting argument provides a
${2(k+1)}$-approximation for the coloring problem on $B_k$-EPG graphs without
knowing the representation of the graph. It generalizes a result of [Epstein et
al, 2013] on $B_1$-EPG graphs (where the representation was needed).
",Computer Science,Computer Science
"Survey of Visual Question Answering: Datasets and Techniques   Visual question answering (or VQA) is a new and exciting problem that
combines natural language processing and computer vision techniques. We present
a survey of the various datasets and models that have been used to tackle this
task. The first part of the survey details the various datasets for VQA and
compares them along some common factors. The second part of this survey details
the different approaches for VQA, classified into four types: non-deep learning
models, deep learning models without attention, deep learning models with
attention, and other models which do not fit into the first three. Finally, we
compare the performances of these approaches and provide some directions for
future work.
",Computer Science,Computer Science
"Analysis of the Polya-Gamma block Gibbs sampler for Bayesian logistic linear mixed models   In this article, we construct a two-block Gibbs sampler using Polson et al.
(2013) data augmentation technique with Polya-Gamma latent variables for
Bayesian logistic linear mixed models under proper priors. Furthermore, we
prove the uniform ergodicity of this Gibbs sampler, which guarantees the
existence of the central limit theorems for MCMC based estimators.
",Mathematics; Statistics,Mathematics; Statistics
"Achieving rental harmony with a secretive roommate   Given the subjective preferences of n roommates in an n-bedroom apartment,
one can use Sperner's lemma to find a division of the rent such that each
roommate is content with a distinct room. At the given price distribution, no
roommate has a strictly stronger preference for a different room. We give a new
elementary proof that the subjective preferences of only n-1 of the roommates
actually suffice to achieve this envy-free rent division. Our proof, in
particular, yields an algorithm to find such a fair division of rent. The
techniques also give generalizations of Sperner's lemma including a new proof
of a conjecture of the third author.
",Mathematics,Computer Science
"Automaton Semigroups and Groups: on the Undecidability of Problems Related to Freeness and Finiteness   In this paper, we study algorithmic problems for automaton semigroups and
automaton groups related to freeness and finiteness. In the course of this
study, we also exhibit some connections between the algebraic structure of
automaton (semi)groups and their dynamics on the boundary.
First, we show that it is undecidable to check whether the group generated by
a given invertible automaton has a positive relation, i. e. a relation p = 1
such that p only contains positive generators. Besides its obvious relation to
the freeness of the group, the absence of positive relations has previously
been studied and is connected to the triviality of some stabilizers of the
boundary. We show that the emptiness of the set of positive relations is
equivalent to the dynamical property that all (directed positive) orbital
graphs centered at non-singular points are acyclic. Our approach also works to
show undecidability of the freeness problem for automaton semigroups; in fact,
it shows undecidability of a strengthened version where the input automaton is
complete and invertible.
Gillibert showed that the finiteness problem for automaton semigroups is
undecidable. In the second part of the paper, we show that this undecidability
result also holds if the input is restricted to be bi-reversible and invertible
(but, in general, not complete). As an immediate consequence, we obtain that
the finiteness problem for automaton subsemigroups of semigroups generated by
invertible, yet partial automata, so called automaton-inverse semigroups, is
also undecidable.
",Computer Science; Mathematics,Computer Science
"Anticipation: an effective evolutionary strategy for a sub-optimal population in a cyclic environment   We built a two-state model of an asexually reproducing organism in a periodic
environment endowed with the capability to anticipate an upcoming environmental
change and undergo pre-emptive switching. By virtue of these anticipatory
transitions, the organism oscillates between its two states that is a time
$\theta$ out of sync with the environmental oscillation. We show that an
anticipation-capable organism increases its long-term fitness over an organism
that oscillates in-sync with the environment, provided $\theta$ does not exceed
a threshold. We also show that the long-term fitness is maximized for an
optimal anticipation time that decreases approximately as $1/n$, $n$ being the
number of cell divisions in time $T$. Furthermore, we demonstrate that optimal
""anticipators"" outperforms ""bet-hedgers"" in the range of parameters considered.
For a sub-optimal ensemble of anticipators, anticipation performs better to
bet-hedging only when the variance in anticipation is small compared to the
mean and the rate of pre-emptive transition is high. Taken together, our work
suggests that anticipation increases overall fitness of an organism in a
periodic environment and it is a viable alternative to bet-hedging provided the
error in anticipation is small.
",Quantitative Biology,Quantitative Biology
"Finite-sample bounds for the multivariate Behrens-Fisher distribution with proportional covariances   The Behrens-Fisher problem is a well-known hypothesis testing problem in
statistics concerning two-sample mean comparison. In this article, we confirm
one conjecture in Eaton and Olshen (1972), which provides stochastic bounds for
the multivariate Behrens-Fisher test statistic under the null hypothesis. We
also extend their results on the stochastic ordering of random quotients to the
arbitrary finite dimensional case. This work can also be seen as a
generalization of Hsu (1938) that provided the bounds for the univariate
Behrens-Fisher problem. The results obtained in this article can be used to
derive a testing procedure for the multivariate Behrens-Fisher problem that
strongly controls the Type I error.
",Mathematics; Statistics,Mathematics; Statistics
"Controlling plasmon modes and damping in buckled two-dimensional material open systems   Full ranges of both hybrid plasmon-mode dispersions and their damping are
studied systematically by our recently developed mean-field theory in open
systems involving a conducting substrate and a two-dimensional (2D) material
with a buckled honeycomb lattice, such as silicene, germanene, and a group
\rom{4} dichalcogenide as well. In this hybrid system, the single plasmon mode
for a free-standing 2D layer is split into one acoustic-like and one
optical-like mode, leading to a dramatic change in the damping of plasmon
modes. In comparison with gapped graphene, critical features associated with
plasmon modes and damping in silicene and molybdenum disulfide are found with
various spin-orbit and lattice asymmetry energy bandgaps, doping types and
levels, and coupling strengths between 2D materials and the conducting
substrate. The obtained damping dependence on both spin and valley degrees of
freedom is expected to facilitate measuring the open-system dielectric property
and the spin-orbit coupling strength of individual 2D materials. The unique
linear dispersion of the acoustic-like plasmon mode introduces additional
damping from the intraband particle-hole modes which is absent for a
free-standing 2D material layer, and the use of molybdenum disulfide with a
large bandgap simultaneously suppresses the strong damping from the interband
particle-hole modes.
",Physics,Physics
"Data-driven Advice for Applying Machine Learning to Bioinformatics Problems   As the bioinformatics field grows, it must keep pace not only with new data
but with new algorithms. Here we contribute a thorough analysis of 13
state-of-the-art, commonly used machine learning algorithms on a set of 165
publicly available classification problems in order to provide data-driven
algorithm recommendations to current researchers. We present a number of
statistical and visual comparisons of algorithm performance and quantify the
effect of model selection and algorithm tuning for each algorithm and dataset.
The analysis culminates in the recommendation of five algorithms with
hyperparameters that maximize classifier performance across the tested
problems, as well as general guidelines for applying machine learning to
supervised classification problems.
",Computer Science; Statistics,Computer Science; Statistics
"HATS-43b, HATS-44b, HATS-45b, and HATS-46b: Four Short Period Transiting Giant Planets in the Neptune-Jupiter Mass Range   We report the discovery of four short period extrasolar planets transiting
moderately bright stars from photometric measurements of the HATSouth network
coupled to additional spectroscopic and photometric follow-up observations.
While the planet masses range from 0.26 to 0.90 M$_J$, the radii are all
approximately a Jupiter radii, resulting in a wide range of bulk densities. The
orbital period of the planets range from 2.7d to 4.7d, with HATS-43b having an
orbit that appears to be marginally non-circular (e= 0.173$\pm$0.089). HATS-44
is notable for a high metallicity ([Fe/H]= 0.320$\pm$0.071). The host stars
spectral types range from late F to early K, and all of them are moderately
bright (13.3<V<14.4), allowing the execution of future detailed follow-up
observations. HATS-43b and HATS-46b, with expected transmission signals of 2350
ppm and 1500 ppm, respectively, are particularly well suited targets for
atmospheric characterisation via transmission spectroscopy.
",Physics,Physics
"Radon background in liquid xenon detectors   The radioactive daughters isotope of 222Rn are one of the highest risk
contaminants in liquid xenon detectors aiming for a small signal rate. The
noble gas is permanently emanated from the detector surfaces and mixed with the
xenon target. Because of its long half-life 222Rn is homogeneously distributed
in the target and its subsequent decays can mimic signal events. Since no
shielding is possible this background source can be the dominant one in future
large scale experiments. This article provides an overview of strategies used
to mitigate this source of background by means of material selection and
on-line radon removal techniques.
",Physics,Physics
"The Faraday room of the CUORE Experiment   The paper describes the Faraday room that shields the CUORE experiment
against electromagnetic fields, from 50 Hz up to high frequency. Practical
contraints led to choose panels made of light shielding materials. The seams
between panels were optimized with simulations to minimize leakage.
Measurements of shielding performance show attenuation of a factor 15 at 50 Hz,
and a factor 1000 above 1 KHz up to about 100 MHz.
",Physics,Physics
"Baby MIND: A magnetized segmented neutrino detector for the WAGASCI experiment   T2K (Tokai-to-Kamioka) is a long-baseline neutrino experiment in Japan
designed to study various parameters of neutrino oscillations. A near detector
complex (ND280) is located 280~m downstream of the production target and
measures neutrino beam parameters before any oscillations occur. ND280's
measurements are used to predict the number and spectra of neutrinos in the
Super-Kamiokande detector at the distance of 295~km. The difference in the
target material between the far (water) and near (scintillator, hydrocarbon)
detectors leads to the main non-cancelling systematic uncertainty for the
oscillation analysis. In order to reduce this uncertainty a new
WAter-Grid-And-SCintillator detector (WAGASCI) has been developed. A magnetized
iron neutrino detector (Baby MIND) will be used to measure momentum and charge
identification of the outgoing muons from charged current interactions. The
Baby MIND modules are composed of magnetized iron plates and long plastic
scintillator bars read out at the both ends with wavelength shifting fibers and
silicon photomultipliers. The front-end electronics board has been developed to
perform the readout and digitization of the signals from the scintillator bars.
Detector elements were tested with cosmic rays and in the PS beam at CERN. The
obtained results are presented in this paper.
",Physics,Physics
"Inflationary magneto-(non)genesis, increasing kinetic couplings, and the strong coupling problem   We study the generation of magnetic fields during inflation making use of a
coupling of the inflaton and moduli fields to electromagnetism via the photon
kinetic term, and assuming that the coupling is an increasing function of time.
We demonstrate that the strong coupling problem of inflationary magnetogenesis
can be avoided by incorporating the destabilization of moduli fields after
inflation. The magnetic field always dominates over the electric one, and thus
the severe constraints on the latter from backreaction, which are the demanding
obstacles in the case of a decreasing coupling function, do not apply to the
current scenario. However, we show that this loophole to the strong coupling
problem comes at a price: the normalization of the amplitude of magnetic fields
is determined by this coupling term and is therefore suppressed by a large
factor after the moduli destabilization completes. From this we conclude that
there is no self-consistent and generic realization of primordial
magnetogenesis producing scale-invariant fields in the case of an increasing
kinetic coupling.
",Physics,Physics
"Exhaled breath barbotage: a new method for pulmonary surfactant dysfunction assessment   Exhaled air contains aerosol of submicron droplets of the alveolar lining
fluid (ALF), which are generated in the small airways of a human lung. Since
the exhaled particles are micro-samples of the ALF, their trapping opens up an
opportunity to collect non-invasively a native material from respiratory tract.
Recent studies of the particle characteristics (such as size distribution,
concentration and composition) in healthy and diseased subjects performed under
various conditions have demonstrated a high potential of the analysis of
exhaled aerosol droplets for identifying and monitoring pathological processes
in the ALF. In this paper we present a new method for sampling of aerosol
particles during the exhaled breath barbotage (EBB) through liquid. The
barbotage procedure results in accumulation of the pulmonary surfactant, being
the main component of ALF, on the liquid surface, which makes possible the
study its surface properties. We also propose a data processing algorithm to
evaluate the surface pressure ($\pi$) -- surface concentration ($\Gamma$)
isotherm from the raw data measured in a Langmuir trough. Finally, we analyze
the $(\pi-\Gamma)$ isotherms obtained for the samples collected in the groups
of healthy volunteers and patients with pulmonary tuberculosis and compare them
with the isotherm measured for the artificial pulmonary surfactant.
",Physics,Physics
"The adaptive zero-error capacity for a class of channels with noisy feedback   The adaptive zero-error capacity of discrete memoryless channels (DMC) with
noiseless feedback has been shown to be positive whenever there exists at least
one channel output ""disprover"", i.e. a channel output that cannot be reached
from at least one of the inputs. Furthermore, whenever there exists a
disprover, the adaptive zero-error capacity attains the Shannon (small-error)
capacity. Here, we study the zero-error capacity of a DMC when the channel
feedback is noisy rather than perfect. We show that the adaptive zero-error
capacity with noisy feedback is lower bounded by the forward channel's
zero-undetected error capacity, and show that under certain conditions this is
tight.
",Computer Science,Computer Science
"SimProp v2r4: Monte Carlo simulation code for UHECR propagation   We introduce the new version of SimProp, a Monte Carlo code for simulating
the propagation of ultra-high energy cosmic rays in intergalactic space. This
version, SimProp v2r4, together with an overall improvement of the code
capabilities with a substantial reduction in the computation time, also
computes secondary cosmogenic particles such as electron-positron pairs and
gamma rays produced during the propagation of ultra-high energy cosmic rays. As
recently pointed out by several authors, the flux of this secondary radiation
and its products, within reach of the current observatories, provides useful
information about models of ultra-high energy cosmic ray sources which would be
hard to discriminate otherwise.
",Physics,Physics
"X-Cube Fracton Model on Generic Lattices: Phases and Geometric Order   Fracton order is a new kind of quantum order characterized by topological
excitations that exhibit remarkable mobility restrictions and a robust ground
state degeneracy (GSD) which can increase exponentially with system size. In
this paper, we present a generic lattice construction (in three dimensions) for
a generalized X-cube model of fracton order, where the mobility restrictions of
the subdimensional particles inherit the geometry of the lattice. This helps
explain a previous result that lattice curvature can produce a robust GSD, even
on a manifold with trivial topology. We provide explicit examples to show that
the (zero temperature) phase of matter is sensitive to the lattice geometry. In
one example, the lattice geometry confines the dimension-1 particles to small
loops, which allows the fractons to be fully mobile charges, and the resulting
phase is equivalent to (3+1)-dimensional toric code. However, the phase is
sensitive to more than just lattice curvature; different lattices without
curvature (e.g. cubic or stacked kagome lattices) also result in different
phases of matter, which are separated by phase transitions. Unintuitively
however, according to a previous definition of phase [Chen, Gu, Wen 2010], even
just a rotated or rescaled cubic lattice results in different phases of matter,
which motivates us to propose a new and coarser definition of phase for gapped
ground states and fracton order. The new equivalence relation between ground
states is given by the composition of a local unitary transformation and a
quasi-isometry (which can rotate and rescale the lattice); equivalently, ground
states are in the same phase if they can be adiabatically connected by varying
both the Hamiltonian and the positions of the degrees of freedom (via a
quasi-isometry). In light of the importance of geometry, we further propose
that fracton orders should be regarded as a geometric order.
",Physics,Physics
"Multifractal analysis of the time series of daily means of wind speed in complex regions   In this paper, we applied the multifractal detrended fluctuation analysis to
the daily means of wind speed measured by 119 weather stations distributed over
the territory of Switzerland. The analysis was focused on the inner time
fluctuations of wind speed, which could be more linked with the local
conditions of the highly varying topography of Switzerland. Our findings point
out to a persistent behaviour of all the measured wind speed series (indicated
by a Hurst exponent significantly larger than 0.5), and to a high
multifractality degree indicating a relative dominance of the large
fluctuations in the dynamics of wind speed, especially in the Swiss plateau,
which is comprised between the Jura and Alp mountain ranges. The study
represents a contribution to the understanding of the dynamical mechanisms of
wind speed variability in mountainous regions.
",Statistics,Physics
"Causal Effect Inference with Deep Latent-Variable Models   Learning individual-level causal effects from observational data, such as
inferring the most effective medication for a specific patient, is a problem of
growing importance for policy makers. The most important aspect of inferring
causal effects from observational data is the handling of confounders, factors
that affect both an intervention and its outcome. A carefully designed
observational study attempts to measure all important confounders. However,
even if one does not have direct access to all confounders, there may exist
noisy and uncertain measurement of proxies for confounders. We build on recent
advances in latent variable modeling to simultaneously estimate the unknown
latent space summarizing the confounders and the causal effect. Our method is
based on Variational Autoencoders (VAE) which follow the causal structure of
inference with proxies. We show our method is significantly more robust than
existing methods, and matches the state-of-the-art on previous benchmarks
focused on individual treatment effects.
",Computer Science; Statistics,Statistics
"Low-cost Autonomous Navigation System Based on Optical Flow Classification   This work presents a low-cost robot, controlled by a Raspberry Pi, whose
navigation system is based on vision. The strategy used consisted of
identifying obstacles via optical flow pattern recognition. Its estimation was
done using the Lucas-Kanade algorithm, which can be executed by the Raspberry
Pi without harming its performance. Finally, an SVM-based classifier was used
to identify patterns of this signal associated with obstacles movement. The
developed system was evaluated considering its execution over an optical flow
pattern dataset extracted from a real navigation environment. In the end, it
was verified that the acquisition cost of the system was inferior to that
presented by most of the cited works, while its performance was similar to
theirs.
",Computer Science,Computer Science
"Euler characteristics of cominuscule quantum K-theory   We prove an identity relating the product of two opposite Schubert varieties
in the (equivariant) quantum K-theory ring of a cominuscule flag variety to the
minimal degree of a rational curve connecting the Schubert varieties. We deduce
that the sum of the structure constants associated to any product of Schubert
classes is equal to one. Equivalently, the sheaf Euler characteristic map
extends to a ring homomorphism defined on the quantum K-theory ring.
",Mathematics,Mathematics
"Big Data Regression Using Tree Based Segmentation   Scaling regression to large datasets is a common problem in many application
areas. We propose a two step approach to scaling regression to large datasets.
Using a regression tree (CART) to segment the large dataset constitutes the
first step of this approach. The second step of this approach is to develop a
suitable regression model for each segment. Since segment sizes are not very
large, we have the ability to apply sophisticated regression techniques if
required. A nice feature of this two step approach is that it can yield models
that have good explanatory power as well as good predictive performance.
Ensemble methods like Gradient Boosted Trees can offer excellent predictive
performance but may not provide interpretable models. In the experiments
reported in this study, we found that the predictive performance of the
proposed approach matched the predictive performance of Gradient Boosted Trees.
",Computer Science; Statistics,Statistics
"Galactic Dark Matter Halos and Globular Cluster Populations. III: Extension to Extreme Environments   The total mass M_GCS in the globular cluster (GC) system of a galaxy is
empirically a near-constant fraction of the total mass M_h = M_bary + M_dark of
the galaxy, across a range of 10^5 in galaxy mass. This trend is radically
unlike the strongly nonlinear behavior of total stellar mass M_star versus M_h.
We discuss extensions of this trend to two more extreme situations: (a) entire
clusters of galaxies, and (b) the Ultra-Diffuse Galaxies (UDGs) recently
discovered in Coma and elsewhere. Our calibration of the ratio \eta_M = M_GCS /
M_h from normal galaxies, accounting for new revisions in the adopted
mass-to-light ratio for GCs, now gives \eta_M = 2.9 \times 10^{-5} as the mean
absolute mass fraction. We find that the same ratio appears valid for galaxy
clusters and UDGs. Estimates of \eta_M in the four clusters we examine tend to
be slightly higher than for individual galaxies, butmore data and better
constraints on the mean GC mass in such systems are needed to determine if this
difference is significant. We use the constancy of \eta_M to estimate total
masses for several individual cases; for example, the total mass of the Milky
Way is calculated to be M_h = 1.1 \times 10^{12} M_sun. Physical explanations
for the uniformity of \eta_M are still descriptive, but point to a picture in
which massive, dense star clusters in their formation stages were relatively
immune to the feedback that more strongly influenced lower-density regions
where most stars form.
",Physics,Physics
"Reversing Parallel Programs with Blocks and Procedures   We show how to reverse a while language extended with blocks, local
variables, procedures and the interleaving parallel composition. Annotation is
defined along with a set of operational semantics capable of storing necessary
reversal information, and identifiers are introduced to capture the
interleaving order of an execution. Inversion is defined with a set of
operational semantics that use saved information to undo an execution. We prove
that annotation does not alter the behaviour of the original program, and that
inversion correctly restores the initial program state.
",Computer Science,Computer Science
"The effect of prior probabilities on quantification and propagation of imprecise probabilities resulting from small datasets   This paper outlines a methodology for Bayesian multimodel uncertainty
quantification (UQ) and propagation and presents an investigation into the
effect of prior probabilities on the resulting uncertainties. The UQ
methodology is adapted from the information-theoretic method previously
presented by the authors (Zhang and Shields, 2018) to a fully Bayesian
construction that enables greater flexibility in quantifying uncertainty in
probability model form. Being Bayesian in nature and rooted in UQ from small
datasets, prior probabilities in both probability model form and model
parameters are shown to have a significant impact on quantified uncertainties
and, consequently, on the uncertainties propagated through a physics-based
model. These effects are specifically investigated for a simplified plate
buckling problem with uncertainties in material properties derived from a small
number of experiments using noninformative priors and priors derived from past
studies of varying appropriateness. It is illustrated that prior probabilities
can have a significant impact on multimodel UQ for small datasets and
inappropriate (but seemingly reasonable) priors may even have lingering effects
that bias probabilities even for large datasets. When applied to uncertainty
propagation, this may result in probability bounds on response quantities that
do not include the true probabilities.
",Statistics,Physics; Statistics
"Barnacles and Gravity   Theories with more than one vacuum allow quantum transitions between them,
which may proceed via bubble nucleation; theories with more than two vacua
posses additional decay modes in which the wall of a bubble may further decay.
The instantons which mediate such a process have $O(3)$ symmetry (in four
dimensions, rather than the usual $O(4)$ symmetry of homogeneous vacuum decay),
and have been called `barnacles'; previously they have been studied in flat
space, in the thin wall limit, and this paper extends the analysis to include
gravity. It is found that there are regions of parameter space in which, given
an initial bubble, barnacles are the favoured subsequent decay process, and
that the inclusion of gravity can enlarge this region. The relation to other
heterogeneous vacuum decay scenarios, as well as some of the phenomenological
implications of barnacles are briefly discussed.
",Physics,Physics
"Learning Deep Visual Object Models From Noisy Web Data: How to Make it Work   Deep networks thrive when trained on large scale data collections. This has
given ImageNet a central role in the development of deep architectures for
visual object classification. However, ImageNet was created during a specific
period in time, and as such it is prone to aging, as well as dataset bias
issues. Moving beyond fixed training datasets will lead to more robust visual
systems, especially when deployed on robots in new environments which must
train on the objects they encounter there. To make this possible, it is
important to break free from the need for manual annotators. Recent work has
begun to investigate how to use the massive amount of images available on the
Web in place of manual image annotations. We contribute to this research thread
with two findings: (1) a study correlating a given level of noisily labels to
the expected drop in accuracy, for two deep architectures, on two different
types of noise, that clearly identifies GoogLeNet as a suitable architecture
for learning from Web data; (2) a recipe for the creation of Web datasets with
minimal noise and maximum visual variability, based on a visual and natural
language processing concept expansion strategy. By combining these two results,
we obtain a method for learning powerful deep object models automatically from
the Web. We confirm the effectiveness of our approach through object
categorization experiments using our Web-derived version of ImageNet on a
popular robot vision benchmark database, and on a lifelong object discovery
task on a mobile robot.
",Computer Science,Computer Science
"Driver Distraction Identification with an Ensemble of Convolutional Neural Networks   The World Health Organization (WHO) reported 1.25 million deaths yearly due
to road traffic accidents worldwide and the number has been continuously
increasing over the last few years. Nearly fifth of these accidents are caused
by distracted drivers. Existing work of distracted driver detection is
concerned with a small set of distractions (mostly, cell phone usage).
Unreliable ad-hoc methods are often used.In this paper, we present the first
publicly available dataset for driver distraction identification with more
distraction postures than existing alternatives. In addition, we propose a
reliable deep learning-based solution that achieves a 90% accuracy. The system
consists of a genetically-weighted ensemble of convolutional neural networks,
we show that a weighted ensemble of classifiers using a genetic algorithm
yields in a better classification confidence. We also study the effect of
different visual elements in distraction detection by means of face and hand
localizations, and skin segmentation. Finally, we present a thinned version of
our ensemble that could achieve 84.64% classification accuracy and operate in a
real-time environment.
",Computer Science; Statistics,Computer Science; Statistics
"Robust Optimal Design of Energy Efficient Series Elastic Actuators: Application to a Powered Prosthetic Ankle   Design of robotic systems that safely and efficiently operate in uncertain
operational conditions, such as rehabilitation and physical assistance robots,
remains an important challenge in the field. Current methods for the design of
energy efficient series elastic actuators use an optimization formulation that
typically assumes known operational conditions. This approach could lead to
actuators that cannot perform in uncertain environments because elongation,
speed, or torque requirements may be beyond actuator specifications when the
operation deviates from its nominal conditions. Addressing this gap, we propose
a convex optimization formulation to design the stiffness of series elastic
actuators to minimize energy consumption and satisfy actuator constraints
despite uncertainty due to manufacturing of the spring, unmodeled dynamics,
efficiency of the transmission, and the kinematics and kinetics of the load. In
our formulation, we express energy consumption as a scalar convex-quadratic
function of compliance. In the unconstrained case, this quadratic equation
provides an analytical solution to the optimal value of stiffness that
minimizes energy consumption for arbitrary periodic reference trajectories. As
actuator constraints, we consider peak motor torque, peak motor velocity,
limitations due to the speed-torque relationship of DC motors, and peak
elongation of the spring. As a simulation case study, we apply our formulation
to the robust design of a series elastic actuator for a powered prosthetic
ankle. Our simulation results indicate that a small trade-off between energy
efficiency and robustness is justified to design actuators that can operate
with uncertainty.
",Computer Science,Computer Science
"A Parameterized Approach to Personalized Variable Length Summarization of Soccer Matches   We present a parameterized approach to produce personalized variable length
summaries of soccer matches. Our approach is based on temporally segmenting the
soccer video into 'plays', associating a user-specifiable 'utility' for each
type of play and using 'bin-packing' to select a subset of the plays that add
up to the desired length while maximizing the overall utility (volume in
bin-packing terms). Our approach systematically allows a user to override the
default weights assigned to each type of play with individual preferences and
thus see a highly personalized variable length summarization of soccer matches.
We demonstrate our approach based on the output of an end-to-end pipeline that
we are building to produce such summaries. Though aspects of the overall
end-to-end pipeline are human assisted at present, the results clearly show
that the proposed approach is capable of producing semantically meaningful and
compelling summaries. Besides the obvious use of producing summaries of
superior league matches for news broadcasts, we anticipate our work to promote
greater awareness of the local matches and junior leagues by producing
consumable summaries of them.
",Computer Science,Computer Science
"Gradient-based Filter Design for the Dual-tree Wavelet Transform   The wavelet transform has seen success when incorporated into neural network
architectures, such as in wavelet scattering networks. More recently, it has
been shown that the dual-tree complex wavelet transform can provide better
representations than the standard transform. With this in mind, we extend our
previous method for learning filters for the 1D and 2D wavelet transforms into
the dual-tree domain. We show that with few modifications to our original
model, we can learn directional filters that leverage the properties of the
dual-tree wavelet transform.
",Statistics,Computer Science; Statistics
"From rate distortion theory to metric mean dimension: variational principle   The purpose of this paper is to point out a new connection between
information theory and dynamical systems. In the information theory side, we
consider rate distortion theory, which studies lossy data compression of
stochastic processes under distortion constraints. In the dynamical systems
side, we consider mean dimension theory, which studies how many parameters per
second we need to describe a dynamical system. The main results are new
variational principles connecting rate distortion function to metric mean
dimension.
",Computer Science; Mathematics,Computer Science; Mathematics
"Five-dimensional Perfect Simplices   Let $Q_n=[0,1]^n$ be the unit cube in ${\mathbb R}^n$, $n \in {\mathbb N}$.
For a nondegenerate simplex $S\subset{\mathbb R}^n$, consider the value
$\xi(S)=\min \{\sigma>0: Q_n\subset \sigma S\}$. Here $\sigma S$ is a
homothetic image of $S$ with homothety center at the center of gravity of $S$
and coefficient of homothety $\sigma$. Let us introduce the value $\xi_n=\min
\{\xi(S): S\subset Q_n\}$. We call $S$ a perfect simplex if $S\subset Q_n$ and
$Q_n$ is inscribed into the simplex $\xi_n S$. It is known that such simplices
exist for $n=1$ and $n=3$. The exact values of $\xi_n$ are known for $n=2$ and
in the case when there exist an Hadamard matrix of order $n+1$, in the latter
situation $\xi_n=n$. In this paper we show that $\xi_5=5$ and $\xi_9=9$. We
also describe infinite families of simplices $S\subset Q_n$ such that
$\xi(S)=\xi_n$ for $n=5,7,9$. The main result of the paper is the existence of
perfect simplices in ${\mathbb R}^5$.
Keywords: simplex, cube, homothety, axial diameter, Hadamard matrix
",Mathematics,Mathematics
"Polarization, plasmon, and Debye screening in doped 3D ani-Weyl semimetal   We compute the polarization function in a doped three-dimensional
anisotropic-Weyl semimetal, in which the fermion energy dispersion is linear in
two components of the momenta and quadratic in the third. Through detailed
calculations, we find that the long wavelength plasmon mode depends on the
fermion density $n_e$ in the form $\Omega_{p}^{\bot}\propto n_{e}^{3/10}$
within the basal plane and behaves as $\Omega_{p}^{z}\propto n_{e}^{1/2}$ along
the third direction. This unique characteristic of the plasmon mode can be
probed by various experimental techniques, such as electron energy-loss
spectroscopy. The Debye screening at finite chemical potential and finite
temperature is also analyzed based on the polarization function.
",Physics,Physics
"Dose finding for new vaccines: the role for immunostimulation/immunodynamic modelling   Current methods to optimize vaccine dose are purely empirically based,
whereas in the drug development field, dosing determinations use far more
advanced quantitative methodology to accelerate decision-making. Applying these
established methods in the field of vaccine development may reduce the
currently large clinical trial sample sizes, long time frames, high costs, and
ultimately have a better potential to save lives. We propose the field of
immunostimulation/immunodynamic (IS/ID) modelling, which aims to translate
mathematical frameworks used for drug dosing towards optimizing vaccine dose
decision-making. Analogous to PK/PD modelling, IS/ID modelling approaches apply
mathematical models to describe the underlying mechanisms by which the immune
response is stimulated by vaccination (IS) and the resulting measured immune
response dynamics (ID). To move IS/ID modelling forward, existing datasets and
further data on vaccine allometry and dose-dependent dynamics need to be
generated and collate, requiring a collaborative environment with input from
academia, industry, regulators, governmental and non-governmental agencies to
share modelling expertise, and connect modellers to vaccine data.
",Quantitative Biology,Statistics
"Predicting how and when hidden neurons skew measured synaptic interactions   A major obstacle to understanding neural coding and computation is the fact
that experimental recordings typically sample only a small fraction of the
neurons in a circuit. Measured neural properties are skewed by interactions
between recorded neurons and the ""hidden"" portion of the network. To properly
interpret neural data and determine how biological structure gives rise to
neural circuit function, we thus need a better understanding of the
relationships between measured effective neural properties and the true
underlying physiological properties. Here, we focus on how the effective
spatiotemporal dynamics of the synaptic interactions between neurons are
reshaped by coupling to unobserved neurons. We find that the effective
interactions from a pre-synaptic neuron $r'$ to a post-synaptic neuron $r$ can
be decomposed into a sum of the true interaction from $r'$ to $r$ plus
corrections from every directed path from $r'$ to $r$ through unobserved
neurons. Importantly, the resulting formula reveals when the hidden units
have---or do not have---major effects on reshaping the interactions among
observed neurons. As a particular example of interest, we derive a formula for
the impact of hidden units in random networks with ""strong""
coupling---connection weights that scale with $1/\sqrt{N}$, where $N$ is the
network size, precisely the scaling observed in recent experiments. With this
quantitative relationship between measured and true interactions, we can study
how network properties shape effective interactions, which properties are
relevant for neural computations, and how to manipulate effective interactions.
",Physics,Quantitative Biology
"Image Registration for the Alignment of Digitized Historical Documents   In this work, we conducted a survey on different registration algorithms and
investigated their suitability for hyperspectral historical image registration
applications. After the evaluation of different algorithms, we choose an
intensity based registration algorithm with a curved transformation model. For
the transformation model, we select cubic B-splines since they should be
capable to cope with all non-rigid deformations in our hyperspectral images.
From a number of similarity measures, we found that residual complexity and
localized mutual information are well suited for the task at hand. In our
evaluation, both measures show an acceptable performance in handling all
difficulties, e.g., capture range, non-stationary and spatially varying
intensity distortions or multi-modality that occur in our application.
",Computer Science,Computer Science
"Control Interpretations for First-Order Optimization Methods   First-order iterative optimization methods play a fundamental role in large
scale optimization and machine learning. This paper presents control
interpretations for such optimization methods. First, we give loop-shaping
interpretations for several existing optimization methods and show that they
are composed of basic control elements such as PID and lag compensators. Next,
we apply the small gain theorem to draw a connection between the convergence
rate analysis of optimization methods and the input-output gain computations of
certain complementary sensitivity functions. These connections suggest that
standard classical control synthesis tools may be brought to bear on the design
of optimization algorithms.
",Computer Science; Mathematics,Computer Science; Statistics
"Bayesian adaptive bandit-based designs using the Gittins index for multi-armed trials with normally distributed endpoints   Adaptive designs for multi-armed clinical trials have become increasingly
popular recently in many areas of medical research because of their potential
to shorten development times and to increase patient response. However,
developing response-adaptive trial designs that offer patient benefit while
ensuring the resulting trial avoids bias and provides a statistically rigorous
comparison of the different treatments included is highly challenging. In this
paper, the theory of Multi-Armed Bandit Problems is used to define a family of
near optimal adaptive designs in the context of a clinical trial with a
normally distributed endpoint with known variance. Through simulation studies
based on an ongoing trial as a motivation we report the operating
characteristics (type I error, power, bias) and patient benefit of these
approaches and compare them to traditional and existing alternative designs.
These results are then compared to those recently published in the context of
Bernoulli endpoints. Many limitations and advantages are similar in both cases
but there are also important differences, specially with respect to type I
error control. This paper proposes a simulation-based testing procedure to
correct for the observed type I error inflation that bandit-based and adaptive
rules can induce. Results presented extend recent work by considering a
normally distributed endpoint, a very common case in clinical practice yet
mostly ignored in the response-adaptive theoretical literature, and illustrate
the potential advantages of using these methods in a rare disease context. We
also recommend a suitable modified implementation of the bandit-based adaptive
designs for the case of common diseases.
",Statistics,Statistics
"Multi-rendezvous Spacecraft Trajectory Optimization with Beam P-ACO   The design of spacecraft trajectories for missions visiting multiple
celestial bodies is here framed as a multi-objective bilevel optimization
problem. A comparative study is performed to assess the performance of
different Beam Search algorithms at tackling the combinatorial problem of
finding the ideal sequence of bodies. Special focus is placed on the
development of a new hybridization between Beam Search and the Population-based
Ant Colony Optimization algorithm. An experimental evaluation shows all
algorithms achieving exceptional performance on a hard benchmark problem. It is
found that a properly tuned deterministic Beam Search always outperforms the
remaining variants. Beam P-ACO, however, demonstrates lower parameter
sensitivity, while offering superior worst-case performance. Being an anytime
algorithm, it is then found to be the preferable choice for certain practical
applications.
",Computer Science; Physics,Computer Science
"Parameter Learning and Change Detection Using a Particle Filter With Accelerated Adaptation   This paper presents the construction of a particle filter, which incorporates
elements inspired by genetic algorithms, in order to achieve accelerated
adaptation of the estimated posterior distribution to changes in model
parameters. Specifically, the filter is designed for the situation where the
subsequent data in online sequential filtering does not match the model
posterior filtered based on data up to a current point in time. The examples
considered encompass parameter regime shifts and stochastic volatility. The
filter adapts to regime shifts extremely rapidly and delivers a clear heuristic
for distinguishing between regime shifts and stochastic volatility, even though
the model dynamics assumed by the filter exhibit neither of those features.
",Statistics; Quantitative Finance,Statistics
"Training Group Orthogonal Neural Networks with Privileged Information   Learning rich and diverse representations is critical for the performance of
deep convolutional neural networks (CNNs). In this paper, we consider how to
use privileged information to promote inherent diversity of a single CNN model
such that the model can learn better representations and offer stronger
generalization ability. To this end, we propose a novel group orthogonal
convolutional neural network (GoCNN) that learns untangled representations
within each layer by exploiting provided privileged information and enhances
representation diversity effectively. We take image classification as an
example where image segmentation annotations are used as privileged information
during the training process. Experiments on two benchmark datasets -- ImageNet
and PASCAL VOC -- clearly demonstrate the strong generalization ability of our
proposed GoCNN model. On the ImageNet dataset, GoCNN improves the performance
of state-of-the-art ResNet-152 model by absolute value of 1.2% while only uses
privileged information of 10% of the training images, confirming effectiveness
of GoCNN on utilizing available privileged knowledge to train better CNNs.
",Computer Science,Computer Science; Statistics
"Trail-Mediated Self-Interaction   A number of microorganisms leave persistent trails while moving along
surfaces. For single-cell organisms, the trail-mediated self-interaction will
influence its dynamics. It has been discussed recently [Kranz \textit{et al.}
Phys. Rev. Lett. \textbf{117}, 8101 (2016)] that the self-interaction may
localize the organism above a critical coupling $\chi_c$ to the trail. Here we
will derive a generalized active particle model capturing the key features of
the self-interaction and analyze its behavior for smaller couplings $\chi <
\chi_c$. We find that fluctuations in propulsion speed shift the localization
transition to stronger couplings.
",Quantitative Biology,Physics
"V2X Meets NOMA: Non-Orthogonal Multiple Access for 5G Enabled Vehicular Networks   Benefited from the widely deployed infrastructure, the LTE network has
recently been considered as a promising candidate to support the
vehicle-to-everything (V2X) services. However, with a massive number of devices
accessing the V2X network in the future, the conventional OFDM-based LTE
network faces the congestion issues due to its low efficiency of orthogonal
access, resulting in significant access delay and posing a great challenge
especially to safety-critical applications. The non-orthogonal multiple access
(NOMA) technique has been well recognized as an effective solution for the
future 5G cellular networks to provide broadband communications and massive
connectivity. In this article, we investigate the applicability of NOMA in
supporting cellular V2X services to achieve low latency and high reliability.
Starting with a basic V2X unicast system, a novel NOMA-based scheme is proposed
to tackle the technical hurdles in designing high spectral efficient scheduling
and resource allocation schemes in the ultra dense topology. We then extend it
to a more general V2X broadcasting system. Other NOMA-based extended V2X
applications and some open issues are also discussed.
",Computer Science,Computer Science
"Optimal rate list decoding over bounded alphabets using algebraic-geometric codes   We give new constructions of two classes of algebraic code families which are
efficiently list decodable with small output list size from a fraction
$1-R-\epsilon$ of adversarial errors where $R$ is the rate of the code, for any
desired positive constant $\epsilon$. The alphabet size depends only $\epsilon$
and is nearly-optimal.
The first class of codes are obtained by folding algebraic-geometric codes
using automorphisms of the underlying function field. The list decoding
algorithm is based on a linear-algebraic approach, which pins down the
candidate messages to a subspace with a nice ""periodic"" structure. The list is
pruned by precoding into a special form of ""subspace-evasive"" sets, which are
constructed pseudorandomly. Instantiating this construction with the
Garcia-Stichtenoth function field tower yields codes list-decodable up to a
$1-R-\epsilon$ error fraction with list size bounded by $O(1/\epsilon)$,
matching the existential bound up to constant factors. The parameters we
achieve are thus quite close to the existential bounds in all three aspects:
error-correction radius, alphabet size, and list-size.
The second class of codes are obtained by restricting evaluation points of an
algebraic-geometric code to rational points from a subfield. Once again, the
linear-algebraic approach to list decoding to pin down candidate messages to a
periodic subspace. We develop an alternate approach based on ""subspace designs""
to precode messages. Together with the subsequent explicit constructions of
subspace designs, this yields a deterministic construction of an algebraic code
family of rate $R$ with efficient list decoding from $1-R-\epsilon$ fraction of
errors over a constant-sized alphabet. The list size is bounded by a very
slowly growing function of the block length $N$; in particular, it is at most
$O(\log^{(r)} N)$ (the $r$'th iterated logarithm) for any fixed integer $r$.
",Computer Science; Mathematics,Computer Science
"Resilience: A Criterion for Learning in the Presence of Arbitrary Outliers   We introduce a criterion, resilience, which allows properties of a dataset
(such as its mean or best low rank approximation) to be robustly computed, even
in the presence of a large fraction of arbitrary additional data. Resilience is
a weaker condition than most other properties considered so far in the
literature, and yet enables robust estimation in a broader variety of settings.
We provide new information-theoretic results on robust distribution learning,
robust estimation of stochastic block models, and robust mean estimation under
bounded $k$th moments. We also provide new algorithmic results on robust
distribution learning, as well as robust mean estimation in $\ell_p$-norms.
Among our proof techniques is a method for pruning a high-dimensional
distribution with bounded $1$st moments to a stable ""core"" with bounded $2$nd
moments, which may be of independent interest.
",Computer Science; Statistics,Statistics
"Causal Mediation Analysis Leveraging Multiple Types of Summary Statistics Data   Summary statistics of genome-wide association studies (GWAS) teach causal
relationship between millions of genetic markers and tens and thousands of
phenotypes. However, underlying biological mechanisms are yet to be elucidated.
We can achieve necessary interpretation of GWAS in a causal mediation
framework, looking to establish a sparse set of mediators between genetic and
downstream variables, but there are several challenges. Unlike existing methods
rely on strong and unrealistic assumptions, we tackle practical challenges
within a principled summary-based causal inference framework. We analyzed the
proposed methods in extensive simulations generated from real-world genetic
data. We demonstrated only our approach can accurately redeem causal genes,
even without knowing actual individual-level data, despite the presence of
competing non-causal trails.
",Computer Science; Statistics; Quantitative Biology,Statistics
"Ballistic magnon heat conduction and possible Poiseuille flow in the helimagnetic insulator Cu$_2$OSeO$_3$   We report on the observation of magnon thermal conductivity $\kappa_m\sim$ 70
W/mK near 5 K in the helimagnetic insulator Cu$_2$OSeO$_3$, exceeding that
measured in any other ferromagnet by almost two orders of magnitude. Ballistic,
boundary-limited transport for both magnons and phonons is established below 1
K, and Poiseuille flow of magnons is proposed to explain a magnon mean-free
path substantially exceeding the specimen width for the least defective
specimens in the range 2 K $<T<$ 10 K. These observations establish
Cu$_2$OSeO$_3$ as a model system for studying long-wavelength magnon dynamics.
",Physics,Physics
"Entanglement spectroscopy on a quantum computer   We present a quantum algorithm to compute the entanglement spectrum of
arbitrary quantum states. The interesting universal part of the entanglement
spectrum is typically contained in the largest eigenvalues of the density
matrix which can be obtained from the lower Renyi entropies through the
Newton-Girard method. Obtaining the $p$ largest eigenvalues
($\lambda_1>\lambda_2\ldots>\lambda_p$) requires a parallel circuit depth of
$\mathcal{O}(p(\lambda_1/\lambda_p)^p)$ and $\mathcal{O}(p\log(N))$ qubits
where up to $p$ copies of the quantum state defined on a Hilbert space of size
$N$ are needed as the input. We validate this procedure for the entanglement
spectrum of the topologically-ordered Laughlin wave function corresponding to
the quantum Hall state at filling factor $\nu=1/3$. Our scaling analysis
exposes the tradeoffs between time and number of qubits for obtaining the
entanglement spectrum in the thermodynamic limit using finite-size digital
quantum computers. We also illustrate the utility of the second Renyi entropy
in predicting a topological phase transition and in extracting the localization
length in a many-body localized system.
",Physics,Physics
"Image-derived generative modeling of pseudo-macromolecular structures - towards the statistical assessment of Electron CryoTomography template matching   Cellular Electron CryoTomography (CECT) is a 3D imaging technique that
captures information about the structure and spatial organization of
macromolecular complexes within single cells, in near-native state and at
sub-molecular resolution. Although template matching is often used to locate
macromolecules in a CECT image, it is insufficient as it only measures the
relative structural similarity. Therefore, it is preferable to assess the
statistical credibility of the decision through hypothesis testing, requiring
many templates derived from a diverse population of macromolecular structures.
Due to the very limited number of known structures, we need a generative model
to efficiently and reliably sample pseudo-structures from the complex
distribution of macromolecular structures. To address this challenge, we
propose a novel image-derived approach for performing hypothesis testing for
template matching by constructing generative models using the generative
adversarial network. Finally, we conducted hypothesis testing experiments for
template matching on both simulated and experimental subtomograms, allowing us
to conclude the identity of subtomograms with high statistical credibility and
significantly reducing false positives.
",Statistics; Quantitative Biology,Statistics
"Casimir free energy of dielectric films: Classical limit, low-temperature behavior and control   The Casimir free energy of dielectric films, both free-standing in vacuum and
deposited on metallic or dielectric plates, is investigated. It is shown that
the values of the free energy depend considerably on whether the calculation
approach used neglects or takes into account the dc conductivity of film
material. We demonstrate that there are the material-dependent and universal
classical limits in the former and latter cases, respectively. The analytic
behavior of the Casimir free energy and entropy for a free-standing dielectric
film at low temperature in found. According to our results, the Casimir entropy
goes to zero when the temperature vanishes if the calculation approach with
neglected dc conductivity of a film is employed. If the dc conductivity is
taken into account, the Casimir entropy takes the positive value at zero
temperature, depending on the parameters of a film, i.e., the Nernst heat
theorem is violated. By considering the Casimir free energy of silica and
sapphire films deposited on a Au plate in the framework of two calculation
approaches, we argue that physically correct values are obtained by
disregarding the role of dc conductivity. A comparison with the well known
results for the configuration of two parallel plates is made. Finally, we
compute the Casimir free energy of silica, sapphire and Ge films deposited on
high-resistivity Si plates of different thicknesses and demonstrate that it can
be positive, negative and equal to zero. Possible applications of the obtained
results to thin films used in microelectronics are discussed.
",Physics,Physics
"Scalable Magnetic Field SLAM in 3D Using Gaussian Process Maps   We present a method for scalable and fully 3D magnetic field simultaneous
localisation and mapping (SLAM) using local anomalies in the magnetic field as
a source of position information. These anomalies are due to the presence of
ferromagnetic material in the structure of buildings and in objects such as
furniture. We represent the magnetic field map using a Gaussian process model
and take well-known physical properties of the magnetic field into account. We
build local maps using three-dimensional hexagonal block tiling. To make our
approach computationally tractable we use reduced-rank Gaussian process
regression in combination with a Rao-Blackwellised particle filter. We show
that it is possible to obtain accurate position and orientation estimates using
measurements from a smartphone, and that our approach provides a scalable
magnetic field SLAM algorithm in terms of both computational complexity and map
storage.
",Computer Science; Statistics,Computer Science
"On the link between column density distribution and density scaling relation in star formation regions   We present a method to derive the density scaling relation $\langle n\rangle
\propto L^{-\alpha}$ in regions of star formation or in their turbulent
vicinities from straightforward binning of the column-density distribution
($N$-pdf). The outcome of the method is studied for three types of $N$-pdf:
power law ($7/5\le\alpha\le5/3$), lognormal ($0.7\lesssim\alpha\lesssim1.4$)
and combination of lognormals. In the last case, the method of Stanchev et al.
(2015) was also applied for comparison and a very weak (or close to zero)
correlation was found. We conclude that the considered `binning approach'
reflects rather the local morphology of the $N$-pdf with no reference to the
physical conditions in a considered region. The rough consistency of the
derived slopes with the widely adopted Larson's (1981) value $\alpha\sim1.1$ is
suggested to support claims that the density-size relation in molecular clouds
is indeed an artifact of the observed $N$-pdf.
",Physics,Physics
"Infinite ergodic index of the ehrenfest wind-tree model   The set of all possible configurations of the Ehrenfest wind-tree model
endowed with the Hausdorff topology is a compact metric space. For a typical
configuration we show that the wind-tree dynamics has infinite ergodic index in
almost every direction. In particular some ergodic theorems can be applied to
show that if we start with a large number of initially parallel particles their
directions decorrelate as the dynamics evolve answering the question posed by
the Ehrenfests.
",Mathematics,Mathematics
"Low-dose cryo electron ptychography via non-convex Bayesian optimization   Electron ptychography has seen a recent surge of interest for phase sensitive
imaging at atomic or near-atomic resolution. However, applications are so far
mainly limited to radiation-hard samples because the required doses are too
high for imaging biological samples at high resolution. We propose the use of
non-convex, Bayesian optimization to overcome this problem and reduce the dose
required for successful reconstruction by two orders of magnitude compared to
previous experiments. We suggest to use this method for imaging single
biological macromolecules at cryogenic temperatures and demonstrate 2D
single-particle reconstructions from simulated data with a resolution of 7.9
\AA$\,$ at a dose of 20 $e^- / \AA^2$. When averaging over only 15 low-dose
datasets, a resolution of 4 \AA$\,$ is possible for large macromolecular
complexes. With its independence from microscope transfer function, direct
recovery of phase contrast and better scaling of signal-to-noise ratio,
cryo-electron ptychography may become a promising alternative to Zernike
phase-contrast microscopy.
",Physics; Mathematics; Statistics,Physics
"On the structure of join tensors with applications to tensor eigenvalue problems   We investigate the structure of join tensors, which may be regarded as the
multivariable extension of lattice-theoretic join matrices. Explicit formulae
for a polyadic decomposition (i.e., a linear combination of rank-1 tensors) and
a tensor-train decomposition of join tensors are derived on general join
semilattices. We discuss conditions under which the obtained decompositions are
optimal in rank, and examine numerically the storage complexity of the obtained
decompositions for a class of LCM tensors as a special case of join tensors. In
addition, we investigate numerically the sharpness of a theoretical upper bound
on the tensor eigenvalues of LCM tensors.
",Mathematics,Mathematics
"A Comparison of Spatial-based Targeted Disease Containment Strategies using Mobile Phone Data   Epidemic outbreaks are an important healthcare challenge, especially in
developing countries where they represent one of the major causes of mortality.
Approaches that can rapidly target subpopulations for surveillance and control
are critical for enhancing containment processes during epidemics.
Using a real-world dataset from Ivory Coast, this work presents an attempt to
unveil the socio-geographical heterogeneity of disease transmission dynamics.
By employing a spatially explicit meta-population epidemic model derived from
mobile phone Call Detail Records (CDRs), we investigate how the differences in
mobility patterns may affect the course of a realistic infectious disease
outbreak. We consider different existing measures of the spatial dimension of
human mobility and interactions, and we analyse their relevance in identifying
the highest risk sub-population of individuals, as the best candidates for
isolation countermeasures. The approaches presented in this paper provide
further evidence that mobile phone data can be effectively exploited to
facilitate our understanding of individuals' spatial behaviour and its
relationship with the risk of infectious diseases' contagion. In particular, we
show that CDRs-based indicators of individuals' spatial activities and
interactions hold promise for gaining insight of contagion heterogeneity and
thus for developing containment strategies to support decision-making during
country-level pandemics.
",Computer Science; Physics,Computer Science
"Bit Complexity of Computing Solutions for Symmetric Hyperbolic Systems of PDEs with Guaranteed Precision   We establish upper bounds of bit complexity of computing solution operators
for symmetric hyperbolic systems of PDEs. Here we continue the research started
in in our revious publications where computability, in the rigorous sense of
computable analysis, has been established for solution operators of Cauchy and
dissipative boundary-value problems for such systems.
",Computer Science,Computer Science; Mathematics
"Noise2Noise: Learning Image Restoration without Clean Data   We apply basic statistical reasoning to signal reconstruction by machine
learning -- learning to map corrupted observations to clean signals -- with a
simple and powerful conclusion: it is possible to learn to restore images by
only looking at corrupted examples, at performance at and sometimes exceeding
training using clean data, without explicit image priors or likelihood models
of the corruption. In practice, we show that a single model learns photographic
noise removal, denoising synthetic Monte Carlo images, and reconstruction of
undersampled MRI scans -- all corrupted by different processes -- based on
noisy data only.
",Statistics,Computer Science; Statistics
"Low-Dose CT with a Residual Encoder-Decoder Convolutional Neural Network (RED-CNN)   Given the potential X-ray radiation risk to the patient, low-dose CT has
attracted a considerable interest in the medical imaging field. The current
main stream low-dose CT methods include vendor-specific sinogram domain
filtration and iterative reconstruction, but they need to access original raw
data whose formats are not transparent to most users. Due to the difficulty of
modeling the statistical characteristics in the image domain, the existing
methods for directly processing reconstructed images cannot eliminate image
noise very well while keeping structural details. Inspired by the idea of deep
learning, here we combine the autoencoder, the deconvolution network, and
shortcut connections into the residual encoder-decoder convolutional neural
network (RED-CNN) for low-dose CT imaging. After patch-based training, the
proposed RED-CNN achieves a competitive performance relative to
the-state-of-art methods in both simulated and clinical cases. Especially, our
method has been favorably evaluated in terms of noise suppression, structural
preservation and lesion detection.
",Computer Science; Physics,Computer Science; Statistics
"Accelerating Innovation Through Analogy Mining   The availability of large idea repositories (e.g., the U.S. patent database)
could significantly accelerate innovation and discovery by providing people
with inspiration from solutions to analogous problems. However, finding useful
analogies in these large, messy, real-world repositories remains a persistent
challenge for either human or automated methods. Previous approaches include
costly hand-created databases that have high relational structure (e.g.,
predicate calculus representations) but are very sparse. Simpler
machine-learning/information-retrieval similarity metrics can scale to large,
natural-language datasets, but struggle to account for structural similarity,
which is central to analogy. In this paper we explore the viability and value
of learning simpler structural representations, specifically, ""problem
schemas"", which specify the purpose of a product and the mechanisms by which it
achieves that purpose. Our approach combines crowdsourcing and recurrent neural
networks to extract purpose and mechanism vector representations from product
descriptions. We demonstrate that these learned vectors allow us to find
analogies with higher precision and recall than traditional
information-retrieval methods. In an ideation experiment, analogies retrieved
by our models significantly increased people's likelihood of generating
creative ideas compared to analogies retrieved by traditional methods. Our
results suggest a promising approach to enabling computational analogy at scale
is to learn and leverage weaker structural representations.
",Computer Science; Statistics,Computer Science
"Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations   Neural networks are among the most accurate supervised learning methods in
use today, but their opacity makes them difficult to trust in critical
applications, especially when conditions in training differ from those in test.
Recent work on explanations for black-box models has produced tools (e.g. LIME)
to show the implicit rules behind predictions, which can help us identify when
models are right for the wrong reasons. However, these methods do not scale to
explaining entire datasets and cannot correct the problems they reveal. We
introduce a method for efficiently explaining and regularizing differentiable
models by examining and selectively penalizing their input gradients, which
provide a normal to the decision boundary. We apply these penalties both based
on expert annotation and in an unsupervised fashion that encourages diverse
models with qualitatively different decision boundaries for the same
classification problem. On multiple datasets, we show our approach generates
faithful explanations and models that generalize much better when conditions
differ between training and test.
",Computer Science; Statistics,Computer Science; Statistics
"The hypotensive effect of activated apelin receptor is correlated with \b{eta}-arrestin recruitment   The apelinergic system is an important player in the regulation of both
vascular tone and cardiovascular function, making this physiological system an
attractive target for drug development for hypertension, heart failure and
ischemic heart disease. Indeed, apelin exerts a positive inotropic effect in
humans whilst reducing peripheral vascular resistance. In this study, we
investigated the signaling pathways through which apelin exerts its hypotensive
action. We synthesized a series of apelin-13 analogs whereby the C-terminal
Phe13 residue was replaced by natural or unnatural amino acids. In HEK293 cells
expressing APJ, we evaluated the relative efficacy of these compounds to
activate G{\alpha}i1 and G{\alpha}oA G-proteins, recruit \b{eta}-arrestins 1
and 2 (\b{eta}arrs), and inhibit cAMP production. Calculating the transduction
ratio for each pathway allowed us to identify several analogs with distinct
signaling profiles. Furthermore, we found that these analogs delivered i.v. to
Sprague-Dawley rats exerted a wide range of hypotensive responses. Indeed, two
compounds lost their ability to lower blood pressure, while other analogs
significantly reduced blood pressure as apelin-13. Interestingly, analogs that
did not lower blood pressure were less effective at recruiting \b{eta}arrs.
Finally, using Spearman correlations, we established that the hypotensive
response was significantly correlated with \b{eta}arr recruitment but not with
G protein- dependent signaling. In conclusion, our results demonstrated that
the \b{eta}arr recruitment potency is involved in the hypotensive efficacy of
activated APJ.
",Quantitative Biology,Quantitative Biology
"CN rings in full protoplanetary disks around young stars as probes of disk structure   Bright ring-like structure emission of the CN molecule has been observed in
protoplanetary disks. We investigate whether such structures are due to the
morphology of the disk itself or if they are instead an intrinsic feature of CN
emission. With the intention of using CN as a diagnostic, we also address to
which physical and chemical parameters CN is most sensitive. A set of disk
models were run for different stellar spectra, masses, and physical structures
via the 2D thermochemical code DALI. An updated chemical network that accounts
for the most relevant CN reactions was adopted. Ring-shaped emission is found
to be a common feature of all adopted models; the highest abundance is found in
the upper outer regions of the disk, and the column density peaks at 30-100 AU
for T Tauri stars with standard accretion rates. Higher mass disks generally
show brighter CN. Higher UV fields, such as those appropriate for T Tauri stars
with high accretion rates or for Herbig Ae stars or for higher disk flaring,
generally result in brighter and larger rings. These trends are due to the main
formation paths of CN, which all start with vibrationally excited H2*
molecules, that are produced through far ultraviolet (FUV) pumping of H2. The
model results compare well with observed disk-integrated CN fluxes and the
observed location of the CN ring for the TW Hya disk. CN rings are produced
naturally in protoplanetary disks and do not require a specific underlying disk
structure such as a dust cavity or gap. The strong link between FUV flux and CN
emission can provide critical information regarding the vertical structure of
the disk and the distribution of dust grains which affects the UV penetration,
and could help to break some degeneracies in the SED fitting. In contrast with
C2H or c-C3H2, the CN flux is not very sensitive to carbon and oxygen
depletion.
",Physics,Physics
"Universality in numerical computation with random data. Case studies, analytic results and some speculations   We discuss various universality aspects of numerical computations using
standard algorithms. These aspects include empirical observations and rigorous
results. We also make various speculations about computation in a broader
sense.
",Physics; Mathematics,Computer Science; Mathematics; Statistics
"Modelling thermo-electro-mechanical effects in orthotropic cardiac tissue   In this paper we introduce a new mathematical model for the active
contraction of cardiac muscle, featuring different thermo-electric and
nonlinear conductivity properties. The passive hyperelastic response of the
tissue is described by an orthotropic exponential model, whereas the ionic
activity dictates active contraction incorporated through the concept of
orthotropic active strain. We use a fully incompressible formulation, and the
generated strain modifies directly the conductivity mechanisms in the medium
through the pull-back transformation. We also investigate the influence of
thermo-electric effects in the onset of multiphysics emergent spatiotemporal
dynamics, using nonlinear diffusion. It turns out that these ingredients have a
key role in reproducing pathological chaotic dynamics such as ventricular
fibrillation during inflammatory events, for instance. The specific structure
of the governing equations suggests to cast the problem in mixed-primal form
and we write it in terms of Kirchhoff stress, displacements, solid pressure,
electric potential, activation generation, and ionic variables. We also propose
a new mixed-primal finite element method for its numerical approximation, and
we use it to explore the properties of the model and to assess the importance
of coupling terms, by means of a few computational experiments in 3D.
",Quantitative Biology,Physics
"Fast learning rate of deep learning via a kernel perspective   We develop a new theoretical framework to analyze the generalization error of
deep learning, and derive a new fast learning rate for two representative
algorithms: empirical risk minimization and Bayesian deep learning. The series
of theoretical analyses of deep learning has revealed its high expressive power
and universal approximation capability. Although these analyses are highly
nonparametric, existing generalization error analyses have been developed
mainly in a fixed dimensional parametric model. To compensate this gap, we
develop an infinite dimensional model that is based on an integral form as
performed in the analysis of the universal approximation capability. This
allows us to define a reproducing kernel Hilbert space corresponding to each
layer. Our point of view is to deal with the ordinary finite dimensional deep
neural network as a finite approximation of the infinite dimensional one. The
approximation error is evaluated by the degree of freedom of the reproducing
kernel Hilbert space in each layer. To estimate a good finite dimensional
model, we consider both of empirical risk minimization and Bayesian deep
learning. We derive its generalization error bound and it is shown that there
appears bias-variance trade-off in terms of the number of parameters of the
finite dimensional approximation. We show that the optimal width of the
internal layers can be determined through the degree of freedom and the
convergence rate can be faster than $O(1/\sqrt{n})$ rate which has been shown
in the existing studies.
",Computer Science; Mathematics; Statistics,Computer Science; Statistics
"Improving SIEM capabilities through an enhanced probe for encrypted Skype traffic detection   Nowadays, the Security Information and Event Management (SIEM) systems take
on great relevance in handling security issues for critical infrastructures as
Internet Service Providers. Basically, a SIEM has two main functions: i) the
collection and the aggregation of log data and security information from
disparate network devices (routers, firewalls, intrusion detection systems, ad
hoc probes and others) and ii) the analysis of the gathered data by
implementing a set of correlation rules aimed at detecting potential suspicious
events as the presence of encrypted real-time traffic. In the present work, the
authors propose an enhanced implementation of a SIEM where a particular focus
is given to the detection of encrypted Skype traffic by using an ad-hoc
developed enhanced probe (ESkyPRO) conveniently governed by the SIEM itself.
Such enhanced probe, able to interact with an agent counterpart deployed into
the SIEM platform, is designed by exploiting some machine learning concepts.
The main purpose of the proposed ad-hoc SIEM is to correlate the information
received by ESkyPRO and other types of data obtained by an Intrusion Detection
System (IDS) probe in order to make the encrypted Skype traffic detection as
accurate as possible.
",Computer Science,Computer Science
"Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks   The paper evaluates three variants of the Gated Recurrent Unit (GRU) in
recurrent neural networks (RNN) by reducing parameters in the update and reset
gates. We evaluate the three variant GRU models on MNIST and IMDB datasets and
show that these GRU-RNN variant models perform as well as the original GRU RNN
model while reducing the computational expense.
",Computer Science; Statistics,Computer Science; Statistics
"Adversarial Variational Inference and Learning in Markov Random Fields   Markov random fields (MRFs) find applications in a variety of machine
learning areas, while the inference and learning of such models are challenging
in general. In this paper, we propose the Adversarial Variational Inference and
Learning (AVIL) algorithm to solve the problems with a minimal assumption about
the model structure of an MRF. AVIL employs two variational distributions to
approximately infer the latent variables and estimate the partition function,
respectively. The variational distributions, which are parameterized as neural
networks, provide an estimate of the negative log likelihood of the MRF. On one
hand, the estimate is in an intuitive form of approximate contrastive free
energy. On the other hand, the estimate is a minimax optimization problem,
which is solved by stochastic gradient descent in an alternating manner. We
apply AVIL to various undirected generative models in a fully black-box manner
and obtain better results than existing competitors on several real datasets.
",Computer Science; Statistics,Statistics
"Infinitesimal perturbation analysis for risk measures based on the Smith max-stable random field   When using risk or dependence measures based on a given underlying model, it
is essential to be able to quantify the sensitivity or robustness of these
measures with respect to the model parameters. In this paper, we consider an
underlying model which is very popular in spatial extremes, the Smith
max-stable random field. We study the sensitivity properties of risk or
dependence measures based on the values of this field at a finite number of
locations. Max-stable fields play a key role, e.g., in the modelling of natural
disasters. As their multivariate density is generally not available for more
than three locations, the Likelihood Ratio Method cannot be used to estimate
the derivatives of the risk measures with respect to the model parameters.
Thus, we focus on a pathwise method, the Infinitesimal Perturbation Analysis
(IPA). We provide a convenient and tractable sufficient condition for
performing IPA, which is intricate to obtain because of the very structure of
max-stable fields involving pointwise maxima over an infinite number of random
functions. IPA enables the consistent estimation of the considered measures'
derivatives with respect to the parameters characterizing the spatial
dependence. We carry out a simulation study which shows that the approach
performs well in various configurations.
",Quantitative Finance,Physics; Mathematics
"Protonation induced high-Tc phases in iron-based superconductors evidenced by NMR and magnetization measurements   Chemical substitution during growth is a well-established method to
manipulate electronic states of quantum materials, and leads to rich spectra of
phase diagrams in cuprate and iron-based superconductors. Here we report a
novel and generic strategy to achieve nonvolatile electron doping in series of
(i.e. 11 and 122 structures) Fe-based superconductors by ionic liquid gating
induced protonation at room temperature. Accumulation of protons in bulk
compounds induces superconductivity in the parent compounds, and enhances the
Tc largely in some superconducting ones. Furthermore, the existence of proton
in the lattice enables the first proton nuclear magnetic resonance (NMR) study
to probe directly superconductivity. Using FeS as a model system, our NMR study
reveals an emergent high-Tc phase with no coherence peak which is hard to
measure by NMR with other isotopes. This novel electric-field-induced proton
evolution opens up an avenue for manipulation of competing electronic states
(e.g. Mott insulators), and may provide an innovative way for a broad
perspective of NMR measurements with greatly enhanced detecting resolution.
",Physics,Physics
"Report: Dynamic Eye Movement Matching and Visualization Tool in Neuro Gesture   In the research of the impact of gestures using by a lecturer, one
challenging task is to infer the attention of a group of audiences. Two
important measurements that can help infer the level of attention are eye
movement data and Electroencephalography (EEG) data. Under the fundamental
assumption that a group of people would look at the same place if they all pay
attention at the same time, we apply a method, ""Time Warp Edit Distance"", to
calculate the similarity of their eye movement trajectories. Moreover, we also
cluster eye movement pattern of audiences based on these pair-wised similarity
metrics. Besides, since we don't have a direct metric for the ""attention""
ground truth, a visual assessment would be beneficial to evaluate the
gesture-attention relationship. Thus we also implement a visualization tool.
",Computer Science,Computer Science
"Fuzzy logic based approaches for gene regulatory network inference   The rapid advancement in high-throughput techniques has fueled the generation
of large volume of biological data rapidly with low cost. Some of these
techniques are microarray and next generation sequencing which provides genome
level insight of living cells. As a result, the size of most of the biological
databases, such as NCBI-GEO, NCBI-SRA, is exponentially growing. These
biological data are analyzed using computational techniques for knowledge
discovery - which is one of the objectives of bioinformatics research. Gene
regulatory network (GRN) is a gene-gene interaction network which plays pivotal
role in understanding gene regulation process and disease studies. From the
last couple of decades, the researchers are interested in developing
computational algorithms for GRN inference (GRNI) using high-throughput
experimental data. Several computational approaches have been applied for
inferring GRN from gene expression data including statistical techniques
(correlation coefficient), information theory (mutual information), regression
based approaches, probabilistic approaches (Bayesian networks, naive byes),
artificial neural networks, and fuzzy logic. The fuzzy logic, along with its
hybridization with other intelligent approach, is well studied in GRNI due to
its several advantages. In this paper, we present a consolidated review on
fuzzy logic and its hybrid approaches for GRNI developed during last two
decades.
",Quantitative Biology,Statistics
"Prediction-Constrained Topic Models for Antidepressant Recommendation   Supervisory signals can help topic models discover low-dimensional data
representations that are more interpretable for clinical tasks. We propose a
framework for training supervised latent Dirichlet allocation that balances two
goals: faithful generative explanations of high-dimensional data and accurate
prediction of associated class labels. Existing approaches fail to balance
these goals by not properly handling a fundamental asymmetry: the intended task
is always predicting labels from data, not data from labels. Our new
prediction-constrained objective trains models that predict labels from heldout
data well while also producing good generative likelihoods and interpretable
topic-word parameters. In a case study on predicting depression medications
from electronic health records, we demonstrate improved recommendations
compared to previous supervised topic models and high- dimensional logistic
regression from words alone.
",Computer Science; Statistics,Statistics
"On the geometry of the moduli space of sheaves supported on curves of genus two in a quadric surface   We study the moduli space of stable sheaves of Euler characteristic 2,
supported on curves of arithmetic genus 2 contained in a smooth quadric
surface. We show that this moduli space is rational. We compute its Betti
numbers and we give a classification of the stable sheaves involving locally
free resolutions.
",Mathematics,Mathematics
"Emergence of Topological Nodal Lines and Type II Weyl Nodes in Strong Spin--Orbit Coupling System InNbX2(X=S,Se)   Using first--principles density functional calculations, we systematically
investigate electronic structures and topological properties of InNbX2 (X=S,
Se). In the absence of spin--orbit coupling (SOC), both compounds show nodal
lines protected by mirror symmetry. Including SOC, the Dirac rings in InNbS2
split into two Weyl rings. This unique property is distinguished from other
dicovered nodal line materials which normally requires the absence of SOC. On
the other hand, SOC breaks the nodal lines in InNbSe2 and the compound becomes
a type II Weyl semimetal with 12 Weyl points in the Brillouin Zone. Using a
supercell slab calculation we study the dispersion of Fermi arcs surface states
in InNbSe2, we also utilize a coherent potential approximation to probe their
tolernace to the surface disorder effects. The quasi two--dimensionality and
the absence of toxic elements makes these two compounds an ideal experimental
platform for investigating novel properties of topological semimetals.
",Physics,Physics
"YUI and HANA: Control and Visualization Programs for HRC in J-PARC   We developed control and visualization programs, YUI and HANA, for High-
Resolution Chopper spectrometer (HRC) installed at BL12 in MLF, J-PARC. YUI is
a comprehensive program to control DAQ-middleware, the accessories, and sample
environment devices. HANA is a program for the data transformation and
visualization of inelastic neutron scattering spectra. In this paper, we
describe the basic system structures and unique functions of these programs
from the viewpoint of users.
",Physics,Physics
"A Novel Model of Cancer-Induced Peripheral Neuropathy and the Role of TRPA1 in Pain Transduction   Background. Models of cancer-induced neuropathy are designed by injecting
cancer cells near the peripheral nerves. The interference of tissue-resident
immune cells does not allow a direct contact with nerve fibres which affects
the tumor microenvironment and the invasion process. Methods. Anaplastic
tumor-1 (AT-1) cells were inoculated within the sciatic nerves (SNs) of male
Copenhagen rats. Lumbar dorsal root ganglia (DRGs) and the SNs were collected
on days 3, 7, 14, and 21. SN tissues were examined for morphological changes
and DRG tissues for immunofluorescence, electrophoretic tendency, and mRNA
quantification. Hypersensitivities to cold, mechanical, and thermal stimuli
were determined. HC-030031, a selective TRPA1 antagonist, was used to treat
cold allodynia. Results. Nociception thresholds were identified on day 6.
Immunofluorescent micrographs showed overexpression of TRPA1 on days 7 and 14
and of CGRP on day 14 until day 21. Both TRPA1 and CGRP were coexpressed on the
same cells. Immunoblots exhibited an increase in TRPA1 expression on day 14.
TRPA1 mRNA underwent an increase on day 7 (normalized to 18S). Injection of
HC-030031 transiently reversed the cold allodynia. Conclusion. A novel and a
promising model of cancer-induced neuropathy was established, and the role of
TRPA1 and CGRP in pain transduction was examined.
",Quantitative Biology,Quantitative Biology
"An evaluation of cosmological models from expansion and growth of structure measurements   We compare a large suite of theoretical cosmological models to observational
data from the cosmic microwave background, baryon acoustic oscillation
measurements of expansion, Type Ia SNe measurements of expansion, redshift
space distortion measurements of the growth of structure, and the local Hubble
constant. Our theoretical models include parametrizations of dark energy as
well as physical models of dark energy and modified gravity. We determine the
constraints on the model parameters, incorporating the redshift space
distortion data directly in the analysis. To determine whether models can be
ruled out, we evaluate the $p$ value (the probability under the model of
obtaining data as bad or worse than the observed data). In our comparison, we
find the well known tension of H$_0$ with the other data; no model resolves
this tension successfully. Among the models we consider, the large scale growth
of structure data does not affect the modified gravity models as a category
particularly differently than dark energy models; it matters for some modified
gravity models but not others, and the same is true for dark energy models. We
compute predicted observables for each model under current observational
constraints, and identify models for which future observational constraints
will be particularly informative.
",Physics,Physics
"Persistent Currents in Ferromagnetic Condensates   Persistent currents in Bose condensates with a scalar order parameter are
stabilized by the topology of the order parameter manifold. In condensates with
multicomponent order parameters it is topologically possible for supercurrents
to `unwind' without leaving the manifold. We study the energetics of this
process in the case of ferromagnetic condensates using a long wavelength energy
functional that includes both the superfluid and spin stiffnesses. Exploiting
analogies to an elastic rod and rigid body motion, we show that the current
carrying state in a 1D ring geometry transitions between a spin helix in the
energy minima and a soliton-like configuration at the maxima. The relevance to
recent experiments in ultracold atoms is briefly discussed.
",Physics,Physics
"Leveraging the Path Signature for Skeleton-based Human Action Recognition   Human action recognition in videos is one of the most challenging tasks in
computer vision. One important issue is how to design discriminative features
for representing spatial context and temporal dynamics. Here, we introduce a
path signature feature to encode information from intra-frame and inter-frame
contexts. A key step towards leveraging this feature is to construct the proper
trajectories (paths) for the data steam. In each frame, the correlated
constraints of human joints are treated as small paths, then the spatial path
signature features are extracted from them. In video data, the evolution of
these spatial features over time can also be regarded as paths from which the
temporal path signature features are extracted. Eventually, all these features
are concatenated to constitute the input vector of a fully connected neural
network for action classification. Experimental results on four standard
benchmark action datasets, J-HMDB, SBU Dataset, Berkeley MHAD, and NTURGB+D
demonstrate that the proposed approach achieves state-of-the-art accuracy even
in comparison with recent deep learning based models.
",Computer Science,Computer Science; Statistics
"The Query Complexity of Cake Cutting   We study the query complexity of cake cutting and give lower and upper bounds
for computing approximately envy-free, perfect, and equitable allocations with
the minimum number of cuts. The lower bounds are tight for computing connected
envy-free allocations among n=3 players and for computing perfect and equitable
allocations with minimum number of cuts between n=2 players.
We also formalize moving knife procedures and show that a large subclass of
this family, which captures all the known moving knife procedures, can be
simulated efficiently with arbitrarily small error in the Robertson-Webb query
model.
",Computer Science,Computer Science
"Supercongruences related to ${}_3F_2(1)$ involving harmonic numbers   We show various supercongruences for truncated series which involve central
binomial coefficients and harmonic numbers. The corresponding infinite series
are also evaluated.
",Mathematics,Mathematics
"Zero-Delay Source-Channel Coding with a One-Bit ADC Front End and Correlated Side Information at the Receiver   Zero-delay transmission of a Gaussian source over an additive white Gaussian
noise (AWGN) channel is considered with a one-bit analog-to-digital converter
(ADC) front end and a correlated side information at the receiver. The design
of the optimal encoder and decoder is studied for two performance criteria,
namely, the mean squared error (MSE) distortion and the distortion outage
probability (DOP), under an average power constraint on the channel input. For
both criteria, necessary optimality conditions for the encoder and the decoder
are derived. Using these conditions, it is observed that the numerically
optimized encoder (NOE) under the MSE distortion criterion is periodic, and its
period increases with the correlation between the source and the receiver side
information. For the DOP, it is instead seen that the NOE mappings periodically
acquire positive and negative values, which decay to zero with increasing
source magnitude, and the interval over which the mapping takes non-zero
values, becomes wider with the correlation between the source and the side
information.
",Computer Science,Computer Science
"Shared urbanism: Big data on accommodation sharing in urban Australia   As affordability pressures and tight rental markets in global cities mount,
online shared accommodation sites proliferate. Home sharing arrangements
present dilemmas for planning that aims to improve health and safety standards,
while supporting positives such as the usage of dormant stock and the relieving
of rental pressures on middle/lower income earners. Currently, no formal data
exists on this internationally growing trend. Here, we present a first
quantitative glance on shared accommodation practices across all major urban
centers of Australia enabled via collection and analysis of thousands of online
listings. We examine, countrywide, the spatial and short time scale temporal
characteristics of this market, along with preliminary analysis on rents,
dwelling types and other characteristics. Findings have implications for
housing policy makers and planning practitioners seeking to monitor and respond
to housing policy and affordability pressures in formal and informal housing
markets.
",Computer Science; Physics,Computer Science
"Bio-Inspired Multi-Layer Spiking Neural Network Extracts Discriminative Features from Speech Signals   Spiking neural networks (SNNs) enable power-efficient implementations due to
their sparse, spike-based coding scheme. This paper develops a bio-inspired SNN
that uses unsupervised learning to extract discriminative features from speech
signals, which can subsequently be used in a classifier. The architecture
consists of a spiking convolutional/pooling layer followed by a fully connected
spiking layer for feature discovery. The convolutional layer of leaky,
integrate-and-fire (LIF) neurons represents primary acoustic features. The
fully connected layer is equipped with a probabilistic spike-timing-dependent
plasticity learning rule. This layer represents the discriminative features
through probabilistic, LIF neurons. To assess the discriminative power of the
learned features, they are used in a hidden Markov model (HMM) for spoken digit
recognition. The experimental results show performance above 96% that compares
favorably with popular statistical feature extraction methods. Our results
provide a novel demonstration of unsupervised feature acquisition in an SNN.
",Computer Science,Computer Science; Statistics
"Accurate Multi-physics Numerical Analysis of Particle Preconcentration Based on Ion Concentration Polarization   This paper studies mechanism of preconcentration of charged particles in a
straight micro-channel embedded with permselective membranes, by numerically
solving coupled transport equations of ions, charged particles and solvent
fluid without any simplifying assumptions. It is demonstrated that trapping and
preconcentration of charged particles are determined by the interplay between
drag force from the electroosmotic fluid flow and the electrophoretic force
applied trough the electric field. Several insightful characteristics are
revealed, including the diverse dynamics of co-ions and counter ions,
replacement of co-ions by focused particles, lowered ion concentrations in
particle enriched zone, and enhanced electroosmotic pumping effect etc.
Conditions for particles that may be concentrated are identified in terms of
charges, sizes and electrophoretic mobilities of particles and co-ions.
Dependences of enrichment factor on cross-membrane voltage, initial particle
concentration and buffer ion concentrations are analyzed and the underlying
reasons are elaborated. Finally, post priori a condition for validity of
decoupled simulation model is given based on charges carried by focused charge
particles and that by buffer co-ions. These results provide important guidance
in the design and optimization of nanofluidic preconcentration and other
related devices.
",Physics,Physics
"A fast numerical method for ideal fluid flow in domains with multiple stirrers   A collection of arbitrarily-shaped solid objects, each moving at a constant
speed, can be used to mix or stir ideal fluid, and can give rise to interesting
flow patterns. Assuming these systems of fluid stirrers are two-dimensional,
the mathematical problem of resolving the flow field - given a particular
distribution of any finite number of stirrers of specified shape and speed -
can be formulated as a Riemann-Hilbert problem. We show that this
Riemann-Hilbert problem can be solved numerically using a fast and accurate
algorithm for any finite number of stirrers based around a boundary integral
equation with the generalized Neumann kernel. Various systems of fluid stirrers
are considered, and our numerical scheme is shown to handle highly multiply
connected domains (i.e. systems of many fluid stirrers) with minimal
computational expense.
",Mathematics,Mathematics
"Distance-based Protein Folding Powered by Deep Learning   Contact-assisted protein folding has made very good progress, but two
challenges remain. One is accurate contact prediction for proteins lack of many
sequence homologs and the other is that time-consuming folding simulation is
often needed to predict good 3D models from predicted contacts. We show that
protein distance matrix can be predicted well by deep learning and then
directly used to construct 3D models without folding simulation at all. Using
distance geometry to construct 3D models from our predicted distance matrices,
we successfully folded 21 of the 37 CASP12 hard targets with a median family
size of 58 effective sequence homologs within 4 hours on a Linux computer of 20
CPUs. In contrast, contacts predicted by direct coupling analysis (DCA) cannot
fold any of them in the absence of folding simulation and the best CASP12 group
folded 11 of them by integrating predicted contacts into complex,
fragment-based folding simulation. The rigorous experimental validation on 15
CASP13 targets show that among the 3 hardest targets of new fold our
distance-based folding servers successfully folded 2 large ones with <150
sequence homologs while the other servers failed on all three, and that our ab
initio folding server also predicted the best, high-quality 3D model for a
large homology modeling target. Further experimental validation in CAMEO shows
that our ab initio folding server predicted correct fold for a membrane protein
of new fold with 200 residues and 229 sequence homologs while all the other
servers failed. These results imply that deep learning offers an efficient and
accurate solution for ab initio folding on a personal computer.
",Quantitative Biology,Computer Science; Statistics
"Playing a true Parrondo's game with a three state coin on a quantum walk   Playing a Parrondo's game with a qutrit is the subject of this paper. We show
that a true quantum Parrondo's game can be played with a 3 state coin(qutrit)
in a 1D quantum walk in contrast to the fact that playing a true Parrondo's
game with a 2 state coin(qubit) in 1D quantum walk fails in the asymptotic
limits.
",Computer Science; Physics,Mathematics
"Towards Audio to Scene Image Synthesis using Generative Adversarial Network   Humans can imagine a scene from a sound. We want machines to do so by using
conditional generative adversarial networks (GANs). By applying the techniques
including spectral norm, projection discriminator and auxiliary classifier,
compared with naive conditional GAN, the model can generate images with better
quality in terms of both subjective and objective evaluations. Almost
three-fourth of people agree that our model have the ability to generate images
related to sounds. By inputting different volumes of the same sound, our model
output different scales of changes based on the volumes, showing that our model
truly knows the relationship between sounds and images to some extent.
",Computer Science,Computer Science
"Fast, Better Training Trick -- Random Gradient   In this paper, we will show an unprecedented method to accelerate training
and improve performance, which called random gradient (RG). This method can be
easier to the training of any model without extra calculation cost, we use
Image classification, Semantic segmentation, and GANs to confirm this method
can improve speed which is training model in computer vision. The central idea
is using the loss multiplied by a random number to random reduce the
back-propagation gradient. We can use this method to produce a better result in
Pascal VOC, Cifar, Cityscapes datasets.
",Statistics,Computer Science; Statistics
"Logics for Word Transductions with Synthesis   We introduce a logic, called LT, to express properties of transductions, i.e.
binary relations from input to output (finite) words. In LT, the input/output
dependencies are modelled via an origin function which associates to any
position of the output word, the input position from which it originates. LT is
well-suited to express relations (which are not necessarily functional), and
can express all regular functional transductions, i.e. transductions definable
for instance by deterministic two-way transducers. Despite its high expressive
power, LT has decidable satisfiability and equivalence problems, with tight
non-elementary and elementary complexities, depending on specific
representation of LT-formulas. Our main contribution is a synthesis result:
from any transduction R defined in LT , it is possible to synthesise a regular
functional transduction f such that for all input words u in the domain of R, f
is defined and (u,f(u)) belongs to R. As a consequence, we obtain that any
functional transduction is regular iff it is LT-definable. We also investigate
the algorithmic and expressiveness properties of several extensions of LT, and
explicit a correspondence between transductions and data words. As a
side-result, we obtain a new decidable logic for data words.
",Computer Science,Computer Science
"Probabilistic Combination of Noisy Points and Planes for RGB-D Odometry   This work proposes a visual odometry method that combines points and plane
primitives, extracted from a noisy depth camera. Depth measurement uncertainty
is modelled and propagated through the extraction of geometric primitives to
the frame-to-frame motion estimation, where pose is optimized by weighting the
residuals of 3D point and planes matches, according to their uncertainties.
Results on an RGB-D dataset show that the combination of points and planes,
through the proposed method, is able to perform well in poorly textured
environments, where point-based odometry is bound to fail.
",Computer Science,Computer Science
"Active Community Detection: A Maximum Likelihood Approach   We propose novel semi-supervised and active learning algorithms for the
problem of community detection on networks. The algorithms are based on
optimizing the likelihood function of the community assignments given a graph
and an estimate of the statistical model that generated it. The optimization
framework is inspired by prior work on the unsupervised community detection
problem in Stochastic Block Models (SBM) using Semi-Definite Programming (SDP).
In this paper we provide the next steps in the evolution of learning
communities in this context which involves a constrained semi-definite
programming algorithm, and a newly presented active learning algorithm. The
active learner intelligently queries nodes that are expected to maximize the
change in the model likelihood. Experimental results show that this active
learning algorithm outperforms the random-selection semi-supervised version of
the same algorithm as well as other state-of-the-art active learning
algorithms. Our algorithms significantly improved performance is demonstrated
on both real-world and SBM-generated networks even when the SBM has a signal to
noise ratio (SNR) below the known unsupervised detectability threshold.
",Computer Science; Statistics,Computer Science; Statistics
"Meridian Surfaces on Rotational Hypersurfaces with Lightlike Axis in ${\mathbb E}^4_2$   We construct a special class of Lorentz surfaces in the pseudo-Euclidean
4-space with neutral metric which are one-parameter systems of meridians of
rotational hypersurfaces with lightlike axis and call them meridian surfaces.
We give the complete classification of the meridian surfaces with constant
Gauss curvature and prove that there are no meridian surfaces with parallel
mean curvature vector field other than CMC surfaces lying in a hyperplane. We
also classify the meridian surfaces with parallel normalized mean curvature
vector field. We show that in the family of the meridian surfaces there exist
Lorentz surfaces which have parallel normalized mean curvature vector field but
not parallel mean curvature vector.
",Mathematics,Mathematics
"Deep Learning for Real-time Gravitational Wave Detection and Parameter Estimation: Results with Advanced LIGO Data   The recent Nobel-prize-winning detections of gravitational waves from merging
black holes and the subsequent detection of the collision of two neutron stars
in coincidence with electromagnetic observations have inaugurated a new era of
multimessenger astrophysics. To enhance the scope of this emergent field of
science, we pioneered the use of deep learning with convolutional neural
networks, that take time-series inputs, for rapid detection and
characterization of gravitational wave signals. This approach, Deep Filtering,
was initially demonstrated using simulated LIGO noise. In this article, we
present the extension of Deep Filtering using real data from LIGO, for both
detection and parameter estimation of gravitational waves from binary black
hole mergers using continuous data streams from multiple LIGO detectors. We
demonstrate for the first time that machine learning can detect and estimate
the true parameters of real events observed by LIGO. Our results show that Deep
Filtering achieves similar sensitivities and lower errors compared to
matched-filtering while being far more computationally efficient and more
resilient to glitches, allowing real-time processing of weak time-series
signals in non-stationary non-Gaussian noise with minimal resources, and also
enables the detection of new classes of gravitational wave sources that may go
unnoticed with existing detection algorithms. This unified framework for data
analysis is ideally suited to enable coincident detection campaigns of
gravitational waves and their multimessenger counterparts in real-time.
",Computer Science; Physics,Physics
"Miraculous cancellations for quantum $SL_2$   In earlier work, Helen Wong and the author discovered certain ""miraculous
cancellations"" for the quantum trace map connecting the Kauffman bracket skein
algebra of a surface to its quantum Teichmueller space, occurring when the
quantum parameter $q$ is a root of unity. The current paper is devoted to
giving a more representation theoretic interpretation of this phenomenon, in
terms of the quantum group $U_q(sl_2)$ and its dual Hopf algebra $SL_2^q$.
",Mathematics,Mathematics
"Curious Minds Wonder Alike: Studying Multimodal Behavioral Dynamics to Design Social Scaffolding of Curiosity   Curiosity is the strong desire to learn or know more about something or
someone. Since learning is often a social endeavor, social dynamics in
collaborative learning may inevitably influence curiosity. There is a scarcity
of research, however, focusing on how curiosity can be evoked in group learning
contexts. Inspired by a recently proposed theoretical framework that
articulates an integrated socio-cognitive infrastructure of curiosity, in this
work, we use data-driven approaches to identify fine-grained social scaffolding
of curiosity in child-child interaction, and propose how they can be used to
elicit and maintain curiosity in technology-enhanced learning environments. For
example, we discovered sequential patterns of multimodal behaviors across group
members and we describe those that maximize an individual's utility, or
likelihood, of demonstrating curiosity during open-ended problem-solving in
group work. We also discovered, and describe here, behaviors that directly or
in a mediated manner cause curiosity related conversational behaviors in the
interaction, with twice as many interpersonal causal influences compared to
intrapersonal ones. We explain how these findings form a solid foundation for
developing curiosity-increasing learning technologies or even assisting a human
coach to induce curiosity among learners.
",Computer Science,Computer Science; Statistics
"Weak multiplier Hopf algebras III. Integrals and duality   Let $(A,\Delta)$ be a weak multiplier Hopf algebra. It is a pair of a
non-degenerate algebra $A$, with or without identity, and a coproduct $\Delta$
on $A$, satisfying certain properties. The main difference with multiplier Hopf
algebras is that now, the canonical maps $T_1$ and $T_2$ on $A\otimes A$,
defined by $$T_1(a\otimes b)=\Delta(a)(1\otimes b)
\qquad\quad\text{and}\qquad\quad T_2(c\otimes a)=(c\otimes 1)\Delta(a),$$ are
no longer assumed to be bijective. Also recall that a weak multiplier Hopf
algebra is called regular if its antipode is a bijective map from $A$ to
itself.
In this paper, we introduce and study the notion of integrals on such regular
weak multiplier Hopf algebras. A left integral is a non-zero linear functional
on $A$ that is left invariant (in an appropriate sense). Similarly for a right
integral. For a regular weak multiplier Hopf algebra $(A,\Delta)$ with
(sufficiently many) integrals, we construct the dual $(\widehat
A,\widehat\Delta)$. It is again a regular weak multiplier Hopf algebra with
(sufficiently many) integrals. This duality extends the known duality of
finite-dimensional weak Hopf algebras to this more general case. It also
extends the duality of multiplier Hopf algebras with integrals, the so-called
algebraic quantum groups. For this reason, we will sometimes call a regular
weak multiplier Hopf algebra with enough integrals an algebraic quantum
groupoid.
We discuss the relation of our work with the work on duality for algebraic
quantum groupoids by Timmermann.
We also illustrate this duality with a particular example in a separate
paper. In this paper, we only mention the main definitions and results for this
example. However, we do consider the two natural weak multiplier Hopf algebras
associated with a groupoid in detail and show that they are dual to each other
in the sense of the above duality.
",Mathematics,Mathematics
"Comparing the dark matter models, modified Newtonian dynamics and modified gravity in accounting for the galaxy rotation curves   We compare six models (including the baryonic model, two dark matter models,
two modified Newtonian dynamics models and one modified gravity model) in
accounting for the galaxy rotation curves. For the dark matter models, we
assume NFW profile and core-modified profile for the dark halo, respectively.
For the modified Newtonian dynamics models, we discuss Milgrom's MOND theory
with two different interpolation functions, i.e. the standard and the simple
interpolation functions. As for the modified gravity, we focus on Moffat's MSTG
theory. We fit these models to the observed rotation curves of 9 high-surface
brightness and 9 low-surface brightness galaxies. We apply the Bayesian
Information Criterion and the Akaike Information Criterion to test the
goodness-of-fit of each model. It is found that non of the six models can well
fit all the galaxy rotation curves. Two galaxies can be best fitted by the
baryonic model without involving the nonluminous dark matter. MOND can fit the
largest number of galaxies, and only one galaxy can be best fitted by MSTG
model. Core-modified model can well fit about one half LSB galaxies but no HSB
galaxy, while NFW model can fit only a small fraction of HSB galaxies but no
LSB galaxy. This may imply that the oversimplified NFW and Core-modified
profiles couldn't well mimic the postulated dark matter halo.
",Physics,Physics
"Interior transmission eigenvalue problems on compact manifolds with boundary conductivity parameters   In this paper, we consider an interior transmission eigenvalue (ITE) problem
on some compact $C^{\infty }$-Riemannian manifolds with a common smooth
boundary. In particular, these manifolds may have different topologies, but we
impose some conditions of Riemannian metrics, indices of refraction and
boundary conductivity parameters on the boundary. Then we prove the
discreteness of the set of ITEs, the existence of infinitely many ITEs, and its
Weyl type lower bound. For our settings, we can adopt the argument by
Lakshtanov and Vainberg, considering the Dirichlet-to-Neumann map. As an
application, we derive the existence of non-scattering energies for
time-harmonic acoustic equations. For the sake of simplicity, we consider the
scattering theory on the Euclidean space. However, the argument is applicable
for certain kinds of non-compact manifolds with ends on which we can define the
scattering matrix.
",Mathematics,Physics
"Cubic Fields: A Primer   We classify all cubic extensions of any field of arbitrary characteristic, up
to isomorphism, via an explicit construction involving three fundamental types
of cubic forms. We deduce a classification of any Galois cubic extension of a
field. The splitting and ramification of places in a separable cubic extension
of any global function field are completely determined, and precise
Riemann-Hurwitz formulae are given. In doing so, we determine the decomposition
of any cubic polynomial over a finite field.
",Mathematics,Mathematics
"A cross-vendor and cross-state analysis of the GPS-probe data latency   Crowdsourced GPS probe data has become a major source of real-time traffic
information applications. In addition to traditional traveler advisory systems
such as dynamic message signs (DMS) and 511 systems, probe data is being used
for automatic incident detection, Integrated Corridor Management (ICM), end of
queue warning systems, and mobility-related smartphone applications. Several
private sector vendors offer minute by minute network-wide travel time and
speed probe data. The quality of such data in terms of deviation of the
reported travel time and speeds from ground-truth has been extensively studied
in recent years, and as a result concerns over the accuracy of probe data has
mostly faded away. However, the latency of probe data, defined as the lag
between the time that disturbance in traffic speed is reported in the
outsourced data feed, and the time that the traffic is perturbed, has become a
subject of interest. The extent of latency of probe data for real-time
applications is critical, so it is important to have a good understanding of
the amount of latency and its influencing factors. This paper uses high-quality
independent Bluetooth/Wi-Fi re-identification data collected on multiple
freeway segments in three different states, to measure the latency of the
vehicle probe data provided by three major vendors. The statistical
distribution of the latency and its sensitivity to speed slowdown and recovery
periods are discussed.
",Statistics,Computer Science
"Diversification-Based Learning in Computing and Optimization   Diversification-Based Learning (DBL) derives from a collection of principles
and methods introduced in the field of metaheuristics that have broad
applications in computing and optimization. We show that the DBL framework goes
significantly beyond that of the more recent Opposition-based learning (OBL)
framework introduced in Tizhoosh (2005), which has become the focus of numerous
research initiatives in machine learning and metaheuristic optimization. We
unify and extend earlier proposals in metaheuristic search (Glover, 1997,
Glover and Laguna, 1997) to give a collection of approaches that are more
flexible and comprehensive than OBL for creating intensification and
diversification strategies in metaheuristic search. We also describe potential
applications of DBL to various subfields of machine learning and optimization.
",Computer Science,Computer Science; Statistics
"Nonlinear dynamics on branched structures and networks   Nonlinear dynamics on graphs has rapidly become a topical issue with many
physical applications, ranging from nonlinear optics to Bose-Einstein
condensation. Whenever in a physical experiment a ramified structure is
involved, it can prove useful to approximate such a structure by a metric
graph, or network. For the Schroedinger equation it turns out that the sixth
power in the nonlinear term of the energy is critical in the sense that below
that power the constrained energy is lower bounded irrespectively of the value
of the mass (subcritical case). On the other hand, if the nonlinearity power
equals six, then the lower boundedness depends on the value of the mass: below
a critical mass, the constrained energy is lower bounded, beyond it, it is not.
For powers larger than six the constrained energy functional is never lower
bounded, so that it is meaningless to speak about ground states (supercritical
case). These results are the same as in the case of the nonlinear Schrodinger
equation on the real line. In fact, as regards the existence of ground states,
the results for systems on graphs differ, in general, from the ones for systems
on the line even in the subcritical case: in the latter case, whenever the
constrained energy is lower bounded there always exist ground states (the
solitons, whose shape is explicitly known), whereas for graphs the existence of
a ground state is not guaranteed. For the critical case, our results show a
phenomenology much richer than the analogous on the line.
",Mathematics,Physics
"Integrable modules over affine Lie superalgebras sl(1|n)^   We describe the category of integrable sl(1|n)^ -modules with the positive
central charge and show that the irreducible modules provide the full set of
irreducible representations for the corresponding simple vertex algebra.
",Mathematics,Mathematics
"Permutation complexity of images of Sturmian words by marked morphisms   We show that the permutation complexity of the image of a Sturmian word by a
binary marked morphism is $n+k$ for some constant $k$ and all lengths $n$
sufficiently large.
",Computer Science,Mathematics
"A GPU Poisson-Fermi Solver for Ion Channel Simulations   The Poisson-Fermi model is an extension of the classical Poisson-Boltzmann
model to include the steric and correlation effects of ions and water treated
as nonuniform spheres in aqueous solutions. Poisson-Boltzmann electrostatic
calculations are essential but computationally very demanding for molecular
dynamics or continuum simulations of complex systems in molecular biophysics
and electrochemistry. The graphic processing unit (GPU) with enormous
arithmetic capability and streaming memory bandwidth is now a powerful engine
for scientific as well as industrial computing. We propose two parallel GPU
algorithms, one for linear solver and the other for nonlinear solver, for
solving the Poisson-Fermi equation approximated by the standard finite
difference method in 3D to study biological ion channels with crystallized
structures from the Protein Data Bank, for example. Numerical methods for both
linear and nonlinear solvers in the parallel algorithms are given in detail to
illustrate the salient features of the CUDA (compute unified device
architecture) software platform of GPU in implementation. It is shown that the
parallel algorithms on GPU over the sequential algorithms on CPU (central
processing unit) can achieve 22.8x and 16.9x speedups for the linear solver
time and total runtime, respectively.
",Physics,Physics
"Transkernel: An Executor for Commodity Kernels on Peripheral Cores   Modern mobile and embedded platforms see a large number of ephemeral tasks
driven by background activities. In order to execute such a task, the OS kernel
wakes up the platform beforehand and puts it back to sleep afterwards. In doing
so, the kernel operates various IO devices and orchestrates their power state
transitions. Such kernel execution phases are lengthy, having high energy cost,
and yet difficult to optimize. We advocate for relieving the CPU from these
kernel phases by executing them on a low-power, microcontroller-like core,
dubbed peripheral core, hence leaving the CPU off. Yet, for a peripheral core
to execute phases in a complex commodity kernel (e.g. Linux), existing
approaches either incur high engineering effort or high runtime overhead. We
take a radical approach with a new executor model called transkernel. Running
on a peripheral core, a transkernel executes the binary of the commodity kernel
through cross-ISA, dynamic binary translation (DBT). The transkernel translates
stateful kernel code while emulating a small set of stateless kernel services;
it sets a narrow, stable binary interface for emulated services; it specializes
for kernel's beaten paths; it exploits ISA similarities for low DBT cost. With
a concrete implementation on a heterogeneous ARM SoC, we demonstrate the
feasibility and benefit of transkernel. Our result contributes a new OS
structure that combines cross-ISA DBT and emulation for harnessing a
heterogeneous SoC. Our result demonstrates that while cross-ISA DBT is
typically used under the assumption of efficiency loss, it can be used for
efficiency gain, even atop off-the-shelf hardware.
",Computer Science,Computer Science
"Smooth Pinball Neural Network for Probabilistic Forecasting of Wind Power   Uncertainty analysis in the form of probabilistic forecasting can
significantly improve decision making processes in the smart power grid for
better integrating renewable energy sources such as wind. Whereas point
forecasting provides a single expected value, probabilistic forecasts provide
more information in the form of quantiles, prediction intervals, or full
predictive densities. This paper analyzes the effectiveness of a novel approach
for nonparametric probabilistic forecasting of wind power that combines a
smooth approximation of the pinball loss function with a neural network
architecture and a weighting initialization scheme to prevent the quantile
cross over problem. A numerical case study is conducted using publicly
available wind data from the Global Energy Forecasting Competition 2014.
Multiple quantiles are estimated to form 10%, to 90% prediction intervals which
are evaluated using a quantile score and reliability measures. Benchmark models
such as the persistence and climatology distributions, multiple quantile
regression, and support vector quantile regression are used for comparison
where results demonstrate the proposed approach leads to improved performance
while preventing the problem of overlapping quantile estimates.
",Statistics,Computer Science; Statistics
"Tamed to compatible when b^(2+) = 1 and b^1 = 2   Weiyi Zhang noticed recently a gap in the proof of the main theorem of the
authors article ""Tamed to compatible: Symplectic forms via moduli space
integration"" [T] for the case when the symplectic 4-manifold in question has
first Betti number 2 (and necessarily self-dual second Betti number 1). This
note explains how to fill this gap.
",Mathematics,Mathematics
"Critical fields and fluctuations determined from specific heat and magnetoresistance in the same nanogram SmFeAs(O,F) single crystal   Through a direct comparison of specific heat and magneto-resistance we
critically asses the nature of superconducting fluctuations in the same
nano-gram crystal of SmFeAs(O, F). We show that although the superconducting
fluctuation contribution to conductivity scales well within the 2D-LLL scheme
its predictions contrast the inherently 3D nature of SmFeAs(O, F) in the
vicinity T_{c}. Furthermore the transition seen in specific heat cannot be
satisfactory described either by the LLL or the XY scaling. Additionally we
have validated, through comparing Hc2 values obtained from the entropy
conservation construction (Hab=-19.5 T/K and Hab=-2.9 T/K), the analysis of
fluctuation contribution to conductivity as a reasonable method for estimating
the Hc2 slope.
",Physics,Physics
"Weighted Contrastive Divergence   Learning algorithms for energy based Boltzmann architectures that rely on
gradient descent are in general computationally prohibitive, typically due to
the exponential number of terms involved in computing the partition function.
In this way one has to resort to approximation schemes for the evaluation of
the gradient. This is the case of Restricted Boltzmann Machines (RBM) and its
learning algorithm Contrastive Divergence (CD). It is well-known that CD has a
number of shortcomings, and its approximation to the gradient has several
drawbacks. Overcoming these defects has been the basis of much research and new
algorithms have been devised, such as persistent CD. In this manuscript we
propose a new algorithm that we call Weighted CD (WCD), built from small
modifications of the negative phase in standard CD. However small these
modifications may be, experimental work reported in this paper suggest that WCD
provides a significant improvement over standard CD and persistent CD at a
small additional computational cost.
",Statistics,Computer Science; Statistics
"Hydrodynamic signatures of stationary Marangoni-driven surfactant transport   We experimentally study steady Marangoni-driven surfactant transport on the
interface of a deep water layer. Using hydrodynamic measurements, and without
using any knowledge of the surfactant physico-chemical properties, we show that
sodium dodecyl sulphate and Tergitol 15-S-9 introduced in low concentrations
result in a flow driven by adsorbed surfactant. At higher surfactant
concentration, the flow is dominated by the dissolved surfactant. Using
Camphoric acid, whose properties are {\it a priori} unknown, we demonstrate
this method's efficacy by showing its spreading is adsorption dominated.
",Physics,Physics
"Continuum of classical-field ensembles from canonical to grand canonical and the onset of their equivalence   The canonical and grand-canonical ensembles are two usual marginal cases for
ultracold Bose gases, but real collections of experimental runs commonly have
intermediate properties. Here we study the continuum of intermediate cases, and
look into the appearance of ensemble equivalence as interaction rises for
mesoscopic 1d systems. We demonstrate how at sufficient interaction strength
the distributions of condensate and excited atoms become practically identical
regardless of the ensemble used. Importantly, we find that features that are
fragile in the ideal gas and appear only in a strict canonical ensemble can
become robust in all ensembles when interactions become strong. As evidence,
the steep cliff in the distribution of the number of excited atoms is
preserved. To make this study, a straightforward approach for generating
canonical and intermediate classical field ensembles using a modified
stochastic Gross-Pitaevskii equation (SGPE) is developed.
",Physics,Physics
"Joint Syntacto-Discourse Parsing and the Syntacto-Discourse Treebank   Discourse parsing has long been treated as a stand-alone problem independent
from constituency or dependency parsing. Most attempts at this problem are
pipelined rather than end-to-end, sophisticated, and not self-contained: they
assume gold-standard text segmentations (Elementary Discourse Units), and use
external parsers for syntactic features. In this paper we propose the first
end-to-end discourse parser that jointly parses in both syntax and discourse
levels, as well as the first syntacto-discourse treebank by integrating the
Penn Treebank with the RST Treebank. Built upon our recent span-based
constituency parser, this joint syntacto-discourse parser requires no
preprocessing whatsoever (such as segmentation or feature extraction), achieves
the state-of-the-art end-to-end discourse parsing accuracy.
",Computer Science,Computer Science
"Motivic infinite loop spaces   We prove a recognition principle for motivic infinite P1-loop spaces over a
perfect field. This is achieved by developing a theory of framed motivic
spaces, which is a motivic analogue of the theory of E-infinity-spaces. A
framed motivic space is a motivic space equipped with transfers along finite
syntomic morphisms with trivialized cotangent complex in K-theory. Our main
result is that grouplike framed motivic spaces are equivalent to the full
subcategory of motivic spectra generated under colimits by suspension spectra.
As a consequence, we deduce some representability results for suspension
spectra of smooth varieties, and in particular for the motivic sphere spectrum,
in terms of Hilbert schemes of points in affine spaces.
",Mathematics,Mathematics
"Toward Incorporation of Relevant Documents in word2vec   Recent advances in neural word embedding provide significant benefit to
various information retrieval tasks. However as shown by recent studies,
adapting the embedding models for the needs of IR tasks can bring considerable
further improvements. The embedding models in general define the term
relatedness by exploiting the terms' co-occurrences in short-window contexts.
An alternative (and well-studied) approach in IR for related terms to a query
is using local information i.e. a set of top-retrieved documents. In view of
these two methods of term relatedness, in this work, we report our study on
incorporating the local information of the query in the word embeddings. One
main challenge in this direction is that the dense vectors of word embeddings
and their estimation of term-to-term relatedness remain difficult to interpret
and hard to analyze. As an alternative, explicit word representations propose
vectors whose dimensions are easily interpretable, and recent methods show
competitive performance to the dense vectors. We introduce a neural-based
explicit representation, rooted in the conceptual ideas of the word2vec
Skip-Gram model. The method provides interpretable explicit vectors while
keeping the effectiveness of the Skip-Gram model. The evaluation of various
explicit representations on word association collections shows that the newly
proposed method out- performs the state-of-the-art explicit representations
when tasked with ranking highly similar terms. Based on the introduced ex-
plicit representation, we discuss our approaches on integrating local documents
in globally-trained embedding models and discuss the preliminary results.
",Computer Science,Computer Science
"Game-Theoretic Design of Secure and Resilient Distributed Support Vector Machines with Adversaries   With a large number of sensors and control units in networked systems,
distributed support vector machines (DSVMs) play a fundamental role in scalable
and efficient multi-sensor classification and prediction tasks. However, DSVMs
are vulnerable to adversaries who can modify and generate data to deceive the
system to misclassification and misprediction. This work aims to design defense
strategies for DSVM learner against a potential adversary. We establish a
game-theoretic framework to capture the conflicting interests between the DSVM
learner and the attacker. The Nash equilibrium of the game allows predicting
the outcome of learning algorithms in adversarial environments, and enhancing
the resilience of the machine learning through dynamic distributed learning
algorithms. We show that the DSVM learner is less vulnerable when he uses a
balanced network with fewer nodes and higher degree. We also show that adding
more training samples is an efficient defense strategy against an attacker. We
present secure and resilient DSVM algorithms with verification method and
rejection method, and show their resiliency against adversary with numerical
experiments.
",Computer Science; Statistics,Computer Science; Statistics
"A Debris Backwards Flow Simulation System for Malaysia Airlines Flight 370   This paper presents a system based on a Two-Way Particle-Tracking Model to
analyze possible crash positions of flight MH370. The particle simulator
includes a simple flow simulation of the debris based on a Lagrangian approach
and a module to extract appropriated ocean current data from netCDF files. The
influence of wind, waves, immersion depth and hydrodynamic behavior are not
considered in the simulation.
",Computer Science; Physics,Physics
"Discrete configuration spaces of squares and hexagons   We consider generalizations of the familiar fifteen-piece sliding puzzle on
the 4 by 4 square grid. On larger grids with more pieces and more holes,
asymptotically how fast can we move the puzzle into the solved state? We also
give a variation with sliding hexagons. The square puzzles and the hexagon
puzzles are both discrete versions of configuration spaces of disks, which are
of interest in statistical mechanics and topological robotics. The
combinatorial theorems and proofs in this paper suggest followup questions in
both combinatorics and topology, and may turn out to be useful for proving
topological statements about configuration spaces.
",Computer Science; Mathematics,Computer Science; Mathematics
"HARPO: 1.7 - 74 MeV gamma-ray beam validation of a high angular resolutio n, high linear polarisation dilution, gas time projection chamber telescope and polarimeter   A presentation at the SciNeGHE conference of the past achievements, of the
present activities and of the perspectives for the future of the HARPO project,
the development of a time projection chamber as a high-performance gamma-ray
telescope and linear polarimeter in the e+e- pair creation regime.
",Physics,Physics
"On certain families of planar patterns and fractals   This survey article is dedicated to some families of fractals that were
introduced and studied during the last decade, more precisely, families of
Sierpiński carpets: limit net sets, generalised Sierpiński carpets and
labyrinth fractals. We give a unifying approach of these fractals and several
of their topological and geometrical properties, by using the framework of
planar patterns.
",Mathematics,Mathematics
"L1188: a promising candidate of cloud-cloud collision triggering the formation of the low- and intermediate-mass stars   We present a new large-scale (4 square degrees) simultaneous $^{12}$CO,
$^{13}$CO, and C$^{18}$O ($J$=1$-$0) mapping of L1188 with the PMO 13.7-m
telescope. Our observations have revealed that L1188 consists of two nearly
orthogonal filamentary molecular clouds at two clearly separated velocities.
Toward the intersection showing large velocity spreads, we find several
bridging features connecting the two clouds in velocity, and an open arc
structure which exhibits high excitation temperatures, enhanced $^{12}$CO and
$^{13}$CO emission, and broad $^{12}$CO line wings. This agrees with the
scenario that the two clouds are colliding with each other. The distribution of
young stellar object (YSO) candidates implies an enhancement of star formation
in the intersection of the two clouds. We suggest that a cloud-cloud collision
happened in L1188 about 1~Myr ago, possibly triggering the formation of low-
and intermediate-mass YSOs in the intersection.
",Physics,Physics
"Observation of a Lamb band gap in a polymer waveguide with periodic cross-like cavities   The quest for large and low frequency band gaps is one of the principal
objectives pursued in a number of engineering applications, ranging from noise
absorption to vibration control, to seismic wave abatement. For this purpose, a
plethora of complex architectures (including multi-phase materials) and
multi-physics approaches have been proposed in the past, often involving
difficulties in their practical realization.
To address this issue, in this work we propose an easy-to-manufacture design
able to open large, low frequency complete Lamb band gaps exploiting a suitable
arrangement of masses and stiffnesses produced by cavities in a monolithic
material. The performance of the designed structure is evaluated by numerical
simulations and confirmed by Scanning Laser Doppler Vibrometer (SLDV)
measurements on an isotropic polyvinyl chloride plate in which a square ring
region of cross-like cavities is fabricated. The full wave field reconstruction
clearly confirms the ability of even a limited number of unit cell rows of the
proposed design to efficiently attenuate Lamb waves. In addition, numerical
simulations show that the structure allows to shift of the central frequency of
the BG through geometrical modifications. The design may be of interest for
applications in which large BGs at low frequencies are required.
",Physics,Physics
"Model comparison for Gibbs random fields using noisy reversible jump Markov chain Monte Carlo   The reversible jump Markov chain Monte Carlo (RJMCMC) method offers an
across-model simulation approach for Bayesian estimation and model comparison,
by exploring the sampling space that consists of several models of possibly
varying dimensions. A naive implementation of RJMCMC to models like Gibbs
random fields suffers from computational difficulties: the posterior
distribution for each model is termed doubly-intractable since computation of
the likelihood function is rarely available. Consequently, it is simply
impossible to simulate a transition of the Markov chain in the presence of
likelihood intractability. A variant of RJMCMC is presented, called noisy
RJMCMC, where the underlying transition kernel is replaced with an
approximation based on unbiased estimators. Based on previous theoretical
developments, convergence guarantees for the noisy RJMCMC algorithm are
provided. The experiments show that the noisy RJMCMC algorithm can be much more
efficient than other exact methods, provided that an estimator with controlled
Monte Carlo variance is used, a fact which is in agreement with the theoretical
analysis.
",Statistics,Statistics
"Artificial Intelligence Assisted Power Grid Hardening in Response to Extreme Weather Events   In this paper, an artificial intelligence based grid hardening model is
proposed with the objective of improving power grid resilience in response to
extreme weather events. At first, a machine learning model is proposed to
predict the component states (either operational or outage) in response to the
extreme event. Then, these predictions are fed into a hardening model, which
determines strategic locations for placement of distributed generation (DG)
units. In contrast to existing literature in hardening and resilience
enhancement, this paper co-optimizes grid economic and resilience objectives by
considering the intricate dependencies of the two. The numerical simulations on
the standard IEEE 118-bus test system illustrate the merits and applicability
of the proposed hardening model. The results indicate that the proposed
hardening model through decentralized and distributed local energy resources
can produce a more robust solution that can protect the system significantly
against multiple component outages due to an extreme event.
",Computer Science,Computer Science
"Differentiable Compositional Kernel Learning for Gaussian Processes   The generalization properties of Gaussian processes depend heavily on the
choice of kernel, and this choice remains a dark art. We present the Neural
Kernel Network (NKN), a flexible family of kernels represented by a neural
network. The NKN architecture is based on the composition rules for kernels, so
that each unit of the network corresponds to a valid kernel. It can compactly
approximate compositional kernel structures such as those used by the Automatic
Statistician (Lloyd et al., 2014), but because the architecture is
differentiable, it is end-to-end trainable with gradient-based optimization. We
show that the NKN is universal for the class of stationary kernels. Empirically
we demonstrate pattern discovery and extrapolation abilities of NKN on several
tasks that depend crucially on identifying the underlying structure, including
time series and texture extrapolation, as well as Bayesian optimization.
",Statistics,Computer Science; Statistics
"Ultra-light and strong: the massless harmonic oscillator and its singular path integral   In classical mechanics, a light particle bound by a strong elastic force just
oscillates at high frequency in the region allowed by its initial position and
velocity. In quantum mechanics, instead, the ground state of the particle
becomes completely de-localized in the limit $m \to 0$. The harmonic oscillator
thus ceases to be a useful microscopic physical model in the limit $m \to 0$,
but its Feynman path integral has interesting singularities which make it a
prototype of other systems exhibiting a ""quantum runaway"" from the classical
configurations near the minimum of the action. The probability density of the
coherent runaway modes can be obtained as the solution of a Fokker-Planck
equation associated to the condition $S=S_{min}$. This technique can be applied
also to other systems, notably to a dimensional reduction of the
Einstein-Hilbert action.
",Physics,Physics
"Learning Distributions of Meant Color   When a speaker says the name of a color, the color that they picture is not
necessarily the same as the listener imagines. Color is a grounded semantic
task, but that grounding is not a mapping of a single word (or phrase) to a
single point in color-space. Proper understanding of color language requires
the capacity to map a sequence of words to a probability distribution in
color-space. A distribution is required as there is no clear agreement between
people as to what a particular color describes -- different people have a
different idea of what it means to be `very dark orange'. We propose a novel
GRU-based model to handle this case. Learning how each word in a color name
contributes to the color described, allows for knowledge sharing between uses
of the words in different color names. This knowledge sharing significantly
improves predicative capacity for color names with sparse training data. The
extreme case of this challenge in data sparsity is for color names without any
direct training data. Our model is able to predict reasonable distributions for
these cases, as evaluated on a held-out dataset consisting only of such terms.
",Computer Science,Computer Science
"A Search for Laser Emission with Megawatt Thresholds from 5600 FGKM Stars   We searched high resolution spectra of 5600 nearby stars for emission lines
that are both inconsistent with a natural origin and unresolved spatially, as
would be expected from extraterrestrial optical lasers. The spectra were
obtained with the Keck 10-meter telescope, including light coming from within
0.5 arcsec of the star, corresponding typically to within a few to tens of au
of the star, and covering nearly the entire visible wavelength range from 3640
to 7890 angstroms. We establish detection thresholds by injecting synthetic
laser emission lines into our spectra and blindly analyzing them for
detections. We compute flux density detection thresholds for all wavelengths
and spectral types sampled. Our detection thresholds for the power of the
lasers themselves range from 3 kW to 13 MW, independent of distance to the star
but dependent on the competing ""glare"" of the spectral energy distribution of
the star and on the wavelength of the laser light, launched from a benchmark,
diffraction-limited 10-meter class telescope. We found no such laser emission
coming from the planetary region around any of the 5600 stars. As they contain
roughly 2000 lukewarm, Earth-size planets, we rule out models of the Milky Way
in which over 0.1 percent of warm, Earth-size planets harbor technological
civilizations that, intentionally or not, are beaming optical lasers toward us.
A next generation spectroscopic laser search will be done by the Breakthrough
Listen initiative, targeting more stars, especially stellar types overlooked
here including spectral types O, B, A, early F, late M, and brown dwarfs, and
astrophysical exotica.
",Physics,Physics
"Reminiscences of Julian Schwinger: Late Harvard, Early UCLA Years (1968-1981)   These are reminiscences of my interactions with Julian Schwinger from 1968
through 1981 and beyond.
",Physics,Computer Science
"Predictability of escape for a stochastic saddle-node bifurcation: when rare events are typical   Transitions between multiple stable states of nonlinear systems are
ubiquitous in physics, chemistry, and beyond. Two types of behaviors are
usually seen as mutually exclusive: unpredictable noise-induced transitions and
predictable bifurcations of the underlying vector field. Here, we report a new
situation, corresponding to a fluctuating system approaching a bifurcation,
where both effects collaborate. We show that the problem can be reduced to a
single control parameter governing the competition between deterministic and
stochastic effects. Two asymptotic regimes are identified: when the control
parameter is small (e.g. small noise), deviations from the deterministic case
are well described by the Freidlin-Wentzell theory. In particular, escapes over
the potential barrier are very rare events. When the parameter is large (e.g.
large noise), such events become typical. Unlike pure noise-induced
transitions, the distribution of the escape time is peaked around a value which
is asymptotically predicted by an adiabatic approximation. We show that the two
regimes are characterized by qualitatively different reacting trajectories,
with algebraic and exponential divergence, respectively.
",Physics,Physics
"Improvement in the UAV position estimation with low-cost GPS, INS and vision-based system: Application to a quadrotor UAV   In this paper, we develop a position estimation system for Unmanned Aerial
Vehicles formed by hardware and software. It is based on low-cost devices: GPS,
commercial autopilot sensors and dense optical flow algorithm implemented in an
onboard microcomputer. Comparative tests were conducted using our approach and
the conventional one, where only fusion of GPS and inertial sensors are used.
Experiments were conducted using a quadrotor in two flying modes: hovering and
trajectory tracking in outdoor environments. Results demonstrate the
effectiveness of the proposed approach in comparison with the conventional
approaches presented in the vast majority of commercial drones.
",Computer Science,Computer Science
"Invariance of Ideal Limit Points   Let $\mathcal{I}$ be an analytic P-ideal [respectively, a summable ideal] on
the positive integers and let $(x_n)$ be a sequence taking values in a metric
space $X$. First, it is shown that the set of ideal limit points of $(x_n)$ is
an $F_\sigma$-set [resp., a closet set]. Let us assume that $X$ is also
separable and the ideal $\mathcal{I}$ satisfies certain additional assumptions,
which however includes several well-known examples, e.g., the collection of
sets with zero asymptotic density, sets with zero logarithmic density, and some
summable ideals. Then, it is shown that the set of ideal limit points of
$(x_n)$ is equal to the set of ideal limit points of almost all its
subsequences.
",Mathematics,Mathematics
"Multiple Source Domain Adaptation with Adversarial Training of Neural Networks   While domain adaptation has been actively researched in recent years, most
theoretical results and algorithms focus on the single-source-single-target
adaptation setting. Naive application of such algorithms on multiple source
domain adaptation problem may lead to suboptimal solutions. As a step toward
bridging the gap, we propose a new generalization bound for domain adaptation
when there are multiple source domains with labeled instances and one target
domain with unlabeled instances. Compared with existing bounds, the new bound
does not require expert knowledge about the target distribution, nor the
optimal combination rule for multisource domains. Interestingly, our theory
also leads to an efficient learning strategy using adversarial neural networks:
we show how to interpret it as learning feature representations that are
invariant to the multiple domain shifts while still being discriminative for
the learning task. To this end, we propose two models, both of which we call
multisource domain adversarial networks (MDANs): the first model optimizes
directly our bound, while the second model is a smoothed approximation of the
first one, leading to a more data-efficient and task-adaptive model. The
optimization tasks of both models are minimax saddle point problems that can be
optimized by adversarial training. To demonstrate the effectiveness of MDANs,
we conduct extensive experiments showing superior adaptation performance on
three real-world datasets: sentiment analysis, digit classification, and
vehicle counting.
",Computer Science; Statistics,Computer Science; Statistics
"On a Possible Giant Impact Origin for the Colorado Plateau   It is proposed and substantiated that an extraterrestrial object of the
approximate size and mass of Planet Mars, impacting the Earth in an oblique
angle along an approximately NE-SW route (with respect to the current
orientation of the North America continent) around 750 million years ago (750
Ma), is likely to be the direct cause of a chain of events which led to the
rifting of the Rodinia supercontinent and the severing of the foundation of the
Colorado Plateau from its surrounding craton.
It is further argued that the impactor most likely originated as a rouge
exoplanet produced during one of the past crossings of our Solar System through
the Galactic spiral arms in its orbital motion around the center of the Milky
Way Galaxy. Recent work has shown that the sites of galactic spiral arms are
locations of density-wave collisionless shocks. The perturbations from such
shock are known lead to the formation of massive stars, which evolve quickly
and die as supernovae. The blastwaves from supernova explosions, in addition to
the collisionless shocks at the spiral arms, can perturb the orbits of the
streaming disk matter, occasionally producing rogue exoplanets that can reach
the inner confines of our Solar System. The similarity between the period of
spiral-arm crossings of our Solar System to the period of major extinction
events in the Phanerozoic Eon of the Earth's history, as well as to the period
of the supercontinent cycle (the so-called Wilson Cycle), indicates that the
global environment of the Milky Way Galaxy may have played a major role in
initiating Earth's past tectonic activities.
",Physics,Physics
"Highly accurate model for prediction of lung nodule malignancy with CT scans   Computed tomography (CT) examinations are commonly used to predict lung
nodule malignancy in patients, which are shown to improve noninvasive early
diagnosis of lung cancer. It remains challenging for computational approaches
to achieve performance comparable to experienced radiologists. Here we present
NoduleX, a systematic approach to predict lung nodule malignancy from CT data,
based on deep learning convolutional neural networks (CNN). For training and
validation, we analyze >1000 lung nodules in images from the LIDC/IDRI cohort.
All nodules were identified and classified by four experienced thoracic
radiologists who participated in the LIDC project. NoduleX achieves high
accuracy for nodule malignancy classification, with an AUC of ~0.99. This is
commensurate with the analysis of the dataset by experienced radiologists. Our
approach, NoduleX, provides an effective framework for highly accurate nodule
malignancy prediction with the model trained on a large patient population. Our
results are replicable with software available at
this http URL.
",Statistics; Quantitative Biology,Statistics
"How to place an obstacle having a dihedral symmetry centered at a given point inside a disk so as to optimize the fundamental Dirichlet eigenvalue   A generic model for the shape optimization problems we consider in this paper
is the optimization of the Dirichlet eigenvalues of the Laplace operator with a
volume constraint. We deal with an obstacle placement problem which can be
formulated as the following eigenvalue optimization problem: Fix two positive
real numbers $r_1$ and $A$. We consider a disk $B\subset \mathbb{R}^2$ having
radius $r_1$. We want to place an obstacle $P$ of area $A$ within $B$ so as to
maximize or minimize the fundamental Dirichlet eigenvalue $\lambda_1$ for the
Laplacian on $B\setminus P$. That is, we want to study the behavior of the
function $\rho \mapsto \lambda_1(B\setminus\rho(P))$, where $\rho$ runs over
the set of all rigid motions of the plane fixing the center of mass for $P$
such that $\rho(P)\subset B$. In this paper, we consider a non-concentric
obstacle placement problem. The extremal configurations correspond to the cases
where an axis of symmetry of $P$ coincide with an axis of symmetry of $B$. We
also characterize the maximizing and the minimizing configurations in our main
result, viz., Theorem 4.1. Equation (6), Propositions 5.1 and 5.2 imply Theorem
4.1. We give many different generalizations of our result. At the end, we
provide some numerical evidence to validate our main theorem for the case where
the obstacle $P$ has $\mathbb{D}_4$ symmetry. For the $n$ odd case, we identify
some of the extremal configuration for $\lambda_1$. We prove that equation (6)
and Proposition 5.1 hold true for $n$ odd too. We highlight some of the
difficulties faced in proving Proposition 5.2 for this case. We provide
numerical evidence for $n=5$ and conjecture that Theorem 4.1 holds true for $n$
odd too.
",Mathematics,Mathematics
"Catalyzed bimolecular reactions in responsive nanoreactors   We describe a general theory for surface-catalyzed bimolecular reactions in
responsive nanoreactors, catalytically active nanoparticles coated by a
stimuli-responsive 'gating' shell, whose permeability controls the activity of
the process. We address two archetypal scenarios encountered in this system:
The first, where two species diffusing from a bulk solution react at the
catalyst's surface; the second where only one of the reactants diffuses from
the bulk while the other one is produced at the nanoparticle surface, e.g., by
light conversion. We find that in both scenarios the total catalytic rate has
the same mathematical structure, once diffusion rates are properly redefined.
Moreover, the diffusional fluxes of the different reactants are strongly
coupled, providing a richer behavior than that arising in unimolecular
reactions. We also show that in stark contrast to bulk reactions, the
identification of a limiting reactant is not simply determined by the relative
bulk concentrations but controlled by the nanoreactor shell permeability.
Finally, we describe an application of our theory by analyzing experimental
data on the reaction between hexacyanoferrate (III) and borohydride ions in
responsive hydrogel-based core-shell nanoreactors.
",Physics,Physics
"TADPOLE Challenge: Prediction of Longitudinal Evolution in Alzheimer's Disease   The Alzheimer's Disease Prediction Of Longitudinal Evolution (TADPOLE)
Challenge compares the performance of algorithms at predicting future evolution
of individuals at risk of Alzheimer's disease. TADPOLE Challenge participants
train their models and algorithms on historical data from the Alzheimer's
Disease Neuroimaging Initiative (ADNI) study or any other datasets to which
they have access. Participants are then required to make monthly forecasts over
a period of 5 years from January 2018, of three key outcomes for ADNI-3
rollover participants: clinical diagnosis, Alzheimer's Disease Assessment Scale
Cognitive Subdomain (ADAS-Cog13), and total volume of the ventricles. These
individual forecasts are later compared with the corresponding future
measurements in ADNI-3 (obtained after the TADPOLE submission deadline). The
first submission phase of TADPOLE was open for prize-eligible submissions
between 15 June and 15 November 2017. The submission system remains open via
the website: this https URL, although since 15 November
2017 submissions are not eligible for the first round of prizes. This paper
describes the design of the TADPOLE Challenge.
",Statistics; Quantitative Biology,Computer Science; Statistics
"Modeling Temporally Evolving and Spatially Globally Dependent Data   The last decades have seen an unprecedented increase in the availability of
data sets that are inherently global and temporally evolving, from remotely
sensed networks to climate model ensembles. This paper provides a view of
statistical modeling techniques for space-time processes, where space is the
sphere representing our planet. In particular, we make a distintion between (a)
second order-based, and (b) practical approaches to model temporally evolving
global processes. The former are based on the specification of a class of
space-time covariance functions, with space being the two-dimensional sphere.
The latter are based on explicit description of the dynamics of the space-time
process, i.e., by specifying its evolution as a function of its past history
with added spatially dependent noise.
We especially focus on approach (a), where the literature has been sparse. We
provide new models of space-time covariance functions for random fields defined
on spheres cross time. Practical approaches, (b), are also discussed, with
special emphasis on models built directly on the sphere, without projecting the
spherical coordinate on the plane.
We present a case study focused on the analysis of air pollution from the
2015 wildfires in Equatorial Asia, an event which was classified as the year's
worst environmental disaster. The paper finishes with a list of the main
theoretical and applied research problems in the area, where we expect the
statistical community to engage over the next decade.
",Mathematics; Statistics,Computer Science; Physics
"Trace Properties from Separation Logic Specifications   We propose a formal approach for relating abstract separation logic library
specifications with the trace properties they enforce on interactions between a
client and a library. Separation logic with abstract predicates enforces a
resource discipline that constrains when and how calls may be made between a
client and a library. Intuitively, this can enforce a protocol on the
interaction trace. This intuition is broadly used in the separation logic
community but has not previously been formalised. We provide just such a
formalisation. Our approach is based on using wrappers which instrument library
code to induce execution traces for the properties under examination. By
considering a separation logic extended with trace resources, we prove that
when a library satisfies its separation logic specification then its wrapped
version satisfies the same specification and, moreover, maintains the trace
properties as an invariant. Consequently, any client and library implementation
that are correct with respect to the separation logic specification will
satisfy the trace properties.
",Computer Science,Computer Science
"Scalable methods for Bayesian selective inference   Modeled along the truncated approach in Panigrahi (2016), selection-adjusted
inference in a Bayesian regime is based on a selective posterior. Such a
posterior is determined together by a generative model imposed on data and the
selection event that enforces a truncation on the assumed law. The effective
difference between the selective posterior and the usual Bayesian framework is
reflected in the use of a truncated likelihood. The normalizer of the truncated
law in the adjusted framework is the probability of the selection event; this
is typically intractable and it leads to the computational bottleneck in
sampling from such a posterior. The current work lays out a primal-dual
approach of solving an approximating optimization problem to provide valid
post-selective Bayesian inference. The selection procedures are posed as
data-queries that solve a randomized version of a convex learning program which
have the advantage of preserving more left-over information for inference. We
propose a randomization scheme under which the optimization has separable
constraints that result in a partially separable objective in lower dimensions
for many commonly used selective queries to approximate the otherwise
intractable selective posterior. We show that the approximating optimization
under a Gaussian randomization gives a valid exponential rate of decay for the
selection probability on a large deviation scale. We offer a primal-dual method
to solve the optimization problem leading to an approximate posterior; this
allows us to exploit the usual merits of a Bayesian machinery in both low and
high dimensional regimes where the underlying signal is effectively sparse. We
show that the adjusted estimates empirically demonstrate better frequentist
properties in comparison to the unadjusted estimates based on the usual
posterior, when applied to a wide range of constrained, convex data queries.
",Statistics,Statistics
"ROPPERI - A TPC readout with GEMs, pads and Timepix   The concept of a hybrid readout of a time projection chamber is presented. It
combines a GEM-based amplification and a pad-based anode plane with a pixel
chip as readout electronics. This way, a high granularity enabling to identify
electron clusters from the primary ionisation is achieved as well as
flexibility and large anode coverage. The benefits of this high granularity, in
particular for dE/dx measurements, are outlined and the current software and
hardware development status towards a proof-of-principle is given.
",Physics,Physics
"Predicate Specialization for Definitional Higher-order Logic Programs   Higher-order logic programming is an interesting extension of traditional
logic programming that allows predicates to appear as arguments and variables
to be used where predicates typically occur. Higher-order characteristics are
indeed desirable but on the other hand they are also usually more expensive to
support. In this paper we propose a program specialization technique based on
partial evaluation that can be applied to a modest but useful class of
higher-order logic programs and can transform them into first-order programs
without introducing additional data structures. The resulting first-order
programs can be executed by conventional logic programming interpreters and
benefit from other optimizations that might be available. We provide an
implementation and experimental results that suggest the efficiency of the
transformation.
",Computer Science,Computer Science
"Robust Bayesian Model Selection for Variable Clustering with the Gaussian Graphical Model   Variable clustering is important for explanatory analysis. However, only few
dedicated methods for variable clustering with the Gaussian graphical model
have been proposed. Even more severe, small insignificant partial correlations
due to noise can dramatically change the clustering result when evaluating for
example with the Bayesian Information Criteria (BIC). In this work, we try to
address this issue by proposing a Bayesian model that accounts for negligible
small, but not necessarily zero, partial correlations. Based on our model, we
propose to evaluate a variable clustering result using the marginal likelihood.
To address the intractable calculation of the marginal likelihood, we propose
two solutions: one based on a variational approximation, and another based on
MCMC. Experiments on simulated data shows that the proposed method is similarly
accurate as BIC in the no noise setting, but considerably more accurate when
there are noisy partial correlations. Furthermore, on real data the proposed
method provides clustering results that are intuitively sensible, which is not
always the case when using BIC or its extensions.
",Statistics,Statistics
"Comparing Graph Clusterings: Set partition measures vs. Graph-aware measures   In this paper, we propose a family of graph partition similarity measures
that take the topology of the graph into account. These graph-aware measures
are alternatives to using set partition similarity measures that are not
specifically designed for graph partitions. The two types of measures,
graph-aware and set partition measures, are shown to have opposite behaviors
with respect to resolution issues and provide complementary information
necessary to assess that two graph partitions are similar.
",Statistics,Computer Science
"Towards a Science of Mind   The ancient mind/body problem continues to be one of deepest mysteries of
science and of the human spirit. Despite major advances in many fields, there
is still no plausible link between subjective experience (qualia) and its
realization in the body. This paper outlines some of the elements of a rigorous
science of mind (SoM) - key ideas include scientific realism of mind, agnostic
mysterianism, careful attention to language, and a focus on concrete
(touchstone) questions and results.
",Computer Science; Quantitative Biology,Quantitative Biology
"Raman LIDARs and atmospheric calibration for the Cherenkov Telescope Array   The Cherenkov Telescope Array (CTA) is the next generation of Imaging
Atmospheric Cherenkov Telescopes. It will reach a sensitivity and energy
resolution never obtained until now by any other high energy gamma-ray
experiment. Understanding the systematic uncertainties in general will be a
crucial issue for the performance of CTA. It is well known that atmospheric
conditions contribute particularly in this aspect.Within the CTA consortium
several groups are currently building Raman LIDARs to be installed on the two
sites. Raman LIDARs are devices composed of a powerful laser that shoots into
the atmosphere, a collector that gathers the backscattered light from molecules
and aerosols, a photo-sensor, an optical module that spectrally selects
wavelengths of interest, and a read--out system.Unlike currently used elastic
LIDARs, they can help reduce the systematic uncertainties of the molecular and
aerosol components of the atmosphere to <5% so that CTA can achieve its energy
resolution requirements of<10% uncertainty at 1 TeV.All the Raman LIDARs in
this work have design features that make them different than typical Raman
LIDARs used in atmospheric science and are characterized by large collecting
mirrors (2.5m2) and reduced acquisition time.They provide both multiple elastic
and Raman read-out channels and custom made optics design.In this paper, the
motivation for Raman LIDARs, the design and the status of advance of these
technologies are described.
",Physics,Physics
"Ground state properties of 3d metals from self-consistent GW approach   Self consistent GW approach (scGW) has been applied to calculate the ground
state properties (equilibrium Wigner-Seitz radius $S_{WZ}$ and bulk modulus
$B$) of 3d transition metals Sc, Ti, V, Fe, Co, Ni, and Cu. The approach
systematically underestimates $S_{WZ}$ with average relative deviation from the
experimental data about 1% and it overestimates the calculated bulk modulus
with relative error about 25%. It is shown that scGW is superior in accuracy as
compared to the local density approximation (LDA) but it is less accurate than
the generalized gradient approach (GGA) for the materials studied. If compared
to the random phase approximation (RPA), scGW is slightly less accurate, but
its error for the 3d metals looks more systematic. The systematic nature of the
deviation from the experimental data suggests that the next order of the
perturbation theory should allow one to reduce the error.
",Physics,Physics
"Magnetic Flux Tailoring through Lenz Lenses in Toroidal Diamond Indenter Cells: A New Pathway to High Pressure Nuclear Magnetic Resonance   A new pathway to nuclear magnetic resonance spectroscopy in high pressure
diamond anvil cells is introduced, using inductively coupled broadband passive
electro-magnetic lenses to locally amplify the magnetic flux at the isolated
sample, leading to an increase in sensitivity. The lenses are adopted for the
geometrical restrictions imposed by a toroidal diamond indenter cell, and yield
high signal-to-noise ratios at pressures as high as 72 GPa, at initial sample
volumes of only 230 pl. The corresponding levels of detection, LODt, are found
to be up to four orders of magnitude lower compared to formerly used solenoidal
micro-coils in diamond anvil cells, as shown by Proton-NMR measurements on
paraffin oil. This approach opens up the field of ultra-high pressure sciences
for one of the most versatile spectroscopic methods available in a pressure
range unprecedended up to now.
",Physics,Physics
"Stability and performance analysis of linear positive systems with delays using input-output methods   It is known that input-output approaches based on scaled small-gain theorems
with constant $D$-scalings and integral linear constraints are non-conservative
for the analysis of some classes of linear positive systems interconnected with
uncertain linear operators. This dramatically contrasts with the case of
general linear systems with delays where input-output approaches provide, in
general, sufficient conditions only. Using these results we provide simple
alternative proofs for many of the existing results on the stability of linear
positive systems with discrete/distributed/neutral time-invariant/-varying
delays and linear difference equations. In particular, we give a simple proof
for the characterization of diagonal Riccati stability for systems with
discrete-delays and generalize this equation to other types of delay systems.
The fact that all those results can be reproved in a very simple way
demonstrates the importance and the efficiency of the input-output framework
for the analysis of linear positive systems. The approach is also used to
derive performance results evaluated in terms of the $L_1$-, $L_2$- and
$L_\infty$-gains. It is also flexible enough to be used for design purposes.
",Computer Science; Mathematics,Mathematics
"Robot human interface for housekepeer with wireless capabilities   This paper presents the design and implementation of a Human Interface for a
housekeeper robot. It bases on the idea of making the robot understand the
human needs without making the human go through the details of robots work, for
example, the way that the robot implements the work or the method that the
robot uses to plan the path in order to reach the work area. The interface
commands based on idioms of the natural human language and designed in a manner
that the user gives the robot several commands with their execution date/time.
",Computer Science,Computer Science
"(non)-automaticity of completely multiplicative sequences having negligible many non-trivial prime factors   In this article we consider the completely multiplicative sequences $(a_n)_{n
\in \mathbf{N}}$ defined on a field $\mathbf{K}$ and satisfying $$\sum_{p| p
\leq n, a_p \neq 1, p \in \mathbf{P}}\frac{1}{p}<\infty,$$ where $\mathbf{P}$
is the set of prime numbers. We prove that if such sequences are automatic then
they cannot have infinitely many prime numbers $p$ such that $a_{p}\neq 1$.
Using this fact, we prove that if a completely multiplicative sequence
$(a_n)_{n \in \mathbf{N}}$, vanishing or not, can be written in the form
$a_n=b_n\chi_n$ such that $(b_n)_{n \in \mathbf{N}}$ is a non ultimately
periodic, completely multiplicative automatic sequence satisfying the above
condition, and $(\chi_n)_{n \in \mathbf{N}}$ is a Dirichlet character or a
constant sequence, then there exists only one prime number $p$ such that $b_p
\neq 1$ or $0$.
",Mathematics,Mathematics
"On the Semantics and Complexity of Probabilistic Logic Programs   We examine the meaning and the complexity of probabilistic logic programs
that consist of a set of rules and a set of independent probabilistic facts
(that is, programs based on Sato's distribution semantics). We focus on two
semantics, respectively based on stable and on well-founded models. We show
that the semantics based on stable models (referred to as the ""credal
semantics"") produces sets of probability models that dominate infinitely
monotone Choquet capacities, we describe several useful consequences of this
result. We then examine the complexity of inference with probabilistic logic
programs. We distinguish between the complexity of inference when a
probabilistic program and a query are given (the inferential complexity), and
the complexity of inference when the probabilistic program is fixed and the
query is given (the query complexity, akin to data complexity as used in
database theory). We obtain results on the inferential and query complexity for
acyclic, stratified, and cyclic propositional and relational programs,
complexity reaches various levels of the counting hierarchy and even
exponential levels.
",Computer Science,Computer Science; Statistics
"Cross-validation in high-dimensional spaces: a lifeline for least-squares models and multi-class LDA   Least-squares models such as linear regression and Linear Discriminant
Analysis (LDA) are amongst the most popular statistical learning techniques.
However, since their computation time increases cubically with the number of
features, they are inefficient in high-dimensional neuroimaging datasets.
Fortunately, for k-fold cross-validation, an analytical approach has been
developed that yields the exact cross-validated predictions in least-squares
models without explicitly training the model. Its computation time grows with
the number of test samples. Here, this approach is systematically investigated
in the context of cross-validation and permutation testing. LDA is used
exemplarily but results hold for all other least-squares methods. Furthermore,
a non-trivial extension to multi-class LDA is formally derived. The analytical
approach is evaluated using complexity calculations, simulations, and
permutation testing of an EEG/MEG dataset. Depending on the ratio between
features and samples, the analytical approach is up to 10,000x faster than the
standard approach (retraining the model on each training set). This allows for
a fast cross-validation of least-squares models and multi-class LDA in
high-dimensional data, with obvious applications in multi-dimensional datasets,
Representational Similarity Analysis, and permutation testing.
",Statistics,Statistics
"Spatial Risk Measure for Max-Stable and Max-Mixture Processes   In this paper, we consider isotropic and stationary max-stable, inverse
max-stable and max-mixture processes $X=(X(s))\_{s\in\bR^2}$ and the damage
function $\cD\_X^{\nu}= |X|^\nu$ with $0<\nu<1/2$. We study the quantitative
behavior of a risk measure which is the variance of the average of
$\cD\_X^{\nu}$ over a region $\mathcal{A}\subset \bR^2$.} This kind of risk
measure has already been introduced and studied for \vero{some} max-stable
processes in \cite{koch2015spatial}. %\textcolor{red}{In this study, we
generalised this risk measure to be applicable for several models: asymptotic
dependence represented by max-stable, asymptotic independence represented by
inverse max-stable and mixing between of them.} We evaluated the proposed risk
measure by a simulation study.
",Mathematics; Statistics,Mathematics
"Applications of Fractional Calculus to Newtonian Mechanics   We investigate some basic applications of Fractional Calculus (FC) to
Newtonian mechanics. After a brief review of FC, we consider a possible
generalization of Newton's second law of motion and apply it to the case of a
body subject to a constant force. In our second application of FC to Newtonian
gravity, we consider a generalized fractional gravitational potential and
derive the related circular orbital velocities. This analysis might be used as
a tool to model galactic rotation curves, in view of the dark matter problem.
Both applications have a pedagogical value in connecting fractional calculus to
standard mechanics and can be used as a starting point for a more advanced
treatment of fractional mechanics.
",Physics,Physics
"prDeep: Robust Phase Retrieval with a Flexible Deep Network   Phase retrieval algorithms have become an important component in many modern
computational imaging systems. For instance, in the context of ptychography and
speckle correlation imaging, they enable imaging past the diffraction limit and
through scattering media, respectively. Unfortunately, traditional phase
retrieval algorithms struggle in the presence of noise. Progress has been made
recently on more robust algorithms using signal priors, but at the expense of
limiting the range of supported measurement models (e.g., to Gaussian or coded
diffraction patterns). In this work we leverage the regularization-by-denoising
framework and a convolutional neural network denoiser to create prDeep, a new
phase retrieval algorithm that is both robust and broadly applicable. We test
and validate prDeep in simulation to demonstrate that it is robust to noise and
can handle a variety of system models.
A MatConvNet implementation of prDeep is available at
this https URL.
",Statistics,Computer Science; Statistics
"A general family of congruences for Bernoulli numbers   We prove a general family of congruences for Bernoulli numbers whose index is
a polynomial function of a prime, modulo a power of that prime. Our family
generalizes many known results, including the von Staudt--Clausen theorem and
Kummer's congruence.
",Mathematics,Mathematics
"Design discussion on the ISDA Common Domain Model   A new initiative from the International Swaps and Derivatives Association
(ISDA) aims to establish a ""Common Domain Model"" (ISDA CDM): a new standard for
data and process representation across the full range of derivatives
instruments. Design of the ISDA CDM is at an early stage and the draft
definition contains considerable complexity. This paper contributes by offering
insight, analysis and discussion relating to key topics in the design space
such as data lineage, timestamps, consistency, operations, events, state and
state transitions.
",Computer Science,Computer Science
"Topology data analysis of critical transitions in financial networks   We develop a topology data analysis-based method to detect early signs for
critical transitions in financial data. From the time-series of multiple stock
prices, we build time-dependent correlation networks, which exhibit topological
structures. We compute the persistent homology associated to these structures
in order to track the changes in topology when approaching a critical
transition. As a case study, we investigate a portfolio of stocks during a
period prior to the US financial crisis of 2007-2008, and show the presence of
early signs of the critical transition.
",Physics; Mathematics,Quantitative Finance
"From which world is your graph?   Discovering statistical structure from links is a fundamental problem in the
analysis of social networks. Choosing a misspecified model, or equivalently, an
incorrect inference algorithm will result in an invalid analysis or even
falsely uncover patterns that are in fact artifacts of the model. This work
focuses on unifying two of the most widely used link-formation models: the
stochastic blockmodel (SBM) and the small world (or latent space) model (SWM).
Integrating techniques from kernel learning, spectral graph theory, and
nonlinear dimensionality reduction, we develop the first statistically sound
polynomial-time algorithm to discover latent patterns in sparse graphs for both
models. When the network comes from an SBM, the algorithm outputs a block
structure. When it is from an SWM, the algorithm outputs estimates of each
node's latent position.
",Computer Science; Statistics,Computer Science; Statistics
"Sources of inter-model scatter in TRACMIP, the Tropical Rain belts with an Annual cycle and a Continent Model Intercomparison Project   We analyze the source of inter-model scatter in the surface temperature
response to quadrupling CO2 in two sets of GCM simulations from the Tropical
Rain Belts with an Annual cycle and a Continent Model Intercomparison Project
(TRACMIP; Voigt et al, 2016). TRACMIP provides simulations of idealized
climates that allow for studying the fundamental dynamics of tropical rainfall
and its response to climate change. One configuration is an aquaplanet
atmosphere (i.e., with zonally-symmetric boundary conditions) coupled to a slab
ocean (AquaCTL and Aqua4x). The other includes an equatorial continent
represented by a thin slab ocean with increased surface albedo and decreased
evaporation (LandCTL and Land4x).
",Physics,Physics
"Ternary and $n$-ary $f$-distributive Structures   We introduce and study ternary $f$-distributive structures, Ternary
$f$-quandles and more generally their higher $n$-ary analogues. A
classification of ternary $f$-quandles is provided in low dimensions. Moreover,
we study extension theory and introduce a cohomology theory for ternary, and
more generally $n$-ary, $f$-quandles. Furthermore, we give some computational
examples.
",Mathematics,Mathematics
"Calibrated Fairness in Bandits   We study fairness within the stochastic, \emph{multi-armed bandit} (MAB)
decision making framework. We adapt the fairness framework of ""treating similar
individuals similarly"" to this setting. Here, an `individual' corresponds to an
arm and two arms are `similar' if they have a similar quality distribution.
First, we adopt a {\em smoothness constraint} that if two arms have a similar
quality distribution then the probability of selecting each arm should be
similar. In addition, we define the {\em fairness regret}, which corresponds to
the degree to which an algorithm is not calibrated, where perfect calibration
requires that the probability of selecting an arm is equal to the probability
with which the arm has the best quality realization. We show that a variation
on Thompson sampling satisfies smooth fairness for total variation distance,
and give an $\tilde{O}((kT)^{2/3})$ bound on fairness regret. This complements
prior work, which protects an on-average better arm from being less favored. We
also explain how to extend our algorithm to the dueling bandit setting.
",Computer Science,Computer Science; Statistics
"Quivers with potentials for cluster varieties associated to braid semigroups   Let $C$ be a simply laced generalized Cartan matrix. Given an element $b$ of
the generalized braid semigroup related to $C$, we construct a collection of
mutation-equivalent quivers with potentials. A quiver with potential in such a
collection corresponds to an expression of $b$ in terms of the standard
generators. For two expressions that differ by a braid relation, the
corresponding quivers with potentials are related by a mutation.
The main application of this result is a construction of a family of $CY_3$
$A_\infty$-categories associated to elements of the braid semigroup related to
$C$. In particular, we construct a canonical up to equivalence $CY_3$
$A_\infty$-category associated to quotient of any Double Bruhat cell
$G^{u,v}/{\rm Ad} H$ in a simply laced reductive Lie group $G$.
We describe the full set of parameters these categories depend on by defining
a 2-dimensional CW-complex and proving that the set of parameters is identified
with second cohomology group of this complex.
",Mathematics,Mathematics
"Scraping and Preprocessing Commercial Auction Data for Fraud Classification   In the last three decades, we have seen a significant increase in trading
goods and services through online auctions. However, this business created an
attractive environment for malicious moneymakers who can commit different types
of fraud activities, such as Shill Bidding (SB). The latter is predominant
across many auctions but this type of fraud is difficult to detect due to its
similarity to normal bidding behaviour. The unavailability of SB datasets makes
the development of SB detection and classification models burdensome.
Furthermore, to implement efficient SB detection models, we should produce SB
data from actual auctions of commercial sites. In this study, we first scraped
a large number of eBay auctions of a popular product. After preprocessing the
raw auction data, we build a high-quality SB dataset based on the most reliable
SB strategies. The aim of our research is to share the preprocessed auction
dataset as well as the SB training (unlabelled) dataset, thereby researchers
can apply various machine learning techniques by using authentic data of
auctions and fraud.
",Statistics,Computer Science
"Proton-induced halo formation in charged meteors   Despite a very long history of meteor science, our understanding of meteor
ablation and its shocked plasma physics is still far from satisfactory as we
are still missing the microphysics of meteor shock formation and its plasma
dynamics. Here we argue that electrons and ions in the meteor plasma above
$\sim$100 km altitude undergo spatial separation due to electrons being trapped
by gyration in the Earth's magnetic field, while the ions are carried by the
meteor as their dynamics is dictated by collisions. This separation process
charges the meteor and creates a strong local electric field. We show how
acceleration of protons in this field leads to the collisional excitation of
ionospheric N$_2$ on the scale of many 100 m. This mechanism explains the
puzzling large halo detected around Leonid meteors, while it also fits into the
theoretical expectations of several other unexplained meteor related phenomena.
We expect our work to lead to more advanced models of meteor-ionosphere
interaction, combined with the electrodynamics of meteor trail evolution.
",Physics,Physics
"Decentralized Tube-based Model Predictive Control of Uncertain Nonlinear Multi-Agent Systems   This paper addresses the problem of decentralized tube-based nonlinear Model
Predictive Control (NMPC) for a class of uncertain nonlinear continuous-time
multi-agent systems with additive and bounded disturbance. In particular, the
problem of robust navigation of a multi-agent system to predefined states of
the workspace while using only local information is addressed, under certain
distance and control input constraints. We propose a decentralized feedback
control protocol that consists of two terms: a nominal control input, which is
computed online and is the outcome of a Decentralized Finite Horizon Optimal
Control Problem (DFHOCP) that each agent solves at every sampling time, for its
nominal system dynamics; and an additive state feedback law which is computed
offline and guarantees that the real trajectories of each agent will belong to
a hyper-tube centered along the nominal trajectory, for all times. The volume
of the hyper-tube depends on the upper bound of the disturbances as well as the
bounds of the derivatives of the dynamics. In addition, by introducing certain
distance constraints, the proposed scheme guarantees that the initially
connected agents remain connected for all times. Under standard assumptions
that arise in nominal NMPC schemes, controllability assumptions as well as
communication capabilities between the agents, we guarantee that the
multi-agent system is ISS (Input to State Stable) with respect to the
disturbances, for all initial conditions satisfying the state constraints.
Simulation results verify the correctness of the proposed framework.
",Computer Science,Computer Science
"Switching divergences for spectral learning in blind speech dereverberation   When recorded in an enclosed room, a sound signal will most certainly get
affected by reverberation. This not only undermines audio quality, but also
poses a problem for many human-machine interaction technologies that use speech
as their input. In this work, a new blind, two-stage dereverberation approach
based in a generalized \beta-divergence as a fidelity term over a non-negative
representation is proposed. The first stage consists of learning the spectral
structure of the signal solely from the observed spectrogram, while the second
stage is devoted to model reverberation. Both steps are taken by minimizing a
cost function in which the aim is put either in constructing a dictionary or a
good representation by changing the divergence involved. In addition, an
approach for finding an optimal fidelity parameter for dictionary learning is
proposed. An algorithm for implementing the proposed method is described and
tested against state-of-the-art methods. Results show improvements for both
artificial reverberation and real recordings.
",Computer Science,Computer Science
"First results from the IllustrisTNG simulations: the stellar mass content of groups and clusters of galaxies   The IllustrisTNG project is a new suite of cosmological
magneto-hydrodynamical simulations of galaxy formation performed with the Arepo
code and updated models for feedback physics. Here we introduce the first two
simulations of the series, TNG100 and TNG300, and quantify the stellar mass
content of about 4000 massive galaxy groups and clusters ($10^{13} \leq M_{\rm
200c}/M_{\rm sun} \leq 10^{15}$) at recent times ($z \leq 1$). The richest
clusters have half of their total stellar mass bound to satellite galaxies,
with the other half being associated with the central galaxy and the diffuse
intra-cluster light. The exact ICL fraction depends sensitively on the
definition of a central galaxy's mass and varies in our most massive clusters
between 20 to 40% of the total stellar mass. Haloes of $5\times 10^{14}M_{\rm
sun}$ and above have more diffuse stellar mass outside 100 kpc than within 100
kpc, with power-law slopes of the radial mass density distribution as shallow
as the dark matter's ( $-3.5 < \alpha_{\rm 3D} < -3$). Total halo mass is a
very good predictor of stellar mass, and vice versa: at $z=0$, the 3D stellar
mass measured within 30 kpc scales as $\propto (M_{\rm 500c})^{0.49}$ with a
$\sim 0.12$ dex scatter. This is possibly too steep in comparison to the
available observational constraints, even though the abundance of TNG less
massive galaxies ($< 10^{11}M_{\rm sun}$ in stars) is in good agreement with
the measured galaxy stellar mass functions at recent epochs. The 3D sizes of
massive galaxies fall too on a tight ($\sim$0.16 dex scatter) power-law
relation with halo mass, with $r^{\rm stars}_{\rm 0.5} \propto (M_{\rm
500c})^{0.53}$. Even more fundamentally, halo mass alone is a good predictor
for the whole stellar mass profiles beyond the inner few kpc, and we show how
on average these can be precisely recovered given a single mass measurement of
the galaxy or its halo.
",Physics,Physics
"Quasinonexpansive Iterations on the Affine Hull of Orbits: From Mann's Mean Value Algorithm to Inertial Methods   Fixed point iterations play a central role in the design and the analysis of
a large number of optimization algorithms. We study a new iterative scheme in
which the update is obtained by applying a composition of quasinonexpansive
operators to a point in the affine hull of the orbit generated up to the
current iterate. This investigation unifies several algorithmic constructs,
including Mann's mean value method, inertial methods, and multi-layer
memoryless methods. It also provides a framework for the development of new
algorithms, such as those we propose for solving monotone inclusion and
minimization problems.
",Mathematics,Computer Science; Mathematics
"Strong electron-hole symmetric Rashba spin-orbit coupling in graphene/monolayer transition metal dichalcogenide heterostructures   Despite its extremely weak intrinsic spin-orbit coupling (SOC), graphene has
been shown to acquire considerable SOC by proximity coupling with exfoliated
transition metal dichalcogenides (TMDs). Here we demonstrate strong induced
Rashba SOC in graphene that is proximity coupled to a monolayer TMD film, MoS2
or WSe2, grown by chemical vapor deposition with drastically different Fermi
level positions. Graphene/TMD heterostructures are fabricated with a
pickup-transfer technique utilizing hexagonal boron nitride, which serves as a
flat template to promote intimate contact and therefore a strong interfacial
interaction between TMD and graphene as evidenced by quenching of the TMD
photoluminescence. We observe strong induced graphene SOC that manifests itself
in a pronounced weak anti-localization (WAL) effect in the graphene
magnetoconductance. The spin relaxation rate extracted from the WAL analysis
varies linearly with the momentum scattering time and is independent of the
carrier type. This indicates a dominantly Dyakonov-Perel spin relaxation
mechanism caused by the induced Rashba SOC. Our analysis yields a Rashba SOC
energy of ~1.5 meV in graphene/WSe2 and ~0.9 meV in graphene/MoS2,
respectively. The nearly electron-hole symmetric nature of the induced Rashba
SOC provides a clue to possible underlying SOC mechanisms.
",Physics,Physics
"Enhanced Network Embeddings via Exploiting Edge Labels   Network embedding methods aim at learning low-dimensional latent
representation of nodes in a network. While achieving competitive performance
on a variety of network inference tasks such as node classification and link
prediction, these methods treat the relations between nodes as a binary
variable and ignore the rich semantics of edges. In this work, we attempt to
learn network embeddings which simultaneously preserve network structure and
relations between nodes. Experiments on several real-world networks illustrate
that by considering different relations between different node pairs, our
method is capable of producing node embeddings of higher quality than a number
of state-of-the-art network embedding methods, as evaluated on a challenging
multi-label node classification task.
",Computer Science,Computer Science; Statistics
"Group Field theory and Tensor Networks: towards a Ryu-Takayanagi formula in full quantum gravity   We establish a dictionary between group field theory (thus, spin networks and
random tensors) states and generalized random tensor networks. Then, we use
this dictionary to compute the Rényi entropy of such states and recover the
Ryu-Takayanagi formula, in two different cases corresponding to two different
truncations/approximations, suggested by the established correspondence.
",Physics,Physics
"Graph Convolutional Networks for Classification with a Structured Label Space   It is a usual practice to ignore any structural information underlying
classes in multi-class classification. In this paper, we propose a graph
convolutional network (GCN) augmented neural network classifier to exploit a
known, underlying graph structure of labels. The proposed approach resembles an
(approximate) inference procedure in, for instance, a conditional random field
(CRF). We evaluate the proposed approach on document classification and object
recognition and report both accuracies and graph-theoretic metrics that
correspond to the consistency of the model's prediction. The experiment results
reveal that the proposed model outperforms a baseline method which ignores the
graph structures of a label space in terms of graph-theoretic metrics.
",Computer Science; Statistics,Computer Science; Statistics
"Virtual Constraints and Hybrid Zero Dynamics for Realizing Underactuated Bipedal Locomotion   Underactuation is ubiquitous in human locomotion and should be ubiquitous in
bipedal robotic locomotion as well. This chapter presents a coherent theory for
the design of feedback controllers that achieve stable walking gaits in
underactuated bipedal robots. Two fundamental tools are introduced, virtual
constraints and hybrid zero dynamics. Virtual constraints are relations on the
state variables of a mechanical model that are imposed through a time-invariant
feedback controller. One of their roles is to synchronize the robot's joints to
an internal gait phasing variable. A second role is to induce a low dimensional
system, the zero dynamics, that captures the underactuated aspects of a robot's
model, without any approximations. To enhance intuition, the relation between
physical constraints and virtual constraints is first established. From here,
the hybrid zero dynamics of an underactuated bipedal model is developed, and
its fundamental role in the design of asymptotically stable walking motions is
established. The chapter includes numerous references to robots on which the
highlighted techniques have been implemented.
",Computer Science; Mathematics,Computer Science
"Effects of Interactions on Dynamic Correlations of Hard-Core Bosons at Finite Temperatures   We investigate how dynamic correlations of hard-core bosonic excitation at
finite temperature are affected by additional interactions besides the
hard-core repulsion which prevents them from occupying the same site. We focus
especially on dimerized spin systems, where these additional interactions
between the elementary excitations, triplons, lead to the formation of bound
states, relevant for the correct description of scattering processes. In order
to include these effects quantitatively we extend the previously developed
Brückner approach to include also nearest-neighbor (NN) and next-nearest
neighbor (NNN) interactions correctly in a low-temperature expansion. This
leads to the extension of the scalar Bethe-Salpeter equation to a matrix-valued
equation. Exemplarily, we consider the Heisenberg spin ladder to illustrate the
significance of the additional interactions on the spectral functions at finite
temperature which are proportional to inelastic neutron scattering rates.
",Physics,Physics
"The SLUGGS Survey: Dark matter fractions at large radii and assembly epochs of early-type galaxies from globular cluster kinematics   We use globular cluster kinematics data, primarily from the SLUGGS survey, to
measure the dark matter fraction ($f_{\rm DM}$) and the average dark matter
density ($\left< \rho_{\rm DM} \right>$) within the inner 5 effective radii
($R_{\rm e}$) for 32 nearby early--type galaxies (ETGs) with stellar mass log
$(M_*/\rm M_\odot)$ ranging from $10.1$ to $11.8$. We compare our results with
a simple galaxy model based on scaling relations as well as with cosmological
hydrodynamical simulations where the dark matter profile has been modified
through various physical processes.
We find a high $f_{\rm DM}$ ($\geq0.6$) within 5~$R_{\rm e}$ in most of our
sample, which we interpret as a signature of a late mass assembly history that
is largely devoid of gas-rich major mergers. However, around log $(M_*/M_\odot)
\sim 11$, there is a wide range of $f_{\rm DM}$ which may be challenging to
explain with any single cosmological model. We find tentative evidence that
lenticulars (S0s), unlike ellipticals, have mass distributions that are similar
to spiral galaxies, with decreasing $f_{\rm DM}$ within 5~$R_{\rm e}$ as galaxy
luminosity increases. However, we do not find any difference between the
$\left< \rho_{\rm DM} \right>$ of S0s and ellipticals in our sample, despite
the differences in their stellar populations. We have also used $\left<
\rho_{\rm DM} \right>$ to infer the epoch of halo assembly ($z{\sim}2-4$). By
comparing the age of their central stars with the inferred epoch of halo
formation, we are able to gain more insight into their mass assembly histories.
Our results suggest a fundamental difference in the dominant late-phase mass
assembly channel between lenticulars and elliptical galaxies.
",Physics,Physics
"A SAT+CAS Approach to Finding Good Matrices: New Examples and Counterexamples   We enumerate all circulant good matrices with odd orders divisible by 3 up to
order 70. As a consequence of this we find a previously overlooked set of good
matrices of order 27 and a new set of good matrices of order 57. We also find
that circulant good matrices do not exist in the orders 51, 63, and 69, thereby
finding three new counterexamples to the conjecture that such matrices exist in
all odd orders. Additionally, we prove a new relationship between the entries
of good matrices and exploit this relationship in our enumeration algorithm.
Our method applies the SAT+CAS paradigm of combining computer algebra
functionality with modern SAT solvers to efficiently search large spaces which
are specified by both algebraic and logical constraints.
",Computer Science,Computer Science
"Support Feature Machines   Support Vector Machines (SVMs) with various kernels have played dominant role
in machine learning for many years, finding numerous applications. Although
they have many attractive features interpretation of their solutions is quite
difficult, the use of a single kernel type may not be appropriate in all areas
of the input space, convergence problems for some kernels are not uncommon, the
standard quadratic programming solution has $O(m^3)$ time and $O(m^2)$ space
complexity for $m$ training patterns. Kernel methods work because they
implicitly provide new, useful features. Such features, derived from various
kernels and other vector transformations, may be used directly in any machine
learning algorithm, facilitating multiresolution, heterogeneous models of data.
Therefore Support Feature Machines (SFM) based on linear models in the extended
feature spaces, enabling control over selection of support features, give at
least as good results as any kernel-based SVMs, removing all problems related
to interpretation, scaling and convergence. This is demonstrated for a number
of benchmark datasets analyzed with linear discrimination, SVM, decision trees
and nearest neighbor methods.
",Computer Science; Statistics,Computer Science; Statistics
"Social Events in a Time-Varying Mobile Phone Graph   The large-scale study of human mobility has been significantly enhanced over
the last decade by the massive use of mobile phones in urban populations.
Studying the activity of mobile phones allows us, not only to infer social
networks between individuals, but also to observe the movements of these
individuals in space and time. In this work, we investigate how these two
related sources of information can be integrated within the context of
detecting and analyzing large social events. We show that large social events
can be characterized not only by an anomalous increase in activity of the
antennas in the neighborhood of the event, but also by an increase in social
relationships of the attendants present in the event. Moreover, having detected
a large social event via increased antenna activity, we can use the network
connections to infer whether an unobserved user was present at the event. More
precisely, we address the following three challenges: (i) automatically
detecting large social events via increased antenna activity; (ii)
characterizing the social cohesion of the detected event; and (iii) analyzing
the feasibility of inferring whether unobserved users were in the event.
",Computer Science; Physics,Computer Science
"Towards Efficient Verification of Population Protocols   Population protocols are a well established model of computation by
anonymous, identical finite state agents. A protocol is well-specified if from
every initial configuration, all fair executions reach a common consensus. The
central verification question for population protocols is the
well-specification problem: deciding if a given protocol is well-specified.
Esparza et al. have recently shown that this problem is decidable, but with
very high complexity: it is at least as hard as the Petri net reachability
problem, which is EXPSPACE-hard, and for which only algorithms of non-primitive
recursive complexity are currently known.
In this paper we introduce the class WS3 of well-specified strongly-silent
protocols and we prove that it is suitable for automatic verification. More
precisely, we show that WS3 has the same computational power as general
well-specified protocols, and captures standard protocols from the literature.
Moreover, we show that the membership problem for WS3 reduces to solving
boolean combinations of linear constraints over N. This allowed us to develop
the first software able to automatically prove well-specification for all of
the infinitely many possible inputs.
",Computer Science,Computer Science
"Analytical solution of the integral equation for partial wave Coulomb t-matrices at excited-state energy   Starting from the integral representation of the three-dimensional Coulomb
transition matrix elaborated by us formerly with the use of specific symmetry
of the interaction in a four-dimensional Euclidean space introduced by Fock,
the possibility of the analytical solving of the integral equation for the
partial wave transition matrices at the excited bound state energy has been
studied. New analytical expressions for the partial s-, p- and d-wave Coulomb
t-matrices for like-charged particles and the expression for the partial d-wave
t-matrix for unlike-charged particles at the energy of the first excited bound
state have been derived.
",Physics,Physics
"Wave-Shaped Round Functions and Primitive Groups   Round functions used as building blocks for iterated block ciphers, both in
the case of Substitution-Permutation Networks and Feistel Networks, are often
obtained as the composition of different layers which provide confusion and
diffusion, and key additions. The bijectivity of any encryption function,
crucial in order to make the decryption possible, is guaranteed by the use of
invertible layers or by the Feistel structure. In this work a new family of
ciphers, called wave ciphers, is introduced. In wave ciphers, round functions
feature wave functions, which are vectorial Boolean functions obtained as the
composition of non-invertible layers, where the confusion layer enlarges the
message which returns to its original size after the diffusion layer is
applied. This is motivated by the fact that relaxing the requirement that all
the layers are invertible allows to consider more functions which are optimal
with regard to non-linearity. In particular it allows to consider injective APN
S-boxes. In order to guarantee efficient decryption we propose to use wave
functions in Feistel Networks. With regard to security, the immunity from some
group-theoretical attacks is investigated. In particular, it is shown how to
avoid that the group generated by the round functions acts imprimitively, which
represent a serious flaw for the cipher.
",Computer Science; Mathematics,Computer Science
"Occupants in simplicial complexes   Let $M$ be a smooth manifold and $K\subset M$ be a simplicial complex of
codimension at least 3. Functor calculus methods lead to a homotopical formula
of $M\setminus K$ in terms of spaces $M\setminus T$ where $T$ is a finite
subset of $K$. This is a generalization of the author's previous work with
Michael Weiss where the subset $K$ is assumed to be a smooth submanifold of $M$
and uses his generalization of manifold calculus adapted for simplicial
complexes.
",Mathematics,Mathematics
"It's Like Python But: Towards Supporting Transfer of Programming Language Knowledge   Expertise in programming traditionally assumes a binary novice-expert divide.
Learning resources typically target programmers who are learning programming
for the first time, or expert programmers for that language. An
underrepresented, yet important group of programmers are those that are
experienced in one programming language, but desire to author code in a
different language. For this scenario, we postulate that an effective form of
feedback is presented as a transfer from concepts in the first language to the
second. Current programming environments do not support this form of feedback.
In this study, we apply the theory of learning transfer to teach a language
that programmers are less familiar with--such as R--in terms of a programming
language they already know--such as Python. We investigate learning transfer
using a new tool called Transfer Tutor that presents explanations for R code in
terms of the equivalent Python code. Our study found that participants
leveraged learning transfer as a cognitive strategy, even when unprompted.
Participants found Transfer Tutor to be useful across a number of affordances
like stepping through and highlighting facts that may have been missed or
misunderstood. However, participants were reluctant to accept facts without
code execution or sometimes had difficulty reading explanations that are
verbose or complex. These results provide guidance for future designs and
research directions that can support learning transfer when learning new
programming languages.
",Computer Science,Computer Science
"On the relevance of generalized disclinations in defect mechanics   The utility of the notion of generalized disclinations in materials science
is discussed within the physical context of modeling interfacial and bulk line
defects like defected grain and phase boundaries, dislocations and
disclinations. The Burgers vector of a disclination dipole in linear elasticity
is derived, clearly demonstrating the equivalence of its stress field to that
of an edge dislocation. We also prove that the inverse deformation/displacement
jump of a defect line is independent of the cut-surface when its g.disclination
strength vanishes. An explicit formula for the displacement jump of a single
localized composite defect line in terms of given g.disclination and
dislocation strengths is deduced based on the Weingarten theorem for
g.disclination theory (Weingarten-gd theorem) at finite deformation. The
Burgers vector of a g.disclination dipole at finite deformation is also
derived.
",Physics,Physics
"DFTerNet: Towards 2-bit Dynamic Fusion Networks for Accurate Human Activity Recognition   Deep Convolutional Neural Networks (DCNNs) are currently popular in human
activity recognition applications. However, in the face of modern artificial
intelligence sensor-based games, many research achievements cannot be
practically applied on portable devices. DCNNs are typically resource-intensive
and too large to be deployed on portable devices, thus this limits the
practical application of complex activity detection. In addition, since
portable devices do not possess high-performance Graphic Processing Units
(GPUs), there is hardly any improvement in Action Game (ACT) experience.
Besides, in order to deal with multi-sensor collaboration, all previous human
activity recognition models typically treated the representations from
different sensor signal sources equally. However, distinct types of activities
should adopt different fusion strategies. In this paper, a novel scheme is
proposed. This scheme is used to train 2-bit Convolutional Neural Networks with
weights and activations constrained to {-0.5,0,0.5}. It takes into account the
correlation between different sensor signal sources and the activity types.
This model, which we refer to as DFTerNet, aims at producing a more reliable
inference and better trade-offs for practical applications. Our basic idea is
to exploit quantization of weights and activations directly in pre-trained
filter banks and adopt dynamic fusion strategies for different activity types.
Experiments demonstrate that by using dynamic fusion strategy can exceed the
baseline model performance by up to ~5% on activity recognition like
OPPORTUNITY and PAMAP2 datasets. Using the quantization method proposed, we
were able to achieve performances closer to that of full-precision counterpart.
These results were also verified using the UniMiB-SHAR dataset. In addition,
the proposed method can achieve ~9x acceleration on CPUs and ~11x memory
saving.
",Statistics,Computer Science
"Susceptibility Propagation by Using Diagonal Consistency   A susceptibility propagation that is constructed by combining a belief
propagation and a linear response method is used for approximate computation
for Markov random fields. Herein, we formulate a new, improved susceptibility
propagation by using the concept of a diagonal matching method that is based on
mean-field approaches to inverse Ising problems. The proposed susceptibility
propagation is robust for various network structures, and it is reduced to the
ordinary susceptibility propagation and to the adaptive
Thouless-Anderson-Palmer equation in special cases.
",Mathematics; Statistics,Mathematics
"Direct simulation of liquid-gas-solid flow with a free surface lattice Boltzmann method   Direct numerical simulation of liquid-gas-solid flows is uncommon due to the
considerable computational cost. As the grid spacing is determined by the
smallest involved length scale, large grid sizes become necessary -- in
particular if the bubble-particle aspect ratio is on the order of 10 or larger.
Hence, it arises the question of both feasibility and reasonability. In this
paper, we present a fully parallel, scalable method for direct numerical
simulation of bubble-particle interaction at a size ratio of 1-2 orders of
magnitude that makes simulations feasible on currently available
super-computing resources. With the presented approach, simulations of bubbles
in suspension columns consisting of more than $100\,000$ fully resolved
particles become possible. Furthermore, we demonstrate the significance of
particle-resolved simulations by comparison to previous unresolved solutions.
The results indicate that fully-resolved direct numerical simulation is indeed
necessary to predict the flow structure of bubble-particle interaction problems
correctly.
",Physics,Physics
"Pair Correlation and Gap Distributions for Substitution Tilings and Generalized Ulam Sets in the Plane   We study empirical statistical and gap distributions of several important
tilings of the plane. In particular, we consider the slope distributions, the
angle distributions, pair correlation, squared-distance pair correlation, angle
gap distributions, and slope gap distributions for the Ammann Chair tiling, the
recently discovered fifteenth pentagonal tiling, and a few pertinent tilings
related to these famous examples. We also consider the spatial statistics of
generalized Ulam sets in two dimensions. Additionally, we carefully prove a
tight asymptotic formula for the time steps in which Ulam set points at certain
prescribed geometric positions in their plots in the plane formally enter the
recursively-defined sets.
The software we have developed to these generate numerical approximations to
the distributions for the tilings we consider here is written in Python under
the Sage environment and is released as open-source software which is available
freely on our websites. In addition to the small subset of tilings and other
point sets in the plane we study within the article, our program supports many
other tiling variants and is easily extended for researchers to explore related
tilings and iterative sets.
",Mathematics,Computer Science
"Interferometric Monitoring of Gamma-ray Bright AGNs: S5 0716+714   We present the results of very long baseline interferometry (VLBI)
observations of gamma-ray bright blazar S5 0716+714 using the Korean VLBI
Network (KVN) at the 22, 43, 86, and 129 GHz bands, as part of the
Interferometric Monitoring of Gamma-ray Bright AGNs (iMOGABA) KVN key science
program. Observations were conducted in 29 sessions from January 16, 2013 to
March 1, 2016, with the source being detected and imaged at all available
frequencies. In all epochs, the source was compact on the milliarcsecond (mas)
scale, yielding a compact VLBI core dominating the synchrotron emission on
these scales. Based on the multi-wavelength data between 15 GHz (Owens Valley
Radio Observatory) and 230 GHz (Submillimeter Array), we found that the source
shows multiple prominent enhancements of the flux density at the centimeter
(cm) and millimeter (mm) wavelengths, with mm enhancements leading cm
enhancements by -16$\pm$8 days. The turnover frequency was found to vary
between 21 to 69GHz during our observations. By assuming a synchrotron
self-absorption model for the relativistic jet emission in S5 0716+714, we
found the magnetic field strength in the mas emission region to be $\le$5 mG
during the observing period, yielding a weighted mean of 1.0$\pm$0.6 mG for
higher turnover frequencies (e.g., >45 GHz).
",Physics,Physics
"Gross-Hopkins Duals of Higher Real K-theory Spectra   We determine the Gross-Hopkins duals of certain higher real K-theory spectra.
More specifically, let p be an odd prime, and consider the Morava E-theory
spectrum of height n=p-1. It is known, in the expert circles, that for certain
finite subgroups G of the Morava stabilizer group, the homotopy fixed point
spectra E_n^{hG} are Gross-Hopkins self-dual up to a shift. In this paper, we
determine the shift for those finite subgroups G which contain p-torsion. This
generalizes previous results for n=2 and p=3.
",Mathematics,Mathematics
"Measuring LDA Topic Stability from Clusters of Replicated Runs   Background: Unstructured and textual data is increasing rapidly and Latent
Dirichlet Allocation (LDA) topic modeling is a popular data analysis methods
for it. Past work suggests that instability of LDA topics may lead to
systematic errors. Aim: We propose a method that relies on replicated LDA runs,
clustering, and providing a stability metric for the topics. Method: We
generate k LDA topics and replicate this process n times resulting in n*k
topics. Then we use K-medioids to cluster the n*k topics to k clusters. The k
clusters now represent the original LDA topics and we present them like normal
LDA topics showing the ten most probable words. For the clusters, we try
multiple stability metrics, out of which we recommend Rank-Biased Overlap,
showing the stability of the topics inside the clusters. Results: We provide an
initial validation where our method is used for 270,000 Mozilla Firefox commit
messages with k=20 and n=20. We show how our topic stability metrics are
related to the contents of the topics. Conclusions: Advances in text mining
enable us to analyze large masses of text in software engineering but
non-deterministic algorithms, such as LDA, may lead to unreplicable
conclusions. Our approach makes LDA stability transparent and is also
complementary rather than alternative to many prior works that focus on LDA
parameter tuning.
",Computer Science,Computer Science
"Model Agnostic Time Series Analysis via Matrix Estimation   We propose an algorithm to impute and forecast a time series by transforming
the observed time series into a matrix, utilizing matrix estimation to recover
missing values and de-noise observed entries, and performing linear regression
to make predictions. At the core of our analysis is a representation result,
which states that for a large model class, the transformed time series matrix
is (approximately) low-rank. In effect, this generalizes the widely used
Singular Spectrum Analysis (SSA) in time series literature, and allows us to
establish a rigorous link between time series analysis and matrix estimation.
The key to establishing this link is constructing a Page matrix with
non-overlapping entries rather than a Hankel matrix as is commonly done in the
literature (e.g., SSA). This particular matrix structure allows us to provide
finite sample analysis for imputation and prediction, and prove the asymptotic
consistency of our method. Another salient feature of our algorithm is that it
is model agnostic with respect to both the underlying time dynamics and the
noise distribution in the observations. The noise agnostic property of our
approach allows us to recover the latent states when only given access to noisy
and partial observations a la a Hidden Markov Model; e.g., recovering the
time-varying parameter of a Poisson process without knowing that the underlying
process is Poisson. Furthermore, since our forecasting algorithm requires
regression with noisy features, our approach suggests a matrix estimation based
method - coupled with a novel, non-standard matrix estimation error metric - to
solve the error-in-variable regression problem, which could be of interest in
its own right. Through synthetic and real-world datasets, we demonstrate that
our algorithm outperforms standard software packages (including R libraries) in
the presence of missing data as well as high levels of noise.
",Statistics,Computer Science; Statistics
"Balanced Quantization: An Effective and Efficient Approach to Quantized Neural Networks   Quantized Neural Networks (QNNs), which use low bitwidth numbers for
representing parameters and performing computations, have been proposed to
reduce the computation complexity, storage size and memory usage. In QNNs,
parameters and activations are uniformly quantized, such that the
multiplications and additions can be accelerated by bitwise operations.
However, distributions of parameters in Neural Networks are often imbalanced,
such that the uniform quantization determined from extremal values may under
utilize available bitwidth. In this paper, we propose a novel quantization
method that can ensure the balance of distributions of quantized values. Our
method first recursively partitions the parameters by percentiles into balanced
bins, and then applies uniform quantization. We also introduce computationally
cheaper approximations of percentiles to reduce the computation overhead
introduced. Overall, our method improves the prediction accuracies of QNNs
without introducing extra computation during inference, has negligible impact
on training speed, and is applicable to both Convolutional Neural Networks and
Recurrent Neural Networks. Experiments on standard datasets including ImageNet
and Penn Treebank confirm the effectiveness of our method. On ImageNet, the
top-5 error rate of our 4-bit quantized GoogLeNet model is 12.7\%, which is
superior to the state-of-the-arts of QNNs.
",Computer Science,Computer Science; Statistics
"CODA: Enabling Co-location of Computation and Data for Near-Data Processing   Recent studies have demonstrated that near-data processing (NDP) is an
effective technique for improving performance and energy efficiency of
data-intensive workloads. However, leveraging NDP in realistic systems with
multiple memory modules introduces a new challenge. In today's systems, where
no computation occurs in memory modules, the physical address space is
interleaved at a fine granularity among all memory modules to help improve the
utilization of processor-memory interfaces by distributing the memory traffic.
However, this is at odds with efficient use of NDP, which requires careful
placement of data in memory modules such that near-data computations and their
exclusively used data can be localized in individual memory modules, while
distributing shared data among memory modules to reduce hotspots. In order to
address this new challenge, we propose a set of techniques that (1) enable
collections of OS pages to either be fine-grain interleaved among memory
modules (as is done today) or to be placed contiguously on individual memory
modules (as is desirable for NDP private data), and (2) decide whether to
localize or distribute each memory object based on its anticipated access
pattern and steer computations to the memory where the data they access is
located. Our evaluations across a wide range of workloads show that the
proposed mechanism improves performance by 31% and reduces 38% remote data
accesses over a baseline system that cannot exploit computate-data affinity
characteristics.
",Computer Science,Computer Science
"Scalable Inference for Nested Chinese Restaurant Process Topic Models   Nested Chinese Restaurant Process (nCRP) topic models are powerful
nonparametric Bayesian methods to extract a topic hierarchy from a given text
corpus, where the hierarchical structure is automatically determined by the
data. Hierarchical Latent Dirichlet Allocation (hLDA) is a popular instance of
nCRP topic models. However, hLDA has only been evaluated at small scale,
because the existing collapsed Gibbs sampling and instantiated weight
variational inference algorithms either are not scalable or sacrifice inference
quality with mean-field assumptions. Moreover, an efficient distributed
implementation of the data structures, such as dynamically growing count
matrices and trees, is challenging.
In this paper, we propose a novel partially collapsed Gibbs sampling (PCGS)
algorithm, which combines the advantages of collapsed and instantiated weight
algorithms to achieve good scalability as well as high model quality. An
initialization strategy is presented to further improve the model quality.
Finally, we propose an efficient distributed implementation of PCGS through
vectorization, pre-processing, and a careful design of the concurrent data
structures and communication strategy.
Empirical studies show that our algorithm is 111 times more efficient than
the previous open-source implementation for hLDA, with comparable or even
better model quality. Our distributed implementation can extract 1,722 topics
from a 131-million-document corpus with 28 billion tokens, which is 4-5 orders
of magnitude larger than the previous largest corpus, with 50 machines in 7
hours.
",Computer Science; Statistics,Computer Science; Statistics
"Quantum Query Algorithms are Completely Bounded Forms   We prove a characterization of $t$-query quantum algorithms in terms of the
unit ball of a space of degree-$2t$ polynomials. Based on this, we obtain a
refined notion of approximate polynomial degree that equals the quantum query
complexity, answering a question of Aaronson et al. (CCC'16). Our proof is
based on a fundamental result of Christensen and Sinclair (J. Funct. Anal.,
1987) that generalizes the well-known Stinespring representation for quantum
channels to multilinear forms. Using our characterization, we show that many
polynomials of degree four are far from those coming from two-query quantum
algorithms. We also give a simple and short proof of one of the results of
Aaronson et al. showing an equivalence between one-query quantum algorithms and
bounded quadratic polynomials.
",Computer Science; Mathematics,Computer Science
"Bayesian Uncertainty Quantification and Information Fusion in CALPHAD-based Thermodynamic Modeling   Calculation of phase diagrams is one of the fundamental tools in alloy
design---more specifically under the framework of Integrated Computational
Materials Engineering. Uncertainty quantification of phase diagrams is the
first step required to provide confidence for decision making in property- or
performance-based design. As a manner of illustration, a thorough probabilistic
assessment of the CALPHAD model parameters is performed against the available
data for a Hf-Si binary case study using a Markov Chain Monte Carlo sampling
approach. The plausible optimum values and uncertainties of the parameters are
thus obtained, which can be propagated to the resulting phase diagram. Using
the parameter values obtained from deterministic optimization in a
computational thermodynamic assessment tool (in this case Thermo-Calc) as the
prior information for the parameter values and ranges in the sampling process
is often necessary to achieve a reasonable cost for uncertainty quantification.
This brings up the problem of finding an appropriate CALPHAD model with
high-level of confidence which is a very hard and costly task that requires
considerable expert skill. A Bayesian hypothesis testing based on Bayes'
factors is proposed to fulfill the need of model selection in this case, which
is applied to compare four recommended models for the Hf-Si system. However, it
is demonstrated that information fusion approaches, i.e., Bayesian model
averaging and an error correlation-based model fusion, can be used to combine
the useful information existing in all the given models rather than just using
the best selected model, which may lack some information about the system being
modelled.
",Statistics,Computer Science; Statistics
"Dichotomy for Digraph Homomorphism Problems (two algorithms)   Update : An issue has been found in the correctness of our algorithm, and we
are working to resolve the issue. Until a resolution is found, we retract our
main claim that our approach gives a combinatorial solution to the CSP
conjecture. We remain hopeful that we can resolve the issues. We thank Ross
Willard for carefully checking the algorithm and pointing out the mistake in
the version of this manuscript. We briefly explain one issue at the beginning
of the text, and leave the rest of the manuscript intact for the moment . Ross
Willard is posting a more involved description of a counter-example to the
algorithm in the present manuscript. We have an updated manuscript that
corrects some issues while still not arriving at a full solution; we will keep
this private as long as unresolved issues remain.
Previous abstract : We consider the problem of finding a homomorphism from an
input digraph G to a fixed digraph H. We show that if H admits a
weak-near-unanimity polymorphism $\phi$ then deciding whether G admits a
homomorphism to H (HOM(H)) is polynomial time solvable. This confirms the
conjecture of Maroti and McKenzie, and consequently implies the validity of the
celebrated dichotomy conjecture due to Feder and Vardi. We transform the
problem into an instance of the list homomorphism problem where initially all
the lists are full (contain all the vertices of H). Then we use the
polymorphism $\phi$ as a guide to reduce the lists to singleton lists, which
yields a homomorphism if one exists.
",Computer Science,Computer Science
"Uniform rank gradient, cost and local-global convergence   We analyze the rank gradient of finitely generated groups with respect to
sequences of subgroups of finite index that do not necessarily form a chain, by
connecting it to the cost of p.m.p. actions. We generalize several results that
were only known for chains before. The connection is made by the notion of
local-global convergence.
In particular, we show that for a finitely generated group $\Gamma$ with
fixed price $c$, every Farber sequence has rank gradient $c-1$. By adapting
Lackenby's trichotomy theorem to this setting, we also show that in a finitely
presented amenable group, every sequence of subgroups with index tending to
infinity has vanishing rank gradient.
",Mathematics,Mathematics
"The cauchy problem for radially symmetric homogeneous boltzmann equation with shubin class initial datum and gelfand-shilov smoothing effect   In this paper, we study the Cauchy problem for radially symmetric homogeneous
non-cutoff Boltzmann equation with Maxwellian molecules, the initial datum
belongs to Shubin space of the negative index which can be characterized by
spectral decomposition of the harmonic oscillators. The Shubin space of the
negative index contains the measure functions. Based on this spectral
decomposition, we construct the weak solution with Shubin class initial datum,
we also prove that the Cauchy problem enjoys Gelfand-Shilov smoothing effect,
meaning that the smoothing properties are the same as the Cauchy problem
defined by the evolution equation associated to a fractional harmonic
oscillator.
",Mathematics,Mathematics
"Laplace operators on holomorphic Lie algebroids   The paper introduces Laplace-type operators for functions defined on the
tangent space of a Finsler Lie algebroid, using a volume form on the
prolongation of the algebroid. It also presents the construction of a
horizontal Laplace operator for forms defined on the prolongation of the
algebroid. All of the Laplace operators considered in the paper are also
locally expressed using the Chern-Finsler connection of the algebroid.
",Mathematics,Mathematics
"Tunable Optoelectronic Properties of Triply-Bonded Carbon Molecules with Linear and Graphyne Substructures   In this paper we present a detailed computational study of the electronic
structure and optical properties of triply-bonded hydrocarbons with linear, and
graphyne substructures, with the aim of identifying their potential in
opto-electronic device applications. For the purpose, we employed a correlated
electron methodology based upon the Pariser-Parr-Pople model Hamiltonian,
coupled with the configuration interaction (CI) approach, and studied
structures containing up to 42 carbon atoms. Our calculations, based upon
large-scale CI expansions, reveal that the linear structures have intense
optical absorption at the HOMO-LUMO gap, while the graphyne ones have those at
higher energies. Thus, the opto-electronic properties depend on the topology of
the {graphyne substructures, suggesting that they can be tuned by means of
structural modifications. Our results are in very good agreement with the
available experimental data.
",Physics,Physics
"Feynman-Kac equation for anomalous processes with space- and time-dependent forces   Functionals of a stochastic process Y(t) model many physical time-extensive
observables, e.g. particle positions, local and occupation times or accumulated
mechanical work. When Y(t) is a normal diffusive process, their statistics are
obtained as the solution of the Feynman-Kac equation. This equation provides
the crucial link between the expected values of diffusion processes and the
solutions of deterministic second-order partial differential equations. When
Y(t) is an anomalous diffusive process, generalizations of the Feynman-Kac
equation that incorporate power-law or more general waiting time distributions
of the underlying random walk have recently been derived. A general
representation of such waiting times is provided in terms of a Lévy process
whose Laplace exponent is related to the memory kernel appearing in the
generalized Feynman-Kac equation. The corresponding anomalous processes have
been shown to capture nonlinear mean square displacements exhibiting crossovers
between different scaling regimes, which have been observed in biological
systems like migrating cells or diffusing macromolecules in intracellular
environments. However, the case where both space- and time-dependent forces
drive the dynamics of the generalized anomalous process has not been solved
yet. Here, we present the missing derivation of the Feynman-Kac equation in
such general case by using the subordination technique. Furthermore, we discuss
its extension to functionals explicitly depending on time, which are relevant
for the stochastic thermodynamics of anomalous diffusive systems. Exact results
on the work fluctuations of a simple non-equilibrium model are obtained. In
this paper we also provide a pedagogical introduction to Lévy processes,
semimartingales and their associated stochastic calculus, which underlie the
mathematical formulation of anomalous diffusion as a subordinated process.
",Physics; Mathematics,Physics
"Would You Like to Motivate Software Testers? Ask Them How   Context. Considering the importance of software testing to the development of
high quality and reliable software systems, this paper aims to investigate how
can work-related factors influence the motivation of software testers. Method.
We applied a questionnaire that was developed using a previous theory of
motivation and satisfaction of software engineers to conduct a survey-based
study to explore and understand how professional software testers perceive and
value work-related factors that could influence their motivation at work.
Results. With a sample of 80 software testers we observed that software testers
are strongly motivated by variety of work, creative tasks, recognition for
their work, and activities that allow them to acquire new knowledge, but in
general the social impact of this activity has low influence on their
motivation. Conclusion. This study discusses the difference of opinions among
software testers, regarding work-related factors that could impact their
motivation, which can be relevant for managers and leaders in software
engineering practice.
",Computer Science,Computer Science
"Non-locality of the meet levels of the Trotter-Weil Hierarchy   We prove that the meet level $m$ of the Trotter-Weil, $\mathsf{V}_m$ is not
local for all $m \geq 1$, as conjectured in a paper by Kufleitner and Lauser.
In order to show this, we explicitly provide a language whose syntactic
semigroup is in $L \mathsf{V}_m$ and not in $\mathsf{V}_m*\mathsf{D}$.
",Computer Science; Mathematics,Mathematics
"Intelligent Pothole Detection and Road Condition Assessment   Poor road conditions are a public nuisance, causing passenger discomfort,
damage to vehicles, and accidents. In the U.S., road-related conditions are a
factor in 22,000 of the 42,000 traffic fatalities each year. Although we often
complain about bad roads, we have no way to detect or report them at scale. To
address this issue, we developed a system to detect potholes and assess road
conditions in real-time. Our solution is a mobile application that captures
data on a car's movement from gyroscope and accelerometer sensors in the phone.
To assess roads using this sensor data, we trained SVM models to classify road
conditions with 93% accuracy and potholes with 92% accuracy, beating the base
rate for both problems. As the user drives, the models use the sensor data to
classify whether the road is good or bad, and whether it contains potholes.
Then, the classification results are used to create data-rich maps that
illustrate road conditions across the city. Our system will empower civic
officials to identify and repair damaged roads which inconvenience passengers
and cause accidents. This paper details our data science process for collecting
training data on real roads, transforming noisy sensor data into useful
signals, training and evaluating machine learning models, and deploying those
models to production through a real-time classification app. It also highlights
how cities can use our system to crowdsource data and deliver road repair
resources to areas in need.
",Computer Science,Computer Science
"On the coefficients of the Alekseev Torossian associator   This paper explains a method to calculate the coefficients of the
Alekseev-Torossian associator as linear combinations of iterated integrals of
Kontsevich weight forms of Lie graphs.
",Mathematics,Mathematics
"Training DNNs with Hybrid Block Floating Point   The wide adoption of DNNs has given birth to unrelenting computing
requirements, forcing datacenter operators to adopt domain-specific
accelerators to train them. These accelerators typically employ densely packed
full precision floating-point arithmetic to maximize performance per area.
Ongoing research efforts seek to further increase that performance density by
replacing floating-point with fixed-point arithmetic. However, a significant
roadblock for these attempts has been fixed point's narrow dynamic range, which
is insufficient for DNN training convergence. We identify block floating point
(BFP) as a promising alternative representation since it exhibits wide dynamic
range and enables the majority of DNN operations to be performed with
fixed-point logic. Unfortunately, BFP alone introduces several limitations that
preclude its direct applicability. In this work, we introduce HBFP, a hybrid
BFP-FP approach, which performs all dot products in BFP and other operations in
floating point. HBFP delivers the best of both worlds: the high accuracy of
floating point at the superior hardware density of fixed point. For a wide
variety of models, we show that HBFP matches floating point's accuracy while
enabling hardware implementations that deliver up to 8.5x higher throughput.
",Computer Science; Statistics,Computer Science
"A Framework for Algorithm Stability   We say that an algorithm is stable if small changes in the input result in
small changes in the output. This kind of algorithm stability is particularly
relevant when analyzing and visualizing time-varying data. Stability in general
plays an important role in a wide variety of areas, such as numerical analysis,
machine learning, and topology, but is poorly understood in the context of
(combinatorial) algorithms. In this paper we present a framework for analyzing
the stability of algorithms. We focus in particular on the tradeoff between the
stability of an algorithm and the quality of the solution it computes. Our
framework allows for three types of stability analysis with increasing degrees
of complexity: event stability, topological stability, and Lipschitz stability.
We demonstrate the use of our stability framework by applying it to kinetic
Euclidean minimum spanning trees.
",Computer Science,Computer Science; Statistics
"Learning from Label Proportions in Brain-Computer Interfaces: Online Unsupervised Learning with Guarantees   Objective: Using traditional approaches, a Brain-Computer Interface (BCI)
requires the collection of calibration data for new subjects prior to online
use. Calibration time can be reduced or eliminated e.g.~by transfer of a
pre-trained classifier or unsupervised adaptive classification methods which
learn from scratch and adapt over time. While such heuristics work well in
practice, none of them can provide theoretical guarantees. Our objective is to
modify an event-related potential (ERP) paradigm to work in unison with the
machine learning decoder to achieve a reliable calibration-less decoding with a
guarantee to recover the true class means.
Method: We introduce learning from label proportions (LLP) to the BCI
community as a new unsupervised, and easy-to-implement classification approach
for ERP-based BCIs. The LLP estimates the mean target and non-target responses
based on known proportions of these two classes in different groups of the
data. We modified a visual ERP speller to meet the requirements of the LLP. For
evaluation, we ran simulations on artificially created data sets and conducted
an online BCI study with N=13 subjects performing a copy-spelling task.
Results: Theoretical considerations show that LLP is guaranteed to minimize
the loss function similarly to a corresponding supervised classifier. It
performed well in simulations and in the online application, where 84.5% of
characters were spelled correctly on average without prior calibration.
Significance: The continuously adapting LLP classifier is the first
unsupervised decoder for ERP BCIs guaranteed to find the true class means. This
makes it an ideal solution to avoid a tedious calibration and to tackle
non-stationarities in the data. Additionally, LLP works on complementary
principles compared to existing unsupervised methods, allowing for their
further enhancement when combined with LLP.
",Computer Science; Statistics,Computer Science; Statistics
"Multi-Labelled Value Networks for Computer Go   This paper proposes a new approach to a novel value network architecture for
the game Go, called a multi-labelled (ML) value network. In the ML value
network, different values (win rates) are trained simultaneously for different
settings of komi, a compensation given to balance the initiative of playing
first. The ML value network has three advantages, (a) it outputs values for
different komi, (b) it supports dynamic komi, and (c) it lowers the mean
squared error (MSE). This paper also proposes a new dynamic komi method to
improve game-playing strength. This paper also performs experiments to
demonstrate the merits of the architecture. First, the MSE of the ML value
network is generally lower than the value network alone. Second, the program
based on the ML value network wins by a rate of 67.6% against the program based
on the value network alone. Third, the program with the proposed dynamic komi
method significantly improves the playing strength over the baseline that does
not use dynamic komi, especially for handicap games. To our knowledge, up to
date, no handicap games have been played openly by programs using value
networks. This paper provides these programs with a useful approach to playing
handicap games.
",Computer Science,Computer Science
"Implications of right-handed neutrinos in $B-L$ extended standard model with scalar dark matter   We investigate the Standard Model (SM) with a $U(1)_{B-L}$ gauge extension
where a $B-L$ charged scalar is a viable dark matter (DM) candidate. The
dominant annihilation process, for the DM particle is through the $B-L$
symmetry breaking scalar to right-handed neutrino pair. We exploit the effect
of decay and inverse decay of the right-handed neutrino in thermal relic
abundance of the DM. Depending on the values of the decay rate, the DM relic
density can be significantly different from what is obtained in the standard
calculation assuming the right-handed neutrino is in thermal equilibrium and
there appear different regions of the parameter space satisfying the observed
DM relic density. For a DM mass less than $\mathcal{O}$(TeV), the direct
detection experiments impose a competitive bound on the mass of the
$U(1)_{B-L}$ gauge boson $Z^\prime$ with the collider experiments. Utilizing
the non-observation of the displaced vertices arising from the right-handed
neutrino decays, bound on the mass of $Z^\prime$ has been obtained at present
and higher luminosities at the LHC with 14 TeV centre of mass energy where an
integrated luminosity of 100fb$^{-1}$ is sufficient to probe $m_{Z'} \sim 5.5$
TeV.
",Physics,Physics
"On ramification in transcendental extensions of local fields   Let $L/K$ be an extension of complete discrete valuation fields, and assume
that the residue field of $K$ is perfect and of positive characteristic. The
residue field of $L$ is not assumed to be perfect.
In this paper, we prove a formula for the Swan conductor of the image of a
character $\chi \in H^1(K, \mathbb{Q}/\mathbb{Z})$ in $H^1(L,
\mathbb{Q}/\mathbb{Z})$ for $\chi$ sufficiently ramified. Further, we define
generalizations $\psi_{L/K}^{\mathrm{ab}}$ and $\psi_{L/K}^{\mathrm{AS}}$ of
the classical Hasse-Herbrand $\psi$-function and prove a formula for
$\psi_{L/K}^{\mathrm{ab}}(t)$ for sufficiently large $t\in \mathbb{R}$.
",Mathematics,Mathematics
"Exploring Features for Predicting Policy Citations   In this study we performed an initial investigation and evaluation of
altmetrics and their relationship with public policy citation of research
papers. We examined methods for using altmetrics and other data to predict
whether a research paper is cited in public policy and applied receiver
operating characteristic curve on various feature groups in order to evaluate
their potential usefulness. From the methods we tested, classifying based on
tweet count provided the best results, achieving an area under the ROC curve of
0.91.
",Computer Science,Computer Science
"Short-time behavior of the heat kernel and Weyl's law on $RCD^*(K, N)$-spaces   In this paper, we prove pointwise convergence of heat kernels for
mGH-convergent sequences of $RCD^*(K,N)$-spaces. We obtain as a corollary
results on the short-time behavior of the heat kernel in $RCD^*(K,N)$-spaces.
We use then these results to initiate the study of Weyl's law in the $RCD$
setting
",Mathematics,Mathematics
"Comments on the National Toxicology Program Report on Cancer, Rats and Cell Phone Radiation   With the National Toxicology Program issuing its final report on cancer, rats
and cell phone radiation, one can draw the following conclusions from their
data. There is a roughly linear relationship between gliomas (brain cancers)
and schwannomas (cancers of the nerve sheaths around the heart) with increased
absorption of 900 MHz radiofrequency radiation for male rats. The rate of these
cancers in female rats is about one third the rate in male rats; the rate of
gliomas in female humans is about two thirds the rate in male humans. Both of
these observations can be explained by a decrease in sensitivity to chemical
carcinogenesis in both female rats and female humans. The increase in male rat
life spans with increased radiofrequency absorption is due to a reduction in
kidney failure from a decrease in food intake. No such similar increase in the
life span of humans who use cell phones is expected.
",Quantitative Biology,Quantitative Biology
"New concepts of inertial measurements with multi-species atom interferometry   In the field of cold atom inertial sensors, we present and analyze innovative
configurations for improving their measurement range and sensitivity,
especially attracting for onboard applications. These configurations rely on
multi-species atom interferometry, involving the simultaneous manipulation of
different atomic species in a unique instrument to deduce inertial
measurements. Using a dual-species atom accelerometer manipulating
simultaneously both isotopes of rubidium, we report a preliminary experimental
realization of original concepts involving the implementation of two atom
interferometers first with different interrogation times and secondly in phase
quadrature. These results open the door to a new generation of atomic sensors
relying on high performance multi-species atom interferometric measurements.
",Physics,Physics
"Long-Term Sequential Prediction Using Expert Advice   For the prediction with experts' advice setting, we consider some methods to
construct forecasting algorithms that suffer loss not much more than any expert
in the pool. In contrast to the standard approach, we investigate the case of
long-term forecasting of time series. This approach implies that each expert
issues a forecast for a time point ahead (or a time interval), and then the
master algorithm combines these forecasts into one aggregated forecast
(sequence of forecasts). We introduce two new approaches to aggregating
experts' long-term interval predictions. Both are based on Vovk's aggregating
algorithm. The first approach applies the method of Mixing Past Posteriors
method to the long-term prediction. The second approach is used for the
interval forecasting and considers overlapping experts. The upper bounds for
regret of these algorithms for adversarial case are obtained. We also present
the results of numerical experiments on time series long-term prediction.
",Computer Science; Statistics,Computer Science; Statistics
"Radial anisotropy in omega Cen limiting the room for an intermediate-mass black hole   Finding an intermediate-mass black hole (IMBH) in a globular cluster (or
proving its absence) would provide valuable insights into our understanding of
galaxy formation and evolution. However, it is challenging to identify a unique
signature of an IMBH that cannot be accounted for by other processes.
Observational claims of IMBH detection are indeed often based on analyses of
the kinematics of stars in the cluster core, the most common signature being a
rise in the velocity dispersion profile towards the centre of the system.
Unfortunately, this IMBH signal is degenerate with the presence of
radially-biased pressure anisotropy in the globular cluster. To explore the
role of anisotropy in shaping the observational kinematics of clusters, we
analyse the case of omega Cen by comparing the observed profiles to those
calculated from the family of LIMEPY models, that account for the presence of
anisotropy in the system in a physically motivated way. The best-fit radially
anisotropic models reproduce the observational profiles well, and describe the
central kinematics as derived from Hubble Space Telescope proper motions
without the need for an IMBH.
",Physics,Physics
"Academic Engagement and Commercialization in an Institutional Transition Environment: Evidence from Shanghai Maritime University   Does academic engagement accelerate or crowd out the commercialization of
university knowledge? Research on this topic seldom considers the impact of the
institutional environment, especially when a formal institution for encouraging
the commercial activities of scholars has not yet been established. This study
investigates this question in the context of China, which is in the
institutional transition stage. Based on a survey of scholars from Shanghai
Maritime University, we demonstrate that academic engagement has a positive
impact on commercialization and that this impact is greater for risk-averse
scholars than for other risk-seeking scholars. Our results suggest that in an
institutional transition environment, the government should consider
encouraging academic engagement to stimulate the commercialization activities
of conservative scholars.
",Quantitative Finance,Computer Science; Statistics
"The univalence axiom in cubical sets   In this note we show that Voevodsky's univalence axiom holds in the model of
type theory based on symmetric cubical sets. We will also discuss Swan's
construction of the identity type in this variation of cubical sets. This
proves that we have a model of type theory supporting dependent products,
dependent sums, univalent universes, and identity types with the usual
judgmental equality, and this model is formulated in a constructive metatheory.
",Computer Science; Mathematics,Mathematics
"Multi-hop assortativities for networks classification   Several social, medical, engineering and biological challenges rely on
discovering the functionality of networks from their structure and node
metadata, when it is available. For example, in chemoinformatics one might want
to detect whether a molecule is toxic based on structure and atomic types, or
discover the research field of a scientific collaboration network. Existing
techniques rely on counting or measuring structural patterns that are known to
show large variations from network to network, such as the number of triangles,
or the assortativity of node metadata. We introduce the concept of multi-hop
assortativity, that captures the similarity of the nodes situated at the
extremities of a randomly selected path of a given length. We show that
multi-hop assortativity unifies various existing concepts and offers a
versatile family of 'fingerprints' to characterize networks. These fingerprints
allow in turn to recover the functionalities of a network, with the help of the
machine learning toolbox. Our method is evaluated empirically on established
social and chemoinformatic network benchmarks. Results reveal that our
assortativity based features are competitive providing highly accurate results
often outperforming state of the art methods for the network classification
task.
",Computer Science; Statistics,Computer Science; Physics
"An anti-incursion algorithm for unknown probabilistic adversaries on connected graphs   A gambler moves on the vertices $1, \ldots, n$ of a graph using the
probability distribution $p_{1}, \ldots, p_{n}$. A cop pursues the gambler on
the graph, only being able to move between adjacent vertices. What is the
expected number of moves that the gambler can make until the cop catches them?
Komarov and Winkler proved an upper bound of approximately $1.97n$ for the
expected capture time on any connected $n$-vertex graph when the cop does not
know the gambler's distribution. We improve this upper bound to approximately
$1.95n$ by modifying the cop's pursuit algorithm.
",Computer Science; Mathematics,Computer Science; Mathematics
"An EM Based Probabilistic Two-Dimensional CCA with Application to Face Recognition   Recently, two-dimensional canonical correlation analysis (2DCCA) has been
successfully applied for image feature extraction. The method instead of
concatenating the columns of the images to the one-dimensional vectors,
directly works with two-dimensional image matrices. Although 2DCCA works well
in different recognition tasks, it lacks a probabilistic interpretation. In
this paper, we present a probabilistic framework for 2DCCA called probabilistic
2DCCA (P2DCCA) and an iterative EM based algorithm for optimizing the
parameters. Experimental results on synthetic and real data demonstrate
superior performance in loading factor estimation for P2DCCA compared to 2DCCA.
For real data, three subsets of AR face database and also the UMIST face
database confirm the robustness of the proposed algorithm in face recognition
tasks with different illumination conditions, facial expressions, poses and
occlusions.
",Computer Science; Statistics,Computer Science
"Gould's Belt: Local Large Scale Structure in the Milky Way   Gould's Belt is a flat local system composed of young OB stars, molecular
clouds and neutral hydrogen within 500 pc from the Sun. It is inclined about 20
degrees to the galactic plane and its velocity field significantly deviates
from rotation around the distant center of the Milky Way. We discuss possible
models of its origin: free expansion from a point or from a ring, expansion of
a shell, or a collision of a high velocity cloud with the plane of the Milky
Way. Currently, no convincing model exists. Similar structures are identified
in HI and CO distribution in our and other nearby galaxies.
",Physics,Physics
"Automatic Segmentation of the Left Ventricle in Cardiac CT Angiography Using Convolutional Neural Network   Accurate delineation of the left ventricle (LV) is an important step in
evaluation of cardiac function. In this paper, we present an automatic method
for segmentation of the LV in cardiac CT angiography (CCTA) scans. Segmentation
is performed in two stages. First, a bounding box around the LV is detected
using a combination of three convolutional neural networks (CNNs).
Subsequently, to obtain the segmentation of the LV, voxel classification is
performed within the defined bounding box using a CNN. The study included CCTA
scans of sixty patients, fifty scans were used to train the CNNs for the LV
localization, five scans were used to train LV segmentation and the remaining
five scans were used for testing the method. Automatic segmentation resulted in
the average Dice coefficient of 0.85 and mean absolute surface distance of 1.1
mm. The results demonstrate that automatic segmentation of the LV in CCTA scans
using voxel classification with convolutional neural networks is feasible.
",Computer Science; Physics,Computer Science; Statistics
"Effects of Arrival Type and Degree of Saturation on Queue Length Estimation at Signalized Intersections   Purpose of this study is evaluation of the relationship between different
arrival types and degree of saturation (X) with overestimations of HCM 2010
procedure for estimating the back of queue within a study area. Further
analysis is performed to establish the relationship between queue length and
delay and also between each of them individually and X in cases with
overestimation. The analyses are based on the 50th percentile queue lengths for
data collected at four signalized intersections along a corridor in 4 time
periods (off peak period and AM, Noon and PM peak periods). Based on the
statistical test results, arrival type did not play a role in overestimations.
However, there is a significant relationship between the overestimations on
minor and major street and different ranges of X. On minor streets, about 59%
of the overestimations are at X values less than half; while near 23% of the
overestimations are at oversaturation condition with X values greater than 1.
The relationship between amount of overestimations and degree of saturation
should be established based on the numerical amount of overestimations versus X
values rather than the relative amounts; since the statistical comparison
between the relative amount of overestimations and X values, resulted in a
wrong idea of the real world condition. There was a significant correlation
between field queue and delay data of the cases with overestimated queue length
in all cases on major and minor streets. Also, field queue is correlated to X,
in all cases on minor and major streets.
",Statistics,Computer Science; Mathematics
"Abdominal aortic aneurysms and endovascular sealing: deformation and dynamic response   Endovascular sealing is a new technique for the repair of abdominal aortic
aneurysms. Commercially available in Europe since~2013, it takes a
revolutionary approach to aneurysm repair through minimally invasive
techniques. Although aneurysm sealing may be thought as more stable than
conventional endovascular stent graft repairs, post-implantation movement of
the endoprosthesis has been described, potentially leading to late
complications. The paper presents for the first time a model, which explains
the nature of forces, in static and dynamic regimes, acting on sealed abdominal
aortic aneurysms, with references to real case studies. It is shown that
elastic deformation of the aorta and of the endoprosthesis induced by static
forces and vibrations during daily activities can potentially promote undesired
movements of the endovascular sealing structure.
",Physics,Physics
"Introduction to Formal Concept Analysis and Its Applications in Information Retrieval and Related Fields   This paper is a tutorial on Formal Concept Analysis (FCA) and its
applications. FCA is an applied branch of Lattice Theory, a mathematical
discipline which enables formalisation of concepts as basic units of human
thinking and analysing data in the object-attribute form. Originated in early
80s, during the last three decades, it became a popular human-centred tool for
knowledge representation and data analysis with numerous applications. Since
the tutorial was specially prepared for RuSSIR 2014, the covered FCA topics
include Information Retrieval with a focus on visualisation aspects, Machine
Learning, Data Mining and Knowledge Discovery, Text Mining and several others.
",Computer Science; Statistics,Computer Science
"ASDA : Analyseur Syntaxique du Dialecte Alg{é}rien dans un but d'analyse s{é}mantique   Opinion mining and sentiment analysis in social media is a research issue
having a great interest in the scientific community. However, before begin this
analysis, we are faced with a set of problems. In particular, the problem of
the richness of languages and dialects within these media. To address this
problem, we propose in this paper an approach of construction and
implementation of Syntactic analyzer named ASDA. This tool represents a parser
for the Algerian dialect that label the terms of a given corpus. Thus, we
construct a labeling table containing for each term its stem, different
prefixes and suffixes, allowing us to determine the different grammatical parts
a sort of POS tagging. This labeling will serve us later in the semantic
processing of the Algerian dialect, like the automatic translation of this
dialect or sentiment analysis
",Computer Science,Computer Science
"An Approach to Controller Design Based on the Generalized Cloud Model   In this paper, an approach to controller design based on the cloud models,
without using the analog plant model is presented.
",Computer Science,Computer Science
"Phasebook and Friends: Leveraging Discrete Representations for Source Separation   Deep learning based speech enhancement and source separation systems have
recently reached unprecedented levels of quality, to the point that performance
is reaching a new ceiling. Most systems rely on estimating the magnitude of a
target source by estimating a real-valued mask to be applied to a
time-frequency representation of the mixture signal. A limiting factor in such
approaches is a lack of phase estimation: the phase of the mixture is most
often used when reconstructing the estimated time-domain signal. Here, we
propose `MagBook', `phasebook', and `Combook', three new types of layers based
on discrete representations that can be used to estimate complex time-frequency
masks. MagBook layers extend classical sigmoidal units and a recently
introduced convex softmax activation for mask-based magnitude estimation.
Phasebook layers use a similar structure to give an estimate of the phase mask
without suffering from phase wrapping issues. Combook layers are an alternative
to the MagBook-Phasebook combination that directly estimate complex masks. We
present various training and inference regimes involving these representations,
and explain in particular how to include them in an end-to-end learning
framework. We also present an oracle study to assess upper bounds on
performance for various types of masks using discrete phase representations. We
evaluate the proposed methods on the wsj0-2mix dataset, a well-studied corpus
for single-channel speaker-independent speaker separation, matching the
performance of state-of-the-art mask-based approaches without requiring
additional phase reconstruction steps.
",Computer Science,Computer Science
"Jamming-Resistant Receivers for the Massive MIMO Uplink   We design a jamming-resistant receiver scheme to enhance the robustness of a
massive MIMO uplink system against jamming. We assume that a jammer attacks the
system both in the pilot and data transmission phases. The key feature of the
proposed scheme is that, in the pilot phase, we estimate not only the
legitimate channel, but also the jamming channel by exploiting a purposely
unused pilot sequence. The jamming channel estimate is used to constructed
linear receive filters that reject the impact of the jamming signal. The
performance of the proposed scheme is analytically evaluated using asymptotic
properties of massive MIMO. The optimal regularized zero-forcing receiver and
the optimal power allocation are also studied. Numerical results are provided
to verify our analysis and show that the proposed scheme greatly improves the
achievable rates, as compared to conventional receivers. Interestingly, the
proposed scheme works particularly well under strong jamming attacks, since the
improved estimate of the jamming channel outweighs the extra jamming power.
",Computer Science,Computer Science
"The list chromatic number of graphs with small clique number   We prove that every triangle-free graph with maximum degree $\Delta$ has list
chromatic number at most $(1+o(1))\frac{\Delta}{\ln \Delta}$. This matches the
best-known bound for graphs of girth at least 5. We also provide a new proof
that for any $r\geq 4$ every $K_r$-free graph has list-chromatic number at most
$200r\frac{\Delta\ln\ln\Delta}{\ln\Delta}$.
",Mathematics,Mathematics
"A robust RUV-testing procedure via gamma-divergence   Identification of differentially expressed genes (DE-genes) is commonly
conducted in modern biomedical researches. However, unwanted variation
inevitably arises during the data collection process, which could make the
detection results heavily biased. It is suggested to remove the unwanted
variation while keeping the biological variation to ensure a reliable analysis
result. Removing Unwanted Variation (RUV) is recently proposed for this purpose
by the virtue of negative control genes. On the other hand, outliers are
frequently appear in modern high-throughput genetic data that can heavily
affect the performances of RUV and its downstream analysis. In this work, we
propose a robust RUV-testing procedure via gamma-divergence. The advantages of
our method are twofold: (1) it does not involve any modeling for the outlier
distribution, which is applicable to various situations, (2) it is easy to
implement in the sense that its robustness is controlled by a single tuning
parameter gamma of gamma-divergence, and a data-driven criterion is developed
to select $\gamma$. In the Gender Study, our method can successfully remove
unwanted variation, and is able to identify more DE-genes than conventional
methods.
",Statistics,Statistics
"Bayesian Model-Agnostic Meta-Learning   Learning to infer Bayesian posterior from a few-shot dataset is an important
step towards robust meta-learning due to the model uncertainty inherent in the
problem. In this paper, we propose a novel Bayesian model-agnostic
meta-learning method. The proposed method combines scalable gradient-based
meta-learning with nonparametric variational inference in a principled
probabilistic framework. During fast adaptation, the method is capable of
learning complex uncertainty structure beyond a point estimate or a simple
Gaussian approximation. In addition, a robust Bayesian meta-update mechanism
with a new meta-loss prevents overfitting during meta-update. Remaining an
efficient gradient-based meta-learner, the method is also model-agnostic and
simple to implement. Experiment results show the accuracy and robustness of the
proposed method in various tasks: sinusoidal regression, image classification,
active learning, and reinforcement learning.
",Statistics,Statistics
"Transient behavior of the solutions to the second order difference equations by the renormalization method based on Newton-Maclaurin expansion   The renormalization method based on the Newton-Maclaurin expansion is applied
to study the transient behavior of the solutions to the difference equations as
they tend to the steady-states. The key and also natural step is to make the
renormalization equations to be continuous such that the elementary functions
can be used to describe the transient behavior of the solutions to difference
equations. As the concrete examples, we deal with the important second order
nonlinear difference equations with a small parameter. The result shows that
the method is more natural than the multi-scale method.
",Physics; Mathematics,Mathematics
"Emotion Controlled Spectrum Mobility Scheme for Efficient Syntactic Interoperability In Cognitive Radio Based Internet of Vehicles   Blind spots are one of the causes of road accidents in the hilly and flat
areas. These blind spot accidents can be decreased by establishing an Internet
of Vehicles (IoV) using Vehicle-2-Vehicle (V2V) and Vehicle-2-Infrastrtructure
(V2I) communication systems. But the problem with these IoV is that most of
them are using DSRC or single Radio Access Technology (RAT) as a wireless
technology, which has been proven to be failed for efficient communication
between vehicles. Recently, Cognitive Radio (CR) based IoV have to be proven
best wireless communication systems for vehicular networks. However, the
spectrum mobility is a challenging task to keep CR based vehicular networks
interoperable and has not been addressed sufficiently in existing research. In
our previous research work, the Cognitive Radio Site (CR-Site) has been
proposed as in-vehicle CR-device, which can be utilized to establish efficient
IoV systems. H In this paper, we have introduced the Emotions Inspired
Cognitive Agent (EIC_Agent) based spectrum mobility mechanism in CR-Site and
proposed a novel emotions controlled spectrum mobility scheme for efficient
syntactic interoperability between vehicles. For this purpose, a probabilistic
deterministic finite automaton using fear factor is proposed to perform
efficient spectrum mobility using fuzzy logic. In addition, the quantitative
computation of different fear intensity levels has been performed with the help
of fuzzy logic. The system has been tested using active data from different GSM
service providers on Mangla-Mirpur road. This is supplemented by extensive
simulation experiments which validate the proposed scheme for CR based
high-speed vehicular networks. The qualitative comparison with the
existing-state-of the-art has proven the superiority of the proposed emotions
controlled syntactic interoperable spectrum mobility scheme within cognitive
radio based IoV systems.
",Computer Science,Computer Science
"Pressure Induced Superconductivity in the New Compound ScZrCo1-$δ$   It is widely perceived that the correlation effect may play an important role
in several unconventional superconducting families, such as cuprate, iron-based
and heavy-fermion superconductors. The application of high pressure can tune
the ground state properties and balance the localization and itineracy of
electrons in correlated systems, which may trigger unconventional
superconductivity. Moreover, non-centrosymmetric structure may induce the spin
triplet pairing which is very rare in nature. Here, we report a new compound
ScZrCo1-${\delta}$ crystallizing in the Ti2Ni structure with the space group of
FD3-MS without a spatial inversion center. The resistivity of the material at
ambient pressure shows a bad metal and weak semiconducting behavior.
Furthermore, specific heat and magnetic susceptibility measurements yield a
rather large value of Wilson ratio ~4.47. Both suggest a ground state with
correlation effect. By applying pressure, the up-going behavior of resistivity
in lowering temperature at ambient pressure is suppressed and gradually it
becomes metallic. At a pressure of about 19.5 GPa superconductivity emerges. Up
to 36.05 GPa, a superconducting transition at about 3.6 K with a quite high
upper critical field is observed. Our discovery here provides a new platform
for investigating the relationship between correlation effect and
superconductivity.
",Physics,Physics
"Fast Global Convergence via Landscape of Empirical Loss   While optimizing convex objective (loss) functions has been a powerhouse for
machine learning for at least two decades, non-convex loss functions have
attracted fast growing interests recently, due to many desirable properties
such as superior robustness and classification accuracy, compared with their
convex counterparts. The main obstacle for non-convex estimators is that it is
in general intractable to find the optimal solution. In this paper, we study
the computational issues for some non-convex M-estimators. In particular, we
show that the stochastic variance reduction methods converge to the global
optimal with linear rate, by exploiting the statistical property of the
population loss. En route, we improve the convergence analysis for the batch
gradient method in \cite{mei2016landscape}.
",Statistics,Statistics
"Algebraic and logistic investigations on free lattices   Lorenzen's ""Algebraische und logistische Untersuchungen über freie
Verbände"" appeared in 1951 in The journal of symbolic logic. These
""Investigations"" have immediately been recognised as a landmark in the history
of infinitary proof theory, but their approach and method of proof have not
been incorporated into the corpus of proof theory. More precisely, Lorenzen
proves the admissibility of cut by double induction, on the cut formula and on
the complexity of the derivations, without using any ordinal assignment,
contrary to the presentation of cut elimination in most standard texts on proof
theory. This translation has the intent of giving a new impetus to their
reception.
The ""Investigations"" are best known for providing a constructive proof of
consistency for ramified type theory without axiom of reducibility. They do so
by showing that it is a part of a trivially consistent ""inductive calculus""
that describes our knowledge of arithmetic without detour. The proof resorts
only to the inductive definition of formulas and theorems.
They propose furthermore a definition of a semilattice, of a distributive
lattice, of a pseudocomplemented semilattice, and of a countably complete
boolean lattice as deductive calculuses, and show how to present them for
constructing the respective free object over a given preordered set.
This translation is published with the kind permission of Lorenzen's
daughter, Jutta Reinhardt.
",Mathematics,Mathematics
"City-Scale Intelligent Systems and Platforms   As of 2014, 54% of the earth's population resides in urban areas, and it is
steadily increasing, expecting to reach 66% by 2050. Urban areas range from
small cities with tens of thousands of people to megacities with greater than
10 million people. Roughly 12% of the global population today lives in 28
megacities, and at least 40 are projected by 2030. At these scales, the urban
infrastructure such as roads, buildings, and utility networks will cover areas
as large as New England. This steady urbanization and the resulting expansion
of infrastructure, combined with renewal of aging urban infrastructure,
represent tens of trillion of dollars in new urban infrastructure investment
over the coming decades. These investments must balance factors including
impact on clean air and water, energy and maintenance costs, and the
productivity and health of city dwellers. Moreover, cost-effective management
and sustainability of these growing urban areas will be one of the most
critical challenges to our society, motivating the concept of science- and
data-driven urban design, retrofit, and operation-that is, ""Smart Cities"".
",Computer Science,Computer Science
"The near-critical Gibbs measure of the branching random walk   Consider the supercritical branching random walk on the real line in the
boundary case and the associated Gibbs measure $\nu_{n,\beta}$ on the
$n^\text{th}$ generation, which is also the polymer measure on a disordered
tree with inverse temperature $\beta$. The convergence of the partition
function $W_{n,\beta}$, after rescaling, towards a nontrivial limit has been
proved by A\""{\i}dékon and Shi in the critical case $\beta = 1$ and by
Madaule when $\beta >1$. We study here the near-critical case, where $\beta_n
\to 1$, and prove the convergence of $W_{n,\beta_n}$, after rescaling, towards
a constant multiple of the limit of the derivative martingale. Moreover,
trajectories of particles chosen according to the Gibbs measure $\nu_{n,\beta}$
have been studied by Madaule in the critical case, with convergence towards the
Brownian meander, and by Chen, Madaule and Mallein in the strong disorder
regime, with convergence towards the normalized Brownian excursion. We prove
here the convergence for trajectories of particles chosen according to the
near-critical Gibbs measure and display continuous families of processes from
the meander to the excursion or to the Brownian motion.
",Mathematics,Mathematics
"On the Genus of the Moonshine Module   We provide a novel and simple description of Schellekens' seventy-one affine
Kac-Moody structures of self-dual vertex operator algebras of central charge 24
by utilizing cyclic subgroups of the glue codes of the Niemeier lattices with
roots. We also discuss a possible uniform construction procedure of the
self-dual vertex operator algebras of central charge 24 starting from the Leech
lattice. This also allows us to consider the uniqueness question for all
non-trivial affine Kac-Moody structures. We finally discuss our description
from a Lorentzian viewpoint.
",Mathematics,Physics
"TF Boosted Trees: A scalable TensorFlow based framework for gradient boosting   TF Boosted Trees (TFBT) is a new open-sourced frame-work for the distributed
training of gradient boosted trees. It is based on TensorFlow, and its
distinguishing features include a novel architecture, automatic loss
differentiation, layer-by-layer boosting that results in smaller ensembles and
faster prediction, principled multi-class handling, and a number of
regularization techniques to prevent overfitting.
",Computer Science; Statistics,Computer Science; Statistics
"Multiprocessor Approximate Message Passing with Column-Wise Partitioning   Solving a large-scale regularized linear inverse problem using multiple
processors is important in various real-world applications due to the
limitations of individual processors and constraints on data sharing policies.
This paper focuses on the setting where the matrix is partitioned column-wise.
We extend the algorithmic framework and the theoretical analysis of approximate
message passing (AMP), an iterative algorithm for solving linear inverse
problems, whose asymptotic dynamics are characterized by state evolution (SE).
In particular, we show that column-wise multiprocessor AMP (C-MP-AMP) obeys an
SE under the same assumptions when the SE for AMP holds. The SE results imply
that (i) the SE of C-MP-AMP converges to a state that is no worse than that of
AMP and (ii) the asymptotic dynamics of C-MP-AMP and AMP can be identical.
Moreover, for a setting that is not covered by SE, numerical results show that
damping can improve the convergence performance of C-MP-AMP.
",Computer Science; Mathematics,Computer Science
"Face Deidentification with Generative Deep Neural Networks   Face deidentification is an active topic amongst privacy and security
researchers. Early deidentification methods relying on image blurring or
pixelization were replaced in recent years with techniques based on formal
anonymity models that provide privacy guaranties and at the same time aim at
retaining certain characteristics of the data even after deidentification. The
latter aspect is particularly important, as it allows to exploit the
deidentified data in applications for which identity information is irrelevant.
In this work we present a novel face deidentification pipeline, which ensures
anonymity by synthesizing artificial surrogate faces using generative neural
networks (GNNs). The generated faces are used to deidentify subjects in images
or video, while preserving non-identity-related aspects of the data and
consequently enabling data utilization. Since generative networks are very
adaptive and can utilize a diverse set of parameters (pertaining to the
appearance of the generated output in terms of facial expressions, gender,
race, etc.), they represent a natural choice for the problem of face
deidentification. To demonstrate the feasibility of our approach, we perform
experiments using automated recognition tools and human annotators. Our results
show that the recognition performance on deidentified images is close to
chance, suggesting that the deidentification process based on GNNs is highly
effective.
",Computer Science,Computer Science
"A parallel orbital-updating based plane-wave basis method for electronic structure calculations   Motivated by the recently proposed parallel orbital-updating approach in real
space method, we propose a parallel orbital-updating based plane-wave basis
method for electronic structure calculations, for solving the corresponding
eigenvalue problems. In addition, we propose two new modified parallel
orbital-updating methods. Compared to the traditional plane-wave methods, our
methods allow for two-level parallelization, which is particularly interesting
for large scale parallelization. Numerical experiments show that these new
methods are more reliable and efficient for large scale calculations on modern
supercomputers
",Physics; Mathematics,Computer Science
"Generative Mixture of Networks   A generative model based on training deep architectures is proposed. The
model consists of K networks that are trained together to learn the underlying
distribution of a given data set. The process starts with dividing the input
data into K clusters and feeding each of them into a separate network. After
few iterations of training networks separately, we use an EM-like algorithm to
train the networks together and update the clusters of the data. We call this
model Mixture of Networks. The provided model is a platform that can be used
for any deep structure and be trained by any conventional objective function
for distribution modeling. As the components of the model are neural networks,
it has high capability in characterizing complicated data distributions as well
as clustering data. We apply the algorithm on MNIST hand-written digits and
Yale face datasets. We also demonstrate the clustering ability of the model
using some real-world and toy examples.
",Computer Science; Statistics,Computer Science; Statistics
"Computational complexity, torsion-freeness of homoclinic Floer homology, and homoclinic Morse inequalities   Floer theory was originally devised to estimate the number of 1-periodic
orbits of Hamiltonian systems. In earlier works, we constructed Floer homology
for homoclinic orbits on two dimensional manifolds using combinatorial
techniques. In the present paper, we study theoretic aspects of computational
complexity of homoclinic Floer homology. More precisely, for finding the
homoclinic points and immersions that generate the homology and its boundary
operator, we establish sharp upper bounds in terms of iterations of the
underlying symplectomorphism. This prepares the ground for future numerical
works.
Although originally aimed at numerics, the above bounds provide also purely
algebraic applications, namely
1) Torsion-freeness of primary homoclinic Floer homology.
2) Morse type inequalities for primary homoclinic orbits.
",Mathematics,Mathematics
"Differentially Private High Dimensional Sparse Covariance Matrix Estimation   In this paper, we study the problem of estimating the covariance matrix under
differential privacy, where the underlying covariance matrix is assumed to be
sparse and of high dimensions. We propose a new method, called DP-Thresholding,
to achieve a non-trivial $\ell_2$-norm based error bound, which is
significantly better than the existing ones from adding noise directly to the
empirical covariance matrix. We also extend the $\ell_2$-norm based error bound
to a general $\ell_w$-norm based one for any $1\leq w\leq \infty$, and show
that they share the same upper bound asymptotically. Our approach can be easily
extended to local differential privacy. Experiments on the synthetic datasets
show consistent results with our theoretical claims.
",Computer Science; Statistics,Computer Science; Mathematics; Statistics
"Influence maximization on correlated networks through community identification   The identification of the minimal set of nodes that maximizes the propagation
of information is one of the most important problems in network science. In
this paper, we introduce a new method to find the set of initial spreaders to
maximize the information propagation in complex networks. We evaluate this
method in assortative networks and verify that degree-degree correlation plays
a fundamental role on the spreading dynamics. Simulation results show that our
algorithm is statistically similar, in terms of the average size of outbreaks,
to the greedy approach. However, our method is much less time consuming than
the greedy algorithm.
",Computer Science; Physics,Computer Science; Physics
"Higher-rank graph algebras are iterated Cuntz-Pimsner algebras   Given a finitely aligned $k$-graph $\Lambda$, we let $\Lambda^i$ denote the
$(k-1)$-graph formed by removing all edges of degree $e_i$ from $\Lambda$. We
show that the Toeplitz-Cuntz-Krieger algebra of $\Lambda$, denoted by
$\mathcal{T}C^*(\Lambda)$, may be realised as the Toeplitz algebra of a Hilbert
$\mathcal{T}C^*(\Lambda^i)$-bimodule. When $\Lambda$ is locally-convex, we show
that the Cuntz-Krieger algebra of $\Lambda$, which we denote by $C^*(\Lambda)$,
may be realised as the Cuntz-Pimsner algebra of a Hilbert
$C^*(\Lambda^i)$-bimodule. Consequently, $\mathcal{T}C^*(\Lambda)$ and
$C^*(\Lambda)$ may be viewed as iterated Toeplitz and iterated Cuntz-Pimsner
algebras over $c_0(\Lambda^0)$ respectively.
",Mathematics,Mathematics
"Population splitting of rodlike swimmers in Couette flow   We present a quantitative analysis on the response of a dilute active
suspension of self-propelled rods (swimmers) in a planar channel subjected to
an imposed shear flow. To best capture the salient features of shear-induced
effects, we consider the case of an imposed Couette flow, providing a constant
shear rate across the channel. We argue that the steady-state behavior of
swimmers can be understood in the light of a population splitting phenomenon,
occurring as the shear rate exceeds a certain threshold, initiating the
reversal of swimming direction for a finite fraction of swimmers from down- to
upstream or vice versa, depending on swimmer position within the channel.
Swimmers thus split into two distinct, statistically significant and oppositely
swimming majority and minority populations. The onset of population splitting
translates into a transition from a self-propulsion-dominated regime to a
shear-dominated regime, corresponding to a unimodal-to-bimodal change in the
probability distribution function of the swimmer orientation. We present a
phase diagram in terms of the swim and flow Peclet numbers showing the
separation of these two regimes by a discontinuous transition line. Our results
shed further light on the behavior of swimmers in a shear flow and provide an
explanation for the previously reported non-monotonic behavior of the mean,
near-wall, parallel-to-flow orientation of swimmers with increasing shear
strength.
",Physics,Physics
"Connection between Fermi contours of zero-field electrons and $ν=\frac12$ composite fermions in two-dimensional systems   We investigate the relation between the Fermi sea (FS) of zero-field carriers
in two-dimensional systems and the FS of the corresponding composite fermions
which emerge in a high magnetic field at filling $\nu = \frac{1}{2}$, as the
kinetic energy dispersion is varied. We study cases both with and without
rotational symmetry, and find that there is generally no straightforward
relation between the geometric shapes and topologies of the two FSs. In
particular, we show analytically that the composite Fermi liquid (CFL) is
completely insensitive to a wide range of changes to the zero-field dispersion
which preserve rotational symmetry, including ones that break the zero-field FS
into multiple disconnected pieces. In the absence of rotational symmetry, we
show that the notion of `valley pseudospin' in many-valley systems is
generically not transferred to the CFL, in agreement with experimental
observations. We also discuss how a rotationally symmetric band structure can
induce a reordering of the Landau levels, opening interesting possibilities of
observing higher-Landau-level physics in the high-field regime.
",Physics,Physics
"Optimized Bacteria are Environmental Prediction Engines   Experimentalists have observed phenotypic variability in isogenic bacteria
populations. We explore the hypothesis that in fluctuating environments this
variability is tuned to maximize a bacterium's expected log growth rate,
potentially aided by epigenetic markers that store information about past
environments. We show that, in a complex, memoryful environment, the maximal
expected log growth rate is linear in the instantaneous predictive
information---the mutual information between a bacterium's epigenetic markers
and future environmental states. Hence, under resource constraints, optimal
epigenetic markers are causal states---the minimal sufficient statistics for
prediction. This is the minimal amount of information about the past needed to
predict the future as well as possible. We suggest new theoretical
investigations into and new experiments on bacteria phenotypic bet-hedging in
fluctuating complex environments.
",Quantitative Biology,Computer Science; Statistics
"The Motivic Cofiber of $τ$   Consider the Tate twist $\tau \in H^{0,1}(S^{0,0})$ in the mod 2 cohomology
of the motivic sphere. After 2-completion, the motivic Adams spectral sequence
realizes this element as a map $\tau \colon S^{0,-1} \to S^{0,0}$, with cofiber
$C\tau$. We show that this motivic 2-cell complex can be endowed with a unique
$E_{\infty}$ ring structure. Moreover, this promotes the known isomorphism
$\pi_{\ast,\ast} C\tau \cong
\mathrm{Ext}^{\ast,\ast}_{BP_{\ast}BP}(BP_{\ast},BP_{\ast})$ to an isomorphism
of rings which also preserves higher products.
We then consider the closed symmetric monoidal category $({
}_{C\tau}\textbf{Mod}, - \wedge_{C\tau} -)$ which lives in the kernel of Betti
realization. Given a motivic spectrum $X$, the $C\tau$-induced spectrum $X
\wedge C\tau$ is usually better behaved and easier to understand than $X$
itself. We specifically illustrate this concept in the examples of the mod 2
Eilenberg-Maclane spectrum $H\mathbb{F}_2$, the mod 2 Moore spectrum
$S^{0,0}/2$ and the connective hermitian $K$-theory spectrum $kq$.
",Mathematics,Mathematics
"A novel quantum dynamical approach in electron microscopy combining wave-packet propagation with Bohmian trajectories   The numerical analysis of the diffraction features rendered by transmission
electron microscopy (TEM) typically relies either on classical approximations
(Monte Carlo simulations) or quantum paraxial tomography (the multislice method
and any of its variants). Although numerically advan- tageous (relatively
simple implementations and low computational costs), they involve important
approximations and thus their range of applicability is limited. To overcome
such limitations, an alternative, more general approach is proposed, based on
an optimal combination of wave-packet propagation with the on-the-fly
computation of associated Bohmian trajectories. For the sake of clarity, but
without loss of generality, the approach is used to analyze the diffraction of
an electron beam by a thin aluminum slab as a function of three different
incidence (work) conditions which are of interest in electron microscopy: the
probe width, the tilting angle, and the beam energy. Specifically, it is shown
that, because there is a dependence on particular thresholds of the beam
energy, this approach provides a clear description of the diffraction process
at any energy, revealing at the same time any diversion of the beam inside the
material towards directions that cannot be accounted for by other conventional
methods, which is of much interest when dealing with relatively low energies
and/or relatively large tilting angles.
",Physics,Physics
"Weighted $1\times1$ cut-and-project sets in bounded distance to a lattice   Recent results of Grepstad and Lev are used to show that weighted
cut-and-project sets with one-dimensional physical space and one-dimensional
internal space are bounded distance equivalent to some lattice if the weight
function $h$ is continuous on the internal space, and if $h$ is either
piecewise linear, or twice differentiable with bounded curvature.
",Mathematics,Mathematics
"Persistent Monitoring of Stochastic Spatio-temporal Phenomena with a Small Team of Robots   This paper presents a solution for persistent monitoring of real-world
stochastic phenomena, where the underlying covariance structure changes sharply
across time, using a small number of mobile robot sensors. We propose an
adaptive solution for the problem where stochastic real-world dynamics are
modeled as a Gaussian Process (GP). The belief on the underlying covariance
structure is learned from recently observed dynamics as a Gaussian Mixture (GM)
in the low-dimensional hyper-parameters space of the GP and adapted across time
using Sequential Monte Carlo methods. Each robot samples a belief point from
the GM and locally optimizes a set of informative regions by greedy
maximization of the submodular entropy function. The key contributions of this
paper are threefold: adapting the belief on the covariance using Markov Chain
Monte Carlo (MCMC) sampling such that particles survive even under sharp
covariance changes across time; exploiting the belief to transform the problem
of entropy maximization into a decentralized one; and developing an
approximation algorithm to maximize entropy on a set of informative regions in
the continuous space. We illustrate the application of the proposed solution
through extensive simulations using an artificial dataset and multiple real
datasets from fixed sensor deployments, and compare it to three competing
state-of-the-art approaches.
",Computer Science; Statistics,Computer Science
"On 2-level polytopes arising in combinatorial settings   2-level polytopes naturally appear in several areas of pure and applied
mathematics, including combinatorial optimization, polyhedral combinatorics,
communication complexity, and statistics. In this paper, we present a study of
some 2-level polytopes arising in combinatorial settings. Our first
contribution is proving that v(P)*f(P) is upper bounded by d*2^(d+1), for a
large collection of families of such polytopes P. Here v(P) (resp. f(P)) is the
number of vertices (resp. facets) of P, and d is its dimension. Whether this
holds for all 2-level polytopes was asked in [Bohn et al., ESA 2015], and
experimental results from [Fiorini et al., ISCO 2016] showed it true up to
dimension 7. The key to most of our proofs is a deeper understanding of the
relations among those polytopes and their underlying combinatorial structures.
This leads to a number of results that we believe to be of independent
interest: a trade-off formula for the number of cliques and stable sets in a
graph; a description of stable matching polytopes as affine projections of
certain order polytopes; and a linear-size description of the base polytope of
matroids that are 2-level in terms of cuts of an associated tree.
",Computer Science; Mathematics,Computer Science; Mathematics
"Mapping stable direct and retrograde orbits around the triple system of asteroids (45) Eugenia   It is well accepted that knowing the composition and the orbital evolution of
asteroids may help us to understand the process of formation of the Solar
System. It is also known that asteroids can represent a threat to our planet.
Such important role made space missions to asteroids a very popular topic in
the current astrodynamics and astronomy studies. By taking into account the
increasingly interest in space missions to asteroids, especially to multiple
systems, we present a study aimed to characterize the stable and unstable
regions around the triple system of asteroids (45) Eugenia. The goal is to
characterize unstable and stable regions of this system and compare with the
system 2001 SN263 - the target of the ASTER mission. Besides, Prado (2014) used
a new concept for mapping orbits considering the disturbance received by the
spacecraft from all the perturbing forces individually. This method was also
applied to (45) Eugenia. We present the stable and unstable regions for
particles with relative inclination between 0 and 180 degrees. We found that
(45) Eugenia presents larger stable regions for both, prograde and retrograde
cases. This is mainly because the satellites of this system are small when
compared to the primary body, and because they are not so close to each other.
We also present a comparison between those two triple systems, and a discussion
on how these results may guide us in the planning of future missions.
",Physics,Physics
"Attacking Similarity-Based Link Prediction in Social Networks   Link prediction is one of the fundamental problems in computational social
science. A particularly common means to predict existence of unobserved links
is via structural similarity metrics, such as the number of common neighbors;
node pairs with higher similarity are thus deemed more likely to be linked.
However, a number of applications of link prediction, such as predicting links
in gang or terrorist networks, are adversarial, with another party incentivized
to minimize its effectiveness by manipulating observed information about the
network. We offer a comprehensive algorithmic investigation of the problem of
attacking similarity-based link prediction through link deletion, focusing on
two broad classes of such approaches, one which uses only local information
about target links, and another which uses global network information. While we
show several variations of the general problem to be NP-Hard for both local and
global metrics, we exhibit a number of well-motivated special cases which are
tractable. Additionally, we provide principled and empirically effective
algorithms for the intractable cases, in some cases proving worst-case
approximation guarantees.
",Computer Science,Computer Science; Statistics
"Emergent SU(N) symmetry in disordered SO(N) spin chains   Strongly disordered spin chains invariant under the SO(N) group are shown to
display antiferromagnetic phases with emergent SU(N) symmetry without fine
tuning. The phases with emergent SU(N) symmetry are of two kinds: one has a
ground state formed of randomly distributed singlets of strongly bound pairs of
SO(N) spins (the `mesonic' phase), while the other has a ground state composed
of singlets made out of strongly bound integer multiples of N SO(N) spins (the
`baryonic' phase). Although the mechanism is general, we argue that the cases
N=2,3,4 and 6 can in principle be realized with the usual spin and orbital
degrees of freedom.
",Physics,Physics
"Improved Accounting for Differentially Private Learning   We consider the problem of differential privacy accounting, i.e. estimation
of privacy loss bounds, in machine learning in a broad sense. We propose two
versions of a generic privacy accountant suitable for a wide range of learning
algorithms. Both versions are derived in a simple and principled way using
well-known tools from probability theory, such as concentration inequalities.
We demonstrate that our privacy accountant is able to achieve state-of-the-art
estimates of DP guarantees and can be applied to new areas like variational
inference. Moreover, we show that the latter enjoys differential privacy at
minor cost.
",Computer Science; Statistics,Computer Science; Statistics
"Dynamic Transition in Symbiotic Evolution Induced by Growth Rate Variation   In a standard bifurcation of a dynamical system, the stationary points (or
more generally attractors) change qualitatively when varying a control
parameter. Here we describe a novel unusual effect, when the change of a
parameter, e.g. a growth rate, does not influence the stationary states, but
nevertheless leads to a qualitative change of dynamics. For instance, such a
dynamic transition can be between the convergence to a stationary state and a
strong increase without stationary states, or between the convergence to one
stationary state and that to a different state. This effect is illustrated for
a dynamical system describing two symbiotic populations, one of which exhibits
a growth rate larger than the other one. We show that, although the stationary
states of the dynamical system do not depend on the growth rates, the latter
influence the boundary of the basins of attraction. This change of the basins
of attraction explains this unusual effect of the quantitative change of
dynamics by growth rate variation.
",Physics,Quantitative Biology
"Activation of Microwave Fields in a Spin-Torque Nano-Oscillator by Neuronal Action Potentials   Action potentials are the basic unit of information in the nervous system and
their reliable detection and decoding holds the key to understanding how the
brain generates complex thought and behavior. Transducing these signals into
microwave field oscillations can enable wireless sensors that report on brain
activity through magnetic induction. In the present work we demonstrate that
action potentials from crayfish lateral giant neuron can trigger microwave
oscillations in spin-torque nano-oscillators. These nanoscale devices take as
input small currents and convert them to microwave current oscillations that
can wirelessly broadcast neuronal activity, opening up the possibility for
compact neuro-sensors. We show that action potentials activate microwave
oscillations in spin-torque nano-oscillators with an amplitude that follows the
action potential signal, demonstrating that the device has both the sensitivity
and temporal resolution to respond to action potentials from a single neuron.
The activation of magnetic oscillations by action potentials, together with the
small footprint and the high frequency tunability, makes these devices
promising candidates for high resolution sensing of bioelectric signals from
neural tissues. These device attributes may be useful for design of
high-throughput bi-directional brain-machine interfaces.
",Physics,Quantitative Biology
"Network Design with Probabilistic Capacities   We consider a network design problem with random arc capacities and give a
formulation with a probabilistic capacity constraint on each cut of the
network. To handle the exponentially-many probabilistic constraints a
separation procedure that solves a nonlinear minimum cut problem is introduced.
For the case with independent arc capacities, we exploit the supermodularity of
the set function defining the constraints and generate cutting planes based on
the supermodular covering knapsack polytope. For the general correlated case,
we give a reformulation of the constraints that allows to uncover and utilize
the submodularity of a related function. The computational results indicate
that exploiting the underlying submodularity and supermodularity arising with
the probabilistic constraints provides significant advantages over the
classical approaches.
",Mathematics,Computer Science
"Master equation for She-Leveque scaling and its classification in terms of other Markov models of developed turbulence   We derive the Markov process equivalent to She-Leveque scaling in homogeneous
and isotropic turbulence. The Markov process is a jump process for velocity
increments $u(r)$ in scale $r$ in which the jumps occur randomly but with
deterministic width in $u$. From its master equation we establish a
prescription to simulate the She-Leveque process and compare it with Kolmogorov
scaling. To put the She-Leveque process into the context of other established
turbulence models on the Markov level, we derive a diffusion process for $u(r)$
from two properties of the Navier-Stokes equation. This diffusion process
already includes Kolmogorov scaling, extended self-similarity and a class of
random cascade models. The fluctuation theorem of this Markov process implies a
""second law"" that puts a loose bound on the multipliers of the random cascade
models. This bound explicitly allows for inverse cascades, which are necessary
to satisfy the fluctuation theorem. By adding a jump process to the diffusion
process, we go beyond Kolmogorov scaling and formulate the most general scaling
law for the class of Markov processes having both diffusion and jump parts.
This Markov scaling law includes She-Leveque scaling and a scaling law derived
by Yakhot.
",Physics,Mathematics
"Iterated failure rate monotonicity and ordering relations within Gamma and Weibull distributions   Stochastic ordering of distributions of random variables may be defined by
the relative convexity of the tail functions. This has been extended to higher
order stochastic orderings, by iteratively reassigning tail-weights. The actual
verification of those stochastic orderings is not simple, as this depends on
inverting distribution functions for which there may be no explicit expression.
The iterative definition of distributions, of course, contributes to make that
verification even harder. We have a look at the stochastic ordering,
introducing a method that allows for explicit usage, applying it to the Gamma
and Weibull distributions, giving a complete description of the order relations
within each of those families.
",Mathematics; Statistics,Mathematics; Statistics
"Existence and uniqueness of steady weak solutions to the Navier-Stokes equations in $\mathbb{R}^2$   The existence of weak solutions to the stationary Navier-Stokes equations in
the whole plane $\mathbb{R}^2$ is proven. This particular geometry was the only
case left open since the work of Leray in 1933. The reason is that due to the
absence of boundaries the local behavior of the solutions cannot be controlled
by the enstrophy in two dimensions. We overcome this difficulty by constructing
approximate weak solutions having a prescribed mean velocity on some given
bounded set. As a corollary, we obtain infinitely many weak solutions in
$\mathbb{R}^2$ parameterized by this mean velocity, which is reminiscent of the
expected convergence of the velocity field at large distances to any prescribed
constant vector field. This explicit parameterization of the weak solutions
allows us to prove a weak-strong uniqueness theorem for small data. The
question of the asymptotic behavior of the weak solutions remains however open,
when the uniqueness theorem doesn't apply.
",Mathematics,Mathematics
"The Simulator: Understanding Adaptive Sampling in the Moderate-Confidence Regime   We propose a novel technique for analyzing adaptive sampling called the {\em
Simulator}. Our approach differs from the existing methods by considering not
how much information could be gathered by any fixed sampling strategy, but how
difficult it is to distinguish a good sampling strategy from a bad one given
the limited amount of data collected up to any given time. This change of
perspective allows us to match the strength of both Fano and change-of-measure
techniques, without succumbing to the limitations of either method. For
concreteness, we apply our techniques to a structured multi-arm bandit problem
in the fixed-confidence pure exploration setting, where we show that the
constraints on the means imply a substantial gap between the
moderate-confidence sample complexity, and the asymptotic sample complexity as
$\delta \to 0$ found in the literature. We also prove the first instance-based
lower bounds for the top-k problem which incorporate the appropriate
log-factors. Moreover, our lower bounds zero-in on the number of times each
\emph{individual} arm needs to be pulled, uncovering new phenomena which are
drowned out in the aggregate sample complexity. Our new analysis inspires a
simple and near-optimal algorithm for the best-arm and top-k identification,
the first {\em practical} algorithm of its kind for the latter problem which
removes extraneous log factors, and outperforms the state-of-the-art in
experiments.
",Computer Science; Statistics,Computer Science; Statistics
"Common Knowledge in a Logic of Gossips   Gossip protocols aim at arriving, by means of point-to-point or group
communications, at a situation in which all the agents know each other secrets.
Recently a number of authors studied distributed epistemic gossip protocols.
These protocols use as guards formulas from a simple epistemic logic, which
makes their analysis and verification substantially easier.
We study here common knowledge in the context of such a logic. First, we
analyze when it can be reduced to iterated knowledge. Then we show that the
semantics and truth for formulas without nested common knowledge operator are
decidable. This implies that implementability, partial correctness and
termination of distributed epistemic gossip protocols that use non-nested
common knowledge operator is decidable, as well. Given that common knowledge is
equivalent to an infinite conjunction of nested knowledge, these results are
non-trivial generalizations of the corresponding decidability results for the
original epistemic logic, established in (Apt & Wojtczak, 2016).
K. R. Apt & D. Wojtczak (2016): On Decidability of a Logic of Gossips. In
Proc. of JELIA 2016, pp. 18-33, doi:10.1007/ 978-3-319-48758-8_2.
",Computer Science,Computer Science
"Time-delay signature suppression in a chaotic semiconductor laser by fiber random grating induced distributed feedback   We demonstrate that a semiconductor laser perturbed by the distributed
feedback from a fiber random grating can emit light chaotically without the
time delay signature. A theoretical model is developed based on the
Lang-Kobayashi model in order to numerically explore the chaotic dynamics of
the laser diode subjected to the random distributed feedback. It is predicted
that the random distributed feedback is superior to the single reflection
feedback in suppressing the time-delay signature. In experiments, a massive
number of feedbacks with randomly varied time delays induced by a fiber random
grating introduce large numbers of external cavity modes into the semiconductor
laser, leading to the high dimension of chaotic dynamics and thus the
concealment of the time delay signature. The obtained time delay signature with
the maximum suppression is 0.0088, which is the smallest to date.
",Physics,Physics
"On the mapping of Points of Interest through StreetView imagery and paid crowdsourcing   The use of volunteers has emerged as low-cost alternative to generate
accurate geographical information, an approach known as Volunteered Geographic
Information (VGI). However, VGI is limited by the number and availability of
volunteers in the area to be mapped, hindering scalability for large areas and
making difficult to map within a time-frame. Fortunately, the availability of
street-view imagery enables the virtual exploration of urban environments,
making possible the recruitment of contributors not necessarily located in the
area to be mapped. In this paper, we describe the design, implementation, and
evaluation of the Virtual City Explorer (VCE), a system to collect the
coordinates of Points of Interest within a bounded area on top of a street view
service with the use of paid crowdworkers. Our evaluation suggests that paid
crowdworkers are effective for finding PoIs, and cover almost all the area.
With respect to completeness, our approach does not find all PoIs found by
experts or VGI communities, but is able to find PoIs that were not found by
them, suggesting complementarity. We also studied the impact of making PoIs
already discovered by a certain number of workers \emph{taboo} for incoming
workers, finding that it encourages more exploration from workers , increase
the number of detected PoIs , and reduce costs.
",Computer Science,Computer Science
"A New Achievable Rate Region for Multiple-Access Channel with States   The problem of reliable communication over the multiple-access channel (MAC)
with states is investigated. We propose a new coding scheme for this problem
which uses quasi-group codes (QGC). We derive a new computable single-letter
characterization of the achievable rate region. As an example, we investigate
the problem of doubly-dirty MAC with modulo-$4$ addition. It is shown that the
sum-rate $R_1+R_2=1$ bits per channel use is achievable using the new scheme.
Whereas, the natural extension of the Gel'fand-Pinsker scheme, sum-rates
greater than $0.32$ are not achievable.
",Computer Science,Computer Science
"An explicit determination of the $K$-theoretic structure constants of the affine Grassmannian associated to $SL_2$   Let $G:=\widehat{SL_2}$ denote the affine Kac-Moody group associated to
$SL_2$ and $\bar{\mathcal{X}}$ the associated affine Grassmannian. We determine
an inductive formula for the Schubert basis structure constants in the
torus-equivariant Grothendieck group of $\bar{\mathcal{X}}$. In the case of
ordinary (non-equivariant) $K$-theory we find an explicit closed form for the
structure constants. We also determine an inductive formula for the structure
constants in the torus-equivariant cohomology ring, and use this formula to
find closed forms for some of the structure constants.
",Mathematics,Mathematics
"From Monte Carlo to Las Vegas: Improving Restricted Boltzmann Machine Training Through Stopping Sets   We propose a Las Vegas transformation of Markov Chain Monte Carlo (MCMC)
estimators of Restricted Boltzmann Machines (RBMs). We denote our approach
Markov Chain Las Vegas (MCLV). MCLV gives statistical guarantees in exchange
for random running times. MCLV uses a stopping set built from the training data
and has maximum number of Markov chain steps K (referred as MCLV-K). We present
a MCLV-K gradient estimator (LVS-K) for RBMs and explore the correspondence and
differences between LVS-K and Contrastive Divergence (CD-K), with LVS-K
significantly outperforming CD-K training RBMs over the MNIST dataset,
indicating MCLV to be a promising direction in learning generative models.
",Computer Science; Statistics,Statistics
"Nichols Algebras and Quantum Principal Bundles   A general procedure for constructing Yetter-Drinfeld modules from quantum
principal bundles is introduced. As an application a Yetter-Drinfeld structure
is put on the cotangent space of the Heckenberger-Kolb calculi of the quantum
Grassmannians. For the special case of quantum projective space the associated
braiding is shown to be non-diagonal and of Hecke type. Moreover, its Nichols
algebra is shown to be finite-dimensional and equal to the anti-holomorphic
part of the total differential calculus.
",Mathematics,Mathematics
"Stochastic Generative Hashing   Learning-based binary hashing has become a powerful paradigm for fast search
and retrieval in massive databases. However, due to the requirement of discrete
outputs for the hash functions, learning such functions is known to be very
challenging. In addition, the objective functions adopted by existing hashing
techniques are mostly chosen heuristically. In this paper, we propose a novel
generative approach to learn hash functions through Minimum Description Length
principle such that the learned hash codes maximally compress the dataset and
can also be used to regenerate the inputs. We also develop an efficient
learning algorithm based on the stochastic distributional gradient, which
avoids the notorious difficulty caused by binary output constraints, to jointly
optimize the parameters of the hash function and the associated generative
model. Extensive experiments on a variety of large-scale datasets show that the
proposed method achieves better retrieval results than the existing
state-of-the-art methods.
",Computer Science; Statistics,Computer Science; Statistics
"A time series distance measure for efficient clustering of input output signals by their underlying dynamics   Starting from a dataset with input/output time series generated by multiple
deterministic linear dynamical systems, this paper tackles the problem of
automatically clustering these time series. We propose an extension to the
so-called Martin cepstral distance, that allows to efficiently cluster these
time series, and apply it to simulated electrical circuits data. Traditionally,
two ways of handling the problem are used. The first class of methods employs a
distance measure on time series (e.g. Euclidean, Dynamic Time Warping) and a
clustering technique (e.g. k-means, k-medoids, hierarchical clustering) to find
natural groups in the dataset. It is, however, often not clear whether these
distance measures effectively take into account the specific temporal
correlations in these time series. The second class of methods uses the
input/output data to identify a dynamic system using an identification scheme,
and then applies a model norm-based distance (e.g. H2, H-infinity) to find out
which systems are similar. This, however, can be very time consuming for large
amounts of long time series data. We show that the new distance measure
presented in this paper performs as good as when every input/output pair is
modelled explicitly, but remains computationally much less complex. The
complexity of calculating this distance between two time series of length N is
O(N logN).
",Computer Science; Statistics,Computer Science; Statistics
"Oblivious Routing via Random Walks   We present novel oblivious routing algorithms for both splittable and
unsplittable multicommodity flow. Our algorithm for minimizing congestion for
\emph{unsplittable} multicommodity flow is the first oblivious routing
algorithm for this setting. As an intermediate step towards this algorithm, we
present a novel generalization of Valiant's classical load balancing scheme for
packet-switched networks to arbitrary graphs, which is of independent interest.
Our algorithm for minimizing congestion for \emph{splittable} multicommodity
flow improves upon the state-of-the-art, in terms of both running time and
performance, for graphs that exhibit good expansion guarantees. Our algorithms
rely on diffusing traffic via iterative applications of the random walk
operator. Consequently, the performance guarantees of our algorithms are
derived from the convergence of the random walk operator to the stationary
distribution and are expressed in terms of the spectral gap of the graph (which
dominates the mixing time).
",Computer Science,Computer Science
"A novel approach to the Lindelöf hypothesis   Lindel{ö}f's hypothesis, one of the most important open problems in the
history of mathematics, states that for large $t$, Riemann's zeta function
$\zeta(\frac{1}{2}+it)$ is of order $O(t^{\varepsilon})$ for any
$\varepsilon>0$. It is well known that for large $t$, the leading order
asymptotics of the Riemann zeta function can be expressed in terms of a
transcendental exponential sum. The usual approach to the Lindelöf hypothesis
involves the use of ingenious techniques for the estimation of this sum.
However, since such estimates can not yield an asymptotic formula for the above
sum, it appears that this approach cannot lead to the proof of the Lindelöf
hypothesis. Here, a completely different approach is introduced: the Riemann
zeta function is embedded in a classical problem in the theory of complex
analysis known as a Riemann-Hilbert problem, and then, the large
$t$-asymptotics of the associated integral equation is formally computed. This
yields two different results. First, the formal proof that a certain Riemann
zeta-type double exponential sum satisfies the asymptotic estimate of the
Lindelöf hypothesis. Second, it is formally shown that the sum of
$|\zeta(1/2+it)|^2$ and of a certain sum which depends on $\epsilon$, satisfies
for large $t$ the estimate of the Lindelöf hypothesis. Hence, since the above
identity is valid for all $\epsilon$, this asymptotic identity suggests the
validity of Lindelöf's hypothesis. The completion of the rigorous derivation
of the above results will be presented in a companion paper.
",Mathematics,Mathematics
"An intracardiac electrogram model to bridge virtual hearts and implantable cardiac devices   Virtual heart models have been proposed to enhance the safety of implantable
cardiac devices through closed loop validation. To communicate with a virtual
heart, devices have been driven by cardiac signals at specific sites. As a
result, only the action potentials of these sites are sensed. However, the real
device implanted in the heart will sense a complex combination of near and
far-field extracellular potential signals. Therefore many device functions,
such as blanking periods and refractory periods, are designed to handle these
unexpected signals. To represent these signals, we develop an intracardiac
electrogram (IEGM) model as an interface between the virtual heart and the
device. The model can capture not only the local excitation but also far-field
signals and pacing afterpotentials. Moreover, the sensing controller can
specify unipolar or bipolar electrogram (EGM) sensing configurations and
introduce various oversensing and undersensing modes. The simulation results
show that the model is able to reproduce clinically observed sensing problems,
which significantly extends the capabilities of the virtual heart model in the
context of device validation.
",Computer Science; Physics,Physics
"Fermions in Two Dimensions: Scattering and Many-Body Properties   Ultracold atomic Fermi gases in two-dimensions (2D) are an increasingly
popular topic of research. The interaction strength between spin-up and
spin-down particles in two-component Fermi gases can be tuned in experiments,
allowing for a strongly interacting regime where the gas properties are yet to
be fully understood. We have probed this regime for 2D Fermi gases by
performing T=0 ab initio diffusion Monte Carlo calculations. The many-body
dynamics are largely dependent on the two-body interactions, therefore we start
with an in-depth look at scattering theory in 2D. We show the partial-wave
expansion and its relation to the scattering length and effective range. Then
we discuss our numerical methods for determining these scattering parameters.
We close out this discussion by illustrating the details of bound states in 2D.
Transitioning to the many-body system, we use variationally optimized wave
functions to calculate ground-state properties of the gas over a range of
interaction strengths. We show results for the energy per particle and
parametrize an equation of state. We then proceed to determine the chemical
potential for the strongly interacting gas.
",Physics,Physics
"Sorted Concave Penalized Regression   The Lasso is biased. Concave penalized least squares estimation (PLSE) takes
advantage of signal strength to reduce this bias, leading to sharper error
bounds in prediction, coefficient estimation and variable selection. For
prediction and estimation, the bias of the Lasso can be also reduced by taking
a smaller penalty level than what selection consistency requires, but such
smaller penalty level depends on the sparsity of the true coefficient vector.
The sorted L1 penalized estimation (Slope) was proposed for adaptation to such
smaller penalty levels. However, the advantages of concave PLSE and Slope do
not subsume each other. We propose sorted concave penalized estimation to
combine the advantages of concave and sorted penalizations. We prove that
sorted concave penalties adaptively choose the smaller penalty level and at the
same time benefits from signal strength, especially when a significant
proportion of signals are stronger than the corresponding adaptively selected
penalty levels. A local convex approximation, which extends the local linear
and quadratic approximations to sorted concave penalties, is developed to
facilitate the computation of sorted concave PLSE and proven to possess desired
prediction and estimation error bounds. We carry out a unified treatment of
penalty functions in a general optimization setting, including the penalty
levels and concavity of the above mentioned sorted penalties and mixed
penalties motivated by Bayesian considerations. Our analysis of prediction and
estimation errors requires the restricted eigenvalue condition on the design,
not beyond, and provides selection consistency under a required minimum signal
strength condition in addition. Thus, our results also sharpens existing
results on concave PLSE by removing the upper sparse eigenvalue component of
the sparse Riesz condition.
",Mathematics,Computer Science; Statistics
"Accelerated Stochastic Quasi-Newton Optimization on Riemann Manifolds   We propose an L-BFGS optimization algorithm on Riemannian manifolds using
minibatched stochastic variance reduction techniques for fast convergence with
constant step sizes, without resorting to linesearch methods designed to
satisfy Wolfe conditions. We provide a new convergence proof for strongly
convex functions without using curvature conditions on the manifold, as well as
a convergence discussion for nonconvex functions. We discuss a couple of ways
to obtain the correction pairs used to calculate the product of the gradient
with the inverse Hessian, and empirically demonstrate their use in synthetic
experiments on computation of Karcher means for symmetric positive definite
matrices and leading eigenvalues of large scale data matrices. We compare our
method to VR-PCA for the latter experiment, along with Riemannian SVRG for both
cases, and show strong convergence results for a range of datasets.
",Mathematics; Statistics,Statistics
"Emergence and complexity in theoretical models of self-organized criticality   In this thesis we present few theoretical studies of the models of
self-organized criticality. Following a brief introduction of self-organized
criticality, we discuss three main problems. The first problem is about growing
patterns formed in the abelian sandpile model (ASM). The patterns exhibit
proportionate growth where different parts of the pattern grow in same rate,
keeping the overall shape unchanged. This non-trivial property, often found in
biological growth, has received increasing attention in recent years. In this
thesis, we present a mathematical characterization of a large class of such
patterns in terms of discrete holomorphic functions. In the second problem, we
discuss a well known model of self-organized criticality introduced by Zhang in
1989. We present an exact analysis of the model and quantitatively explain an
intriguing property known as the emergence of quasi-units. In the third
problem, we introduce an operator algebra to determine the steady state of a
class of stochastic sandpile models.
",Physics; Mathematics,Physics
"Effective mass of quasiparticles from thermodynamics   We discuss the potential advantages of calculating the effective mass of
quasiparticles in the interacting electron liquid from the low-temperature free
energy vis-a-vis the conventional approach, in which the effective mass is
obtained from approximate calculations of the self-energy, or from a quantum
Monte Carlo evaluation of the energy of a variational ""quasiparticle wave
function"". While raw quantum Monte Carlo data are presently too sparse to allow
for an accurate determination of the effective mass, the values estimated by
this method are numerically close to the ones obtained in previous calculations
using diagrammatic many-body theory. In contrast to this, a recently published
parametrization of quantum Monte Carlo data for the free energy of the
homogeneous electron liquid yields effective masses that considerably deviate
from previous calculations and even change sign for low densities, reflecting
an unphysical negative entropy. We suggest that this anomaly is related to the
treatment of the exchange energy at finite temperature.
",Physics,Physics
"Testing for Feature Relevance: The HARVEST Algorithm   Feature selection with high-dimensional data and a very small proportion of
relevant features poses a severe challenge to standard statistical methods. We
have developed a new approach (HARVEST) that is straightforward to apply,
albeit somewhat computer-intensive. This algorithm can be used to pre-screen a
large number of features to identify those that are potentially useful. The
basic idea is to evaluate each feature in the context of many random subsets of
other features. HARVEST is predicated on the assumption that an irrelevant
feature can add no real predictive value, regardless of which other features
are included in the subset. Motivated by this idea, we have derived a simple
statistical test for feature relevance. Empirical analyses and simulations
produced so far indicate that the HARVEST algorithm is highly effective in
predictive analytics, both in science and business.
",Statistics,Statistics
"Universal geometric constraints during epithelial jamming   As an injury heals, an embryo develops, or a carcinoma spreads, epithelial
cells systematically change their shape. In each of these processes cell shape
is studied extensively, whereas variation of shape from cell-to-cell is
dismissed most often as biological noise. But where do cell shape and variation
of cell shape come from? Here we report that cell shape and shape variation are
mutually constrained through a relationship that is purely geometrical. That
relationship is shown to govern maturation of the pseudostratified bronchial
epithelial layer cultured from both non-asthmatic and asthmatic donors as well
as formation of the ventral furrow in the epithelial monolayer of the
Drosophila embryo in vivo. Across these and other vastly different epithelial
systems, cell shape variation collapses to a family of distributions that is
common to all and potentially universal. That distribution, in turn, is
accounted for quantitatively by a mechanistic theory of cell-cell interaction
showing that cell shape becomes progressively less elongated and less variable
as the layer becomes progressively more jammed. These findings thus uncover a
connection between jamming and geometry that is generic -spanning jammed living
and inert systems alike- and demonstrate that proximity of the cell layer to
the jammed state is the principal determinant of the most primitive features of
epithelial cell shape and shape variation.
",Physics,Quantitative Biology
"Randomization-based Inference for Bernoulli-Trial Experiments and Implications for Observational Studies   We present a randomization-based inferential framework for experiments
characterized by a strongly ignorable assignment mechanism where units have
independent probabilities of receiving treatment. Previous works on
randomization tests often assume these probabilities are equal within blocks of
units. We consider the general case where they differ across units and show how
to perform randomization tests and obtain point estimates and confidence
intervals. Furthermore, we develop a rejection-sampling algorithm to conduct
randomization-based inference conditional on ancillary statistics, covariate
balance, or other statistics of interest. Through simulation we demonstrate how
our algorithm can yield powerful randomization tests and thus precise
inference. Our work also has implications for observational studies, which
commonly assume a strongly ignorable assignment mechanism. Most methodologies
for observational studies make additional modeling or asymptotic assumptions,
while our framework only assumes the strongly ignorable assignment mechanism,
and thus can be considered a minimal-assumption approach.
",Statistics,Mathematics; Statistics
"Inner-Scene Similarities as a Contextual Cue for Object Detection   Using image context is an effective approach for improving object detection.
Previously proposed methods used contextual cues that rely on semantic or
spatial information. In this work, we explore a different kind of contextual
information: inner-scene similarity. We present the CISS (Context by Inner
Scene Similarity) algorithm, which is based on the observation that two
visually similar sub-image patches are likely to share semantic identities,
especially when both appear in the same image. CISS uses base-scores provided
by a base detector and performs as a post-detection stage. For each candidate
sub-image (denoted anchor), the CISS algorithm finds a few similar sub-images
(denoted supporters), and, using them, calculates a new enhanced score for the
anchor. This is done by utilizing the base-scores of the supporters and a
pre-trained dependency model. The new scores are modeled as a linear function
of the base scores of the anchor and the supporters and is estimated using a
minimum mean square error optimization. This approach results in: (a) improved
detection of partly occluded objects (when there are similar non-occluded
objects in the scene), and (b) fewer false alarms (when the base detector
mistakenly classifies a background patch as an object). This work relates to
Duncan and Humphreys' ""similarity theory,"" a psychophysical study. which
suggested that the human visual system perceptually groups similar image
regions and that the classification of one region is affected by the estimated
identity of the other. Experimental results demonstrate the enhancement of a
base detector's scores on the PASCAL VOC dataset.
",Computer Science,Computer Science
"Uniform Consistency in Stochastic Block Model with Continuous Community Label   \cite{bickel2009nonparametric} developed a general framework to establish
consistency of community detection in stochastic block model (SBM). In most
applications of this framework, the community label is discrete. For example,
in \citep{bickel2009nonparametric,zhao2012consistency} the degree corrected SBM
is assumed to have a discrete degree parameter. In this paper, we generalize
the method of \cite{bickel2009nonparametric} to give consistency analysis of
maximum likelihood estimator (MLE) in SBM with continuous community label. We
show that there is a standard procedure to transform the $||\cdot||_2$ error
bound to the uniform error bound. We demonstrate the application of our general
results by proving the uniform consistency (strong consistency) of the MLE in
the exponential network model with interaction effect. Unfortunately, in the
continuous parameter case, the condition ensuring uniform consistency we
obtained is much stronger than that in the discrete parameter case, namely
$n\mu_n^5/(\log n)^{8}\rightarrow\infty$ versus $n\mu_n/\log
n\rightarrow\infty$. Where $n\mu_n$ represents the average degree of the
network. But continuous is the limit of discrete. So it is not surprising as we
show that by discretizing the community label space into sufficiently small
(but not too small) pieces and applying the MLE on the discretized community
label space, uniform consistency holds under almost the same condition as in
discrete community label space. Such a phenomenon is surprising since the
discretization does not depend on the data or the model. This reminds us of the
thresholding method.
",Statistics,Computer Science; Statistics
"Banach Algebra of Complex Bounded Radon Measures on Homogeneous Space   Let $ H $ be a compact subgroup of a locally compact group $G$. In this paper
we define a convolution on $ M(G/H) $, the space of all complex bounded Radon
measures on the homogeneous space G/H. Then we prove that the measure space $
M(G/H, *) $ is a non-unital Banach algebra that possesses an approximate
identity. Finally, it is shown that the Banach algebra $ M(G/H, *) $ is not
involutive and also $ L^1(G/H, *) $ is a two-sided ideal of it.
",Mathematics,Mathematics
"SOTER: Programming Safe Robotics System using Runtime Assurance   Autonomous robots increasingly depend on third-party off-the-shelf components
and complex machine-learning techniques. This trend makes it challenging to
provide strong design-time certification of correct operation. To address this
challenge, we present SOTER, a programming framework that integrates the core
principles of runtime assurance to enable the use of uncertified controllers,
while still providing safety guarantees.
Runtime Assurance (RTA) is an approach used for safety-critical systems where
design-time analysis is coupled with run-time techniques to switch between
unverified advanced controllers and verified simple controllers. In this paper,
we present a runtime assurance programming framework for modular design of
provably-safe robotics software. \tool provides language primitives to
declaratively construct a \rta module consisting of an advanced controller
(untrusted), a safe controller (trusted), and the desired safety specification
(S). If the RTA module is well formed then the framework provides a formal
guarantee that it satisfies property S. The compiler generates code for
monitoring system state and switching control between the advanced and safe
controller in order to guarantee S. RTA allows complex systems to be
constructed through the composition of RTA modules.
To demonstrate the efficacy of our framework, we consider a real-world
case-study of building a safe drone surveillance system. Our experiments both
in simulation and on actual drones show that RTA-enabled RTA ensures safety of
the system, including when untrusted third-party components have bugs or
deviate from the desired behavior.
",Computer Science,Computer Science
"Joint Power Allocation and Beamforming for Energy-Efficient Two-Way Multi-Relay Communications   This paper considers the joint design of user power allocation and relay
beamforming in relaying communications, in which multiple pairs of
single-antenna users exchange information with each other via multiple-antenna
relays in two time slots. All users transmit their signals to the relays in the
first time slot while the relays broadcast the beamformed signals to all users
in the second time slot. The aim is to maximize the system's energy efficiency
(EE) subject to quality-of-service (QoS) constraints in terms of exchange
throughput requirements. The QoS constraints are nonconvex with many nonlinear
cross-terms, so finding a feasible point is already computationally
challenging. The sum throughput appears in the numerator while the total
consumption power appears in the denominator of the EE objective function. The
former is a nonconcave function and the latter is a nonconvex function, making
fractional programming useless for EE optimization. Nevertheless, efficient
iterations of low complexity to obtain its optimized solutions are developed.
The performances of the multiple-user and multiple-relay networks under various
scenarios are evaluated to show the merit of the paper development.
",Computer Science,Computer Science
"Idempotent ordered semigroup   An element e of an ordered semigroup $(S,\cdot,\leq)$ is called an ordered
idempotent if $e\leq e^2$. We call an ordered semigroup $S$ idempotent ordered
semigroup if every element of $S$ is an ordered idempotent. Every idempotent
semigroup is a complete semilattice of rectangular idempotent semigroups and in
this way we arrive to many other important classes of idempotent ordered
semigroups.
",Mathematics,Mathematics
"Wall modeling via function enrichment: extension to detached-eddy simulation   We extend the approach of wall modeling via function enrichment to
detached-eddy simulation. The wall model aims at using coarse cells in the
near-wall region by modeling the velocity profile in the viscous sublayer and
log-layer. However, unlike other wall models, the full Navier-Stokes equations
are still discretely fulfilled, including the pressure gradient and convective
term. This is achieved by enriching the elements of the high-order
discontinuous Galerkin method with the law-of-the-wall. As a result, the
Galerkin method can ""choose"" the optimal solution among the polynomial and
enrichment shape functions. The detached-eddy simulation methodology provides a
suitable turbulence model for the coarse near-wall cells. The approach is
applied to wall-modeled LES of turbulent channel flow in a wide range of
Reynolds numbers. Flow over periodic hills shows the superiority compared to an
equilibrium wall model under separated flow conditions.
",Physics,Physics
"The first and second fundamental theorems of invariant theory for the quantum general linear supergroup   We develop the non-commutative polynomial version of the invariant theory for
the quantum general linear supergroup ${\rm{ U}}_q(\mathfrak{gl}_{m|n})$. A
non-commutative ${\rm{ U}}_q(\mathfrak{gl}_{m|n})$-module superalgebra
$\mathcal{P}^{k|l}_{\,r|s}$ is constructed, which is the quantum analogue of
the supersymmetric algebra over $\mathbb{C}^{k|l}\otimes \mathbb{C}^{m|n}\oplus
\mathbb{C}^{r|s}\otimes (\mathbb{C}^{m|n})^{\ast}$. We analyse the structure of
the subalgebra of ${\rm{ U}}_q(\mathfrak{gl}_{m|n})$-invariants in
$\mathcal{P}^{k|l}_{\,r|s}$ by using the quantum super analogue of Howe
duality.
The subalgebra of ${\rm{ U}}_q(\mathfrak{gl}_{m|n})$-invariants in
$\mathcal{P}^{k|l}_{\,r|s}$ is shown to be finitely generated. We determine its
generators and establish a surjective superalgebra homomorphism from a braided
supersymmetric algebra onto it. This establishes the first fundamental theorem
of invariant theory for ${\rm{ U}}_q(\mathfrak{gl}_{m|n})$.
We show that the above mentioned superalgebra homomorphism is an isomorphism
if and only if $m\geq \min\{k,r\}$ and $n\geq \min\{l,s\}$, and obtain a
monomial basis for the subalgebra of invariants in this case. When the
homomorphism is not injective, we give a representation theoretical description
of the generating elements of the kernel associated to the partition
$((m+1)^{n+1})$, producing the second fundamental theorem of invariant theory
for ${\rm{ U}}_q(\mathfrak{gl}_{m|n})$.
We consider two applications of our results. A complete treatment of the
non-commutative polynomial version of invariant theory for ${\rm{
U}}_q(\mathfrak{gl}_{m})$ is obtained as the special case with $n=0$, where an
explicit SFT is proved, which we believe to be new. The FFT and SFT of the
invariant theory for the general linear superalgebra are recovered from the
classical (i.e., $q\to 1$) limit of our results.
",Mathematics,Mathematics
"Distributed Algorithms Made Secure: A Graph Theoretic Approach   In the area of distributed graph algorithms a number of network's entities
with local views solve some computational task by exchanging messages with
their neighbors. Quite unfortunately, an inherent property of most existing
distributed algorithms is that throughout the course of their execution, the
nodes get to learn not only their own output but rather learn quite a lot on
the inputs or outputs of many other entities. This leakage of information might
be a major obstacle in settings where the output (or input) of network's
individual is a private information. In this paper, we introduce a new
framework for \emph{secure distributed graph algorithms} and provide the first
\emph{general compiler} that takes any ""natural"" non-secure distributed
algorithm that runs in $r$ rounds, and turns it into a secure algorithm that
runs in $\widetilde{O}(r \cdot D \cdot poly(\Delta))$ rounds where $\Delta$ is
the maximum degree in the graph and $D$ is its diameter. The security of the
compiled algorithm is information-theoretic but holds only against a
semi-honest adversary that controls a single node in the network.
This compiler is made possible due to a new combinatorial structure called
\emph{private neighborhood trees}: a collection of $n$ trees
$T(u_1),\ldots,T(u_n)$, one for each vertex $u_i \in V(G)$, such that each tree
$T(u_i)$ spans the neighbors of $u_i$ {\em without going through $u_i$}.
Intuitively, each tree $T(u_i)$ allows all neighbors of $u_i$ to exchange a
\emph{secret} that is hidden from $u_i$, which is the basic graph
infrastructure of the compiler. In a $(d,c)$-private neighborhood trees each
tree $T(u_i)$ has depth at most $d$ and each edge $e \in G$ appears in at most
$c$ different trees. We show a construction of private neighborhood trees with
$d=\widetilde{O}(\Delta \cdot D)$ and $c=\widetilde{O}(D)$.
",Computer Science,Computer Science
"Dropout is a special case of the stochastic delta rule: faster and more accurate deep learning   Multi-layer neural networks have lead to remarkable performance on many kinds
of benchmark tasks in text, speech and image processing. Nonlinear parameter
estimation in hierarchical models is known to be subject to overfitting. One
approach to this overfitting and related problems (local minima, colinearity,
feature discovery etc.) is called dropout (Srivastava, et al 2014, Baldi et al
2016). This method removes hidden units with a Bernoulli random variable with
probability $p$ over updates. In this paper we will show that Dropout is a
special case of a more general model published originally in 1990 called the
stochastic delta rule ( SDR, Hanson, 1990). SDR parameterizes each weight in
the network as a random variable with mean $\mu_{w_{ij}}$ and standard
deviation $\sigma_{w_{ij}}$. These random variables are sampled on each forward
activation, consequently creating an exponential number of potential networks
with shared weights. Both parameters are updated according to prediction error,
thus implementing weight noise injections that reflect a local history of
prediction error and efficient model averaging. SDR therefore implements a
local gradient-dependent simulated annealing per weight converging to a bayes
optimal network. Tests on standard benchmarks (CIFAR) using a modified version
of DenseNet shows the SDR outperforms standard dropout in error by over 50% and
in loss by over 50%. Furthermore, the SDR implementation converges on a
solution much faster, reaching a training error of 5 in just 15 epochs with
DenseNet-40 compared to standard DenseNet-40's 94 epochs.
",Statistics,Computer Science; Statistics
"Uniform Rates of Convergence of Some Representations of Extremes : a first approach   Uniform convergence rates are provided for asymptotic representations of
sample extremes. These bounds which are universal in the sense that they do not
depend on the extreme value index are meant to be extended to arbitrary samples
extremes in coming papers.
",Statistics,Mathematics; Statistics
"Greed Works - Online Algorithms For Unrelated Machine Stochastic Scheduling   This paper establishes the first performance guarantees for a combinatorial
online algorithm that schedules stochastic, nonpreemptive jobs on unrelated
machines to minimize the expected total weighted completion time. Prior work on
unrelated machine scheduling with stochastic jobs was restricted to the offline
case, and required sophisticated linear or convex programming relaxations for
the assignment of jobs to machines. The algorithm introduced in this paper is
based on a purely combinatorial assignment of jobs to machines, hence it also
works online. The performance bounds are of the same order of magnitude as
those of earlier work, and depend linearly on an upper bound $\Delta$ on the
squared coefficient of variation of the jobs' processing times. They are
$4+2\Delta$ when there are no release dates, and $12+6\Delta$ when jobs are
released over time. For the special case of deterministic processing times,
without and with release times, this paper shows that the same combinatorial
greedy algorithm has a competitive ratio of 4 and 6, respectively. As to the
technical contribution, the paper shows for the first time how dual fitting
techniques can be used for stochastic and nonpreemptive scheduling problems.
",Computer Science,Computer Science
"Hybrid Optimization Method for Reconfiguration of AC/DC Microgrids in All-Electric Ships   Since the limited power capacity, finite inertia, and dynamic loads make the
shipboard power system (SPS) vulnerable, the automatic reconfiguration for
failure recovery in SPS is an extremely significant but still challenging
problem. It is not only required to operate accurately and optimally, but also
to satisfy operating constraints. In this paper, we consider the
reconfiguration optimization for hybrid AC/DC microgrids in all-electric ships.
Firstly, the multi-zone medium voltage DC (MVDC) SPS model is presented. In
this model, the DC power flow for reconfiguration and a generalized AC/DC
converter are modeled for accurate reconfiguration. Secondly, since this
problem is mixed integer nonlinear programming (MINLP), a hybrid method based
on Newton Raphson and Biogeography based Optimization (NRBBO) is designed
according to the characteristics of system, loads, and faults. This method
facilitates to maximize the weighted load restoration while satisfying
operating constraints. Finally, the simulation results demonstrate this method
has advantages in terms of power restoration and convergence speed.
",Computer Science,Computer Science
"Automatic classification of trees using a UAV onboard camera and deep learning   Automatic classification of trees using remotely sensed data has been a dream
of many scientists and land use managers. Recently, Unmanned aerial vehicles
(UAV) has been expected to be an easy-to-use, cost-effective tool for remote
sensing of forests, and deep learning has attracted attention for its ability
concerning machine vision. In this study, using a commercially available UAV
and a publicly available package for deep learning, we constructed a machine
vision system for the automatic classification of trees. In our method, we
segmented a UAV photography image of forest into individual tree crowns and
carried out object-based deep learning. As a result, the system was able to
classify 7 tree types at 89.0% accuracy. This performance is notable because we
only used basic RGB images from a standard UAV. In contrast, most of previous
studies used expensive hardware such as multispectral imagers to improve the
performance. This result means that our method has the potential to classify
individual trees in a cost-effective manner. This can be a usable tool for many
forest researchers and managements.
",Statistics,Computer Science
"Faster Learning by Reduction of Data Access Time   Nowadays, the major challenge in machine learning is the Big Data challenge.
The big data problems due to large number of data points or large number of
features in each data point, or both, the training of models have become very
slow. The training time has two major components: Time to access the data and
time to process (learn from) the data. So far, the research has focused only on
the second part, i.e., learning from the data. In this paper, we have proposed
one possible solution to handle the big data problems in machine learning. The
idea is to reduce the training time through reducing data access time by
proposing systematic sampling and cyclic/sequential sampling to select
mini-batches from the dataset. To prove the effectiveness of proposed sampling
techniques, we have used Empirical Risk Minimization, which is commonly used
machine learning problem, for strongly convex and smooth case. The problem has
been solved using SAG, SAGA, SVRG, SAAG-II and MBSGD (Mini-batched SGD), each
using two step determination techniques, namely, constant step size and
backtracking line search method. Theoretical results prove the same convergence
for systematic sampling, cyclic sampling and the widely used random sampling
technique, in expectation. Experimental results with bench marked datasets
prove the efficacy of the proposed sampling techniques and show up to six times
faster training.
",Statistics,Computer Science; Statistics
"Mean-variance portfolio selection under partial information with drift uncertainty   This paper studies a mean-variance portfolio selection problem under partial
information with drift uncertainty. It is proved that all the contingent claims
in this model are attainable in the sense of Xiong and Zhou. Further, we
propose a numerical scheme to approximate the optimal portfolio. Malliavin
calculus and the strong law of large numbers play important roles in this
scheme.
",Quantitative Finance,Mathematics; Statistics
"Learning Sparse Adversarial Dictionaries For Multi-Class Audio Classification   Audio events are quite often overlapping in nature, and more prone to noise
than visual signals. There has been increasing evidence for the superior
performance of representations learned using sparse dictionaries for
applications like audio denoising and speech enhancement. This paper
concentrates on modifying the traditional reconstructive dictionary learning
algorithms, by incorporating a discriminative term into the objective function
in order to learn class-specific adversarial dictionaries that are good at
representing samples of their own class at the same time poor at representing
samples belonging to any other class. We quantitatively demonstrate the
effectiveness of our learned dictionaries as a stand-alone solution for both
binary as well as multi-class audio classification problems.
",Computer Science,Computer Science
"Global spectral graph wavelet signature for surface analysis of carpal bones   In this paper, we present a spectral graph wavelet approach for shape
analysis of carpal bones of human wrist. We apply a metric called global
spectral graph wavelet signature for representation of cortical surface of the
carpal bone based on eigensystem of Laplace-Beltrami operator. Furthermore, we
propose a heuristic and efficient way of aggregating local descriptors of a
carpal bone surface to global descriptor. The resultant global descriptor is
not only isometric invariant, but also much more efficient and requires less
memory storage. We perform experiments on shape of the carpal bones of ten
women and ten men from a publicly-available database. Experimental results show
the excellency of the proposed GSGW compared to recent proposed GPS embedding
approach for comparing shapes of the carpal bones across populations.
",Computer Science,Computer Science; Mathematics
"OLÉ: Orthogonal Low-rank Embedding, A Plug and Play Geometric Loss for Deep Learning   Deep neural networks trained using a softmax layer at the top and the
cross-entropy loss are ubiquitous tools for image classification. Yet, this
does not naturally enforce intra-class similarity nor inter-class margin of the
learned deep representations. To simultaneously achieve these two goals,
different solutions have been proposed in the literature, such as the pairwise
or triplet losses. However, such solutions carry the extra task of selecting
pairs or triplets, and the extra computational burden of computing and learning
for many combinations of them. In this paper, we propose a plug-and-play loss
term for deep networks that explicitly reduces intra-class variance and
enforces inter-class margin simultaneously, in a simple and elegant geometric
manner. For each class, the deep features are collapsed into a learned linear
subspace, or union of them, and inter-class subspaces are pushed to be as
orthogonal as possible. Our proposed Orthogonal Low-rank Embedding (OLÉ) does
not require carefully crafting pairs or triplets of samples for training, and
works standalone as a classification loss, being the first reported deep metric
learning framework of its kind. Because of the improved margin between features
of different classes, the resulting deep networks generalize better, are more
discriminative, and more robust. We demonstrate improved classification
performance in general object recognition, plugging the proposed loss term into
existing off-the-shelf architectures. In particular, we show the advantage of
the proposed loss in the small data/model scenario, and we significantly
advance the state-of-the-art on the Stanford STL-10 benchmark.
",Computer Science; Statistics,Computer Science; Statistics
"Limits of Predictability of Cascading Overload Failures in Spatially-Embedded Networks with Distributed Flows   Cascading failures are a critical vulnerability of complex information or
infrastructure networks. Here we investigate the properties of load-based
cascading failures in real and synthetic spatially-embedded network structures,
and propose mitigation strategies to reduce the severity of damages caused by
such failures. We introduce a stochastic method for optimal heterogeneous
distribution of resources (node capacities) subject to a fixed total cost.
Additionally, we design and compare the performance of networks with N-stable
and (N-1)-stable network-capacity allocations by triggering cascades using
various real-world node-attack and node-failure scenarios. We show that failure
mitigation through increased node protection can be effectively achieved
against single node failures. However, mitigating against multiple node
failures is much more difficult due to the combinatorial increase in possible
failures. We analyze the robustness of the system with increasing protection,
and find that a critical tolerance exists at which the system undergoes a phase
transition, and above which the network almost completely survives an attack.
Moreover, we show that cascade-size distributions measured in this region
exhibit a power-law decay. Finally, we find a strong correlation between
cascade sizes induced by individual nodes and sets of nodes. We also show that
network topology alone is a weak factor in determining the progression of
cascading failures.
",Computer Science; Physics,Computer Science
"Chatbots as Conversational Recommender Systems in Urban Contexts   In this paper, we outline the vision of chatbots that facilitate the
interaction between citizens and policy-makers at the city scale. We report the
results of a co-design session attended by more than 60 participants. We give
an outlook of how some challenges associated with such chatbot systems could be
addressed in the future.
",Computer Science,Computer Science
"The Imani Periodic Functions: Genesis and Preliminary Results   The Leah-Hamiltonian, $H(x,y)=y^2/2+3x^{4/3}/4$, is introduced as a
functional equation for $x(t)$ and $y(t)$. By means of a nonlinear
transformation to new independent variables, we show that this functional
equation has a special class of periodic solutions which we designate the Imani
functions. The explicit construction of these functions is done such that they
possess many of the general properties of the standard trigonometric cosine and
sine functions. We conclude by providing a listing of a number of currently
unresolved issues relating to the Imani functions.
",Mathematics,Mathematics
"On irrationality measure of Thue-Morse constant   We provide a non-trivial measure of irrationality for a class of Mahler
numbers defined with infinite products which cover the Thue-Morse constant.
",Mathematics,Mathematics
"Image-based immersed boundary model of the aortic root   Each year, approximately 300,000 heart valve repair or replacement procedures
are performed worldwide, including approximately 70,000 aortic valve
replacement surgeries in the United States alone. This paper describes progress
in constructing anatomically and physiologically realistic immersed boundary
(IB) models of the dynamics of the aortic root and ascending aorta. This work
builds on earlier IB models of fluid-structure interaction (FSI) in the aortic
root, which previously achieved realistic hemodynamics over multiple cardiac
cycles, but which also were limited to simplified aortic geometries and
idealized descriptions of the biomechanics of the aortic valve cusps. By
contrast, the model described herein uses an anatomical geometry reconstructed
from patient-specific computed tomography angiography (CTA) data, and employs a
description of the elasticity of the aortic valve leaflets based on a
fiber-reinforced constitutive model fit to experimental tensile test data.
Numerical tests show that the model is able to resolve the leaflet biomechanics
in diastole and early systole at practical grid spacings. The model is also
used to examine differences in the mechanics and fluid dynamics yielded by
fresh valve leaflets and glutaraldehyde-fixed leaflets similar to those used in
bioprosthetic heart valves. Although there are large differences in the leaflet
deformations during diastole, the differences in the open configurations of the
valve models are relatively small, and nearly identical hemodynamics are
obtained in all cases considered.
",Computer Science; Physics,Physics
"Morphological Simplification of Archaeological Fracture Surfaces   We propose to employ scale spaces of mathematical morphology to
hierarchically simplify fracture surfaces of complementarily fitting
archaeological fragments. This representation preserves contact and is
insensitive to different kinds of abrasion affecting the exact complementarity
of the original fragments. We present a pipeline for morphologically
simplifying fracture surfaces, based on their Lipschitz nature; its core is a
new embedding of fracture surfaces to simultaneously compute both closing and
opening morphological operations, using distance transforms.
",Computer Science,Computer Science; Physics
"On Ordinal Invariants in Well Quasi Orders and Finite Antichain Orders   We investigate the ordinal invariants height, length, and width of well quasi
orders (WQO), with particular emphasis on width, an invariant of interest for
the larger class of orders with finite antichain condition (FAC). We show that
the width in the class of FAC orders is completely determined by the width in
the class of WQOs, in the sense that if we know how to calculate the width of
any WQO then we have a procedure to calculate the width of any given FAC order.
We show how the width of WQO orders obtained via some classical constructions
can sometimes be computed in a compositional way. In particular, this allows
proving that every ordinal can be obtained as the width of some WQO poset. One
of the difficult questions is to give a complete formula for the width of
Cartesian products of WQOs. Even the width of the product of two ordinals is
only known through a complex recursive formula. Although we have not given a
complete answer to this question we have advanced the state of knowledge by
considering some more complex special cases and in particular by calculating
the width of certain products containing three factors. In the course of
writing the paper we have discovered that some of the relevant literature was
written on cross-purposes and some of the notions re-discovered several times.
Therefore we also use the occasion to give a unified presentation of the known
results.
",Mathematics,Mathematics
"Development of ICA and IVA Algorithms with Application to Medical Image Analysis   Independent component analysis (ICA) is a widely used BSS method that can
uniquely achieve source recovery, subject to only scaling and permutation
ambiguities, through the assumption of statistical independence on the part of
the latent sources. Independent vector analysis (IVA) extends the applicability
of ICA by jointly decomposing multiple datasets through the exploitation of the
dependencies across datasets. Though both ICA and IVA algorithms cast in the
maximum likelihood (ML) framework enable the use of all available statistical
information in reality, they often deviate from their theoretical optimality
properties due to improper estimation of the probability density function
(PDF). This motivates the development of flexible ICA and IVA algorithms that
closely adhere to the underlying statistical description of the data. Although
it is attractive minimize the assumptions, important prior information about
the data, such as sparsity, is usually available. If incorporated into the ICA
model, use of this additional information can relax the independence
assumption, resulting in an improvement in the overall separation performance.
Therefore, the development of a unified mathematical framework that can take
into account both statistical independence and sparsity is of great interest.
In this work, we first introduce a flexible ICA algorithm that uses an
effective PDF estimator to accurately capture the underlying statistical
properties of the data. We then discuss several techniques to accurately
estimate the parameters of the multivariate generalized Gaussian distribution,
and how to integrate them into the IVA model. Finally, we provide a
mathematical framework that enables direct control over the influence of
statistical independence and sparsity, and use this framework to develop an
effective ICA algorithm that can jointly exploit these two forms of diversity.
",Statistics,Statistics
"Strongly regular decompositions and symmetric association schemes of a power of two   For any positive integer $m$, the complete graph on $2^{2m}(2^m+2)$ vertices
is decomposed into $2^m+1$ commuting strongly regular graphs, which give rise
to a symmetric association scheme of class $2^{m+2}-2$. Furthermore, the
eigenmatrices of the symmetric association schemes are determined explicitly.
As an application, the eigenmatrix of the commutative strongly regular
decomposition obtained from the strongly regular graphs is derived.
",Mathematics,Mathematics
"Musical Instrument Recognition Using Their Distinctive Characteristics in Artificial Neural Networks   In this study an Artificial Neural Network was trained to classify musical
instruments, using audio samples transformed to the frequency domain. Different
features of the sound, in both time and frequency domain, were analyzed and
compared in relation to how much information that could be derived from that
limited data. The study concluded that in comparison with the base experiment,
that had an accuracy of 93.5%, using the attack only resulted in 80.2% and the
initial 100 Hz in 64.2%.
",Computer Science; Statistics,Computer Science
"Lagrangians of hypergraphs: The Frankl-Füredi conjecture holds almost everywhere   Frankl and Füredi conjectured in 1989 that the maximum Lagrangian of all
$r$-uniform hypergraphs of fixed size $m$ is realised by the initial segment of
the colexicographic order. In particular, in the principal case
$m=\binom{t}{r}$ their conjecture states that every $H\subseteq
\mathbb{N}^{(r)}$ of size $\binom{t}{r}$ satisfies \begin{align*} \max
\{\sum_{A \in H}\prod_{i\in A} y_i \ \colon \ y_1,y_2,\ldots \geq 0; \sum_{i\in
\mathbb{N}} y_i=1 \}&\leq \frac{1}{t^r}\binom{t}{r}. \end{align*}
We prove the above statement for all $r\geq 4$ and large values of $t$ (the
case $r=3$ was settled by Talbot in 2002). More generally, we show for any
$r\geq 4$ that the Frankl-Füredi conjecture holds whenever $\binom{t-1}{r}
\leq m \leq \binom{t}{r}- \gamma_r t^{r-2}$ for a constant $\gamma_r>0$,
thereby verifying it for `most' $m\in \mathbb{N}$.
Furthermore, for $r=3$ we make an improvement on the results of
Talbot~\cite{Tb} and Tang, Peng, Zhang and Zhao~\cite{TPZZ}.
",Mathematics,Mathematics
"Strong-coupling of WSe2 in ultra-compact plasmonic nanocavities at room temperature   Strong-coupling of monolayer metal dichalcogenide semiconductors with light
offers encouraging prospects for realistic exciton devices at room temperature.
However, the nature of this coupling depends extremely sensitively on the
optical confinement and the orientation of electronic dipoles and fields. Here,
we show how plasmon strong coupling can be achieved in compact robust
easily-assembled gold nano-gap resonators at room temperature. We prove that
strong coupling is impossible with monolayers due to the large exciton
coherence size, but resolve clear anti-crossings for 8 layer devices with Rabi
splittings exceeding 135 meV. We show that such structures improve on prospects
for nonlinear exciton functionalities by at least 10^4, while retaining quantum
efficiencies above 50%.
",Physics,Physics
"Proximodistal Exploration in Motor Learning as an Emergent Property of Optimization   To harness the complexity of their high-dimensional bodies during
sensorimotor development, infants are guided by patterns of freezing and
freeing of degrees of freedom. For instance, when learning to reach, infants
free the degrees of freedom in their arm proximodistally, i.e. from joints that
are closer to the body to those that are more distant. Here, we formulate and
study computationally the hypothesis that such patterns can emerge
spontaneously as the result of a family of stochastic optimization processes
(evolution strategies with covariance-matrix adaptation), without an innate
encoding of a maturational schedule. In particular, we present simulated
experiments with an arm where a computational learner progressively acquires
reaching skills through adaptive exploration, and we show that a proximodistal
organization appears spontaneously, which we denote PDFF (ProximoDistal
Freezing and Freeing of degrees of freedom). We also compare this emergent
organization between different arm morphologies -- from human-like to quite
unnatural ones -- to study the effect of different kinematic structures on the
emergence of PDFF. Keywords: human motor learning; proximo-distal exploration;
stochastic optimization; modelling; evolution strategies; cross-entropy
methods; policy search; morphology.}
",Computer Science,Computer Science; Statistics
"Deep Robust Kalman Filter   A Robust Markov Decision Process (RMDP) is a sequential decision making model
that accounts for uncertainty in the parameters of dynamic systems. This
uncertainty introduces difficulties in learning an optimal policy, especially
for environments with large state spaces. We propose two algorithms, RTD-DQN
and Deep-RoK, for solving large-scale RMDPs using nonlinear approximation
schemes such as deep neural networks. The RTD-DQN algorithm incorporates the
robust Bellman temporal difference error into a robust loss function, yielding
robust policies for the agent. The Deep-RoK algorithm is a robust Bayesian
method, based on the Extended Kalman Filter (EKF), that accounts for both the
uncertainty in the weights of the approximated value function and the
uncertainty in the transition probabilities, improving the robustness of the
agent. We provide theoretical results for our approach and test the proposed
algorithms on a continuous state domain.
",Computer Science; Statistics,Computer Science; Statistics
"A variant of Gromov's problem on Hölder equivalence of Carnot groups   It is unknown if there exists a locally $\alpha$-Hölder homeomorphism
$f:\mathbb{R}^3\to \mathbb{H}^1$ for any $\frac{1}{2}< \alpha\le \frac{2}{3}$,
although the identity map $\mathbb{R}^3\to \mathbb{H}^1$ is locally
$\frac{1}{2}$-Hölder. More generally, Gromov asked: Given $k$ and a Carnot
group $G$, for which $\alpha$ does there exist a locally $\alpha$-Hölder
homeomorphism $f:\mathbb{R}^k\to G$? Here, we equip a Carnot group $G$ with the
Carnot-Carathéodory metric. In 2014, Balogh, Hajlasz, and Wildrick considered
a variant of this problem. These authors proved that if $k>n$, there does not
exist an injective, $(\frac{1}{2}+)$-Hölder mapping $f:\mathbb{R}^k\to
\mathbb{H}^n$ that is also locally Lipschitz as a mapping into
$\mathbb{R}^{2n+1}$. For their proof, they use the fact that $\mathbb{H}^n$ is
purely $k$-unrectifiable for $k>n$. In this paper, we will extend their result
from the Heisenberg group to model filiform groups and Carnot groups of step at
most three. We will now require that the Carnot group is purely
$k$-unrectifiable. The main key to our proof will be showing that
$(\frac{1}{2}+)$-Hölder maps $f:\mathbb{R}^k\to G$ that are locally Lipschitz
into Euclidean space, are weakly contact. Proving weak contactness in these two
settings requires understanding the relationship between the algebraic and
metric structures of the Carnot group. We will use coordinates of the first and
second kind for Carnot groups.
",Mathematics,Mathematics
"Topological Representation of the Transit Sets of k-Point Crossover Operators   $k$-point crossover operators and their recombination sets are studied from
different perspectives. We show that transit functions of $k$-point crossover
generate, for all $k>1$, the same convexity as the interval function of the
underlying graph. This settles in the negative an open problem by Mulder about
whether the geodesic convexity of a connected graph $G$ is uniquely determined
by its interval function $I$. The conjecture of Gitchoff and Wagner that for
each transit set $R_k(x,y)$ distinct from a hypercube there is a unique pair of
parents from which it is generated is settled affirmatively. Along the way we
characterize transit functions whose underlying graphs are Hamming graphs, and
those with underlying partial cube graphs. For general values of $k$ it is
shown that the transit sets of $k$-point crossover operators are the subsets
with maximal Vapnik-Chervonenkis dimension. Moreover, the transit sets of
$k$-point crossover on binary strings form topes of uniform oriented matroid of
VC-dimension $k+1$. The Topological Representation Theorem for oriented
matroids therefore implies that $k$-point crossover operators can be
represented by pseudosphere arrangements. This provides the tools necessary to
study the special case $k=2$ in detail.
",Computer Science; Mathematics,Mathematics
"A New Compton-thick AGN in our Cosmic Backyard: Unveiling the Buried Nucleus in NGC 1448 with NuSTAR   NGC 1448 is one of the nearest luminous galaxies ($L_{8-1000\mu m} >$ 10$^{9}
L_{\odot}$) to ours ($z$ $=$ 0.00390), and yet the active galactic nucleus
(AGN) it hosts was only recently discovered, in 2009. In this paper, we present
an analysis of the nuclear source across three wavebands: mid-infrared (MIR)
continuum, optical, and X-rays. We observed the source with the Nuclear
Spectroscopic Telescope Array (NuSTAR), and combined this data with archival
Chandra data to perform broadband X-ray spectral fitting ($\approx$0.5-40 keV)
of the AGN for the first time. Our X-ray spectral analysis reveals that the AGN
is buried under a Compton-thick (CT) column of obscuring gas along our
line-of-sight, with a column density of $N_{\rm H}$(los) $\gtrsim$ 2.5 $\times$
10$^{24}$ cm$^{-2}$. The best-fitting torus models measured an intrinsic 2-10
keV luminosity of $L_{2-10\rm{,int}}$ $=$ (3.5-7.6) $\times$ 10$^{40}$ erg
s$^{-1}$, making NGC 1448 one of the lowest luminosity CTAGNs known. In
addition to the NuSTAR observation, we also performed optical spectroscopy for
the nucleus in this edge-on galaxy using the European Southern Observatory New
Technology Telescope. We re-classify the optical nuclear spectrum as a Seyfert
on the basis of the Baldwin-Philips-Terlevich diagnostic diagrams, thus
identifying the AGN at optical wavelengths for the first time. We also present
high spatial resolution MIR observations of NGC 1448 with Gemini/T-ReCS, in
which a compact nucleus is clearly detected. The absorption-corrected 2-10 keV
luminosity measured from our X-ray spectral analysis agrees with that predicted
from the optical [OIII]$\lambda$5007\AA\ emission line and the MIR 12$\mu$m
continuum, further supporting the CT nature of the AGN.
",Physics,Physics
"The empirical Christoffel function with applications in data analysis   We illustrate the potential applications in machine learning of the
Christoffel function, or more precisely, its empirical counterpart associated
with a counting measure uniformly supported on a finite set of points. Firstly,
we provide a thresholding scheme which allows to approximate the support of a
measure from a finite subset of its moments with strong asymptotic guaranties.
Secondly, we provide a consistency result which relates the empirical
Christoffel function and its population counterpart in the limit of large
samples. Finally, we illustrate the relevance of our results on simulated and
real world datasets for several applications in statistics and machine
learning: (a) density and support estimation from finite samples, (b) outlier
and novelty detection and (c) affine matching.
",Computer Science,Statistics
"Nonparametric Variational Auto-encoders for Hierarchical Representation Learning   The recently developed variational autoencoders (VAEs) have proved to be an
effective confluence of the rich representational power of neural networks with
Bayesian methods. However, most work on VAEs use a rather simple prior over the
latent variables such as standard normal distribution, thereby restricting its
applications to relatively simple phenomena. In this work, we propose
hierarchical nonparametric variational autoencoders, which combines
tree-structured Bayesian nonparametric priors with VAEs, to enable infinite
flexibility of the latent representation space. Both the neural parameters and
Bayesian priors are learned jointly using tailored variational inference. The
resulting model induces a hierarchical structure of latent semantic concepts
underlying the data corpus, and infers accurate representations of data
instances. We apply our model in video representation learning. Our method is
able to discover highly interpretable activity hierarchies, and obtain improved
clustering accuracy and generalization capacity based on the learned rich
representations.
",Computer Science; Statistics,Statistics
"Defend against advanced persistent threats: An optimal control approach   The new cyber attack pattern of advanced persistent threat (APT) has posed a
serious threat to modern society. This paper addresses the APT defense problem,
i.e., the problem of how to effectively defend against an APT campaign. Based
on a novel APT attack-defense model, the effectiveness of an APT defense
strategy is quantified. Thereby, the APT defense problem is modeled as an
optimal control problem, in which an optimal control stands for a most
effective APT defense strategy. The existence of an optimal control is proved,
and an optimality system is derived. Consequently, an optimal control can be
figured out by solving the optimality system. Some examples of the optimal
control are given. Finally, the influence of some factors on the effectiveness
of an optimal control is examined through computer experiments. These findings
help organizations to work out policies of defending against APTs.
",Computer Science,Computer Science
"Evolution of Morphological and Physical Properties of Laboratory Interstellar Organic Residues with Ultraviolet Irradiation   Refractory organic compounds formed in molecular clouds are among the
building blocks of the solar system objects and could be the precursors of
organic matter found in primitive meteorites and cometary materials. However,
little is known about the evolutionary pathways of molecular cloud organics
from dense molecular clouds to planetary systems. In this study, we focus on
the evolution of the morphological and viscoelastic properties of molecular
cloud refractory organic matter. We found that the organic residue,
experimentally synthesized at about 10 K from UV-irradiated H2O-CH3OH-NH3 ice,
changed significantly in terms of its nanometer- to micrometer-scale morphology
and viscoelastic properties after UV irradiation at room temperature. The dose
of this irradiation was equivalent to that experienced after short residence in
diffuse clouds (equal or less than 10,000 years) or irradiation in outer
protoplanetary disks. The irradiated organic residues became highly porous and
more rigid and formed amorphous nanospherules. These nanospherules are
morphologically similar to organic nanoglobules observed in the least-altered
chondrites, chondritic porous interplanetary dust particles, and cometary
samples, suggesting that irradiation of refractory organics could be a possible
formation pathway for such nanoglobules. The storage modulus (elasticity) of
photo-irradiated organic residues is about 100 MPa irrespective of vibrational
frequency, a value that is lower than the storage moduli of minerals and ice.
Dust grains coated with such irradiated organics would therefore stick together
efficiently, but growth to larger grains might be suppressed due to an increase
in aggregate brittleness caused by the strong connections between grains.
",Physics,Physics
"Testing the validity of the local and global GKLS master equations on an exactly solvable model   When deriving a master equation for a multipartite weakly-interacting open
quantum systems, dissipation is often addressed \textit{locally} on each
component, i.e. ignoring the coherent couplings, which are later added `by
hand'. Although simple, the resulting local master equation (LME) is known to
be thermodynamically inconsistent. Otherwise, one may always obtain a
consistent \textit{global} master equation (GME) by working on the energy basis
of the full interacting Hamiltonian. Here, we consider a two-node `quantum
wire' connected to two heat baths. The stationary solution of the LME and GME
are obtained and benchmarked against the exact result. Importantly, in our
model, the validity of the GME is constrained by the underlying secular
approximation. Whenever this breaks down (for resonant weakly-coupled nodes),
we observe that the LME, in spite of being thermodynamically flawed: (a)
predicts the correct steady state, (b) yields the exact asymptotic heat
currents, and (c) reliably reflects the correlations between the nodes. In
contrast, the GME fails at all three tasks. Nonetheless, as the inter-node
coupling grows, the LME breaks down whilst the GME becomes correct. Hence, the
global and local approach may be viewed as \textit{complementary} tools, best
suited to different parameter regimes.
",Physics,Physics
"Training of Deep Neural Networks based on Distance Measures using RMSProp   The vanishing gradient problem was a major obstacle for the success of deep
learning. In recent years it was gradually alleviated through multiple
different techniques. However the problem was not really overcome in a
fundamental way, since it is inherent to neural networks with activation
functions based on dot products. In a series of papers, we are going to analyze
alternative neural network structures which are not based on dot products. In
this first paper, we revisit neural networks built up of layers based on
distance measures and Gaussian activation functions. These kinds of networks
were only sparsely used in the past since they are hard to train when using
plain stochastic gradient descent methods. We show that by using Root Mean
Square Propagation (RMSProp) it is possible to efficiently learn multi-layer
neural networks. Furthermore we show that when appropriately initialized these
kinds of neural networks suffer much less from the vanishing and exploding
gradient problem than traditional neural networks even for deep networks.
",Computer Science; Statistics,Computer Science; Statistics
"Exhaustive Exploration of the Failure-oblivious Computing Search Space   High-availability of software systems requires automated handling of crashes
in presence of errors. Failure-oblivious computing is one technique that aims
to achieve high availability. We note that failure-obliviousness has not been
studied in depth yet, and there is very few study that helps understand why
failure-oblivious techniques work. In order to make failure-oblivious computing
to have an impact in practice, we need to deeply understand failure-oblivious
behaviors in software. In this paper, we study, design and perform an
experiment that analyzes the size and the diversity of the failure-oblivious
behaviors. Our experiment consists of exhaustively computing the search space
of 16 field failures of large-scale open-source Java software. The outcome of
this experiment is a much better understanding of what really happens when
failure-oblivious computing is used, and this opens new promising research
directions.
",Computer Science,Computer Science
"Confluence of Conditional Term Rewrite Systems via Transformations   Conditional term rewriting is an intuitive yet complex extension of term
rewriting. In order to benefit from the simpler framework of unconditional
rewriting, transformations have been defined to eliminate the conditions of
conditional term rewrite systems.
Recent results provide confluence criteria for conditional term rewrite
systems via transformations, yet they are restricted to CTRSs with certain
syntactic properties like weak left-linearity. These syntactic properties imply
that the transformations are sound for the given CTRS.
This paper shows how to use transformations to prove confluence of
operationally terminating, right-stable deterministic conditional term rewrite
systems without the necessity of soundness restrictions. For this purpose, it
is shown that certain rewrite strategies, in particular almost U-eagerness and
innermost rewriting, always imply soundness.
",Computer Science,Computer Science
"From bare interactions, low--energy constants and unitary gas to nuclear density functionals without free parameters: application to neutron matter   We further progress along the line of Ref. [Phys. Rev. {\bf A 94}, 043614
(2016)] where a functional for Fermi systems with anomalously large $s$-wave
scattering length $a_s$ was proposed that has no free parameters. The
functional is designed to correctly reproduce the unitary limit in Fermi gases
together with the leading-order contributions in the s- and p-wave channels at
low density. The functional is shown to be predictive up to densities
$\sim0.01$ fm$^{-3}$ that is much higher densities compared to the Lee-Yang
functional, valid for $\rho < 10^{-6}$ fm$^{-3}$. The form of the functional
retained in this work is further motivated. It is shown that the new functional
corresponds to an expansion of the energy in $(a_s k_F)$ and $(r_e k_F)$ to all
orders, where $r_e$ is the effective range and $k_F$ is the Fermi momentum. One
conclusion from the present work is that, except in the extremely low--density
regime, nuclear systems can be treated perturbatively in $-(a_s k_F)^{-1}$ with
respect to the unitary limit. Starting from the functional, we introduce
density--dependent scales and show that scales associated to the bare
interaction are strongly renormalized by medium effects. As a consequence, some
of the scales at play around saturation are dominated by the unitary gas
properties and not directly to low-energy constants. For instance, we show that
the scale in the s-wave channel around saturation is proportional to the
so-called Bertsch parameter $\xi_0$ and becomes independent of $a_s$. We also
point out that these scales are of the same order of magnitude than those
empirically obtained in the Skyrme energy density functional. We finally
propose a slight modification of the functional such that it becomes accurate
up to the saturation density $\rho\simeq 0.16$ fm$^{-3}$.
",Physics,Physics
"Towards the study of least squares estimators with convex penalty   Penalized least squares estimation is a popular technique in high-dimensional
statistics. It includes such methods as the LASSO, the group LASSO, and the
nuclear norm penalized least squares. The existing theory of these methods is
not fully satisfying since it allows one to prove oracle inequalities with
fixed high probability only for the estimators depending on this probability.
Furthermore, the control of compatibility factors appearing in the oracle
bounds is often not explicit. Some very recent developments suggest that the
theory of oracle inequalities can be revised in an improved way. In this paper,
we provide an overview of ideas and tools leading to such an improved theory.
We show that, along with overcoming the disadvantages mentioned above, the
methodology extends to the hilbertian framework and it applies to a large class
of convex penalties. This paper is partly expository. In particular, we provide
adapted proofs of some results from other recent work.
",Mathematics; Statistics,Mathematics; Statistics
"Deep Residual Learning for Instrument Segmentation in Robotic Surgery   Detection, tracking, and pose estimation of surgical instruments are crucial
tasks for computer assistance during minimally invasive robotic surgery. In the
majority of cases, the first step is the automatic segmentation of surgical
tools. Prior work has focused on binary segmentation, where the objective is to
label every pixel in an image as tool or background. We improve upon previous
work in two major ways. First, we leverage recent techniques such as deep
residual learning and dilated convolutions to advance binary-segmentation
performance. Second, we extend the approach to multi-class segmentation, which
lets us segment different parts of the tool, in addition to background. We
demonstrate the performance of this method on the MICCAI Endoscopic Vision
Challenge Robotic Instruments dataset.
",Computer Science,Computer Science
"Optical bandgap engineering in nonlinear silicon nitride waveguides   Silicon nitride is awell-established material for photonic devices and
integrated circuits. It displays a broad transparency window spanning from the
visible to the mid-IR and waveguides can be manufactured with low losses. An
absence of nonlinear multi-photon absorption in the erbium lightwave
communications band has enabled various nonlinear optic applications in the
past decade. Silicon nitride is a dielectric material whose optical and
mechanical properties strongly depend on the deposition conditions. In
particular, the optical bandgap can be modified with the gas flow ratio during
low-pressure chemical vapor deposition (LPCVD). Here we show that this
parameter can be controlled in a highly reproducible manner, providing an
approach to synthesize the nonlinear Kerr coefficient of the material. This
holistic empirical study provides relevant guidelines to optimize the
properties of LPCVD silicon nitride waveguides for nonlinear optics
applications that rely on the Kerr effect.
",Physics,Physics
"On a Neumann-type series for modified Bessel functions of the first kind   In this paper, we are interested in a Neumann-type series for modified Bessel
functions of the first kind which arises in the study of Dunkl operators
associated with dihedral groups and as an instance of the Laguerre semigroup
constructed by Ben Said-Kobayashi-Orsted. We first revisit the particular case
corresponding to the group of square-preserving symmetries for which we give
two new and different proofs other than the existing ones. The first proof uses
the expansion of powers in a Neumann series of Bessel functions while the
second one is based on a quadratic transformation for the Gauss hypergeometric
function and opens the way to derive further expressions when the orders of the
underlying dihedral groups are powers of two. More generally, we give another
proof of De Bie \& al formula expressing this series as a $\Phi_2$-Horn
confluent hypergeometric function. In the course of proving, we shed the light
on the occurrence of multiple angles in their formula through elementary
symmetric functions, and get a new representation of Gegenbauer polynomials.
",Mathematics,Mathematics
"Antropologia de la Informatica Social: Teoria de la Convergencia Tecno-Social   The traditional humanism of the twentieth century, inspired by the culture of
the book, systematically distanced itself from the new society of digital
information; the Internet and tools of information processing revolutionized
the world, society during this period developed certain adaptive
characteristics based on coexistence (Human - Machine), this transformation
sets based on the impact of three technology segments: devices, applications
and infrastructure of social communication, which are involved in various
physical, behavioural and cognitive changes of the human being; and the
emergence of new models of influence and social control through the new
ubiquitous communication; however in this new process of conviviality new
models like the ""collaborative thinking"" and ""InfoSharing"" develop; managing
social information under three Human ontological dimensions (h) - Information
(i) - Machine (m), which is the basis of a new physical-cyber ecosystem, where
they coexist and develop new social units called ""virtual communities "". This
new communication infrastructure and social management of information given
discovered areas of vulnerability ""social perspective of risk"", impacting all
social units through massive impact vector (i); The virtual environment ""H + i
+ M""; and its components, as well as the life cycle management of social
information allows us to understand the path of integration ""Techno - Social""
and setting a new contribution to cybernetics, within the convergence of
technology with society and the new challenges of coexistence, aimed at a new
holistic and not pragmatic vision, as the human component (h) in the virtual
environment is the precursor of the future and needs to be studied not as an
application, but as the hub of a new society.
",Computer Science,Computer Science
"Identifiability of Gaussian Structural Equation Models with Dependent Errors Having Equal Variances   In this paper, we prove that some Gaussian structural equation models with
dependent errors having equal variances are identifiable from their
corresponding Gaussian distributions. Specifically, we prove identifiability
for the Gaussian structural equation models that can be represented as
Andersson-Madigan-Perlman chain graphs (Andersson et al., 2001). These chain
graphs were originally developed to represent independence models. However,
they are also suitable for representing causal models with additive noise
(Peña, 2016. Our result implies then that these causal models can be
identified from observational data alone. Our result generalizes the result by
Peters and Bühlmann (2014), who considered independent errors having equal
variances. The suitability of the equal error variances assumption should be
assessed on a per domain basis.
",Statistics,Mathematics; Statistics
"Redshift, metallicity and size of two extended dwarf Irregular galaxies. A link between dwarf Irregulars and Ultra Diffuse Galaxies?   We present the results of the spectroscopic and photometric follow-up of two
field galaxies that were selected as possible stellar counterparts of local
high velocity clouds. Our analysis shows that the two systems are distant (D>20
Mpc) dwarf irregular galaxies unrelated to the local HI clouds. However, the
newly derived distance and structural parameters reveal that the two galaxies
have luminosities and effective radii very similar to the recently identified
Ultra Diffuse Galaxies (UDGs). At odds with classical UDGs, they are remarkably
isolated, having no known giant galaxy within ~2.0 Mpc. Moreover, one of them
has a very high gas content compared to galaxies of similar stellar mass, with
a HI to stellar mass ratio M_HI/M_* ~90, typical of almost-dark dwarfs.
Expanding on this finding, we show that extended dwarf irregulars overlap the
distribution of UDGs in the M_V vs. log(r_e) plane and that the sequence
including dwarf spheroidals, dwarf irregulars and UDGs appears as continuously
populated in this plane.
",Physics,Physics
"Statistical foundations for assessing the difference between the classical and weighted-Gini betas   The `beta' is one of the key quantities in the capital asset pricing model
(CAPM). In statistical language, the beta can be viewed as the slope of the
regression line fitted to financial returns on the market against the returns
on the asset under consideration. The insurance counterpart of CAPM, called the
weighted insurance pricing model (WIPM), gives rise to the so-called
weighted-Gini beta. The aforementioned two betas may or may not coincide,
depending on the form of the underlying regression function, and this has
profound implications when designing portfolios and allocating risk capital. To
facilitate these tasks, in this paper we develop large-sample statistical
inference results that, in a straightforward fashion, imply confidence
intervals for, and hypothesis tests about, the equality of the two betas.
",Mathematics; Statistics,Quantitative Finance
"Genetic and Memetic Algorithm with Diversity Equilibrium based on Greedy Diversification   The lack of diversity in a genetic algorithm's population may lead to a bad
performance of the genetic operators since there is not an equilibrium between
exploration and exploitation. In those cases, genetic algorithms present a fast
and unsuitable convergence.
In this paper we develop a novel hybrid genetic algorithm which attempts to
obtain a balance between exploration and exploitation. It confronts the
diversity problem using the named greedy diversification operator. Furthermore,
the proposed algorithm applies a competition between parent and children so as
to exploit the high quality visited solutions. These operators are complemented
by a simple selection mechanism designed to preserve and take advantage of the
population diversity.
Additionally, we extend our proposal to the field of memetic algorithms,
obtaining an improved model with outstanding results in practice.
The experimental study shows the validity of the approach as well as how
important is taking into account the exploration and exploitation concepts when
designing an evolutionary algorithm.
",Computer Science,Computer Science; Statistics
"Characterizing Minimal Semantics-preserving Slices of predicate-linear, Free, Liberal Program Schemas   A program schema defines a class of programs, all of which have identical
statement structure, but whose functions and predicates may differ. A schema
thus defines an entire class of programs according to how its symbols are
interpreted. A subschema of a schema is obtained from a schema by deleting some
of its statements. We prove that given a schema $S$ which is predicate-linear,
free and liberal, such that the true and false parts of every if predicate
satisfy a simple additional condition, and a slicing criterion defined by the
final value of a given variable after execution of any program defined by $S$,
the minimal subschema of $S$ which respects this slicing criterion contains all
the function and predicate symbols `needed' by the variable according to the
data dependence and control dependence relations used in program slicing, which
is the symbol set given by Weiser's static slicing algorithm. Thus this
algorithm gives predicate-minimal slices for classes of programs represented by
schemas satisfying our set of conditions. We also give an example to show that
the corresponding result with respect to the slicing criterion defined by
termination behaviour is incorrect. This complements a result by the authors in
which $S$ was required to be function-linear, instead of predicate-linear.
",Computer Science,Computer Science
"Developing an edge computing platform for real-time descriptive analytics   The Internet of Mobile Things encompasses stream data being generated by
sensors, network communications that pull and push these data streams, as well
as running processing and analytics that can effectively leverage actionable
information for transportation planning, management, and business advantage.
Edge computing emerges as a new paradigm that decentralizes the communication,
computation, control and storage resources from the cloud to the edge of the
network. This paper proposes an edge computing platform where mobile edge nodes
are physical devices deployed on a transit bus where descriptive analytics is
used to uncover meaningful patterns from real-time transit data streams. An
application experiment is used to evaluate the advantages and disadvantages of
our proposed platform to support descriptive analytics at a mobile edge node
and generate actionable information to transit managers.
",Computer Science,Computer Science
"Refracting Metasurfaces without Spurious Diffraction   Refraction represents one of the most fundamental operations that may be
performed by a metasurface. However, simple phasegradient metasurface designs
suffer from restricted angular deflection due to spurious diffraction orders.
It has been recently shown, using a circuit-based approach, that refraction
without spurious diffraction, or diffraction-free, can fortunately be achieved
by a transverse metasurface exhibiting either loss-gain or bianisotropy. Here,
we rederive these conditions using a medium-based - and hence more insightfull
- approach based on Generalized Sheet Transition Conditions (GSTCs) and surface
susceptibility tensors, and experimentally demonstrate two diffraction-free
refractive metasurfaces that are essentially lossless, passive, bianisotropic
and reciprocal.
",Physics,Physics
"On Weyl's asymptotics and remainder term for the orthogonal and unitary groups   We examine the asymptotics of the spectral counting function of a compact
Riemannian manifold by V.G.~Avakumovic \cite{Avakumovic} and L.~Hörmander
\cite{Hormander-eigen} and show that for the scale of orthogonal and unitary
groups ${\bf SO}(N)$, ${\bf SU}(N)$, ${\bf U}(N)$ and ${\bf Spin}(N)$ it is not
sharp. While for negative sectional curvature improvements are possible and
known, {\it cf.} e.g., J.J.~Duistermaat $\&$ V.~Guillemin \cite{Duist-Guill},
here, we give sharp and contrasting examples in the positive Ricci curvature
case [non-negative for ${\bf U}(N)$]. Furthermore here the improvements are
sharp and quantitative relating to the dimension and {\it rank} of the group.
We discuss the implications of these results on the closely related problem of
closed geodesics and the length spectrum.
",Mathematics,Mathematics
"On the Parallel Parameterized Complexity of the Graph Isomorphism Problem   In this paper, we study the parallel and the space complexity of the graph
isomorphism problem (\GI{}) for several parameterizations. Let
$\mathcal{H}=\{H_1,H_2,\cdots,H_l\}$ be a finite set of graphs where
$|V(H_i)|\leq d$ for all $i$ and for some constant $d$. Let $\mathcal{G}$ be an
$\mathcal{H}$-free graph class i.e., none of the graphs $G\in \mathcal{G}$
contain any $H \in \mathcal{H}$ as an induced subgraph. We show that \GI{}
parameterized by vertex deletion distance to $\mathcal{G}$ is in a
parameterized version of $\AC^1$, denoted $\PL$-$\AC^1$, provided the colored
graph isomorphism problem for graphs in $\mathcal{G}$ is in $\AC^1$. From this,
we deduce that \GI{} parameterized by the vertex deletion distance to cographs
is in $\PL$-$\AC^1$.
The parallel parameterized complexity of \GI{} parameterized by the size of a
feedback vertex set remains an open problem. Towards this direction we show
that the graph isomorphism problem is in $\PL$-$\TC^0$ when parameterized by
vertex cover or by twin-cover.
Let $\mathcal{G}'$ be a graph class such that recognizing graphs from
$\mathcal{G}'$ and the colored version of \GI{} for $\mathcal{G}'$ is in
logspace ($\L$). We show that \GI{} for bounded vertex deletion distance to
$\mathcal{G}'$ is in $\L$. From this, we obtain logspace algorithms for \GI{}
for graphs with bounded vertex deletion distance to interval graphs and graphs
with bounded vertex deletion distance to cographs.
",Computer Science,Computer Science
"Semi-supervised Embedding in Attributed Networks with Outliers   In this paper, we propose a novel framework, called Semi-supervised Embedding
in Attributed Networks with Outliers (SEANO), to learn a low-dimensional vector
representation that systematically captures the topological proximity,
attribute affinity and label similarity of vertices in a partially labeled
attributed network (PLAN). Our method is designed to work in both transductive
and inductive settings while explicitly alleviating noise effects from
outliers. Experimental results on various datasets drawn from the web, text and
image domains demonstrate the advantages of SEANO over state-of-the-art methods
in semi-supervised classification under transductive as well as inductive
settings. We also show that a subset of parameters in SEANO is interpretable as
outlier score and can significantly outperform baseline methods when applied
for detecting network outliers. Finally, we present the use of SEANO in a
challenging real-world setting -- flood mapping of satellite images and show
that it is able to outperform modern remote sensing algorithms for this task.
",Computer Science,Computer Science
"Geometrically finite amalgamations of hyperbolic 3-manifold groups are not LERF   We prove that, for any two finite volume hyperbolic $3$-manifolds, the
amalgamation of their fundamental groups along any nontrivial geometrically
finite subgroup is not LERF. This generalizes the author's previous work on
nonLERFness of amalgamations of hyperbolic $3$-manifold groups along abelian
subgroups. A consequence of this result is that closed arithmetic hyperbolic
$4$-manifolds have nonLERF fundamental groups. Along with the author's previous
work, we get that, for any arithmetic hyperbolic manifold with dimension at
least $4$, with possible exceptions in $7$-dimensional manifolds defined by the
octonion, its fundamental group is not LERF.
",Mathematics,Mathematics
"Using polarimetry to retrieve the cloud coverage of Earth-like exoplanets   Context. Clouds have already been detected in exoplanetary atmospheres. They
play crucial roles in a planet's atmosphere and climate and can also create
ambiguities in the determination of atmospheric parameters such as trace gas
mixing ratios. Knowledge of cloud properties is required when assessing the
habitability of a planet. Aims. We aim to show that various types of cloud
cover such as polar cusps, subsolar clouds, and patchy clouds on Earth-like
exoplanets can be distinguished from each other using the polarization and flux
of light that is reflected by the planet. Methods. We have computed the flux
and polarization of reflected starlight for different types of (liquid water)
cloud covers on Earth-like model planets using the adding-doubling method, that
fully includes multiple scattering and polarization. Variations in cloud-top
altitudes and planet-wide cloud cover percentages were taken into account.
Results. We find that the different types of cloud cover (polar cusps, subsolar
clouds, and patchy clouds) can be distinguished from each other and that the
percentage of cloud cover can be estimated within 10%. Conclusions. Using our
proposed observational strategy, one should be able to determine basic orbital
parameters of a planet such as orbital inclination and estimate cloud coverage
with reduced ambiguities from the planet's polarization signals along its
orbit.
",Physics,Physics
"The First Measurement of the $2^{3}S_{1} \rightarrow 3^{3}P - 2^{3}P$ Tune-Out Wavelength in He*   The workhorse of atomic physics: quantum electrodynamics is one of the best
tested theories in physics. However recent discrepancies have shed doubt on its
accuracy for complex atomic systems. To facilitate the development of the
theory further we aim to measure transition dipole matrix elements of
metastable helium (He*) (the ideal 3 body test-bed) to the highest accuracy
thus far. We have undertaken a measurement of the `tune-out wavelength' which
occurs when the contributions to the dynamic polarizability from all atomic
transitions sum to zero; thus illuminating an atom with this wavelength of
light then produces no net energy shift. This provides a strict constraint on
the transition dipole matrix elements without the complication and inaccuracy
of other methods.
Using a novel atom-laser based technique we have made the first measurement
of the the tune-out wavelength in metastable helium between the
$3^{3}P_{1,2,3}$ and $2^{3}P_{1,2,3}$ states at 413.07(2)nm which compares well
with the predicted value\cite{Mitroy2013} of 413.02(9). We have additionally
developed many of the methods necessary to improve this measurement to the
100fm level of accuracy where it will form the most accurate determination of
transition rate information ever made in He* and provide a stringent test for
atomic QED simulations. We believe this measurement to be one of the most
sensitive ever made of an optical dipole potential, able to detect changes in
potentials of $\sim200pK$ and is widely applicable to other species and areas
of atom optics.
",Physics,Physics
"Learning to Optimize Neural Nets   Learning to Optimize is a recently proposed framework for learning
optimization algorithms using reinforcement learning. In this paper, we explore
learning an optimization algorithm for training shallow neural nets. Such
high-dimensional stochastic optimization problems present interesting
challenges for existing reinforcement learning algorithms. We develop an
extension that is suited to learning optimization algorithms in this setting
and demonstrate that the learned optimization algorithm consistently
outperforms other known optimization algorithms even on unseen tasks and is
robust to changes in stochasticity of gradients and the neural net
architecture. More specifically, we show that an optimization algorithm trained
with the proposed method on the problem of training a neural net on MNIST
generalizes to the problems of training neural nets on the Toronto Faces
Dataset, CIFAR-10 and CIFAR-100.
",Computer Science; Mathematics; Statistics,Computer Science; Statistics
"On the Complexity of Simple and Optimal Deterministic Mechanisms for an Additive Buyer   We show that the Revenue-Optimal Deterministic Mechanism Design problem for a
single additive buyer is #P-hard, even when the distributions have support size
2 for each item and, more importantly, even when the optimal solution is
guaranteed to be of a very simple kind: the seller picks a price for each
individual item and a price for the grand bundle of all the items; the buyer
can purchase either the grand bundle at its given price or any subset of items
at their total individual prices. The following problems are also #P-hard, as
immediate corollaries of the proof:
1. determining if individual item pricing is optimal for a given instance,
2. determining if grand bundle pricing is optimal, and
3. computing the optimal (deterministic) revenue.
On the positive side, we show that when the distributions are i.i.d. with
support size 2, the optimal revenue obtainable by any mechanism, even a
randomized one, can be achieved by a simple solution of the above kind
(individual item pricing with a discounted price for the grand bundle) and
furthermore, it can be computed in polynomial time. The problem can be solved
in polynomial time too when the number of items is constant.
",Computer Science,Computer Science
"Run-Wise Simulations for Imaging Atmospheric Cherenkov Telescope Arrays   We present a new paradigm for the simulation of arrays of Imaging Atmospheric
Cherenkov Telescopes (IACTs) which overcomes limitations of current approaches.
Up to now, all major IACT experiments rely on the same Monte-Carlo simulation
strategy, using predefined observation and instrument settings. Simulations
with varying parameters are generated to provide better estimates of the
Instrument Response Functions (IRFs) of different observations. However, a
large fraction of the simulation configuration remains preserved, leading to
complete negligence of all related influences. Additionally, the simulation
scheme relies on interpolations between different array configurations, which
are never fully reproducing the actual configuration for a given observation.
Interpolations are usually performed on zenith angles, off-axis angles, array
multiplicity, and the optical response of the instrument. With the advent of
hybrid systems consisting of a large number of IACTs with different sizes,
types, and camera configurations, the complexity of the interpolation and the
size of the phase space becomes increasingly prohibitive. Going beyond the
existing approaches, we introduce a new simulation and analysis concept which
takes into account the actual observation conditions as well as individual
telescope configurations of each observation run of a given data set. These
run-wise simulations (RWS) thus exhibit considerably reduced systematic
uncertainties compared to the existing approach, and are also more
computationally efficient and simple. The RWS framework has been implemented in
the H.E.S.S. software and tested, and is already being exploited in science
analysis.
",Physics,Physics
"Hierarchical RNN with Static Sentence-Level Attention for Text-Based Speaker Change Detection   Speaker change detection (SCD) is an important task in dialog modeling. Our
paper addresses the problem of text-based SCD, which differs from existing
audio-based studies and is useful in various scenarios, for example, processing
dialog transcripts where speaker identities are missing (e.g., OpenSubtitle),
and enhancing audio SCD with textual information. We formulate text-based SCD
as a matching problem of utterances before and after a certain decision point;
we propose a hierarchical recurrent neural network (RNN) with static
sentence-level attention. Experimental results show that neural networks
consistently achieve better performance than feature-based approaches, and that
our attention-based model significantly outperforms non-attention neural
networks.
",Computer Science,Computer Science
"A Digital Neuromorphic Architecture Efficiently Facilitating Complex Synaptic Response Functions Applied to Liquid State Machines   Information in neural networks is represented as weighted connections, or
synapses, between neurons. This poses a problem as the primary computational
bottleneck for neural networks is the vector-matrix multiply when inputs are
multiplied by the neural network weights. Conventional processing architectures
are not well suited for simulating neural networks, often requiring large
amounts of energy and time. Additionally, synapses in biological neural
networks are not binary connections, but exhibit a nonlinear response function
as neurotransmitters are emitted and diffuse between neurons. Inspired by
neuroscience principles, we present a digital neuromorphic architecture, the
Spiking Temporal Processing Unit (STPU), capable of modeling arbitrary complex
synaptic response functions without requiring additional hardware components.
We consider the paradigm of spiking neurons with temporally coded information
as opposed to non-spiking rate coded neurons used in most neural networks. In
this paradigm we examine liquid state machines applied to speech recognition
and show how a liquid state machine with temporal dynamics maps onto the
STPU-demonstrating the flexibility and efficiency of the STPU for instantiating
neural algorithms.
",Computer Science; Statistics,Computer Science; Statistics
"On Improving the Capacity of Solving Large-scale Wireless Network Design Problems by Genetic Algorithms   Over the last decade, wireless networks have experienced an impressive growth
and now play a main role in many telecommunications systems. As a consequence,
scarce radio resources, such as frequencies, became congested and the need for
effective and efficient assignment methods arose. In this work, we present a
Genetic Algorithm for solving large instances of the Power, Frequency and
Modulation Assignment Problem, arising in the design of wireless networks. To
our best knowledge, this is the first Genetic Algorithm that is proposed for
such problem. Compared to previous works, our approach allows a wider
exploration of the set of power solutions, while eliminating sources of
numerical problems. The performance of the algorithm is assessed by tests over
a set of large realistic instances of a Fixed WiMAX Network.
",Computer Science; Mathematics,Computer Science
"Lorentzian surfaces and the curvature of the Schmidt metric   The b-boundary is a mathematical tool used to attach a topological boundary
to incomplete Lorentzian manifolds using a Riemaniann metric called the Schmidt
metric on the frame bundle. In this paper, we give the general form of the
Schmidt metric in the case of Lorentzian surfaces. Furthermore, we write the
Ricci scalar of the Schmidt metric in terms of the Ricci scalar of the
Lorentzian manifold and give some examples. Finally, we discuss some
applications to general relativity.
",Mathematics,Mathematics
"Phase partitioning in a novel near equi-atomic AlCuFeMn alloy   A novel low cost, near equi-atomic alloy comprising of Al, Cu, Fe and Mn is
synthesized using arc-melting technique. The cast alloy possesses a dendritic
microstructure where the dendrites consist of disordered FCC and ordered FCC
phases. The inter-dendritic region is comprised of ordered FCC phase and
spinodally decomposed BCC phases. A Cu segregation is observed in the
inter-dendritic region while dendritic region is rich in Fe. The bulk hardness
of the alloy is ~ 380 HV, indicating significant yield strength.
",Physics,Physics
"Localization properties and high-fidelity state transfer in electronic hopping models with correlated disorder   We investigate a tight-binding electronic chain featuring diagonal and
off-diagonal disorder, these being modelled through the long-range-correlated
fractional Brownian motion. Particularly, by employing exact diagonalization
methods, we evaluate how the eigenstate spectrum of the system and its related
single-particle dynamics respond to both competing sources of disorder.
Moreover, we report the possibility of carrying out efficient end-to-end
quantum-state transfer protocols even in the presence of such generalized
disorder due to the appearance of extended states around the middle of the band
in the limit of strong correlations.
",Physics,Physics
"Threat Modeling Data Analysis in Socio-technical Systems   Our decision-making processes are becoming more data driven, based on data
from multiple sources, of different types, processed by a variety of
technologies. As technology becomes more relevant for decision processes, the
more likely they are to be subjects of attacks aimed at disrupting their
execution or changing their outcome. With the increasing complexity and
dependencies on technical components, such attempts grow more sophisticated and
their impact will be more severe. This is especially important in scenarios
with shared goals, which had to be previously agreed to, or decisions with
broad social impact. We need to think about our decisions-making and underlying
data analysis processes in a systemic way to correctly evaluate benefits and
risks of specific solutions and to design them to be resistant to attacks. To
reach these goals, we can apply experiences from threat modeling analysis used
in software security. We will need to adapt these practices to new types of
threats, protecting different assets and operating in socio-technical systems.
With these changes, threat modeling can become a foundation for implementing
detailed technical, organizational or legal mitigations and making our
decisions more reliable and trustworthy.
",Computer Science,Computer Science
"Thompson Sampling for a Fatigue-aware Online Recommendation System   In this paper we consider an online recommendation setting, where a platform
recommends a sequence of items to its users at every time period. The users
respond by selecting one of the items recommended or abandon the platform due
to fatigue from seeing less useful items. Assuming a parametric stochastic
model of user behavior, which captures positional effects of these items as
well as the abandoning behavior of users, the platform's goal is to recommend
sequences of items that are competitive to the single best sequence of items in
hindsight, without knowing the true user model a priori. Naively applying a
stochastic bandit algorithm in this setting leads to an exponential dependence
on the number of items. We propose a new Thompson sampling based algorithm with
expected regret that is polynomial in the number of items in this combinatorial
setting, and performs extremely well in practice. We also show a contextual
version of our solution.
",Computer Science; Statistics,Computer Science
"Are multi-factor Gaussian term structure models still useful? An empirical analysis on Italian BTPs   In this paper, we empirically study models for pricing Italian sovereign
bonds under a reduced form framework, by assuming different dynamics for the
short-rate process. We analyze classical Cox-Ingersoll-Ross and Vasicek
multi-factor models, with a focus on optimization algorithms applied in the
calibration exercise. The Kalman filter algorithm together with a maximum
likelihood estimation method are considered to fit the Italian term-structure
over a 12-year horizon, including the global financial crisis and the euro area
sovereign debt crisis. Analytic formulas for the gradient vector and the
Hessian matrix of the likelihood function are provided.
",Quantitative Finance,Mathematics; Statistics
"Coppersmith's lattices and ""focus groups"": an attack on small-exponent RSA   We present a principled technique for reducing the matrix size in some
applications of Coppersmith's lattice method for finding roots of modular
polynomial equations. It relies on an analysis of the actual performance of
Coppersmith's attack for smaller parameter sizes, which can be thought of as
""focus group"" testing. When applied to the small-exponent RSA problem, it
reduces lattice dimensions and consequently running times (sometimes by factors
of two or more). We also argue that existing metrics (such as enabling
condition bounds) are not as important as often thought for measuring the true
performance of attacks based on Coppersmith's method. Finally, experiments are
given to indicate that certain lattice reductive algorithms (such as
Nguyen-Stehlé's L2) may be particularly well-suited for Coppersmith's method.
",Computer Science; Mathematics,Physics; Statistics
"An efficient methodology for the analysis and modeling of computer experiments with large number of inputs   Complex computer codes are often too time expensive to be directly used to
perform uncertainty, sensitivity, optimization and robustness analyses. A
widely accepted method to circumvent this problem consists in replacing
cpu-time expensive computer models by cpu inexpensive mathematical functions,
called metamodels. For example, the Gaussian process (Gp) model has shown
strong capabilities to solve practical problems , often involving several
interlinked issues. However, in case of high dimensional experiments (with
typically several tens of inputs), the Gp metamodel building process remains
difficult, even unfeasible, and application of variable selection techniques
cannot be avoided. In this paper, we present a general methodology allowing to
build a Gp metamodel with large number of inputs in a very efficient manner.
While our work focused on the Gp metamodel, its principles are fully generic
and can be applied to any types of metamodel. The objective is twofold:
estimating from a minimal number of computer experiments a highly predictive
metamodel. This methodology is successfully applied on an industrial computer
code.
",Mathematics; Statistics,Computer Science; Statistics
"Hochschild Cohomology and Deformation Quantization of Affine Toric Varieties   For an affine toric variety $\mathrm{Spec}(A)$, we give a convex geometric
description of the Hodge decomposition of its Hochschild cohomology. Under
certain assumptions we compute the dimensions of the Hodge summands
$T^1_{(i)}(A)$, generalizing the existing results about the Andre-Quillen
cohomology group $T^1_{(1)}(A)$. We prove that every Poisson structure on a
possibly singular affine toric variety can be quantized in the sense of
deformation quantization.
",Mathematics,Mathematics
"Ensemble of Part Detectors for Simultaneous Classification and Localization   Part-based representation has been proven to be effective for a variety of
visual applications. However, automatic discovery of discriminative parts
without object/part-level annotations is challenging. This paper proposes a
discriminative mid-level representation paradigm based on the responses of a
collection of part detectors, which only requires the image-level labels.
Towards this goal, we first develop a detector-based spectral clustering method
to mine the representative and discriminative mid-level patterns for detector
initialization. The advantage of the proposed pattern mining technology is that
the distance metric based on detectors only focuses on discriminative details,
and a set of such grouped detectors offer an effective way for consistent
pattern mining. Relying on the discovered patterns, we further formulate the
detector learning process as a confidence-loss sparse Multiple Instance
Learning (cls-MIL) task, which considers the diversity of the positive samples,
while avoid drifting away the well localized ones by assigning a confidence
value to each positive sample. The responses of the learned detectors can form
an effective mid-level image representation for both image classification and
object localization. Experiments conducted on benchmark datasets demonstrate
the superiority of our method over existing approaches.
",Computer Science,Computer Science
"Are Bitcoin Bubbles Predictable? Combining a Generalized Metcalfe's Law and the LPPLS Model   We develop a strong diagnostic for bubbles and crashes in bitcoin, by
analyzing the coincidence (and its absence) of fundamental and technical
indicators. Using a generalized Metcalfe's law based on network properties, a
fundamental value is quantified and shown to be heavily exceeded, on at least
four occasions, by bubbles that grow and burst. In these bubbles, we detect a
universal super-exponential unsustainable growth. We model this universal
pattern with the Log-Periodic Power Law Singularity (LPPLS) model, which
parsimoniously captures diverse positive feedback phenomena, such as herding
and imitation. The LPPLS model is shown to provide an ex-ante warning of market
instabilities, quantifying a high crash hazard and probabilistic bracket of the
crash time consistent with the actual corrections; although, as always, the
precise time and trigger (which straw breaks the camel's back) being exogenous
and unpredictable. Looking forward, our analysis identifies a substantial but
not unprecedented overvaluation in the price of bitcoin, suggesting many months
of volatile sideways bitcoin prices ahead (from the time of writing, March
2018).
",Quantitative Finance,Quantitative Finance
"Gapless surface states originated from accidentally degenerate quadratic band touching in a three-dimensional tetragonal photonic crystal   A tetragonal photonic crystal composed of high-index pillars can exhibit a
frequency-isolated accidental degeneracy at a high-symmetry point in the first
Brillouin zone. A photonic band gap can be formed there by introducing a
geometrical anisotropy in the pillars. In this gap, gapless surface/domain-wall
states emerge under a certain condition. We analyze their physical property in
terms of an effective hamiltonian, and a good agreement between the effective
theory and numerical calculation is obtained.
",Physics,Physics
"Empirical analysis of non-linear activation functions for Deep Neural Networks in classification tasks   We provide an overview of several non-linear activation functions in a neural
network architecture that have proven successful in many machine learning
applications. We conduct an empirical analysis on the effectiveness of using
these function on the MNIST classification task, with the aim of clarifying
which functions produce the best results overall. Based on this first set of
results, we examine the effects of building deeper architectures with an
increasing number of hidden layers. We also survey the impact of using, on the
same task, different initialisation schemes for the weights of our neural
network. Using these sets of experiments as a base, we conclude by providing a
optimal neural network architecture that yields impressive results in accuracy
on the MNIST classification task.
",Computer Science; Statistics,Computer Science; Statistics
"Coupon Advertising in Online Social Systems: Algorithms and Sampling Techniques   Online social systems have become important platforms for viral marketing
where the advertising of products is carried out with the communication of
users. After adopting the product, the seed buyers may spread the information
to their friends via online messages e.g. posts and tweets. In another issue,
electronic coupon system is one of the relevant promotion vehicles that help
manufacturers and retailers attract more potential customers. By offering
coupons to seed buyers, there is a chance to convince the influential users who
are, however, at first not very interested in the product. In this paper, we
propose a coupon based online influence model and consider the problem that how
to maximize the profit by selecting appropriate seed buyers. The considered
problem herein is markedly different from other influence related problems as
its objective function is not monotone. We provide an algorithmic analysis and
give several algorithms designed with different sampling techniques. In
particular, we propose the RA-T and RA-S algorithms which are not only provably
effective but also scalable on large datasets. The proposed theoretical results
are evaluated by extensive experiments done on large-scale real-world social
networks. The analysis of this paper also provides an algorithmic framework for
non-monotone submodular maximization problems in social networks.
",Computer Science,Computer Science
"Nonlinear elliptic equations on Carnot groups   This article concerns a class of elliptic equations on Carnot groups
depending on one real positive parameter and involving a subcritical
nonlinearity (for the critical case we refer to G. Molica Bisci and D.
Repovš, Yamabe-type equations on Carnot groups, Potential Anal. 46:2
(2017), 369-383; arXiv:1705.10100 [math.AP]). As a special case of our results
we prove the existence of at least one nontrivial solution for a subelliptic
equation defined on a smooth and bounded domain $D$ of the Heisenberg group
$\mathbb{H}^n=\mathbb{C}^n\times \mathbb{R}$. The main approach is based on
variational methods.
",Mathematics,Mathematics
"Application of Spin-Exchange Relaxation-Free Magnetometry to the Cosmic Axion Spin Precession Experiment   The Cosmic Axion Spin Precession Experiment (CASPEr) seeks to measure
oscillating torques on nuclear spins caused by axion or axion-like-particle
(ALP) dark matter via nuclear magnetic resonance (NMR) techniques. A sample
spin-polarized along a leading magnetic field experiences a resonance when the
Larmor frequency matches the axion/ALP Compton frequency, generating precessing
transverse nuclear magnetization. Here we demonstrate a Spin-Exchange
Relaxation-Free (SERF) magnetometer with sensitivity $\approx 1~{\rm
fT/\sqrt{Hz}}$ and an effective sensing volume of 0.1 $\rm{cm^3}$ that may be
useful for NMR detection in CASPEr. A potential drawback of
SERF-magnetometer-based NMR detection is the SERF's limited dynamic range. Use
of a magnetic flux transformer to suppress the leading magnetic field is
considered as a potential method to expand the SERF's dynamic range in order to
probe higher axion/ALP Compton frequencies.
",Physics,Physics
"The Individual Impact Index ($i^3$) Statistic: A Novel Article-Level Citation Metric   Citation metrics are analytic measures used to evaluate the usage, impact and
dissemination of scientific research. Traditionally, citation metrics have been
independently measured at each level of the publication pyramid, namely at the
article-level, at the author-level, and at the journal-level. The most commonly
used metrics have been focused on journal-level measurements, such as the
Impact Factor and the Eigenfactor, as well as on researcher-level metrics like
the Hirsch index (h-index) and i10 index. On the other hand, reliable
article-level metrics are less widespread, and are often reserved to
non-standardized and non-scientific characteristics of individual articles,
such as views, citations, downloads, and mentions in social and news media.
These characteristics are known as 'altmetrics'. However, when the number of
views and citations are similar between two articles, no discriminating measure
currently exists with which to assess and compare each articles' individual
impact. Given the modern, exponentially growing scientific literature,
scientists and readers of Science need optimized, reliable, objective methods
for managing, measuring and comparing research outputs and individual
publications. To this end, I hereby describe and propose a new standardized
article-level metric henceforth known as the 'Individual Impact Index
Statistic', or $i^3$ for short. The $i^3$ is a weighted algorithm that takes
advantage of the peer-review process, and considers a number of characteristics
of individual scientific publications in order to yield a standardized and
readily comparable measure of impact and dissemination. The strengths,
limitations, and potential uses of this novel metric are also discussed.
",Computer Science,Computer Science
"On The Communication Complexity of High-Dimensional Permutations   We study the multiparty communication complexity of high dimensional
permutations, in the Number On the Forehead (NOF) model. This model is due to
Chandra, Furst and Lipton (CFL) who also gave a nontrivial protocol for the
Exactly-n problem where three players receive integer inputs and need to decide
if their inputs sum to a given integer $n$. There is a considerable body of
literature dealing with the same problem, where $(\mathbb{N},+)$ is replaced by
some other abelian group. Our work can be viewed as a far-reaching extension of
this line of work.
We show that the known lower bounds for that group-theoretic problem apply to
all high dimensional permutations. We introduce new proof techniques that
appeal to recent advances in Additive Combinatorics and Ramsey theory. We
reveal new and unexpected connections between the NOF communication complexity
of high dimensional permutations and a variety of well known and thoroughly
studied problems in combinatorics.
Previous protocols for Exactly-n all rely on the construction of large sets
of integers without a 3-term arithmetic progression. No direct algorithmic
protocol was previously known for the problem, and we provide the first such
algorithm. This suggests new ways to significantly improve the CFL protocol.
Many new open questions are presented throughout.
",Computer Science,Computer Science
"Nudging the particle filter   We investigate a new sampling scheme aimed at improving the performance of
particle filters whenever (a) there is a significant mismatch between the
assumed model dynamics and the actual system, or (b) the posterior probability
tends to concentrate in relatively small regions of the state space. The
proposed scheme pushes some particles towards specific regions where the
likelihood is expected to be high, an operation known as nudging in the
geophysics literature. We re-interpret nudging in a form applicable to any
particle filtering scheme, as it does not involve any changes in the rest of
the algorithm. Since the particles are modified, but the importance weights do
not account for this modification, the use of nudging leads to additional bias
in the resulting estimators. However, we prove analytically that nudged
particle filters can still attain asymptotic convergence with the same error
rates as conventional particle methods. Simple analysis also yields an
alternative interpretation of the nudging operation that explains its
robustness to model errors. Finally, we show numerical results that illustrate
the improvements that can be attained using the proposed scheme. In particular,
we present nonlinear tracking examples with synthetic data and a model
inference example using real-world financial data.
",Statistics,Mathematics; Statistics
"Incorporation of prior knowledge of the signal behavior into the reconstruction to accelerate the acquisition of MR diffusion data   Diffusion MRI measurements using hyperpolarized gases are generally acquired
during patient breath hold, which yields a compromise between achievable image
resolution, lung coverage and number of b-values. In this work, we propose a
novel method that accelerates the acquisition of MR diffusion data by
undersampling in both spatial and b-value dimensions, thanks to incorporating
knowledge about the signal decay into the reconstruction (SIDER). SIDER is
compared to total variation (TV) reconstruction by assessing their effect on
both the recovery of ventilation images and estimated mean alveolar dimensions
(MAD). Both methods are assessed by retrospectively undersampling diffusion
datasets of normal volunteers and COPD patients (n=8) for acceleration factors
between x2 and x10. TV led to large errors and artefacts for acceleration
factors equal or larger than x5. SIDER improved TV, presenting lower errors and
histograms of MAD closer to those obtained from fully sampled data for
accelerations factors up to x10. SIDER preserved image quality at all
acceleration factors but images were slightly smoothed and some details were
lost at x10. In conclusion, we have developed and validated a novel compressed
sensing method for lung MRI imaging and achieved high acceleration factors,
which can be used to increase the amount of data acquired during a breath-hold.
This methodology is expected to improve the accuracy of estimated lung
microstructure dimensions and widen the possibilities of studying lung diseases
with MRI.
",Computer Science; Physics,Physics
"Forecasting in the light of Big Data   Predicting the future state of a system has always been a natural motivation
for science and practical applications. Such a topic, beyond its obvious
technical and societal relevance, is also interesting from a conceptual point
of view. This owes to the fact that forecasting lends itself to two equally
radical, yet opposite methodologies. A reductionist one, based on the first
principles, and the naive inductivist one, based only on data. This latter view
has recently gained some attention in response to the availability of
unprecedented amounts of data and increasingly sophisticated algorithmic
analytic techniques. The purpose of this note is to assess critically the role
of big data in reshaping the key aspects of forecasting and in particular the
claim that bigger data leads to better predictions. Drawing on the
representative example of weather forecasts we argue that this is not generally
the case. We conclude by suggesting that a clever and context-dependent
compromise between modelling and quantitative analysis stands out as the best
forecasting strategy, as anticipated nearly a century ago by Richardson and von
Neumann.
",Physics,Computer Science; Statistics
"Diffusion of new products with recovering consumers   We consider the diffusion of new products in the discrete Bass-SIR model, in
which consumers who adopt the product can later ""recover"" and stop influencing
their peers to adopt the product. To gain insight into the effect of the social
network structure on the diffusion, we focus on two extreme cases. In the
""most-connected"" configuration where all consumers are inter-connected
(complete network), averaging over all consumers leads to an aggregate model,
which combines the Bass model for diffusion of new products with the SIR model
for epidemics. In the ""least-connected"" configuration where consumers are
arranged on a circle and each consumer can only be influenced by his left
neighbor (one-sided 1D network), averaging over all consumers leads to a
different aggregate model which is linear, and can be solved explicitly. We
conjecture that for any other network, the diffusion is bounded from below and
from above by that on a one-sided 1D network and on a complete network,
respectively. When consumers are arranged on a circle and each consumer can be
influenced by his left and right neighbors (two-sided 1D network), the
diffusion is strictly faster than on a one-sided 1D network. This is different
from the case of non-recovering adopters, where the diffusion on one-sided and
on two-sided 1D networks is identical. We also propose a nonlinear model for
recoveries, and show that consumers' heterogeneity has a negligible effect on
the aggregate diffusion.
",Computer Science; Physics,Computer Science
"Planar Drawings of Fixed-Mobile Bigraphs   A fixed-mobile bigraph G is a bipartite graph such that the vertices of one
partition set are given with fixed positions in the plane and the mobile
vertices of the other part, together with the edges, must be added to the
drawing. We assume that G is planar and study the problem of finding, for a
given k >= 0, a planar poly-line drawing of G with at most k bends per edge. In
the most general case, we show NP-hardness. For k=0 and under additional
constraints on the positions of the fixed or mobile vertices, we either prove
that the problem is polynomial-time solvable or prove that it belongs to NP.
Finally, we present a polynomial-time testing algorithm for a certain type of
""layered"" 1-bend drawings.
",Computer Science,Computer Science
"The strong ring of simplicial complexes   We define a ring R of geometric objects G generated by finite abstract
simplicial complexes. To every G belongs Hodge Laplacian H as the square of the
Dirac operator determining its cohomology and a unimodular connection matrix
L). The sum of the matrix entries of the inverse of L is the Euler
characteristic. The spectra of H as well as inductive dimension add under
multiplication while the spectra of L multiply. The nullity of the Hodge of H
are the Betti numbers which can now be signed. The map assigning to G its
Poincare polynomial is a ring homomorphism from R the polynomials. Especially
the Euler characteristic is a ring homomorphism. Also Wu characteristic
produces a ring homomorphism. The Kuenneth correspondence between cohomology
groups is explicit as a basis for the product can be obtained from a basis of
the factors. The product in R produces the strong product for the connection
graphs and leads to tensor products of connection Laplacians. The strong ring R
is also a subring of the full Stanley-Reisner ring S Every element G can be
visualized by its Barycentric refinement graph G1 and its connection graph G'.
Gauss-Bonnet, Poincare-Hopf or the Brouwer-Lefschetz extend to the strong ring.
The isomorphism of R with a subring of the strong Sabidussi ring shows that the
multiplicative primes in R are the simplicial complexes and that every
connected element in the strong ring has a unique prime factorization. The
Sabidussi ring is dual to the Zykov ring, in which the Zykov join is the
addition. The connection Laplacian of the d-dimensional lattice remains
invertible in the infinite volume limit: there is a mass gap in any dimension.
",Computer Science; Mathematics,Mathematics
"Construction of dynamical semigroups by a functional regularisation à la Kato   A functional version of the Kato one-parametric regularisation for the
construction of a dynamical semigroup generator of a relative bound one
perturbation is introduced. It does not require that the minus generator of the
unperturbed semigroup is a positivity preserving operator. The regularisation
is illustrated by an example of a boson-number cut-off regularisation.
",Mathematics,Mathematics
"Unveiling Bias Compensation in Turbo-Based Algorithms for (Discrete) Compressed Sensing   In Compressed Sensing, a real-valued sparse vector has to be recovered from
an underdetermined system of linear equations. In many applications, however,
the elements of the sparse vector are drawn from a finite set. Adapted
algorithms incorporating this additional knowledge are required for the
discrete-valued setup. In this paper, turbo-based algorithms for both cases are
elucidated and analyzed from a communications engineering perspective, leading
to a deeper understanding of the algorithm. In particular, we gain the
intriguing insight that the calculation of extrinsic values is equal to the
unbiasing of a biased estimate and present an improved algorithm.
",Computer Science,Computer Science; Statistics
"Cheap Orthogonal Constraints in Neural Networks: A Simple Parametrization of the Orthogonal and Unitary Group   We introduce a novel approach to perform first-order optimization with
orthogonal and unitary constraints. This approach is based on a parametrization
stemming from Lie group theory through the exponential map. The parametrization
transforms the constrained optimization problem into an unconstrained one over
a Euclidean space, for which common first-order optimization methods can be
used. The theoretical results presented are general enough to cover the special
orthogonal group, the unitary group and, in general, any connected compact Lie
group. We discuss how this and other parametrizations can be computed
efficiently through an implementation trick, making numerically complex
parametrizations usable at a negligible runtime cost in neural networks. In
particular, we apply our results to RNNs with orthogonal recurrent weights,
yielding a new architecture called expRNN. We demonstrate how our method
constitutes a more robust approach to optimization with orthogonal constraints,
showing faster, accurate, and more stable convergence in several tasks designed
to test RNNs.
",Computer Science; Statistics,Computer Science; Statistics
"Stability Analysis for Switched Systems with Sequence-based Average Dwell Time   This note investigates the stability of both linear and nonlinear switched
systems with average dwell time. Two new analysis methods are proposed.
Different from existing approaches, the proposed methods take into account the
sequence in which the subsystems are switched. Depending on the predecessor or
successor subsystems to be considered, sequence-based average preceding dwell
time (SBAPDT) and sequence-based average subsequence dwell time (SBASDT)
approaches are proposed and discussed for both continuous and discrete time
systems. These proposed methods, when considering the switch sequence, have the
potential to further reduce the conservativeness of the existing approaches. A
comparative numerical example is also given to demonstrate the advantages of
the proposed approaches.
",Computer Science,Computer Science
"On Convex Programming Relaxations for the Permanent   In recent years, several convex programming relaxations have been proposed to
estimate the permanent of a non-negative matrix, notably in the works of
Gurvits and Samorodnitsky. However, the origins of these relaxations and their
relationships to each other have remained somewhat mysterious. We present a
conceptual framework, implicit in the belief propagation literature, to
systematically arrive at these convex programming relaxations for estimating
the permanent -- as approximations to an exponential-sized max-entropy convex
program for computing the permanent. Further, using standard convex programming
techniques such as duality, we establish equivalence of these aforementioned
relaxations to those based on capacity-like quantities studied by Gurvits and
Anari et al.
",Computer Science; Mathematics,Mathematics
"An Optimal Control Formulation of Pulse-Based Control Using Koopman Operator   In many applications, and in systems/synthetic biology, in particular, it is
desirable to compute control policies that force the trajectory of a bistable
system from one equilibrium (the initial point) to another equilibrium (the
target point), or in other words to solve the switching problem. It was
recently shown that, for monotone bistable systems, this problem admits
easy-to-implement open-loop solutions in terms of temporal pulses (i.e., step
functions of fixed length and fixed magnitude). In this paper, we develop this
idea further and formulate a problem of convergence to an equilibrium from an
arbitrary initial point. We show that this problem can be solved using a static
optimization problem in the case of monotone systems. Changing the initial
point to an arbitrary state allows to build closed-loop, event-based or
open-loop policies for the switching/convergence problems. In our derivations
we exploit the Koopman operator, which offers a linear infinite-dimensional
representation of an autonomous nonlinear system. One of the main advantages of
using the Koopman operator is the powerful computational tools developed for
this framework. Besides the presence of numerical solutions, the
switching/convergence problem can also serve as a building block for solving
more complicated control problems and can potentially be applied to
non-monotone systems. We illustrate this argument on the problem of
synchronizing cardiac cells by defibrillation. Potentially, our approach can be
extended to problems with different parametrizations of control signals since
the only fundamental limitation is the finite time application of the control
signal.
",Computer Science; Mathematics,Computer Science
"Real-time monitoring of the structure of ultra thin Fe$_3$O$_4$ films during growth on Nb-doped SrTiO$_3$(001)   In this work thin magnetite films were deposited on SrTiO$_3$ via reactive
molecular beam epitaxy at different substrate temperatures. The growth process
was monitored in-situ during deposition by means of x-ray diffraction. While
the magnetite film grown at 400$^\circ$C shows a fully relaxed vertical lattice
constant already in the early growth stages, the film deposited at 270$^\circ$C
exhibits a strong vertical compressive strain and relaxes towards the bulk
value with increasing film thickness. Furthermore, a lateral tensile strain was
observed under these growth conditions although the inverse behavior is
expected due to the lattice mismatch of -7.5%. Additionally, the occupancy of
the A and B sublattices of magnetite with tetrahedral and octahedral sites was
investigated showing a lower occupancy of the A sites compared to an ideal
inverse spinel structure. The occupation of A sites decreases for a higher
growth temperature. Thus, we assume a relocation of the iron ions from
tetrahedral sites to octahedral vacancies forming a deficient rock salt
lattice.
",Physics,Physics
"Particle Filters for Partially-Observed Boolean Dynamical Systems   Partially-observed Boolean dynamical systems (POBDS) are a general class of
nonlinear models with application in estimation and control of Boolean
processes based on noisy and incomplete measurements. The optimal minimum mean
square error (MMSE) algorithms for POBDS state estimation, namely, the Boolean
Kalman filter (BKF) and Boolean Kalman smoother (BKS), are intractable in the
case of large systems, due to computational and memory requirements. To address
this, we propose approximate MMSE filtering and smoothing algorithms based on
the auxiliary particle filter (APF) method from sequential Monte-Carlo theory.
These algorithms are used jointly with maximum-likelihood (ML) methods for
simultaneous state and parameter estimation in POBDS models. In the presence of
continuous parameters, ML estimation is performed using the
expectation-maximization (EM) algorithm; we develop for this purpose a special
smoother which reduces the computational complexity of the EM algorithm. The
resulting particle-based adaptive filter is applied to a POBDS model of Boolean
gene regulatory networks observed through noisy RNA-Seq time series data, and
performance is assessed through a series of numerical experiments using the
well-known cell cycle gene regulatory model.
",Mathematics; Statistics,Computer Science; Statistics
"Corrupt Bandits for Preserving Local Privacy   We study a variant of the stochastic multi-armed bandit (MAB) problem in
which the rewards are corrupted. In this framework, motivated by privacy
preservation in online recommender systems, the goal is to maximize the sum of
the (unobserved) rewards, based on the observation of transformation of these
rewards through a stochastic corruption process with known parameters. We
provide a lower bound on the expected regret of any bandit algorithm in this
corrupted setting. We devise a frequentist algorithm, KLUCB-CF, and a Bayesian
algorithm, TS-CF and give upper bounds on their regret. We also provide the
appropriate corruption parameters to guarantee a desired level of local privacy
and analyze how this impacts the regret. Finally, we present some experimental
results that confirm our analysis.
",Computer Science; Statistics,Computer Science
"Approximating the Backbone in the Weighted Maximum Satisfiability Problem   The weighted Maximum Satisfiability problem (weighted MAX-SAT) is a NP-hard
problem with numerous applications arising in artificial intelligence. As an
efficient tool for heuristic design, the backbone has been applied to
heuristics design for many NP-hard problems. In this paper, we investigated the
computational complexity for retrieving the backbone in weighted MAX-SAT and
developed a new algorithm for solving this problem. We showed that it is
intractable to retrieve the full backbone under the assumption that . Moreover,
it is intractable to retrieve a fixed fraction of the backbone as well. And
then we presented a backbone guided local search (BGLS) with Walksat operator
for weighted MAX-SAT. BGLS consists of two phases: the first phase samples the
backbone information from local optima and the backbone phase conducts local
search under the guideline of backbone. Extensive experimental results on the
benchmark showed that BGLS outperforms the existing heuristics in both solution
quality and runtime.
",Computer Science,Computer Science
"On algebraic branching programs of small width   In 1979 Valiant showed that the complexity class VP_e of families with
polynomially bounded formula size is contained in the class VP_s of families
that have algebraic branching programs (ABPs) of polynomially bounded size.
Motivated by the problem of separating these classes we study the topological
closure VP_e-bar, i.e. the class of polynomials that can be approximated
arbitrarily closely by polynomials in VP_e. We describe VP_e-bar with a
strikingly simple complete polynomial (in characteristic different from 2)
whose recursive definition is similar to the Fibonacci numbers. Further
understanding this polynomial seems to be a promising route to new formula
lower bounds.
Our methods are rooted in the study of ABPs of small constant width. In 1992
Ben-Or and Cleve showed that formula size is polynomially equivalent to width-3
ABP size. We extend their result (in characteristic different from 2) by
showing that approximate formula size is polynomially equivalent to approximate
width-2 ABP size. This is surprising because in 2011 Allender and Wang gave
explicit polynomials that cannot be computed by width-2 ABPs at all! The
details of our construction lead to the aforementioned characterization of
VP_e-bar.
As a natural continuation of this work we prove that the class VNP can be
described as the class of families that admit a hypercube summation of
polynomially bounded dimension over a product of polynomially many affine
linear forms. This gives the first separations of algebraic complexity classes
from their nondeterministic analogs.
",Computer Science,Mathematics
"Primordial Black Holes and Slow-Roll Violation   For primordial black holes (PBH) to be the dark matter in single-field
inflation, the slow-roll approximation must be violated by at least ${\cal
O}(1)$ in order to enhance the curvature power spectrum within the required
number of efolds between CMB scales and PBH mass scales. Power spectrum
predictions which rely on the inflaton remaining on the slow-roll attractor can
fail dramatically leading to qualitatively incorrect conclusions in models like
an inflection potential and misestimate the mass scale in a running mass model.
We show that an optimized temporal evaluation of the Hubble slow-roll
parameters to second order remains a good description for a wide range of PBH
formation models where up to a $10^7$ amplification of power occurs in $10$
efolds or more.
",Physics,Physics
"On the origin of the hydraulic jump in a thin liquid film   For more than a century, it has been believed that all hydraulic jumps are
created due to gravity. However, we found that thin-film hydraulic jumps are
not induced by gravity. This study explores the initiation of thin-film
hydraulic jumps. For circular jumps produced by the normal impingement of a jet
onto a solid surface, we found that the jump is formed when surface tension and
viscous forces balance the momentum in the film and gravity plays no
significant role. Experiments show no dependence on the orientation of the
surface and a scaling relation balancing viscous forces and surface tension
collapses the experimental data. Experiments on thin film planar jumps in a
channel also show that the predominant balance is with surface tension,
although for the thickness of the films we studied gravity also played a role
in the jump formation. A theoretical analysis shows that the downstream
transport of surface tension energy is the previously neglected, critical
ingredient in these flows and that capillary waves play the role of gravity
waves in a traditional jump in demarcating the transition from the
supercritical to subcritical flow associated with these jumps.
",Physics,Physics
"Muon detector for the COSINE-100 experiment   The COSINE-100 dark matter search experiment has started taking physics data
with the goal of performing an independent measurement of the annual modulation
signal observed by DAMA/LIBRA. A muon detector was constructed by using plastic
scintillator panels in the outermost layer of the shield surrounding the
COSINE-100 detector. It is used to detect cosmic ray muons in order to
understand the impact of the muon annual modulation on dark matter analysis.
Assembly and initial performance test of each module have been performed at a
ground laboratory. The installation of the detector in Yangyang Underground
Laboratory (Y2L) was completed in the summer of 2016. Using three months of
data, the muon underground flux was measured to be 328 $\pm$ 1(stat.)$\pm$
10(syst.) muons/m$^2$/day. In this report, the assembly of the muon detector
and the results from the analysis are presented.
",Physics,Physics
"Restriction of representations of metaplectic $GL_{2}(F)$ to tori   Let $F$ be a non-Archimedean local field. We study the restriction of an
irreducible admissible genuine representations of the two fold metaplectic
cover $\widetilde{GL}_{2}(F)$ of $GL_{2}(F)$ to the inverse image in
$\widetilde{GL}_{2}(F)$ of a maximal torus in $GL_{2}(F)$.
",Mathematics,Mathematics
"PatternListener: Cracking Android Pattern Lock Using Acoustic Signals   Pattern lock has been widely used for authentication to protect user privacy
on mobile devices (e.g., smartphones and tablets). Given its pervasive usage,
the compromise of pattern lock could lead to serious consequences. Several
attacks have been constructed to crack the lock. However, these approaches
require the attackers to either be physically close to the target device or be
able to manipulate the network facilities (e.g., WiFi hotspots) used by the
victims. Therefore, the effectiveness of the attacks is significantly impacted
by the environment of mobile devices. Also, these attacks are not scalable
since they cannot easily infer unlock patterns of a large number of devices.
Motivated by an observation that fingertip motions on the screen of a mobile
device can be captured by analyzing surrounding acoustic signals on it, we
propose PatternListener, a novel acoustic attack that cracks pattern lock by
analyzing imperceptible acoustic signals reflected by the fingertip. It
leverages speakers and microphones of the victim's device to play imperceptible
audio and record the acoustic signals reflected by the fingertip. In
particular, it infers each unlock pattern by analyzing individual lines that
compose the pattern and are the trajectories of the fingertip. We propose
several algorithms to construct signal segments according to the captured
signals for each line and infer possible candidates of each individual line
according to the signal segments. Finally, we map all line candidates into grid
patterns and thereby obtain the candidates of the entire unlock pattern. We
implement a PatternListener prototype by using off-the-shelf smartphones and
thoroughly evaluate it using 130 unique patterns. The real experimental results
demonstrate that PatternListener can successfully exploit over 90% patterns
within five attempts.
",Computer Science,Computer Science
"Bio-Inspired Local Information-Based Control for Probabilistic Swarm Distribution Guidance   This paper addresses a task allocation problem for a large-scale robotic
swarm, namely swarm distribution guidance problem. Unlike most of the existing
frameworks handling this problem, the proposed framework suggests utilising
local information available to generate its time-varying stochastic policies.
As each agent requires only local consistency on information with neighbouring
agents, rather than the global consistency, the proposed framework offers
various advantages, e.g., a shorter timescale for using new information and
potential to incorporate an asynchronous decision-making process. We perform
theoretical analysis on the properties of the proposed framework. From the
analysis, it is proved that the framework can guarantee the convergence to the
desired density distribution even using local information while maintaining
advantages of global-information-based approaches. The design requirements for
these advantages are explicitly listed in this paper. This paper also provides
specific examples of how to implement the framework developed. The results of
numerical experiments confirm the effectiveness and comparability of the
proposed framework, compared with the global-information-based framework.
",Mathematics; Statistics,Computer Science
"Cantor series and rational numbers   The article is devoted to the investigation of representation of rational
numbers by Cantor series. Necessary and sufficient conditions for a rational
number to be representable by a positive Cantor series are formulated for the
case of an arbitrary sequence $(q_k)$ and some its corollaries are considered.
Results of this article were presented by the author of this article on the
International Conference on Algebra dedicated to 100th anniversary of S. M.
Chernikov (www.researchgate.net/publication/311415815,
www.researchgate.net/publication/301849984). This investigation was also
presented in some reports (links to the reports:
www.researchgate.net/publication/303736670,
www.researchgate.net/publication/303720573, etc.).
",Mathematics,Mathematics
"Automatically Leveraging MapReduce Frameworks for Data-Intensive Applications   MapReduce is a popular programming paradigm for developing large-scale,
data-intensive computation. Many frameworks that implement this paradigm have
recently been developed. To leverage these frameworks, however, developers must
become familiar with their APIs and rewrite existing code. Casper is a new tool
that automatically translates sequential Java programs into the MapReduce
paradigm. Casper identifies potential code fragments to rewrite and translates
them in two steps: (1) Casper uses program synthesis to search for a program
summary (i.e., a functional specification) of each code fragment. The summary
is expressed using a high-level intermediate language resembling the MapReduce
paradigm and verified to be semantically equivalent to the original using a
theorem prover. (2) Casper generates executable code from the summary, using
either the Hadoop, Spark, or Flink API. We evaluated Casper by automatically
converting real-world, sequential Java benchmarks to MapReduce. The resulting
benchmarks perform up to 48.2x faster compared to the original.
",Computer Science,Computer Science
"The effect of stellar and AGN feedback on the low redshift Lyman-$α$ forest in the Sherwood simulation suite   We study the effect of different feedback prescriptions on the properties of
the low redshift ($z\leq1.6$) Ly$\alpha$ forest using a selection of
hydrodynamical simulations drawn from the Sherwood simulation suite. The
simulations incorporate stellar feedback, AGN feedback and a simplified scheme
for efficiently modelling the low column density Ly$\alpha$ forest. We confirm
a discrepancy remains between Cosmic Origins Spectrograph (COS) observations of
the Ly$\alpha$ forest column density distribution function (CDDF) at $z \simeq
0.1$ for high column density systems ($N_{\rm HI}>10^{14}\rm\,cm^{-2}$), as
well as Ly$\alpha$ velocity widths that are too narrow compared to the COS
data. Stellar or AGN feedback -- as currently implemented in our simulations --
have only a small effect on the CDDF and velocity width distribution. We
conclude that resolving the discrepancy between the COS data and simulations
requires an increase in the temperature of overdense gas with $\Delta=4$--$40$,
either through additional He$\,\rm \scriptstyle II\ $ photo-heating at $z>2$ or
fine-tuned feedback that ejects overdense gas into the IGM at just the right
temperature for it to still contribute significantly to the Ly$\alpha$ forest.
Alternatively a larger, currently unresolved turbulent component to the line
width could resolve the discrepancy.
",Physics,Physics
"Positive solutions for nonlinear problems involving the one-dimensional ϕ-Laplacian   Let $\Omega:=\left( a,b\right) \subset\mathbb{R}$, $m\in L^{1}\left(
\Omega\right) $ and $\lambda>0$ be a real parameter. Let $\mathcal{L}$ be the
differential operator given by $\mathcal{L}u:=-\phi\left( u^{\prime}\right)
^{\prime}+r\left( x\right) \phi\left( u\right) $, where $\phi
:\mathbb{R\rightarrow R}$ is an odd increasing homeomorphism and $0\leq r\in
L^{1}\left( \Omega\right) $. We study the existence of positive solutions for
problems of the form $\mathcal{L}u=\lambda m\left( x\right) f\left( u\right)$
in $\Omega,$ $u=0$ on $\partial\Omega$, where $f:\left[ 0,\infty\right)
\rightarrow\left[ 0,\infty\right) $ is a continuos function which is, roughly
speaking, sublinear with respect to $\phi$. Our approach combines the sub and
supersolution method with some estimates on related nonlinear problems. We
point out that our results are new even in the cases $r\equiv0$ and/or
$m\geq0$.
",Mathematics,Mathematics
"The Combinatorics of Weighted Vector Compositions   A vector composition of a vector $\mathbf{\ell}$ is a matrix $\mathbf{A}$
whose rows sum to $\mathbf{\ell}$. We define a weighted vector composition as a
vector composition in which the column values of $\mathbf{A}$ may appear in
different colors. We study vector compositions from different viewpoints: (1)
We show how they are related to sums of random vectors and (2) how they allow
to derive formulas for partial derivatives of composite functions. (3) We study
congruence properties of the number of weighted vector compositions, for fixed
and arbitrary number of parts, many of which are analogous to those of ordinary
binomial coefficients and related quantities. Via the Central Limit Theorem and
their multivariate generating functions, (4) we also investigate the asymptotic
behavior of several special cases of numbers of weighted vector compositions.
Finally, (5) we conjecture an extension of a primality criterion due to Mann
and Shanks in the context of weighted vector compositions.
",Computer Science; Mathematics,Mathematics
"Approximations of the allelic frequency spectrum in general supercritical branching populations   We consider a general branching population where the lifetimes of individuals
are i.i.d.\ with arbitrary distribution and where each individual gives birth
to new individuals at Poisson times independently from each other. In addition,
we suppose that individuals experience mutations at Poissonian rate $\theta$
under the infinitely many alleles assumption assuming that types are
transmitted from parents to offspring. This mechanism leads to a partition of
the population by type, called the allelic partition. The main object of this
work is the frequency spectrum $A(k,t)$ which counts the number of families of
size $k$ in the population at time $t$. The process $(A(k,t),\
t\in\mathbb{R}_+)$ is an example of non-Markovian branching process belonging
to the class of general branching processes counted by random characteristics.
In this work, we propose methods of approximation to replace the frequency
spectrum by simpler quantities. Our main goal is study the asymptotic error
made during these approximations through central limit theorems. In a last
section, we perform several numerical analysis using this model, in particular
to analyze the behavior of one of these approximations with respect to Sabeti's
Extended Haplotype Homozygosity [18].
",Mathematics,Mathematics
"Existence and convexity of local solutions to degenerate hessian equations   In this work, we prove the existence of local convex solution to the
degenerate Hessian equation
",Mathematics,Mathematics
"Quasi-Static Internal Magnetic Field Detected in the Pseudogap Phase of Bi$_{2+x}$Sr$_{2-x}$CaCu$_2$O$_{8+δ}$ by $μ$SR   We report muon spin relaxation ($\mu$SR) measurements of optimally-doped and
overdoped Bi$_{2+x}$Sr$_{2-x}$CaCu$_2$O$_{8+\delta}$ (Bi2212) single crystals
that reveal the presence of a weak temperature-dependent quasi-static internal
magnetic field of electronic origin in the superconducting (SC) and pseudogap
(PG) phases. In both samples the internal magnetic field persists up to 160~K,
but muon diffusion prevents following the evolution of the field to higher
temperatures. We consider the evidence from our measurments in support of PG
order parameter candidates, namely, electronic loop currents and
magnetoelectric quadrupoles.
",Physics,Physics
"Activating spin-forbidden transitions in molecules by the highly localized plasmonic field   Optical spectroscopy has been the primary tool to study the electronic
structure of molecules. However the strict spin selection rule has severely
limited its ability to access states of different spin multiplicities. Here we
propose a new strategy to activate spin-forbidden transitions in molecules by
introducing spatially highly inhomogeneous plasmonic field. The giant
enhancement of the magnetic field strength resulted from the curl of the
inhomogeneous vector potential makes the transition between states of different
spin multiplicities naturally feasible. The dramatic effect of the
inhomogeneity of the plasmonic field on the spin and symmetry selection rules
is well illustrated by first principles calculations of C60. Remarkably, the
intensity of singlet-triplet transitions can even be stronger than that of
singlet-singlet transitions when the plasmon spatial distribution is comparable
with the molecular size. This approach offers a powerful means to completely
map out all excited states of molecules and to actively control their
photochemical processes. The same concept can also be applied to study nano and
biological systems.
",Physics,Physics
"Robust Regulation of Infinite-Dimensional Port-Hamiltonian Systems   We will give general sufficient conditions under which a controller achieves
robust regulation for a boundary control and observation system. Utilizing
these conditions we construct a minimal order robust controller for an
arbitrary order impedance passive linear port-Hamiltonian system. The
theoretical results are illustrated with a numerical example where we implement
a controller for a one-dimensional Euler-Bernoulli beam with boundary controls
and boundary observations.
",Mathematics,Mathematics
"White Matter Network Architecture Guides Direct Electrical Stimulation Through Optimal State Transitions   Electrical brain stimulation is currently being investigated as a therapy for
neurological disease. However, opportunities to optimize such therapies are
challenged by the fact that the beneficial impact of focal stimulation on both
neighboring and distant regions is not well understood. Here, we use network
control theory to build a model of brain network function that makes
predictions about how stimulation spreads through the brain's white matter
network and influences large-scale dynamics. We test these predictions using
combined electrocorticography (ECoG) and diffusion weighted imaging (DWI) data
who volunteered to participate in an extensive stimulation regimen. We posit a
specific model-based manner in which white matter tracts constrain stimulation,
defining its capacity to drive the brain to new states, including states
associated with successful memory encoding. In a first validation of our model,
we find that the true pattern of white matter tracts can be used to more
accurately predict the state transitions induced by direct electrical
stimulation than the artificial patterns of null models. We then use a targeted
optimal control framework to solve for the optimal energy required to drive the
brain to a given state. We show that, intuitively, our model predicts larger
energy requirements when starting from states that are farther away from a
target memory state. We then suggest testable hypotheses about which structural
properties will lead to efficient stimulation for improving memory based on
energy requirements. Our work demonstrates that individual white matter
architecture plays a vital role in guiding the dynamics of direct electrical
stimulation, more generally offering empirical support for the utility of
network control theoretic models of brain response to stimulation.
",Quantitative Biology,Quantitative Biology
"Asymptotics of ABC   We present an informal review of recent work on the asymptotics of
Approximate Bayesian Computation (ABC). In particular we focus on how does the
ABC posterior, or point estimates obtained by ABC, behave in the limit as we
have more data? The results we review show that ABC can perform well in terms
of point estimation, but standard implementations will over-estimate the
uncertainty about the parameters. If we use the regression correction of
Beaumont et al. then ABC can also accurately quantify this uncertainty. The
theoretical results also have practical implications for how to implement ABC.
",Mathematics; Statistics,Mathematics; Statistics
"RSI-CB: A Large Scale Remote Sensing Image Classification Benchmark via Crowdsource Data   Remote sensing image classification is a fundamental task in remote sensing
image processing. Remote sensing field still lacks of such a large-scale
benchmark compared to ImageNet, Place2. We propose a remote sensing image
classification benchmark (RSI-CB) based on crowd-source data which is massive,
scalable, and diversity. Using crowdsource data, we can efficiently annotate
ground objects in remotes sensing image by point of interests, vectors data
from OSM or other crowd-source data. Based on this method, we construct a
worldwide large-scale benchmark for remote sensing image classification. In
this benchmark, there are two sub datasets with 256 * 256 and 128 * 128 size
respectively since different convolution neural networks requirement different
image size. The former sub dataset contains 6 categories with 35 subclasses
with total of more than 24,000 images; the later one contains 6 categories with
45 subclasses with total of more than 36,000 images. The six categories are
agricultural land, construction land and facilities, transportation and
facilities, water and water conservancy facilities, woodland and other land,
and each category has several subclasses. This classification system is defined
according to the national standard of land use classification in China, and is
inspired by the hierarchy mechanism of ImageNet. Finally, we have done a large
number of experiments to compare RSI-CB with SAT-4, UC-Merced datasets on
handcrafted features, such as such as SIFT, and classical CNN models, such as
AlexNet, VGG, GoogleNet, and ResNet. We also show CNN models trained by RSI-CB
have good performance when transfer to other dataset, i.e. UC-Merced, and good
generalization ability. The experiments show that RSI-CB is more suitable as a
benchmark for remote sensing image classification task than other ones in big
data era, and can be potentially used in practical applications.
",Computer Science,Computer Science
"Multi-Entity Dependence Learning with Rich Context via Conditional Variational Auto-encoder   Multi-Entity Dependence Learning (MEDL) explores conditional correlations
among multiple entities. The availability of rich contextual information
requires a nimble learning scheme that tightly integrates with deep neural
networks and has the ability to capture correlation structures among
exponentially many outcomes. We propose MEDL_CVAE, which encodes a conditional
multivariate distribution as a generating process. As a result, the variational
lower bound of the joint likelihood can be optimized via a conditional
variational auto-encoder and trained end-to-end on GPUs. Our MEDL_CVAE was
motivated by two real-world applications in computational sustainability: one
studies the spatial correlation among multiple bird species using the eBird
data and the other models multi-dimensional landscape composition and human
footprint in the Amazon rainforest with satellite images. We show that
MEDL_CVAE captures rich dependency structures, scales better than previous
methods, and further improves on the joint likelihood taking advantage of very
large datasets that are beyond the capacity of previous methods.
",Computer Science; Statistics,Statistics
"Flow-Sensitive Composition of Thread-Modular Abstract Interpretation   We propose a constraint-based flow-sensitive static analysis for concurrent
programs by iteratively composing thread-modular abstract interpreters via the
use of a system of lightweight constraints. Our method is compositional in that
it first applies sequential abstract interpreters to individual threads and
then composes their results. It is flow-sensitive in that the causality
ordering of interferences (flow of data from global writes to reads) is modeled
by a system of constraints. These interference constraints are lightweight
since they only refer to the execution order of program statements as opposed
to their numerical properties: they can be decided efficiently using an
off-the-shelf Datalog engine. Our new method has the advantage of being more
accurate than existing, flow-insensitive, static analyzers while remaining
scalable and providing the expected soundness and termination guarantees even
for programs with unbounded data. We implemented our method and evaluated it on
a large number of benchmarks, demonstrating its effectiveness at increasing the
accuracy of thread-modular abstract interpretation.
",Computer Science,Computer Science
"Hydrodynamic stability in the presence of a stochastic forcing:a case study in convection   We investigate the stability of a statistically stationary conductive state
for Rayleigh-Bénard convection between stress-free plates that arises due to
a bulk stochastic internal heating. This setup may be seen as a generalization
to a stochastic setting of the seminal 1916 study of Lord Rayleigh. Our results
indicate that stochastic forcing at small magnitude has a stabilizing effect,
while strong stochastic forcing has a destabilizing effect. The methodology put
forth in this article, which combines rigorous analysis with careful
computation, also provides an approach to hydrodynamic stability for a variety
of systems subject to a large scale stochastic forcing.
",Physics; Mathematics,Physics
"Power-Sum Denominators   The power sum $1^n + 2^n + \cdots + x^n$ has been of interest to
mathematicians since classical times. Johann Faulhaber, Jacob Bernoulli, and
others who followed expressed power sums as polynomials in $x$ of degree $n+1$
with rational coefficients. Here we consider the denominators of these
polynomials, and prove some of their properties. A remarkable one is that such
a denominator equals $n+1$ times the squarefree product of certain primes $p$
obeying the condition that the sum of the base-$p$ digits of $n+1$ is at least
$p$. As an application, we derive a squarefree product formula for the
denominators of the Bernoulli polynomials.
",Mathematics,Mathematics
"Detecting Recycled Commodity SoCs: Exploiting Aging-Induced SRAM PUF Unreliability   A physical unclonable function (PUF), analogous to a human fingerprint, has
gained an enormous amount of attention from both academia and industry. SRAM
PUF is among one of the popular silicon PUF constructions that exploits random
initial power-up states from SRAM cells to extract hardware intrinsic secrets
for identification and key generation applications. The advantage of SRAM PUFs
is that they are widely embedded into commodity devices, thus such a PUF is
obtained without a custom design and virtually free of implementation costs. A
phenomenon known as `aging' alters the consistent
reproducibility---reliability---of responses that can be extracted from a
readout of a set of SRAM PUF cells. Similar to how a PUF exploits undesirable
manufacturing randomness for generating a hardware intrinsic fingerprint, SRAM
PUF unreliability induced by aging can be exploited to detect recycled
commodity devices requiring no additional cost to the device. In this context,
the SRAM PUF itself acts as an aging sensor by exploiting responses sensitive
to aging. We use SRAMs available in pervasively deployed commercial
off-the-shelf micro-controllers for experimental validations, which complements
recent work demonstrated in FPGA platforms, and we present a simplified
detection methodology along experimental results. We show that less than 1,000
SRAM responses are adequate to guarantee that both false acceptance rate and
false rejection rate are no more than 0.001.
",Computer Science,Computer Science
"Stealthy Deception Attacks Against SCADA Systems   SCADA protocols for Industrial Control Systems (ICS) are vulnerable to
network attacks such as session hijacking. Hence, research focuses on network
anomaly detection based on meta--data (message sizes, timing, command
sequence), or on the state values of the physical process. In this work we
present a class of semantic network-based attacks against SCADA systems that
are undetectable by the above mentioned anomaly detection. After hijacking the
communication channels between the Human Machine Interface (HMI) and
Programmable Logic Controllers (PLCs), our attacks cause the HMI to present a
fake view of the industrial process, deceiving the human operator into taking
manual actions. Our most advanced attack also manipulates the messages
generated by the operator's actions, reversing their semantic meaning while
causing the HMI to present a view that is consistent with the attempted human
actions. The attacks are totaly stealthy because the message sizes and timing,
the command sequences, and the data values of the ICS's state all remain
legitimate.
We implemented and tested several attack scenarios in the test lab of our
local electric company, against a real HMI and real PLCs, separated by a
commercial-grade firewall. We developed a real-time security assessment tool,
that can simultaneously manipulate the communication to multiple PLCs and cause
the HMI to display a coherent system--wide fake view. Our tool is configured
with message-manipulating rules written in an ICS Attack Markup Language (IAML)
we designed, which may be of independent interest. Our semantic attacks all
successfully fooled the operator and brought the system to states of blackout
and possible equipment damage.
",Computer Science,Computer Science
"The Tutte embedding of the mated-CRT map converges to Liouville quantum gravity   We prove that the Tutte embeddings (a.k.a. harmonic/embeddings) of certain
random planar maps converge to $\gamma$-Liouville quantum gravity
($\gamma$-LQG). Specifically, we treat mated-CRT maps, which are discretized
matings of correlated continuum random trees, and $\gamma$ ranges from $0$ to
$2$ as one varies the correlation parameter. We also show that the associated
space-filling path on the embedded map converges to space-filling
SLE$_{\kappa}$ for $\kappa =16/\gamma^2$ (in the annealed sense) and that
simple random walk on the embedded map converges to Brownian motion (in the
quenched sense). Our arguments also yield analogous statements for the Smith
(square tiling) embedding of the mated-CRT map.
This work constitutes the first proof that a discrete conformal embedding of
a random planar map converges to LQG. Many more such statements have been
conjectured. Since the mated-CRT map can be viewed as a coarse-grained
approximation to other random planar maps (the UIPT, tree-weighted maps,
bipolar-oriented maps, etc.), our results indicate a potential approach for
proving that embeddings of these maps converge to LQG as well.
To prove the main result, we establish several (independently interesting)
theorems about LQG surfaces decorated by space-filling SLE. There is a natural
way to use the SLE curve to divide the plane into `cells' corresponding to
vertices of the mated-CRT map. We study the law of the shape of the
origin-containing cell, in particular proving moments for the ratio of its
squared diameter to its area. We also give bounds on the degree of the
origin-containing cell and establish a form of ergodicity for the entire
configuration. Ultimately, we use these properties to show (using a general
theorem proved in a separate paper) that random walk on these cells converges
to a time change of Brownian motion, which in turn leads to the Tutte embedding
result.
",Mathematics,Physics
"Topological orders of strongly interacting particles   We investigate the self-organization of strongly interacting particles
confined in 1D and 2D. We consider hardcore bosons in spinless Hubbard lattice
models with short range interactions. We show that, many-body orders with
topological characteristics emerge, at different energy bands separated by
large gaps. These topological orders manifest in the way the particles organize
in real space to form states with different energy. Each of these states
contains topological defects/condensations whose Euler characteristic can be
used as a topological number to categorize states belonging to the same energy
band. We provide analytical formulas for this topological number and the full
energy spectrum of the system for both sparsely and densely filled systems.
Furthermore, we discuss the connection with the Gauss-Bonnet theorem of
differential geometry, by using the curvature generated in real space by the
particle structures. Our result is a demonstration of how topological orders
can arise in strongly interacting many-body systems with simple underlying
rules, without considering the spin, long-range microscopic interactions, or
external fields.
",Physics,Physics
"Boosting Adversarial Attacks with Momentum   Deep neural networks are vulnerable to adversarial examples, which poses
security concerns on these algorithms due to the potentially severe
consequences. Adversarial attacks serve as an important surrogate to evaluate
the robustness of deep learning models before they are deployed. However, most
of existing adversarial attacks can only fool a black-box model with a low
success rate. To address this issue, we propose a broad class of momentum-based
iterative algorithms to boost adversarial attacks. By integrating the momentum
term into the iterative process for attacks, our methods can stabilize update
directions and escape from poor local maxima during the iterations, resulting
in more transferable adversarial examples. To further improve the success rates
for black-box attacks, we apply momentum iterative algorithms to an ensemble of
models, and show that the adversarially trained models with a strong defense
ability are also vulnerable to our black-box attacks. We hope that the proposed
methods will serve as a benchmark for evaluating the robustness of various deep
models and defense methods. With this method, we won the first places in NIPS
2017 Non-targeted Adversarial Attack and Targeted Adversarial Attack
competitions.
",Computer Science; Statistics,Computer Science; Statistics
"Improved SVD-based Initialization for Nonnegative Matrix Factorization using Low-Rank Correction   Due to the iterative nature of most nonnegative matrix factorization
(\textsc{NMF}) algorithms, initialization is a key aspect as it significantly
influences both the convergence and the final solution obtained. Many
initialization schemes have been proposed for NMF, among which one of the most
popular class of methods are based on the singular value decomposition (SVD).
However, these SVD-based initializations do not satisfy a rather natural
condition, namely that the error should decrease as the rank of factorization
increases. In this paper, we propose a novel SVD-based \textsc{NMF}
initialization to specifically address this shortcoming by taking into account
the SVD factors that were discarded to obtain a nonnegative initialization.
This method, referred to as nonnegative SVD with low-rank correction
(NNSVD-LRC), allows us to significantly reduce the initial error at a
negligible additional computational cost using the low-rank structure of the
discarded SVD factors. NNSVD-LRC has two other advantages compared to previous
SVD-based initializations: (1) it provably generates sparse initial factors,
and (2) it is faster as it only requires to compute a truncated SVD of rank
$\lceil r/2 + 1 \rceil$ where $r$ is the factorization rank of the sought NMF
decomposition (as opposed to a rank-$r$ truncated SVD for other methods). We
show on several standard dense and sparse data sets that our new method
competes favorably with state-of-the-art SVD-based initializations for NMF.
",Computer Science; Statistics,Computer Science; Statistics
"Disorder-protected topological entropy after a quantum quench   Topological phases of matter are considered the bedrock of novel quantum
materials as well as ideal candidates for quantum computers that possess
robustness at the physical level. The robustness of the topological phase at
finite temperature or away from equilibrium is therefore a very desirable
feature. Disorder can improve the lifetime of the encoded topological qubits.
Here we tackle the problem of the survival of the topological phase as detected
by topological entropy, after a sudden quantum quench. We introduce a method to
study analytically the time evolution of the system after a quantum quench and
show that disorder in the couplings of the Hamiltonian of the toric code and
the resulting Anderson localization can make the topological entropy resilient.
",Physics,Physics
"Value Propagation for Decentralized Networked Deep Multi-agent Reinforcement Learning   We consider the networked multi-agent reinforcement learning (MARL) problem
in a fully decentralized setting, where agents learn to coordinate to achieve
the joint success. This problem is widely encountered in many areas including
traffic control, distributed control, and smart grids. We assume that the
reward function for each agent can be different and observed only locally by
the agent itself. Furthermore, each agent is located at a node of a
communication network and can exchanges information only with its neighbors.
Using softmax temporal consistency and a decentralized optimization method, we
obtain a principled and data-efficient iterative algorithm. In the first step
of each iteration, an agent computes its local policy and value gradients and
then updates only policy parameters. In the second step, the agent propagates
to its neighbors the messages based on its value function and then updates its
own value function. Hence we name the algorithm value propagation. We prove a
non-asymptotic convergence rate 1/T with the nonlinear function approximation.
To the best of our knowledge, it is the first MARL algorithm with convergence
guarantee in the control, off-policy and non-linear function approximation
setting. We empirically demonstrate the effectiveness of our approach in
experiments.
",Computer Science; Statistics,Computer Science; Statistics
"Evaluating Compositionality in Sentence Embeddings   An important challenge for human-like AI is compositional semantics. Recent
research has attempted to address this by using deep neural networks to learn
vector space embeddings of sentences, which then serve as input to other tasks.
We present a new dataset for one such task, `natural language inference' (NLI),
that cannot be solved using only word-level knowledge and requires some
compositionality. We find that the performance of state of the art sentence
embeddings (InferSent; Conneau et al., 2017) on our new dataset is poor. We
analyze the decision rules learned by InferSent and find that they are
consistent with simple heuristics that are ecologically valid in its training
dataset. Further, we find that augmenting training with our dataset improves
test performance on our dataset without loss of performance on the original
training dataset. This highlights the importance of structured datasets in
better understanding and improving AI systems.
",Statistics,Computer Science
"Efficient determination of optimised multi-arm multi-stage experimental designs with control of generalised error-rates   Primarily motivated by the drug development process, several publications
have now presented methodology for the design of multi-arm multi-stage
experiments with normally distributed outcome variables of known variance.
Here, we extend these past considerations to allow the design of what we refer
to as an abcd multi-arm multi-stage experiment. We provide a proof of how
strong control of the a-generalised type-I familywise error-rate can be
ensured. We then describe how to attain the power to reject at least b out of c
false hypotheses, which is related to controlling the b-generalised type-II
familywise error-rate. Following this, we detail how a design can be optimised
for a scenario in which rejection of any d null hypotheses brings about
termination of the experiment. We achieve this by proposing a highly
computationally efficient approach for evaluating the performance of a
candidate design. Finally, using a real clinical trial as a motivating example,
we explore the effect of the design's control parameters on the statistical
operating characteristics.
",Statistics,Statistics
"Direct and indirect seismic inversion: interpretation of certain mathematical theorems   Quantitative methods are more familiar to most geophysicists with direct
inversion or indirect inversion. We will discuss seismic inversion in a high
level sense without getting into the actual algorithms. We will stay with
meta-equations and argue pros and cons based on certain mathematical theorems.
",Physics,Mathematics
"Vibrational Density Matrix Renormalization Group   Variational approaches for the calculation of vibrational wave functions and
energies are a natural route to obtain highly accurate results with
controllable errors. However, the unfavorable scaling and the resulting high
computational cost of standard variational approaches limit their application
to small molecules with only few vibrational modes. Here, we demonstrate how
the density matrix renormalization group (DMRG) can be exploited to optimize
vibrational wave functions (vDMRG) expressed as matrix product states. We study
the convergence of these calculations with respect to the size of the local
basis of each mode, the number of renormalized block states, and the number of
DMRG sweeps required. We demonstrate the high accuracy achieved by vDMRG for
small molecules that were intensively studied in the literature. We then
proceed to show that the complete fingerprint region of the sarcosyn-glycin
dipeptide can be calculated with vDMRG.
",Physics,Physics
"Characterizing a CCD detector for astronomical purposes: OAUNI Project   This work verifies the instrumental characteristics of the CCD detector which
is part of the UNI astronomical observatory. We measured the linearity of the
CCD detector of the SBIG STXL6303E camera, along with the associated gain and
readout noise. The linear response to the incident light of the detector is
extremely linear (R2 =99.99%), its effective gain is 1.65 +/- 0.01 e-/ADU and
its readout noise is 12.2 e-. These values are in agreement with the
manufacturer. We confirm that this detector is extremely precise to make
measurements for astronomical purposes.
",Physics,Physics
"Learning from Experience: A Dynamic Closed-Loop QoE Optimization for Video Adaptation and Delivery   The quality of experience (QoE) is known to be subjective and
context-dependent. Identifying and calculating the factors that affect QoE is
indeed a difficult task. Recently, a lot of effort has been devoted to estimate
the users QoE in order to improve video delivery. In the literature, most of
the QoE-driven optimization schemes that realize trade-offs among different
quality metrics have been addressed under the assumption of homogenous
populations. Nevertheless, people perceptions on a given video quality may not
be the same, which makes the QoE optimization harder. This paper aims at taking
a step further in order to address this limitation and meet users profiles. To
do so, we propose a closed-loop control framework based on the
users(subjective) feedbacks to learn the QoE function and optimize it at the
same time. Our simulation results show that our system converges to a steady
state, where the resulting QoE function noticeably improves the users
feedbacks.
",Computer Science,Computer Science
"Motion optimization and parameter identification for a human and lower-back exoskeleton model   Designing an exoskeleton to reduce the risk of low-back injury during lifting
is challenging. Computational models of the human-robot system coupled with
predictive movement simulations can help to simplify this design process. Here,
we present a study that models the interaction between a human model actuated
by muscles and a lower-back exoskeleton. We provide a computational framework
for identifying the spring parameters of the exoskeleton using an optimal
control approach and forward-dynamics simulations. This is applied to generate
dynamically consistent bending and lifting movements in the sagittal plane. Our
computations are able to predict motions and forces of the human and
exoskeleton that are within the torque limits of a subject. The identified
exoskeleton could also yield a considerable reduction of the peak lower-back
torques as well as the cumulative lower-back load during the movements. This
work is relevant to the research communities working on human-robot
interaction, and can be used as a basis for a better human-centered design
process.
",Computer Science,Computer Science
"SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient   In this paper, we propose a StochAstic Recursive grAdient algoritHm (SARAH),
as well as its practical variant SARAH+, as a novel approach to the finite-sum
minimization problems. Different from the vanilla SGD and other modern
stochastic methods such as SVRG, S2GD, SAG and SAGA, SARAH admits a simple
recursive framework for updating stochastic gradient estimates; when comparing
to SAG/SAGA, SARAH does not require a storage of past gradients. The linear
convergence rate of SARAH is proven under strong convexity assumption. We also
prove a linear convergence rate (in the strongly convex case) for an inner loop
of SARAH, the property that SVRG does not possess. Numerical experiments
demonstrate the efficiency of our algorithm.
",Computer Science; Mathematics; Statistics,Computer Science; Statistics
"Self-dual and logarithmic representations of the twisted Heisenberg--Virasoro algebra at level zero   This paper is a continuation of arXiv:1405.1707. We present certain new
applications and generalizations of the free field realization of the twisted
Heisenberg-Virasoro algebra ${\mathcal H}$ at level zero.
We find explicit formulas for singular vectors in certain Verma modules. A
free field realization of self-dual modules for ${\mathcal H}$ is presented by
combining a bosonic construction of Whittaker modules from arXiv:1409.5354 with
a construction of logarithmic modules for vertex algebras. As an application,
we prove that there exists a non-split self-extension of irreducible self-dual
module which is a logarithmic module of rank two.
We construct a large family of logarithmic modules containing different types
of highest weight modules as subquotients. We believe that these logarithmic
modules are related with projective covers of irreducible modules in a suitable
category of ${\mathcal H}$-modules.
",Mathematics,Mathematics
"Modular groups, Hurwitz classes and dynamic portraits of NET maps   An orientation-preserving branched covering $f: S^2 \to S^2$ is a nearly
Euclidean Thurston (NET) map if each critical point is simple and its
postcritical set has exactly four points. Inspired by classical, non-dynamical
notions such as Hurwitz equivalence of branched covers of surfaces, we develop
invariants for such maps. We then apply these notions to the classification and
enumeration of NET maps. As an application, we obtain a complete classification
of the dynamic critical orbit portraits of NET maps.
",Mathematics,Physics
"An Observational Diagnostic for Distinguishing Between Clouds and Haze in Hot Exoplanet Atmospheres   The nature of aerosols in hot exoplanet atmospheres is one of the primary
vexing questions facing the exoplanet field. The complex chemistry, multiple
formation pathways, and lack of easily identifiable spectral features
associated with aerosols make it especially challenging to constrain their key
properties. We propose a transmission spectroscopy technique to identify the
primary aerosol formation mechanism for the most highly irradiated hot Jupiters
(HIHJs). The technique is based on the expectation that the two key types of
aerosols -- photochemically generated hazes and equilibrium condensate clouds
-- are expected to form and persist in different regions of a highly irradiated
planet's atmosphere. Haze can only be produced on the permanent daysides of
tidally-locked hot Jupiters, and will be carried downwind by atmospheric
dynamics to the evening terminator (seen as the trailing limb during transit).
Clouds can only form in cooler regions on the night side and morning terminator
of HIHJs (seen as the leading limb during transit). Because opposite limbs are
expected to be impacted by different types of aerosols, ingress and egress
spectra, which primarily probe opposing sides of the planet, will reveal the
dominant aerosol formation mechanism. We show that the benchmark HIHJ,
WASP-121b, has a transmission spectrum consistent with partial aerosol coverage
and that ingress-egress spectroscopy would constrain the location and formation
mechanism of those aerosols. In general, using this diagnostic we find that
observations with JWST and potentially with HST should be able to distinguish
between clouds and haze for currently known HIHJs.
",Physics,Physics
"Degenerate and chiral states in the extended Heisenberg model in the kagome lattice   We present a study of the low temperature phases of the antiferromagnetic
extended classical Heisenberg model in the kagome lattice, up to third nearest
neighbors. First, we focus on the degenerate lines in the boundaries of the
well-known staggered chiral phases. These boundaries have either semi-extensive
or extensive degeneracy, and we discuss the partial selection of states by
thermal fluctuations. Then, we study the model under an external magnetic field
on these lines and in the staggered chiral phases. We pay particular attention
to the highly frustrated point, where the three exchange couplings are equal.
We show that this point can me mapped to a model with spin liquid behavior and
non-zero chirality. Finally, we explore the effect of Dzyaloshinskii-Moriya
(DM) interactions in two ways: an homogeneous and a staggered DM interaction.
In both cases, there is a rich low temperature phase diagram, with different
spontaneously broken symmetries and non trivial chiral phases.
",Physics,Physics
"Discrete Integrable Systems, Supersymmetric Quantum Mechanics, and Framed BPS States - I   It is possible to understand whether a given BPS spectrum is generated by a
relevant deformation of a 4D N=2 SCFT or of an asymptotically free theory from
the periodicity properties of the corresponding quantum monodromy. With the aim
of giving a better understanding of the above conjecture, in this paper we
revisit the description of framed BPS states of four-dimensional relativistic
quantum field theories with eight conserved supercharges in terms of
supersymmetric quantum mechanics. We unveil aspects of the deep
interrelationship in between the Seiberg-dualities of the latter, the discrete
symmetries of the theory in the bulk, and quantum discrete integrable systems.
",Physics; Mathematics,Mathematics
"Learning Geometric Concepts with Nasty Noise   We study the efficient learnability of geometric concept classes -
specifically, low-degree polynomial threshold functions (PTFs) and
intersections of halfspaces - when a fraction of the data is adversarially
corrupted. We give the first polynomial-time PAC learning algorithms for these
concept classes with dimension-independent error guarantees in the presence of
nasty noise under the Gaussian distribution. In the nasty noise model, an
omniscient adversary can arbitrarily corrupt a small fraction of both the
unlabeled data points and their labels. This model generalizes well-studied
noise models, including the malicious noise model and the agnostic (adversarial
label noise) model. Prior to our work, the only concept class for which
efficient malicious learning algorithms were known was the class of
origin-centered halfspaces.
Specifically, our robust learning algorithm for low-degree PTFs succeeds
under a number of tame distributions -- including the Gaussian distribution
and, more generally, any log-concave distribution with (approximately) known
low-degree moments. For LTFs under the Gaussian distribution, we give a
polynomial-time algorithm that achieves error $O(\epsilon)$, where $\epsilon$
is the noise rate. At the core of our PAC learning results is an efficient
algorithm to approximate the low-degree Chow-parameters of any bounded function
in the presence of nasty noise. To achieve this, we employ an iterative
spectral method for outlier detection and removal, inspired by recent work in
robust unsupervised learning. Our aforementioned algorithm succeeds for a range
of distributions satisfying mild concentration bounds and moment assumptions.
The correctness of our robust learning algorithm for intersections of
halfspaces makes essential use of a novel robust inverse independence lemma
that may be of broader interest.
",Computer Science,Computer Science; Statistics
"Free quantitative fourth moment theorems on Wigner space   We prove a quantitative Fourth Moment Theorem for Wigner integrals of any
order with symmetric kernels, generalizing an earlier result from Kemp et al.
(2012). The proof relies on free stochastic analysis and uses a new biproduct
formula for bi-integrals. A consequence of our main result is a
Nualart-Ortiz-Latorre type characterization of convergence in law to the
semicircular distribution for Wigner integrals. As an application, we provide
Berry-Esseen type bounds in the context of the free Breuer-Major theorem for
the free fractional Brownian motion.
",Mathematics,Mathematics
"Plane graphs without 4- and 5-cycles and without ext-triangular 7-cycles are 3-colorable   Listed as No. 53 among the one hundred famous unsolved problems in [J. A.
Bondy, U. S. R. Murty, Graph Theory, Springer, Berlin, 2008] is Steinberg's
conjecture, which states that every planar graph without 4- and 5-cycles is
3-colorable. In this paper, we show that plane graphs without 4- and 5-cycles
are 3-colorable if they have no ext-triangular 7-cycles. This implies that (1)
planar graphs without 4-, 5-, 7-cycles are 3-colorable, and (2) planar graphs
without 4-, 5-, 8-cycles are 3-colorable, which cover a number of known results
in the literature motivated by Steinberg's conjecture.
",Mathematics,Mathematics
"GuideR: a guided separate-and-conquer rule learning in classification, regression, and survival settings   This article presents GuideR, a user-guided rule induction algorithm, which
overcomes the largest limitation of the existing methods-the lack of the
possibility to introduce user's preferences or domain knowledge to the rule
learning process. Automatic selection of attributes and attribute ranges often
leads to the situation in which resulting rules do not contain interesting
information. We propose an induction algorithm which takes into account user's
requirements. Our method uses the sequential covering approach and is suitable
for classification, regression, and survival analysis problems. The
effectiveness of the algorithm in all these tasks has been verified
experimentally, confirming guided rule induction to be a powerful data analysis
tool.
",Statistics,Computer Science; Statistics
"Grassmannian flows and applications to nonlinear partial differential equations   We show how solutions to a large class of partial differential equations with
nonlocal Riccati-type nonlinearities can be generated from the corresponding
linearized equations, from arbitrary initial data. It is well known that
evolutionary matrix Riccati equations can be generated by projecting linear
evolutionary flows on a Stiefel manifold onto a coordinate chart of the
underlying Grassmann manifold. Our method relies on extending this idea to the
infinite dimensional case. The key is an integral equation analogous to the
Marchenko equation in integrable systems, that represents the coodinate chart
map. We show explicitly how to generate such solutions to scalar partial
differential equations of arbitrary order with nonlocal quadratic
nonlinearities using our approach. We provide numerical simulations that
demonstrate the generation of solutions to
Fisher--Kolmogorov--Petrovskii--Piskunov equations with nonlocal
nonlinearities. We also indicate how the method might extend to more general
classes of nonlinear partial differential systems.
",Physics; Mathematics,Mathematics
"Spin conductance of YIG thin films driven from thermal to subthermal magnons regime by large spin-orbit torque   We report a study on spin conductance in ultra-thin films of Yttrium Iron
Garnet (YIG), where spin transport is provided by propagating spin waves, that
are generated and detected by direct and inverse spin Hall effects in two Pt
wires deposited on top. While at low current the spin conductance is dominated
by transport of thermal magnons, at high current, the spin conductance is
dominated by low-damping non-equilibrium magnons thermalized near the spectral
bottom by magnon-magnon interaction, with consequent a sensitivity to the
applied magnetic field and a longer decay length. This picture is supported by
microfocus Brillouin Light Scattering spectroscopy.
",Physics,Physics
"A Multiscale-Analysis of Stochastic Bistable Reaction-Diffusion Equations   A multiscale analysis of 1D stochastic bistable reaction-diffusion equations
with additive noise is carried out w.r.t. travelling waves within the
variational approach to stochastic partial differential equations. It is shown
with explicit error estimates on appropriate function spaces that up to lower
order w.r.t. the noise amplitude, the solution can be decomposed into the
orthogonal sum of a travelling wave moving with random speed and into Gaussian
fluctuations. A stochastic differential equation describing the speed of the
travelling wave and a linear stochastic partial differential equation
describing the fluctuations are derived in terms of the coefficients. Our
results extend corresponding results obtained for stochastic neural field
equations to the present class of stochastic dynamics.
",Mathematics,Mathematics
"Forest-based methods and ensemble model output statistics for rainfall ensemble forecasting   Rainfall ensemble forecasts have to be skillful for both low precipitation
and extreme events. We present statistical post-processing methods based on
Quantile Regression Forests (QRF) and Gradient Forests (GF) with a parametric
extension for heavy-tailed distributions. Our goal is to improve ensemble
quality for all types of precipitation events, heavy-tailed included, subject
to a good overall performance. Our hybrid proposed methods are applied to daily
51-h forecasts of 6-h accumulated precipitation from 2012 to 2015 over France
using the M{é}t{é}o-France ensemble prediction system called PEARP. They
provide calibrated pre-dictive distributions and compete favourably with
state-of-the-art methods like Analogs method or Ensemble Model Output
Statistics. In particular, hybrid forest-based procedures appear to bring an
added value to the forecast of heavy rainfall.
",Mathematics; Statistics,Statistics
"United Nations Digital Blue Helmets as a Starting Point for Cyber Peacekeeping   Prior works, such as the Tallinn manual on the international law applicable
to cyber warfare, focus on the circumstances of cyber warfare. Many
organizations are considering how to conduct cyber warfare, but few have
discussed methods to reduce, or even prevent, cyber conflict. A recent series
of publications started developing the framework of Cyber Peacekeeping (CPK)
and its legal requirements. These works assessed the current state of
organizations such as ITU IMPACT, NATO CCDCOE and Shanghai Cooperation
Organization, and found that they did not satisfy requirements to effectively
host CPK activities. An assessment of organizations currently working in the
areas related to CPK found that the United Nations (UN) has mandates and
organizational structures that appear to somewhat overlap the needs of CPK.
However, the UN's current approach to Peacekeeping cannot be directly mapped to
cyberspace. In this research we analyze the development of traditional
Peacekeeping in the United Nations, and current initiatives in cyberspace.
Specifically, we will compare the proposed CPK framework with the recent
initiative of the United Nations named the 'Digital Blue Helmets' as well as
with other projects in the UN which helps to predict and mitigate conflicts.
Our goal is to find practical recommendations for the implementation of the CPK
framework in the United Nations, and to examine how responsibilities defined in
the CPK framework overlap with those of the 'Digital Blue Helmets' and the
Global Pulse program.
",Computer Science,Computer Science
"A Submodularity-Based Approach for Multi-Agent Optimal Coverage Problems   We consider the optimal coverage problem where a multi-agent network is
deployed in an environment with obstacles to maximize a joint event detection
probability. The objective function of this problem is non-convex and no global
optimum is guaranteed by gradient-based algorithms developed to date. We first
show that the objective function is monotone submodular, a class of functions
for which a simple greedy algorithm is known to be within 0.63 of the optimal
solution. We then derive two tighter lower bounds by exploiting the curvature
information (total curvature and elemental curvature) of the objective
function. We further show that the tightness of these lower bounds is
complementary with respect to the sensing capabilities of the agents. The
greedy algorithm solution can be subsequently used as an initial point for a
gradient-based algorithm to obtain solutions even closer to the global optimum.
Simulation results show that this approach leads to significantly better
performance relative to previously used algorithms.
",Computer Science; Mathematics,Computer Science; Mathematics
"The finiteness dimension of modules and relative Cohen-Macaulayness   Let $R$ be a commutative Noetherian ring, $\mathfrak a$ and $\mathfrak b$
ideals of $R$. In this paper, we study the finiteness dimension $f_{\mathfrak
a}(M)$ of $M$ relative to $\mathfrak a$ and the $\mathfrak b$-minimum
$\mathfrak a$-adjusted depth $\lambda_{\mathfrak a}^{\mathfrak b}(M)$ of $M$,
where the underlying module $M$ is relative Cohen-Macaulay w.r.t $\mathfrak a$.
Some applications of such modules are given.
",Mathematics,Mathematics
"Riemannian Gaussian distributions on the space of positive-definite quaternion matrices   Recently, Riemannian Gaussian distributions were defined on spaces of
positive-definite real and complex matrices. The present paper extends this
definition to the space of positive-definite quaternion matrices. In order to
do so, it develops the Riemannian geometry of the space of positive-definite
quaternion matrices, which is shown to be a Riemannian symmetric space of
non-positive curvature. The paper gives original formulae for the Riemannian
metric of this space, its geodesics, and distance function. Then, it develops
the theory of Riemannian Gaussian distributions, including the exact expression
of their probability density, their sampling algorithm and statistical
inference.
",Mathematics; Statistics,Mathematics; Statistics
"Attribution of extreme rainfall in Southeast China during May 2015   Anthropogenic climate change increased the probability that a short-duration,
intense rainfall event would occur in parts of southeast China. This type of
event occurred in May 2015, causing serious flooding.
",Physics,Physics
"Topological Kondo insulators in one dimension: Continuous Haldane-type ground-state evolution from the strongly-interacting to the non-interacting limit   We study, by means of the density-matrix renormalization group (DMRG)
technique, the evolution of the ground state in a one-dimensional topological
insulator, from the non-interacting to the strongly-interacting limit, where
the system can be mapped onto a topological Kondo-insulator model. We focus on
a toy model Hamiltonian (i.e., the interacting ""$sp$-ladder"" model), which
could be experimentally realized in optical lattices with higher orbitals
loaded with ultra-cold fermionic atoms. Our goal is to shed light on the
emergence of the strongly-interacting ground state and its topological
classification as the Hubbard-$U$ interaction parameter of the model is
increased. Our numerical results show that the ground state can be generically
classified as a symmetry-protected topological phase of the Haldane-type, even
in the non-interacting case $U=0$ where the system can be additionally
classified as a time-reversal $\mathbb{Z}_{2}$-topological insulator, and
evolves adiabatically between the non-interacting and strongly interacting
limits.
",Physics,Physics
"Binarized octree generation for Cartesian adaptive mesh refinement around immersed geometries   We revisit the generation of balanced octrees for adaptive mesh refinement
(AMR) of Cartesian domains with immersed complex geometries. In a recent short
note [Hasbestan and Senocak, J. Comput. Phys. vol. 351:473-477 (2017)], we
showed that the data-locality of the Z-order curve in hashed linear octree
generation methods may not be perfect because of potential collisions in the
hash table. Building on that observation, we propose a binarized octree
generation method that complies with the Z-order curve exactly. Similar to a
hashed linear octree generation method, we use Morton encoding to index the
nodes of an octree, but use a red-black tree in place of the hash table.
Red-black tree is a special kind of a binary tree, which we use for insertion
and deletion of elements during mesh adaptation. By strictly working with the
bitwise representation of the octree, we remove computer hardware limitations
on the depth of adaptation on a single processor. Additionally, we introduce a
geometry encoding technique for rapidly tagging the solid geometry for
refinement. Our results for several geometries with different levels of
adaptations show that the binarized octree generation outperforms the linear
octree generation in terms of runtime performance at the expense of only a
slight increase in memory usage. We provide the current AMR capability as
open-source software.
",Computer Science; Physics,Computer Science
"Supercharacters and the discrete Fourier, cosine, and sine transforms   Using supercharacter theory, we identify the matrices that are diagonalized
by the discrete cosine and discrete sine transforms, respectively. Our method
affords a combinatorial interpretation for the matrix entries.
",Mathematics,Mathematics
"Advanced Steel Microstructural Classification by Deep Learning Methods   The inner structure of a material is called microstructure. It stores the
genesis of a material and determines all its physical and chemical properties.
While microstructural characterization is widely spread and well known, the
microstructural classification is mostly done manually by human experts, which
gives rise to uncertainties due to subjectivity. Since the microstructure could
be a combination of different phases or constituents with complex substructures
its automatic classification is very challenging and only a few prior studies
exist. Prior works focused on designed and engineered features by experts and
classified microstructures separately from the feature extraction step.
Recently, Deep Learning methods have shown strong performance in vision
applications by learning the features from data together with the
classification step. In this work, we propose a Deep Learning method for
microstructural classification in the examples of certain microstructural
constituents of low carbon steel. This novel method employs pixel-wise
segmentation via Fully Convolutional Neural Networks (FCNN) accompanied by a
max-voting scheme. Our system achieves 93.94% classification accuracy,
drastically outperforming the state-of-the-art method of 48.89% accuracy.
Beyond the strong performance of our method, this line of research offers a
more robust and first of all objective way for the difficult task of steel
quality appreciation.
",Computer Science; Physics,Computer Science; Statistics
"Angle-dependent electron spin resonance of YbRh$_2$Si$_2$ measured with planar microwave resonators and in-situ rotation   We present a new experimental approach to investigate the magnetic properties
of the anisotropic heavy-fermion system YbRh$_2$Si$_2$ as a function of
crystallographic orientation. Angle-dependent electron spin resonance (ESR)
measurements are performed at a low temperature of 1.6 K and at an ESR
frequency of 4.4 GHz utilizing a superconducting planar microwave resonator in
a $^4$He-cryostat in combination with in-situ sample rotation. The obtained ESR
g-factor of YbRh$_2$Si$_2$ as a function of the crystallographic angle is
consistent with results of previous measurements using conventional ESR
spectrometers at higher frequencies and fields. Perspectives to implement this
experimental approach into a dilution refrigerator and to reach the
magnetically ordered phase of YbRh$_2$Si$_2$ are discussed.
",Physics,Physics
"Surface plasmons in superintense laser-solid interactions   We review studies of superintense laser interaction with solid targets where
the generation of propagating surface plasmons (or surface waves) plays a key
role. These studies include the onset of plasma instabilities at the irradiated
surface, the enhancement of secondary emissions (protons, electrons, and
photons as high harmonics in the XUV range) in femtosecond interactions with
grating targets, and the generation of unipolar current pulses with picosecond
duration. The experimental results give evidence of the existence of surface
plasmons in the nonlinear regime of relativistic electron dynamics. These
findings open up a route to the improvement of ultrashort laser-driven sources
of energetic radiation and, more in general, to the extension of plasmonics in
a high field regime.
",Physics,Physics
"Two types of criticality in the brain   Neural networks with equal excitatory and inhibitory feedback show high
computational performance. They operate close to a critical point characterized
by the joint activation of large populations of neurons. Yet, in macaque motor
cortex we observe very different dynamics with weak fluctuations on the
population level. This suggests that motor cortex operates in a sub-optimal
regime. Here we show the opposite: the large dispersion of correlations across
neurons is a signature of a rich dynamical repertoire, hidden from macroscopic
brain signals, but essential for high performance in such concepts as reservoir
computing. Our findings suggest a refinement of the view on criticality in
neural systems: network topology and heterogeneity endow the brain with two
complementary substrates for critical dynamics of largely different
complexities.
",Physics,Quantitative Biology
"FORM version 4.2   We introduce FORM 4.2, a new minor release of the symbolic manipulation
toolkit. We demonstrate several new features, such as a new pattern matching
option, new output optimization, and automatic expansion of rational functions.
",Computer Science,Computer Science
"Sparse Bounds for Discrete Quadratic Phase Hilbert Transform   Consider the discrete quadratic phase Hilbert Transform acting on $\ell^{2}$
finitely supported functions $$ H^{\alpha} f(n) : = \sum_{m \neq 0} \frac{e^{2
\pi i\alpha m^2} f(n - m)}{m}. $$ We prove that, uniformly in $\alpha \in
\mathbb{T}$, there is a sparse bound for the bilinear form $\langle H^{\alpha}
f , g \rangle$. The sparse bound implies several mapping properties such as
weighted inequalities in an intersection of Muckenhoupt and reverse Hölder
classes.
",Mathematics,Mathematics
"Multiband NFC for High-Throughput Wireless Computer Vision Sensor Network   Vision sensors lie in the heart of computer vision. In many computer vision
applications, such as AR/VR, non-contacting near-field communication (NFC) with
high throughput is required to transfer information to algorithms. In this
work, we proposed a novel NFC system which utilizes multiple frequency bands to
achieve high throughput.
",Computer Science,Computer Science
"Incompressible fillings of manifolds   We find boundaries of Borel-Serre compactifications of locally symmetric
spaces, for which any filling is incompressible. We prove this result by
showing that these boundaries have small singular models and using these models
to obstruct compressions. We also show that small singular models of boundaries
obstruct $S^1$-actions (and more generally homotopically trivial $\mathbb
Z/p$-actions) on interiors of aspherical fillings. We use this to bound the
symmetry of complete Riemannian metrics on such interiors in terms of the
fundamental group. We also use small singular models to simplify the proofs of
some already known theorems about moduli spaces (the minimal orbifold theorem
and a topological analogue of Royden's theorem).
",Mathematics,Mathematics
"Metric Reduction and Generalized Holomorphic Structures   In this paper, metric reduction in generalized geometry is investigated. We
show how the Bismut connections on the quotient manifold are obtained from
those on the original manifold. The result facilitates the analysis of
generalized K$\ddot{a}$hler reduction, which motivates the concept of metric
generalized principal bundles and our approach to construct a family of
generalized holomorphic line bundles over $\mathbb{C}P^2$ equipped with some
non-trivial generalized K$\ddot{a}$hler structures.
",Mathematics,Mathematics
"Method for Aspect-Based Sentiment Annotation Using Rhetorical Analysis   This paper fills a gap in aspect-based sentiment analysis and aims to present
a new method for preparing and analysing texts concerning opinion and
generating user-friendly descriptive reports in natural language. We present a
comprehensive set of techniques derived from Rhetorical Structure Theory and
sentiment analysis to extract aspects from textual opinions and then build an
abstractive summary of a set of opinions. Moreover, we propose aspect-aspect
graphs to evaluate the importance of aspects and to filter out unimportant ones
from the summary. Additionally, the paper presents a prototype solution of data
flow with interesting and valuable results. The proposed method's results
proved the high accuracy of aspect detection when applied to the gold standard
dataset.
",Computer Science,Computer Science
"High-field transport properties of a P-doped BaFe2As2 film on technical substrate   High temperature (high-Tc) superconductors like cuprates have superior
critical current properties in magnetic fields over other superconductors.
However, superconducting wires for high-field-magnet applications are still
dominated by low-Tc Nb3Sn due probably to cost and processing issues. The
recent discovery of a second class of high-Tc materials, Fe-based
superconductors, may provide another option for high-field-magnet wires. In
particular, AEFe2As2 (AE: Alkali earth elements, AE-122) is one of the best
candidates for high-field-magnet applications because of its high upper
critical field, Hc2, moderate Hc2 anisotropy, and intermediate Tc. Here we
report on in-field transport properties of P-doped BaFe2As2 (Ba-122) thin films
grown on technical substrates (i.e., biaxially textured oxides templates on
metal tapes) by pulsed laser deposition. The P-doped Ba-122 coated conductor
sample exceeds a transport Jc of 10^5 A/cm^2 at 15 T for both major
crystallographic directions of the applied magnetic field, which is favourable
for practical applications. Our P-doped Ba-122 coated conductors show a
superior in-field Jc over MgB2 and NbTi, and a comparable level to Nb3Sn above
20 T. By analysing the E-J curves for determining Jc, a non-Ohmic linear
differential signature is observed at low field due to flux flow along the
grain boundaries. However, grain boundaries work as flux pinning centres as
demonstrated by the pinning force analysis.
",Physics,Physics
"Remarkably strong chemisorption of nitric oxide on insulating oxide films promoted by hybrid structure   The remarkably strong chemical adsorption behaviors of nitric oxide on
magnesia (001) film deposited on metal substrate have been investigated by
employing periodic density functional calculations with Van der Waals
corrections. The molybdenum supported magnesia (001) show significantly
enhanced adsorption properties and the nitric oxide is chemisorbed strongly and
preferably trapped in flat adsorption configuration on metal supported oxide
film, due to the substantially large adsorption energies and transformation
barriers. The analysis of Bader charges, projected density of states,
differential charge densities, electron localization function, highest occupied
orbital and particular orbital with largest Mg-NO-Mg bonding coefficients, are
applied to reveal the electronic adsorption properties and characteristics of
bonding between nitric oxide and surface as well as the bonding within the
hybrid structure. The strong chemical binding of nitric oxide on magnesia
deposited on molybdenum slab offers new opportunities for toxic gas detection
and treatment. We anticipate that hybrid structure promoted remarkable chemical
adsorption of nitric oxide on magnesia in this study will provide versatile
strategy for enhancing chemical reactivity and properties of insulating oxide.
",Physics,Physics
"Scaling of the Detonation Product State with Reactant Kinetic Energy   This submissions has been withdrawn by arXiv administrators because the
submitter did not have the right to agree to our license.
",Physics,Physics
"Effects of a Price limit Change on Market Stability at the Intraday Horizon in the Korean Stock Market   This paper investigates the effects of a price limit change on the volatility
of the Korean stock market's (KRX) intraday stock price process. Based on the
most recent transaction data from the KRX, which experienced a change in the
price limit on June 15, 2015, we examine the change in realized variance after
the price limit change to investigate the overall effects of the change on the
intraday market volatility. We then analyze the effects in more detail by
applying the discrete Fourier transform (DFT) to the data set. We find evidence
that the market becomes more volatile in the intraday horizon because of the
increase in the amplitudes of the low-frequency components of the price
processes after the price limit change. Therefore, liquidity providers are in a
worse situation than they were prior to the change.
",Quantitative Finance,Quantitative Finance
"Adjusting systematic bias in high dimensional principal component scores   Principal component analysis continues to be a powerful tool in dimension
reduction of high dimensional data. We assume a variance-diverging model and
use the high-dimension, low-sample-size asymptotics to show that even though
the principal component directions are not consistent, the sample and
prediction principal component scores can be useful in revealing the population
structure. We further show that these scores are biased, and the bias is
asymptotically decomposed into rotation and scaling parts. We propose methods
of bias-adjustment that are shown to be consistent and work well in the finite
but high dimensional situations with small sample sizes. The potential
advantage of bias-adjustment is demonstrated in a classification setting.
",Mathematics; Statistics,Statistics
"Heterogeneous Supervision for Relation Extraction: A Representation Learning Approach   Relation extraction is a fundamental task in information extraction. Most
existing methods have heavy reliance on annotations labeled by human experts,
which are costly and time-consuming. To overcome this drawback, we propose a
novel framework, REHession, to conduct relation extractor learning using
annotations from heterogeneous information source, e.g., knowledge base and
domain heuristics. These annotations, referred as heterogeneous supervision,
often conflict with each other, which brings a new challenge to the original
relation extraction task: how to infer the true label from noisy labels for a
given instance. Identifying context information as the backbone of both
relation extraction and true label discovery, we adopt embedding techniques to
learn the distributed representations of context, which bridges all components
with mutual enhancement in an iterative fashion. Extensive experimental results
demonstrate the superiority of REHession over the state-of-the-art.
",Computer Science,Computer Science
"Cusp shape and tunnel number   We show that the set of cusp shapes of hyperbolic tunnel number one manifolds
is dense in the Teichmuller space of the torus. A similar result holds for
tunnel number n manifolds. As a consequence, for fixed n, there are infinitely
many hyperbolic tunnel number n manifolds with at most one exceptional Dehn
filling. This is in contrast to large volume Berge knots, which are tunnel
number one manifolds, but with cusp shapes converging to a single point in
Teichmuller space.
",Mathematics,Mathematics
"Derivation of a multilayer approach to model suspended sediment transport: application to hyperpycnal and hypopycnal plumes   We propose a multi-layer approach to simulate hyperpycnal and hypopycnal
plumes in flows with free surface. The model allows to compute the vertical
profile of the horizontal and the vertical components of the velocity of the
fluid flow. The model can describe as well the vertical profile of the sediment
concentration and the velocity components of each one of the sediment species
that form the turbidity current. To do so, it takes into account the settling
velocity of the particles and their interaction with the fluid. This allows to
better describe the phenomena than a single layer approach. It is in better
agreement with the physics of the problem and gives promising results. The
numerical simulation is carried out by rewriting the multi-layer approach in a
compact formulation, which corresponds to a system with non-conservative
products, and using path-conservative numerical scheme. Numerical results are
presented in order to show the potential of the model.
",Physics,Physics
"A Survey on QoE-oriented Wireless Resources Scheduling   Future wireless systems are expected to provide a wide range of services to
more and more users. Advanced scheduling strategies thus arise not only to
perform efficient radio resource management, but also to provide fairness among
the users. On the other hand, the users' perceived quality, i.e., Quality of
Experience (QoE), is becoming one of the main drivers within the schedulers
design. In this context, this paper starts by providing a comprehension of what
is QoE and an overview of the evolution of wireless scheduling techniques.
Afterwards, a survey on the most recent QoE-based scheduling strategies for
wireless systems is presented, highlighting the application/service of the
different approaches reported in the literature, as well as the parameters that
were taken into account for QoE optimization. Therefore, this paper aims at
helping readers interested in learning the basic concepts of QoE-oriented
wireless resources scheduling, as well as getting in touch with the present
time research frontier.
",Computer Science,Computer Science
"Three hypergraph eigenvector centralities   Eigenvector centrality is a standard network analysis tool for determining
the importance of (or ranking of) entities in a connected system that is
represented by a graph. However, many complex systems and datasets have natural
multi-way interactions that are more faithfully modeled by a hypergraph. Here
we extend the notion of graph eigenvector centrality to uniform hypergraphs.
Traditional graph eigenvector centralities are given by a positive eigenvector
of the adjacency matrix, which is guaranteed to exist by the Perron-Frobenius
theorem under some mild conditions. The natural representation of a hypergraph
is a hypermatrix (colloquially, a tensor). Using recently established
Perron-Frobenius theory for tensors, we develop three tensor eigenvectors
centralities for hypergraphs, each with different interpretations. We show that
these centralities can reveal different information on real-world data by
analyzing hypergraphs constructed from n-gram frequencies, co-tagging on stack
exchange, and drug combinations observed in patient emergency room visits.
",Computer Science,Computer Science; Mathematics
"CTCF Degradation Causes Increased Usage of Upstream Exons in Mouse Embryonic Stem Cells   Transcriptional repressor CTCF is an important regulator of chromatin 3D
structure, facilitating the formation of topologically associating domains
(TADs). However, its direct effects on gene regulation is less well understood.
Here, we utilize previously published ChIP-seq and RNA-seq data to investigate
the effects of CTCF on alternative splicing of genes with CTCF sites. We
compared the amount of RNA-seq signals in exons upstream and downstream of
binding sites following auxin-induced degradation of CTCF in mouse embryonic
stem cells. We found that changes in gene expression following CTCF depletion
were significant, with a general increase in the presence of upstream exons. We
infer that a possible mechanism by which CTCF binding contributes to
alternative splicing is by causing pauses in the transcription mechanism during
which splicing elements are able to concurrently act on upstream exons already
transcribed into RNA.
",Quantitative Biology,Quantitative Biology
"Truncation in Hahn Fields is Undecidable and Wild   We show that in any nontrivial Hahn field with truncation as a primitive
operation we can interpret the monadic second-order logic of the additive
monoid of natural numbers and are thus undecidable. We also specify a definable
binary relation on such a structure that has $\SOP$ and $\TP$.
",Mathematics,Mathematics
"Autocorrelation and Lower Bound on the 2-Adic Complexity of LSB Sequence of $p$-ary $m$-Sequence   In modern stream cipher, there are many algorithms, such as ZUC, LTE
encryption algorithm and LTE integrity algorithm, using bit-component sequences
of $p$-ary $m$-sequences as the input of the algorithm. Therefore, analyzing
their statistical property (For example, autocorrelation, linear complexity and
2-adic complexity) of bit-component sequences of $p$-ary $m$-sequences is
becoming an important research topic. In this paper, we first derive some
autocorrelation properties of LSB (Least Significant Bit) sequences of $p$-ary
$m$-sequences, i.e., we convert the problem of computing autocorrelations of
LSB sequences of period $p^n-1$ for any positive $n\geq2$ to the problem of
determining autocorrelations of LSB sequence of period $p-1$. Then, based on
this property and computer calculation, we list some autocorrelation
distributions of LSB sequences of $p$-ary $m$-sequences with order $n$ for some
small primes $p$'s, such as $p=3,5,7,11,17,31$. Additionally, using their
autocorrelation distributions and the method inspired by Hu, we give the lower
bounds on the 2-adic complexities of these LSB sequences. Our results show that
the main parts of all the lower bounds on the 2-adic complexity of these LSB
sequencesare larger than $\frac{N}{2}$, where $N$ is the period of these
sequences. Therefor, these bounds are large enough to resist the analysis of
RAA (Rational Approximation Algorithm) for FCSR (Feedback with Carry Shift
Register). Especially, for a Mersenne prime $p=2^k-1$, since all its
bit-component sequences of a $p$-ary $m$-sequence are shift equivalent, our
results hold for all its bit-component sequences.
",Computer Science,Computer Science
"Robbins-Monro conditions for persistent exploration learning strategies   We formulate simple assumptions, implying the Robbins-Monro conditions for
the $Q$-learning algorithm with the local learning rate, depending on the
number of visits of a particular state-action pair (local clock) and the number
of iteration (global clock). It is assumed that the Markov decision process is
communicating and the learning policy ensures the persistent exploration. The
restrictions are imposed on the functional dependence of the learning rate on
the local and global clocks. The result partially confirms the conjecture of
Bradkte (1994).
",Statistics,Computer Science; Statistics
"A two-layer shallow water model for bedload sediment transport: convergence to Saint-Venant-Exner model   A two-layer shallow water type model is proposed to describe bedload sediment
transport. The upper layer is filled by water and the lower one by sediment.
The key point falls on the definition of the friction laws between the two
layers, which are a generalization of those introduced in Fernández-Nieto et
al. (ESAIM: M2AN, 51:115-145, 2017). This definition allows to apply properly
the two-layer shallow water model for the case of intense and slow bedload
sediment transport. Moreover, we prove that the two-layer model converges to a
Saint-Venant-Exner system (SVE) including gravitational effects when the ratio
between the hydrodynamic and morphodynamic time scales is small. The SVE with
gravitational effects is a degenerated nonlinear parabolic system. This means
that its numerical approximation is very expensive from a computational point
of view, see for example T. Morales de Luna et al. (J. Sci. Comp., 48(1):
258-273, 2011). In this work, gravitational effects are introduced into the
two-layer system without such extra computational cost. Finally, we also
consider a generalization of the model that includes a non-hydrostatic pressure
correction for the fluid layer and the boundary condition at the sediment
surface. Numerical tests show that the model provides promising results and
behave well in low transport rate regimes as well as in many other situations.
",Physics,Physics
"Uncertainty quantification in graph-based classification of high dimensional data   Classification of high dimensional data finds wide-ranging applications. In
many of these applications equipping the resulting classification with a
measure of uncertainty may be as important as the classification itself. In
this paper we introduce, develop algorithms for, and investigate the properties
of, a variety of Bayesian models for the task of binary classification; via the
posterior distribution on the classification labels, these methods
automatically give measures of uncertainty. The methods are all based around
the graph formulation of semi-supervised learning.
We provide a unified framework which brings together a variety of methods
which have been introduced in different communities within the mathematical
sciences. We study probit classification in the graph-based setting, generalize
the level-set method for Bayesian inverse problems to the classification
setting, and generalize the Ginzburg-Landau optimization-based classifier to a
Bayesian setting; we also show that the probit and level set approaches are
natural relaxations of the harmonic function approach introduced in [Zhu et al
2003].
We introduce efficient numerical methods, suited to large data-sets, for both
MCMC-based sampling as well as gradient-based MAP estimation. Through numerical
experiments we study classification accuracy and uncertainty quantification for
our models; these experiments showcase a suite of datasets commonly used to
evaluate graph-based semi-supervised learning algorithms.
",Computer Science; Statistics,Statistics
"Bayesian Pool-based Active Learning With Abstention Feedbacks   We study pool-based active learning with abstention feedbacks, where a
labeler can abstain from labeling a queried example with some unknown
abstention rate. This is an important problem with many useful applications. We
take a Bayesian approach to the problem and develop two new greedy algorithms
that learn both the classification problem and the unknown abstention rate at
the same time. These are achieved by simply incorporating the estimated
abstention rate into the greedy criteria. We prove that both of our algorithms
have near-optimality guarantees: they respectively achieve a
${(1-\frac{1}{e})}$ constant factor approximation of the optimal expected or
worst-case value of a useful utility function. Our experiments show the
algorithms perform well in various practical scenarios.
",Computer Science; Statistics,Computer Science; Statistics
"A Logical Approach to Cloud Federation   Federated clouds raise a variety of challenges for managing identity,
resource access, naming, connectivity, and object access control. This paper
shows how to address these challenges in a comprehensive and uniform way using
a data-centric approach. The foundation of our approach is a trust logic in
which participants issue authenticated statements about principals, objects,
attributes, and relationships in a logic language, with reasoning based on
declarative policy rules. We show how to use the logic to implement a trust
infrastructure for cloud federation that extends the model of NSF GENI, a
federated IaaS testbed. It captures shared identity management, GENI authority
services, cross-site interconnection using L2 circuits, and a naming and access
control system similar to AWS Identity and Access Management (IAM), but
extended to a federated system without central control.
",Computer Science,Computer Science
"Cyclic Hypergraph Degree Sequences   The problem of efficiently characterizing degree sequences of simple
hypergraphs is a fundamental long-standing open problem in Graph Theory.
Several results are known for restricted versions of this problem. This paper
adds to the list of sufficient conditions for a degree sequence to be {\em
hypergraphic}. This paper proves a combinatorial lemma about cyclically
permuting the columns of a binary table with length $n$ binary sequences as
rows. We prove that for any set of cyclic permutations acting on its columns,
the resulting table has all of its $2^n$ rows distinct. Using this property, we
first define a subset {\em cyclic hyper degrees} of hypergraphic sequences and
show that they admit a polynomial time recognition algorithm. Next, we prove
that there are at least $2^{\frac{(n-1)(n-2)}{2}}$ {\em cyclic hyper degrees},
which also serves as a lower bound on the number of {\em hypergraphic}
sequences. The {\em cyclic hyper degrees} also enjoy a structural
characterization, they are the integral points contained in the union of some
$n$-dimensional rectangles.
",Computer Science,Computer Science
"Amorphous Alloys, Degradation Performance of Azo Dyes: Review   Today freshwater is more important than ever before and it is contaminated
from textile industry. Removal of dyes from effluent of textile using amorphous
alloys has been studied extensively by many researchers. In this review article
it is presented up to date development on the azo dye degradation performance
of amorphous alloys, a new class of catalytic materials. Numerous amorphous
alloys have been developed for increasing higher degradation efficiency in
comparison to conventional ones for the removal of azo dyes in wastewater. One
of the objectives of this review article is to organize the scattered available
information on various aspects on a wide range of potentially effective in the
removal of dyes by using amorphous alloys. This study comprises the affective
removal factors of azo dye such as solution pH, initial dye concentration, and
adsorbent dosage. It was concluded that Fe, Mg, Co, Al and Mn-based amorphous
alloys with wide availability have appreciable for removing several types of
azo dyes from wastewater. Concerning amorphous alloys for future research, some
suggestions are proposed and conclusions have been drawn.
",Physics,Physics
"Topic Modeling on Health Journals with Regularized Variational Inference   Topic modeling enables exploration and compact representation of a corpus.
The CaringBridge (CB) dataset is a massive collection of journals written by
patients and caregivers during a health crisis. Topic modeling on the CB
dataset, however, is challenging due to the asynchronous nature of multiple
authors writing about their health journeys. To overcome this challenge we
introduce the Dynamic Author-Persona topic model (DAP), a probabilistic
graphical model designed for temporal corpora with multiple authors. The
novelty of the DAP model lies in its representation of authors by a persona ---
where personas capture the propensity to write about certain topics over time.
Further, we present a regularized variational inference algorithm, which we use
to encourage the DAP model's personas to be distinct. Our results show
significant improvements over competing topic models --- particularly after
regularization, and highlight the DAP model's unique ability to capture common
journeys shared by different authors.
",Statistics,Computer Science
"Unsupervised Learning by Predicting Noise   Convolutional neural networks provide visual features that perform remarkably
well in many computer vision applications. However, training these networks
requires significant amounts of supervision. This paper introduces a generic
framework to train deep networks, end-to-end, with no supervision. We propose
to fix a set of target representations, called Noise As Targets (NAT), and to
constrain the deep features to align to them. This domain agnostic approach
avoids the standard unsupervised learning issues of trivial solutions and
collapsing of features. Thanks to a stochastic batch reassignment strategy and
a separable square loss function, it scales to millions of images. The proposed
approach produces representations that perform on par with state-of-the-art
unsupervised methods on ImageNet and Pascal VOC.
",Computer Science; Statistics,Computer Science
"Regularity of symbolic powers and Arboricity of matroids   Let $\Delta$ be a simplicial complex of a matroid $M$. In this paper, we
explicitly compute the regularity of all the symbolic powers of a
Stanley-Reisner ideal $I_\Delta$ in terms of combinatorial data of the matroid
$M$. In order to do that, we provide a sharp bound between the arboricity of
$M$ and the circumference of its dual $M^*$.
",Mathematics,Mathematics
"Type-II Dirac Photons   The Dirac equation for relativistic electron waves is the parent model for
Weyl and Majorana fermions as well as topological insulators. Simulation of
Dirac physics in three-dimensional photonic crystals, though fundamentally
important for topological phenomena at optical frequencies, encounters the
challenge of synthesis of both Kramers double degeneracy and parity inversion.
Here we show how type-II Dirac points---exotic Dirac relativistic waves yet to
be discovered---are robustly realized through the nonsymmorphic screw symmetry.
The emergent type-II Dirac points carry nontrivial topology and are the mother
states of type-II Weyl points. The proposed all-dielectric architecture enables
robust cavity states at photonic-crystal---air interfaces and anomalous
refraction, with very low energy dissipation.
",Physics,Physics
"Unveiling Eilenberg-type Correspondences: Birkhoff's Theorem for (finite) Algebras + Duality   The purpose of the present paper is to show that: Eilenberg-type
correspondences = Birkhoff's theorem for (finite) algebras + duality. We
consider algebras for a monad T on a category D and we study (pseudo)varieties
of T-algebras. Pseudovarieties of algebras are also known in the literature as
varieties of finite algebras. Two well-known theorems that characterize
varieties and pseudovarieties of algebras play an important role here:
Birkhoff's theorem and Birkhoff's theorem for finite algebras, the latter also
known as Reiterman's theorem. We prove, under mild assumptions, a categorical
version of Birkhoff's theorem for (finite) algebras to establish a one-to-one
correspondence between (pseudo)varieties of T-algebras and (pseudo)equational
T-theories. Now, if C is a category that is dual to D and B is the comonad on C
that is the dual of T, we get a one-to-one correspondence between
(pseudo)equational T-theories and their dual, (pseudo)coequational B-theories.
Particular instances of (pseudo)coequational B-theories have been already
studied in language theory under the name of ""varieties of languages"" to
establish Eilenberg-type correspondences. All in all, we get a one-to-one
correspondence between (pseudo)varieties of T-algebras and (pseudo)coequational
B-theories, which will be shown to be exactly the nature of Eilenberg-type
correspondences.
",Computer Science; Mathematics,Mathematics
"The Molecular Structures of Local Arm and Perseus Arm in the Galactic Region of l=[139.75,149.75]$^\circ$, b=[-5.25,5.25]$^\circ$   Using the Purple Mountain Observatory Delingha (PMODLH) 13.7 m telescope, we
report a 96-square-degree 12CO/13CO/C18O mapping observation toward the
Galactic region of l = [139.75, 149.75]$^\circ$, b = [-5.25, 5.25]$^\circ$. The
molecular structure of the Local Arm and Perseus Arm are presented. Combining
HI data and part of the Outer Arm results, we obtain that the warp structure of
both atomic and molecular gas is obvious, while the flare structure only exists
in atomic gas in this observing region. In addition, five filamentary giant
molecular clouds on the Perseus Arm are identified. Among them, four are newly
identified. Their relations with the Milky Way large-scale structure are
discussed.
",Physics,Physics
"Continuous Relaxations for the Traveling Salesman Problem   In this work, we aim to explore connections between dynamical systems
techniques and combinatorial optimization problems. In particular, we construct
heuristic approaches for the traveling salesman problem (TSP) based on
embedding the relaxed discrete optimization problem into appropriate manifolds.
We explore multiple embedding techniques -- namely, the construction of new
dynamical systems on the manifold of orthogonal matrices and associated
Procrustes approximations of the TSP cost function. Using these dynamical
systems, we analyze the local neighborhood around the optimal TSP solutions
(which are equilibria) using computations to approximate the associated
\emph{stable manifolds}. We find that these flows frequently converge to
undesirable equilibria. However, the solutions of the dynamical systems and the
associated Procrustes approximation provide an interesting biasing approach for
the popular Lin--Kernighan heuristic which yields fast convergence. The
Lin--Kernighan heuristic is typically based on the computation of edges that
have a `high probability' of being in the shortest tour, thereby effectively
pruning the search space. Our new approach, instead, relies on a natural
relaxation of the combinatorial optimization problem to the manifold of
orthogonal matrices and the subsequent use of this solution to bias the
Lin--Kernighan heuristic. Although the initial cost of computing these edges
using the Procrustes solution is higher than existing methods, we find that the
Procrustes solution, when coupled with a homotopy computation, contains
valuable information regarding the optimal edges. We explore the Procrustes
based approach on several TSP instances and find that our approach often
requires fewer $k$-opt moves than existing approaches. Broadly, we hope that
this work initiates more work in the intersection of dynamical systems theory
and combinatorial optimization.
",Computer Science; Mathematics,Computer Science
"Balancing Selection Pressures, Multiple Objectives, and Neural Modularity to Coevolve Cooperative Agent Behavior   Previous research using evolutionary computation in Multi-Agent Systems
indicates that assigning fitness based on team vs.\ individual behavior has a
strong impact on the ability of evolved teams of artificial agents to exhibit
teamwork in challenging tasks. However, such research only made use of
single-objective evolution. In contrast, when a multiobjective evolutionary
algorithm is used, populations can be subject to individual-level objectives,
team-level objectives, or combinations of the two. This paper explores the
performance of cooperatively coevolved teams of agents controlled by artificial
neural networks subject to these types of objectives. Specifically, predator
agents are evolved to capture scripted prey agents in a torus-shaped grid
world. Because of the tension between individual and team behaviors, multiple
modes of behavior can be useful, and thus the effect of modular neural networks
is also explored. Results demonstrate that fitness rewarding individual
behavior is superior to fitness rewarding team behavior, despite being applied
to a cooperative task. However, the use of networks with multiple modules
allows predators to discover intelligent behavior, regardless of which type of
objectives are used.
",Computer Science,Computer Science
"On the presentation of Hecke-Hopf algebras for non-simply-laced type   Hecke-Hopf algebras were defined by A. Berenstein and D. Kazhdan. We give an
explicit presentation of an Hecke-Hopf algebra when the parameter $m_{ij},$
associated to any two distinct vertices $i$ and $j$ in the presentation of a
Coxeter group, equals $4,$ $5$ or $6$. As an application, we give a proof of a
conjecture of Berenstein and Kazhdan when the Coxeter group is crystallographic
and non-simply-laced. As another application, we show that another conjecture
of Berenstein and Kazhdan holds when $m_{ij},$ associated to any two distinct
vertices $i$ and $j,$ equals $4$ and that the conjecture does not hold when
some $m_{ij}$ equals $6$ by giving a counterexample to it.
",Mathematics,Mathematics
"Surface depression with double-angle geometry during the discharge of close-packed grains from a silo   When rough grains in standard packing conditions are discharged from a silo,
a conical depression with a single slope is formed at the surface. We observed
that the increase of the volume fraction generates a more complex depression
characterized by two angles of discharge: a lower angle close to the one
measured for standard packing and a considerably larger upper angle. The change
in slope appears at the boundary between a densely packed stagnant region at
the periphery and the central flowing channel formed over the aperture. Since
the material in the latter zone is always fluidized, the flow rate is
unaffected by the initial packing of the bed. On the other hand, the contrast
between both angles is markedly smaller when smooth particles of the same size
and density are used, which reveals that high volume fraction and friction must
combine to produce the observed geometry. Our results show that the surface
profile helps to identify by simple visual inspection the packing conditions of
a granular bed, and this can be useful to prevent undesirable collapses during
silo discharge in industry.
",Physics,Physics
"Finsler structures on holomorphic Lie algebroids   Complex Finsler vector bundles have been studied mainly by T. Aikou, who
defined complex Finsler structures on holomorphic vector bundles. In this
paper, we consider the more general case of a holomorphic Lie algebroid E and
we introduce Finsler structures, partial and Chern-Finsler connections on it.
First, we recall some basic notions on holomorphic Lie algebroids. Then, using
an idea from E. Martinez, we introduce the concept of complexified prolongation
of such an algebroid. Also, we study nonlinear and linear connections on the
tangent bundle of E and on the prolongation of E and we investigate the
relation between their coefficients. The analogue of the classical
Chern-Finsler connection is defined and studied in the paper for the case of
the holomorphic Lie algebroid.
",Mathematics,Mathematics
"Advanced reduced-order models for moisture diffusion in porous media   It is of great concern to produce numerically efficient methods for moisture
diffusion through porous media, capable of accurately calculate moisture
distribution with a reduced computational effort. In this way, model reduction
methods are promising approaches to bring a solution to this issue since they
do not degrade the physical model and provide a significant reduction of
computational cost. Therefore, this article explores in details the
capabilities of two model-reduction techniques - the Spectral Reduced-Order
Model (Spectral-ROM) and the Proper Generalised Decomposition (PGD) - to
numerically solve moisture diffusive transfer through porous materials. Both
approaches are applied to three different problems to provide clear examples of
the construction and use of these reduced-order models. The methodology of both
approaches is explained extensively so that the article can be used as a
numerical benchmark by anyone interested in building a reduced-order model for
diffusion problems in porous materials. Linear and non-linear unsteady
behaviors of unidimensional moisture diffusion are investigated. The last case
focuses on solving a parametric problem in which the solution depends on space,
time and the diffusivity properties. Results have highlighted that both methods
provide accurate solutions and enable to reduce significantly the order of the
model around ten times lower than the large original model. It also allows an
efficient computation of the physical phenomena with an error lower than
10^{-2} when compared to a reference solution.
",Computer Science; Physics; Mathematics,Physics
"Dependencies: Formalising Semantic Catenae for Information Retrieval   Building machines that can understand text like humans is an AI-complete
problem. A great deal of research has already gone into this, with astounding
results, allowing everyday people to discuss with their telephones, or have
their reading materials analysed and classified by computers. A prerequisite
for processing text semantics, common to the above examples, is having some
computational representation of text as an abstract object. Operations on this
representation practically correspond to making semantic inferences, and by
extension simulating understanding text. The complexity and granularity of
semantic processing that can be realised is constrained by the mathematical and
computational robustness, expressiveness, and rigour of the tools used.
This dissertation contributes a series of such tools, diverse in their
mathematical formulation, but common in their application to model semantic
inferences when machines process text. These tools are principally expressed in
nine distinct models that capture aspects of semantic dependence in highly
interpretable and non-complex ways. This dissertation further reflects on
present and future problems with the current research paradigm in this area,
and makes recommendations on how to overcome them.
The amalgamation of the body of work presented in this dissertation advances
the complexity and granularity of semantic inferences that can be made
automatically by machines.
",Computer Science,Computer Science
"Discriminative k-shot learning using probabilistic models   This paper introduces a probabilistic framework for k-shot image
classification. The goal is to generalise from an initial large-scale
classification task to a separate task comprising new classes and small numbers
of examples. The new approach not only leverages the feature-based
representation learned by a neural network from the initial task
(representational transfer), but also information about the classes (concept
transfer). The concept information is encapsulated in a probabilistic model for
the final layer weights of the neural network which acts as a prior for
probabilistic k-shot learning. We show that even a simple probabilistic model
achieves state-of-the-art on a standard k-shot learning dataset by a large
margin. Moreover, it is able to accurately model uncertainty, leading to well
calibrated classifiers, and is easily extensible and flexible, unlike many
recent approaches to k-shot learning.
",Computer Science; Statistics,Computer Science; Statistics
"Asymptotically safe cosmology - a status report   Asymptotic Safety, based on a non-Gaussian fixed point of the gravitational
renormalization group flow, provides an elegant mechanism for completing the
gravitational force at sub-Planckian scales. At high energies the fixed point
controls the scaling of couplings such that unphysical divergences are absent
while the emergence of classical low-energy physics is linked to a crossover
between two renormalization group fixed points. These features make Asymptotic
Safety an attractive framework for cosmological model building. The resulting
scenarios may naturally give rise to a quantum gravity driven inflationary
phase in the very early universe and an almost scale-free fluctuation spectrum.
Moreover, effective descriptions arising from an renormalization group
improvement permit a direct comparison to cosmological observations as, e.g.
Planck data.
",Physics,Physics
"A Machine Learning Framework for Stock Selection   This paper demonstrates how to apply machine learning algorithms to
distinguish good stocks from the bad stocks. To this end, we construct 244
technical and fundamental features to characterize each stock, and label stocks
according to their ranking with respect to the return-to-volatility ratio.
Algorithms ranging from traditional statistical learning methods to recently
popular deep learning method, e.g. Logistic Regression (LR), Random Forest
(RF), Deep Neural Network (DNN), and the Stacking, are trained to solve the
classification task. Genetic Algorithm (GA) is also used to implement feature
selection. The effectiveness of the stock selection strategy is validated in
Chinese stock market in both statistical and practical aspects, showing that:
1) Stacking outperforms other models reaching an AUC score of 0.972; 2) Genetic
Algorithm picks a subset of 114 features and the prediction performances of all
models remain almost unchanged after the selection procedure, which suggests
some features are indeed redundant; 3) LR and DNN are radical models; RF is
risk-neutral model; Stacking is somewhere between DNN and RF. 4) The portfolios
constructed by our models outperform market average in back tests.
",Statistics; Quantitative Finance,Statistics
"Carrier driven coupling in ferromagnetic oxide heterostructures   Transition metal oxides are well known for their complex magnetic and
electrical properties. When brought together in heterostructure geometries,
they show particular promise for spintronics and colossal magnetoresistance
applications. In this letter, we propose a new mechanism for the coupling
between layers of itinerant ferromagnetic materials in heterostructures. The
coupling is mediated by charge carriers that strive to maximally delocalize
through the heterostructure to gain kinetic energy. In doing so, they force a
ferromagnetic or antiferromagnetic coupling between the constituent layers. To
illustrate this, we focus on heterostructures composed of SrRuO$_3$ and
La$_{1-x}$A$_{x}$MnO$_3$ (A=Ca/Sr). Our mechanism is consistent with
antiferromagnetic alignment that is known to occur in multilayers of
SrRuO$_3$-La$_{1-x}$A$_{x}$MnO$_3$. To support our assertion, we present a
minimal Kondo-lattice model which reproduces the known magnetization properties
of such multilayers. In addition, we discuss a quantum well model for
heterostructures and argue that the spin-dependent density of states determines
the nature of the coupling. As a smoking gun signature, we propose that
bilayers with the same constituents will oscillate between ferromagnetic and
antiferromagnetic coupling upon tuning the relative thicknesses of the layers.
",Physics,Physics
"Personalized Thread Recommendation for MOOC Discussion Forums   Social learning, i.e., students learning from each other through social
interactions, has the potential to significantly scale up instruction in online
education. In many cases, such as in massive open online courses (MOOCs),
social learning is facilitated through discussion forums hosted by course
providers. In this paper, we propose a probabilistic model for the process of
learners posting on such forums, using point processes. Different from existing
works, our method integrates topic modeling of the post text, timescale
modeling of the decay in post activity over time, and learner topic interest
modeling into a single model, and infers this information from user data. Our
method also varies the excitation levels induced by posts according to the
thread structure, to reflect typical notification settings in discussion
forums. We experimentally validate the proposed model on three real-world MOOC
datasets, with the largest one containing up to 6,000 learners making 40,000
posts in 5,000 threads. Results show that our model excels at thread
recommendation, achieving significant improvement over a number of baselines,
thus showing promise of being able to direct learners to threads that they are
interested in more efficiently. Moreover, we demonstrate analytics that our
model parameters can provide, such as the timescales of different topic
categories in a course.
",Computer Science; Statistics,Computer Science
"Computational Results for Extensive-Form Adversarial Team Games   We provide, to the best of our knowledge, the first computational study of
extensive-form adversarial team games. These games are sequential, zero-sum
games in which a team of players, sharing the same utility function, faces an
adversary. We define three different scenarios according to the communication
capabilities of the team. In the first, the teammates can communicate and
correlate their actions both before and during the play. In the second, they
can only communicate before the play. In the third, no communication is
possible at all. We define the most suitable solution concepts, and we study
the inefficiency caused by partial or null communication, showing that the
inefficiency can be arbitrarily large in the size of the game tree.
Furthermore, we study the computational complexity of the equilibrium-finding
problem in the three scenarios mentioned above, and we provide, for each of the
three scenarios, an exact algorithm. Finally, we empirically evaluate the
scalability of the algorithms in random games and the inefficiency caused by
partial or null communication.
",Computer Science,Computer Science
"Novel Structured Low-rank algorithm to recover spatially smooth exponential image time series   We propose a structured low rank matrix completion algorithm to recover a
time series of images consisting of linear combination of exponential
parameters at every pixel, from under-sampled Fourier measurements. The spatial
smoothness of these parameters is exploited along with the exponential
structure of the time series at every pixel, to derive an annihilation relation
in the $k-t$ domain. This annihilation relation translates into a structured
low rank matrix formed from the $k-t$ samples. We demonstrate the algorithm in
the parameter mapping setting and show significant improvement over state of
the art methods.
",Computer Science,Computer Science
"Unidirectional control of optically induced spin waves   Unidirectional control of optically induced spin waves in a rare-earth iron
garnet crystal is demonstrated. We observed the interference of two spin-wave
packets with different initial phases generated by circularly polarized light
pulses. This interference results in unidirectional propagation if the
spin-wave sources are spaced apart at 1/4 of the wavelength of the spin waves
and the initial phase difference is set to pi/2. The propagating direction of
the spin wave is switched by the polarization helicity of the light pulses.
Moreover, in a numerical simulation, applying more than two spin-wave sources
with a suitable polarization and spot shape, arbitrary manipulation of the spin
wave by the phased array method was replicated.
",Physics,Physics
"Automated Vulnerability Detection in Source Code Using Deep Representation Learning   Increasing numbers of software vulnerabilities are discovered every year
whether they are reported publicly or discovered internally in proprietary
code. These vulnerabilities can pose serious risk of exploit and result in
system compromise, information leaks, or denial of service. We leveraged the
wealth of C and C++ open-source code available to develop a large-scale
function-level vulnerability detection system using machine learning. To
supplement existing labeled vulnerability datasets, we compiled a vast dataset
of millions of open-source functions and labeled it with carefully-selected
findings from three different static analyzers that indicate potential
exploits. The labeled dataset is available at: this https URL. Using
these datasets, we developed a fast and scalable vulnerability detection tool
based on deep feature representation learning that directly interprets lexed
source code. We evaluated our tool on code from both real software packages and
the NIST SATE IV benchmark dataset. Our results demonstrate that deep feature
representation learning on source code is a promising approach for automated
software vulnerability detection.
",Computer Science; Statistics,Computer Science
"A general theory of singular values with applications to signal denoising   We study the Pareto frontier for two competing norms $\|\cdot\|_X$ and
$\|\cdot\|_Y$ on a vector space. For a given vector $c$, the pareto frontier
describes the possible values of $(\|a\|_X,\|b\|_Y)$ for a decomposition
$c=a+b$. The singular value decomposition of a matrix is closely related to the
Pareto frontier for the spectral and nuclear norm. We will develop a general
theory that extends the notion of singular values of a matrix to arbitrary
finite dimensional euclidean vector spaces equipped with dual norms. This also
generalizes the diagonal singular value decompositions for tensors introduced
by the author in previous work. We can apply the results to denoising, where
$c$ is a noisy signal, $a$ is a sparse signal and $b$ is noise. Applications
include 1D total variation denoising, 2D total variation Rudin-Osher-Fatemi
image denoising, LASSO, basis pursuit denoising and tensor decompositions.
",Computer Science; Mathematics; Statistics,Mathematics
"Efficient Model-Based Deep Reinforcement Learning with Variational State Tabulation   Modern reinforcement learning algorithms reach super-human performance on
many board and video games, but they are sample inefficient, i.e. they
typically require significantly more playing experience than humans to reach an
equal performance level. To improve sample efficiency, an agent may build a
model of the environment and use planning methods to update its policy. In this
article we introduce Variational State Tabulation (VaST), which maps an
environment with a high-dimensional state space (e.g. the space of visual
inputs) to an abstract tabular model. Prioritized sweeping with small backups,
a highly efficient planning method, can then be used to update state-action
values. We show how VaST can rapidly learn to maximize reward in tasks like 3D
navigation and efficiently adapt to sudden changes in rewards or transition
probabilities.
",Statistics,Computer Science
"Fractional differential and fractional integral modified-Bloch equations for PFG anomalous diffusion and their general solutions   The studying of anomalous diffusion by pulsed field gradient (PFG) diffusion
technique still faces challenges. Two different research groups have proposed
modified Bloch equation for anomalous diffusion. However, these equations have
different forms and, therefore, yield inconsistent results. The discrepancy in
these reported modified Bloch equations may arise from different ways of
combining the fractional diffusion equation with the precession equation where
the time derivatives have different derivative orders and forms. Moreover, to
the best of my knowledge, the general PFG signal attenuation expression
including finite gradient pulse width (FGPW) effect for time-space fractional
diffusion based on the fractional derivative has yet to be reported by other
methods. Here, based on different combination strategy, two new modified Bloch
equations are proposed, which belong to two significantly different types: a
differential type based on the fractal derivative and an integral type based on
the fractional derivative. The merit of the integral type modified Bloch
equation is that the original properties of the contributions from linear or
nonlinear processes remain unchanged at the instant of the combination. The
general solutions including the FGPW effect were derived from these two
equations as well as from two other methods: a method observing the signal
intensity at the origin and the recently reported effective phase shift
diffusion equation method. The relaxation effect was also considered. It is
found that the relaxation behavior influenced by fractional diffusion based on
the fractional derivative deviates from that of normal diffusion. The general
solution agrees perfectly with continuous-time random walk (CTRW) simulations
as well as reported literature results. The new modified Bloch equations is a
valuable tool to describe PFG anomalous diffusion in NMR and MRI.
",Physics,Physics
"Relaxation of p-growth integral functionals under space-dependent differential constraints   A representation formula for the relaxation of integral energies
$$(u,v)\mapsto\int_{\Omega} f(x,u(x),v(x))\,dx,$$ is obtained, where $f$
satisfies $p$-growth assumptions, $1<p<+\infty$, and the fields $v$ are
subjected to space-dependent first order linear differential constraints in the
framework of $\mathscr{A}$-quasiconvexity with variable coefficients.
",Mathematics,Mathematics
"Event Analysis of Pulse-reclosers in Distribution Systems Through Sparse Representation   The pulse-recloser uses pulse testing technology to verify that the line is
clear of faults before initiating a reclose operation, which significantly
reduces stress on the system components (e.g. substation transformers) and
voltage sags on adjacent feeders. Online event analysis of pulse-reclosers are
essential to increases the overall utility of the devices, especially when
there are numerous devices installed throughout the distribution system. In
this paper, field data recorded from several devices were analyzed to identify
specific activity and fault locations. An algorithm is developed to screen the
data to identify the status of each pole and to tag time windows with a
possible pulse event. In the next step, selected time windows are further
analyzed and classified using a sparse representation technique by solving an
l1-regularized least-square problem. This classification is obtained by
comparing the pulse signature with the reference dictionary to find a set that
most closely matches the pulse features. This work also sheds additional light
on the possibility of fault classification based on the pulse signature. Field
data collected from a distribution system are used to verify the effectiveness
and reliability of the proposed method.
",Computer Science,Computer Science
"Sensor Transformation Attention Networks   Recent work on encoder-decoder models for sequence-to-sequence mapping has
shown that integrating both temporal and spatial attention mechanisms into
neural networks increases the performance of the system substantially. In this
work, we report on the application of an attentional signal not on temporal and
spatial regions of the input, but instead as a method of switching among inputs
themselves. We evaluate the particular role of attentional switching in the
presence of dynamic noise in the sensors, and demonstrate how the attentional
signal responds dynamically to changing noise levels in the environment to
achieve increased performance on both audio and visual tasks in three
commonly-used datasets: TIDIGITS, Wall Street Journal, and GRID. Moreover, the
proposed sensor transformation network architecture naturally introduces a
number of advantages that merit exploration, including ease of adding new
sensors to existing architectures, attentional interpretability, and increased
robustness in a variety of noisy environments not seen during training.
Finally, we demonstrate that the sensor selection attention mechanism of a
model trained only on the small TIDIGITS dataset can be transferred directly to
a pre-existing larger network trained on the Wall Street Journal dataset,
maintaining functionality of switching between sensors to yield a dramatic
reduction of error in the presence of noise.
",Computer Science,Computer Science; Statistics
"Algebraic relations between solutions of Painlevé equations   We calculate model theoretic ranks of Painlevé equations in this article,
showing in particular, that any equation in any of the Painlevé families has
Morley rank one, extending results of Nagloo and Pillay (2011). We show that
the type of the generic solution of any equation in the second Painlevé
family is geometrically trivial, extending a result of Nagloo (2015).
We also establish the orthogonality of various pairs of equations in the
Painlevé families, showing at least generically, that all instances of
nonorthogonality between equations in the same Painlevé family come from
classically studied B{ä}cklund transformations. For instance, we show that if
at least one of $\alpha, \beta$ is transcendental, then $P_{II} (\alpha)$ is
nonorthogonal to $P_{II} ( \beta )$ if and only if $\alpha+ \beta \in \mathbb
Z$ or $\alpha - \beta \in \mathbb Z$. Our results have concrete interpretations
in terms of characterizing the algebraic relations between solutions of
Painlevé equations. We give similar results for orthogonality relations
between equations in different Painlevé families, and formulate some general
questions which extend conjectures of Nagloo and Pillay (2011) on transcendence
and algebraic independence of solutions to Painlevé equations. We also apply
our analysis of ranks to establish some orthogonality results for pairs of
Painlevé equations from different families. For instance, we answer several
open questions of Nagloo (2016), and in the process answer a question of Boalch
(2012).
",Mathematics,Mathematics
"Convergence of the Expectation-Maximization Algorithm Through Discrete-Time Lyapunov Stability Theory   In this paper, we propose a dynamical systems perspective of the
Expectation-Maximization (EM) algorithm. More precisely, we can analyze the EM
algorithm as a nonlinear state-space dynamical system. The EM algorithm is
widely adopted for data clustering and density estimation in statistics,
control systems, and machine learning. This algorithm belongs to a large class
of iterative algorithms known as proximal point methods. In particular, we
re-interpret limit points of the EM algorithm and other local maximizers of the
likelihood function it seeks to optimize as equilibria in its dynamical system
representation. Furthermore, we propose to assess its convergence as asymptotic
stability in the sense of Lyapunov. As a consequence, we proceed by leveraging
recent results regarding discrete-time Lyapunov stability theory in order to
establish asymptotic stability (and thus, convergence) in the dynamical system
representation of the EM algorithm.
",Computer Science,Computer Science; Mathematics; Statistics
"Heating and cooling of coronal loops with turbulent suppression of parallel heat conduction   Using the ""enthalpy-based thermal evolution of loops"" (EBTEL) model, we
investigate the hydrodynamics of the plasma in a flaring coronal loop in which
heat conduction is limited by turbulent scattering of the electrons that
transport the thermal heat flux. The EBTEL equations are solved analytically in
each of the two (conduction-dominated and radiation-dominated) cooling phases.
Comparison of the results with typical observed cooling times in solar flares
shows that the turbulent mean free-path $\lambda_T$ lies in a range
corresponding to a regime in which classical (collision-dominated) conduction
plays at most a limited role. We also consider the magnitude and duration of
the heat input that is necessary to account for the enhanced values of
temperature and density at the beginning of the cooling phase and for the
observed cooling times. We find through numerical modeling that in order to
produce a peak temperature $\simeq 1.5 \times 10^7$~K and a 200~s cooling time
consistent with observations, the flare heating profile must extend over a
significant period of time; in particular, its lingering role must be taken
into consideration in any description of the cooling phase. Comparison with
observationally-inferred values of post-flare loop temperatures, densities, and
cooling times thus leads to useful constraints on both the magnitude and
duration of the magnetic energy release in the loop, as well as on the value of
the turbulent mean free-path $\lambda_T$.
",Physics,Physics
"Hasse diagrams of non-isomorphic posets with $n$ elements, $2\leq n \leq 7,$ and the number of posets with $10$ elements, without the aid of any computer program   Let $P(n)$ be the set of all posets with $n$ elements and $NIP(n)$ the set of
non-isomorphic posets with $n$ elements. Let $P^{(j)}(n)$, $1\leq j\leq 2^n,$
be the number of all posets with $n$ elements possessing exactly $j$
antichains. We have determined the numbers $P^{(j)}(7),$ $1\leq j\leq 128$, and
using a result of M. Erné \cite{EM4}, we compute $|P(10)|$ without the aid of
any computer program. We include the Hasse diagrams of all the non-isomorphic
posets of $P(7)$.
",Mathematics,Computer Science
"A Survey on the Adoption of Cloud Computing in Education Sector   Education is a key factor in ensuring economic growth, especially for
countries with growing economies. Today, students have become more
technologically savvy as teaching and learning uses more advance technology day
in, day out. Due to virtualize resources through the Internet, as well as
dynamic scalability, cloud computing has continued to be adopted by more
organizations. Despite the looming financial crisis, there has been increasing
pressure for educational institutions to deliver better services using minimal
resources. Leaning institutions, both public and private can utilize the
potential advantage of cloud computing to ensure high quality service
regardless of the minimal resources available. Cloud computing is taking a
center stage in academia because of its various benefits. Various learning
institutions use different cloud-based applications provided by the service
providers to ensure that their students and other users can perform both
academic as well as business-related tasks. Thus, this research will seek to
establish the benefits associated with the use of cloud computing in learning
institutions. The solutions provided by the cloud technology ensure that the
research and development, as well as the teaching is more sustainable and
efficient, thus positively influencing the quality of learning and teaching
within educational institutions. This has led to various learning institutions
adopting cloud technology as a solution to various technological challenges
they face on a daily routine.
",Computer Science,Computer Science
"Optimal one-shot quantum algorithm for EQUALITY and AND   We study the computation complexity of Boolean functions in the quantum black
box model. In this model our task is to compute a function
$f:\{0,1\}\to\{0,1\}$ on an input $x\in\{0,1\}^n$ that can be accessed by
querying the black box. Quantum algorithms are inherently probabilistic; we are
interested in the lowest possible probability that the algorithm outputs
incorrect answer (the error probability) for a fixed number of queries. We show
that the lowest possible error probability for $AND_n$ and $EQUALITY_{n+1}$ is
$1/2-n/(n^2+1)$.
",Computer Science,Computer Science
"A high resolution ion microscope for cold atoms   We report on an ion-optical system that serves as a microscope for ultracold
ground state and Rydberg atoms. The system is designed to achieve a
magnification of up to 1000 and a spatial resolution in the 100 nm range,
thereby surpassing many standard imaging techniques for cold atoms. The
microscope consists of four electrostatic lenses and a microchannel plate in
conjunction with a delay line detector in order to achieve single particle
sensitivity with high temporal and spatial resolution. We describe the design
process of the microscope including ion-optical simulations of the imaging
system and characterize aberrations and the resolution limit. Furthermore, we
present the experimental realization of the microscope in a cold atom setup and
investigate its performance by patterned ionization with a structure size down
to 2.7 {\mu}m. The microscope meets the requirements for studying various
many-body effects, ranging from correlations in cold quantum gases up to
Rydberg molecule formation.
",Physics,Physics
"Siamese Network of Deep Fisher-Vector Descriptors for Image Retrieval   This paper addresses the problem of large scale image retrieval, with the aim
of accurately ranking the similarity of a large number of images to a given
query image. To achieve this, we propose a novel Siamese network. This network
consists of two computational strands, each comprising of a CNN component
followed by a Fisher vector component. The CNN component produces dense, deep
convolutional descriptors that are then aggregated by the Fisher Vector method.
Crucially, we propose to simultaneously learn both the CNN filter weights and
Fisher Vector model parameters. This allows us to account for the evolving
distribution of deep descriptors over the course of the learning process. We
show that the proposed approach gives significant improvements over the
state-of-the-art methods on the Oxford and Paris image retrieval datasets.
Additionally, we provide a baseline performance measure for both these datasets
with the inclusion of 1 million distractors.
",Computer Science,Computer Science
"Multi-Agent Diverse Generative Adversarial Networks   We propose MAD-GAN, an intuitive generalization to the Generative Adversarial
Networks (GANs) and its conditional variants to address the well known problem
of mode collapse. First, MAD-GAN is a multi-agent GAN architecture
incorporating multiple generators and one discriminator. Second, to enforce
that different generators capture diverse high probability modes, the
discriminator of MAD-GAN is designed such that along with finding the real and
fake samples, it is also required to identify the generator that generated the
given fake sample. Intuitively, to succeed in this task, the discriminator must
learn to push different generators towards different identifiable modes. We
perform extensive experiments on synthetic and real datasets and compare
MAD-GAN with different variants of GAN. We show high quality diverse sample
generations for challenging tasks such as image-to-image translation and face
generation. In addition, we also show that MAD-GAN is able to disentangle
different modalities when trained using highly challenging diverse-class
dataset (e.g. dataset with images of forests, icebergs, and bedrooms). In the
end, we show its efficacy on the unsupervised feature representation task. In
Appendix, we introduce a similarity based competing objective (MAD-GAN-Sim)
which encourages different generators to generate diverse samples based on a
user defined similarity metric. We show its performance on the image-to-image
translation, and also show its effectiveness on the unsupervised feature
representation task.
",Computer Science; Statistics,Computer Science; Statistics
"Segmentation of the Proximal Femur from MR Images using Deep Convolutional Neural Networks   Magnetic resonance imaging (MRI) has been proposed as a complimentary method
to measure bone quality and assess fracture risk. However, manual segmentation
of MR images of bone is time-consuming, limiting the use of MRI measurements in
the clinical practice. The purpose of this paper is to present an automatic
proximal femur segmentation method that is based on deep convolutional neural
networks (CNNs). This study had institutional review board approval and written
informed consent was obtained from all subjects. A dataset of volumetric
structural MR images of the proximal femur from 86 subject were
manually-segmented by an expert. We performed experiments by training two
different CNN architectures with multiple number of initial feature maps and
layers, and tested their segmentation performance against the gold standard of
manual segmentations using four-fold cross-validation. Automatic segmentation
of the proximal femur achieved a high dice similarity score of 0.94$\pm$0.05
with precision = 0.95$\pm$0.02, and recall = 0.94$\pm$0.08 using a CNN
architecture based on 3D convolution exceeding the performance of 2D CNNs. The
high segmentation accuracy provided by CNNs has the potential to help bring the
use of structural MRI measurements of bone quality into clinical practice for
management of osteoporosis.
",Computer Science; Statistics,Computer Science
"Tensor network method for reversible classical computation   We develop a tensor network technique that can solve universal reversible
classical computational problems, formulated as vertex models on a square
lattice [Nat. Commun. 8, 15303 (2017)]. By encoding the truth table of each
vertex constraint in a tensor, the total number of solutions compatible with
partial inputs/outputs at the boundary can be represented as the full
contraction of a tensor network. We introduce an iterative
compression-decimation (ICD) scheme that performs this contraction efficiently.
The ICD algorithm first propagates local constraints to longer ranges via
repeated contraction-decomposition sweeps over all lattice bonds, thus
achieving compression on a given length scale. It then decimates the lattice
via coarse-graining tensor contractions. Repeated iterations of these two steps
gradually collapse the tensor network and ultimately yield the exact tensor
trace for large systems, without the need for manual control of tensor
dimensions. Our protocol allows us to obtain the exact number of solutions for
computations where a naive enumeration would take astronomically long times.
",Computer Science; Physics,Computer Science; Physics
"Minimax Optimal Estimators for Additive Scalar Functionals of Discrete Distributions   In this paper, we consider estimators for an additive functional of $\phi$,
which is defined as $\theta(P;\phi)=\sum_{i=1}^k\phi(p_i)$, from $n$ i.i.d.
random samples drawn from a discrete distribution $P=(p_1,...,p_k)$ with
alphabet size $k$. We propose a minimax optimal estimator for the estimation
problem of the additive functional. We reveal that the minimax optimal rate is
characterized by the divergence speed of the fourth derivative of $\phi$ if the
divergence speed is high. As a result, we show there is no consistent estimator
if the divergence speed of the fourth derivative of $\phi$ is larger than
$p^{-4}$. Furthermore, if the divergence speed of the fourth derivative of
$\phi$ is $p^{4-\alpha}$ for $\alpha \in (0,1)$, the minimax optimal rate is
obtained within a universal multiplicative constant as $\frac{k^2}{(n\ln
n)^{2\alpha}} + \frac{k^{2-2\alpha}}{n}$.
",Computer Science; Mathematics; Statistics,Mathematics; Statistics
"Multiset Combinatorial Batch Codes   Batch codes, first introduced by Ishai, Kushilevitz, Ostrovsky, and Sahai,
mimic a distributed storage of a set of $n$ data items on $m$ servers, in such
a way that any batch of $k$ data items can be retrieved by reading at most some
$t$ symbols from each server. Combinatorial batch codes, are replication-based
batch codes in which each server stores a subset of the data items.
In this paper, we propose a generalization of combinatorial batch codes,
called multiset combinatorial batch codes (MCBC), in which $n$ data items are
stored in $m$ servers, such that any multiset request of $k$ items, where any
item is requested at most $r$ times, can be retrieved by reading at most $t$
items from each server. The setup of this new family of codes is motivated by
recent work on codes which enable high availability and parallel reads in
distributed storage systems. The main problem under this paradigm is to
minimize the number of items stored in the servers, given the values of
$n,m,k,r,t$, which is denoted by $N(n,k,m,t;r)$. We first give a necessary and
sufficient condition for the existence of MCBCs. Then, we present several
bounds on $N(n,k,m,t;r)$ and constructions of MCBCs. In particular, we
determine the value of $N(n,k,m,1;r)$ for any $n\geq
\left\lfloor\frac{k-1}{r}\right\rfloor{m\choose k-1}-(m-k+1)A(m,4,k-2)$, where
$A(m,4,k-2)$ is the maximum size of a binary constant weight code of length
$m$, distance four and weight $k-2$. We also determine the exact value of
$N(n,k,m,1;r)$ when $r\in\{k,k-1\}$ or $k=m$.
",Computer Science; Mathematics,Computer Science
"Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks   Matrix completion models are among the most common formulations of
recommender systems. Recent works have showed a boost of performance of these
techniques when introducing the pairwise relationships between users/items in
the form of graphs, and imposing smoothness priors on these graphs. However,
such techniques do not fully exploit the local stationarity structures of
user/item graphs, and the number of parameters to learn is linear w.r.t. the
number of users and items. We propose a novel approach to overcome these
limitations by using geometric deep learning on graphs. Our matrix completion
architecture combines graph convolutional neural networks and recurrent neural
networks to learn meaningful statistical graph-structured patterns and the
non-linear diffusion process that generates the known ratings. This neural
network system requires a constant number of parameters independent of the
matrix size. We apply our method on both synthetic and real datasets, showing
that it outperforms state-of-the-art techniques.
",Computer Science; Statistics,Computer Science; Statistics
"ExoMol molecular line lists XX: a comprehensive line list for H$_3^+$   H$_3^+$ is a ubiquitous and important astronomical species whose spectrum has
been observed in the interstellar medium, planets and tentatively in the
remnants of supernova SN1897a. Its role as a cooler is important for gas giant
planets and exoplanets, and possibly the early Universe. All this makes the
spectral properties, cooling function and partition function of H$_3^+$ key
parameters for astronomical models and analysis. A new high-accuracy, very
extensive line list for H$_3^+$ called MiZATeP was computed as part of the
ExoMol project alongside a temperature-dependent cooling function and partition
function as well as lifetimes for %individual excited states. These data are
made available in electronic form as supplementary data to this article and at
this http URL
",Physics,Physics
"Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games   Many artificial intelligence (AI) applications often require multiple
intelligent agents to work in a collaborative effort. Efficient learning for
intra-agent communication and coordination is an indispensable step towards
general AI. In this paper, we take StarCraft combat game as a case study, where
the task is to coordinate multiple agents as a team to defeat their enemies. To
maintain a scalable yet effective communication protocol, we introduce a
Multiagent Bidirectionally-Coordinated Network (BiCNet ['bIknet]) with a
vectorised extension of actor-critic formulation. We show that BiCNet can
handle different types of combats with arbitrary numbers of AI agents for both
sides. Our analysis demonstrates that without any supervisions such as human
demonstrations or labelled data, BiCNet could learn various types of advanced
coordination strategies that have been commonly used by experienced game
players. In our experiments, we evaluate our approach against multiple
baselines under different scenarios; it shows state-of-the-art performance, and
possesses potential values for large-scale real-world applications.
",Computer Science,Computer Science
"Rigidity of volume-minimizing hypersurfaces in Riemannian 5-manifolds   In this paper we generalize the main result of [4] for manifolds that are not
necessarily Einstein. In fact, we obtain an upper bound for the volume of a
locally volume-minimizing closed hypersurface $\Sigma$ of a Riemannian
5-manifold $M$ with scalar curvature bounded from below by a positive constant
in terms of the total traceless Ricci curvature of $\Sigma$. Furthermore, if
$\Sigma$ saturates the respective upper bound and $M$ has nonnegative Ricci
curvature, then $\Sigma$ is isometric to $\mathbb{S}^4$ up to scaling and $M$
splits in a neighborhood of $\Sigma$. Also, we obtain a rigidity result for the
Riemannian cover of $M$ when $\Sigma$ minimizes the volume in its homotopy
class and saturates the upper bound.
",Mathematics,Mathematics
"Hybrid Kinematic Control for Rigid Body Pose Stabilization using Dual Quaternions   In this paper, we address the rigid body pose stabilization problem using
dual quaternion formalism. We propose a hybrid control strategy to design a
switching control law with hysteresis in such a way that the global asymptotic
stability of the closed-loop system is guaranteed and such that the global
attractivity of the stabilization pose does not exhibit chattering, a problem
that is present in all discontinuous-based feedback controllers. Using
numerical simulations, we illustrate the problems that arise from existing
results in the literature -- as unwinding and chattering -- and verify the
effectiveness of the proposed controller to solve the robust global pose
stability problem.
",Computer Science; Mathematics,Computer Science
"Distributed sub-optimal resource allocation over weight-balanced graph via singular perturbation   In this paper, we consider distributed optimization design for resource
allocation problems over weight-balanced graphs. With the help of singular
perturbation analysis, we propose a simple sub-optimal continuous-time
optimization algorithm. Moreover, we prove the existence and uniqueness of the
algorithm equilibrium, and then show the convergence with an exponential rate.
Finally, we verify the sub-optimality of the algorithm, which can approach the
optimal solution as an adjustable parameter tends to zero.
",Mathematics,Computer Science; Mathematics
"Robust and Fast Decoding of High-Capacity Color QR Codes for Mobile Applications   The use of color in QR codes brings extra data capacity, but also inflicts
tremendous challenges on the decoding process due to chromatic distortion,
cross-channel color interference and illumination variation. Particularly, we
further discover a new type of chromatic distortion in high-density color QR
codes, cross-module color interference, caused by the high density which also
makes the geometric distortion correction more challenging. To address these
problems, we propose two approaches, namely, LSVM-CMI and QDA-CMI, which
jointly model these different types of chromatic distortion. Extended from SVM
and QDA, respectively, both LSVM-CMI and QDA-CMI optimize over a particular
objective function to learn a color classifier. Furthermore, a robust geometric
transformation method and several pipeline refinements are proposed to boost
the decoding performance for mobile applications. We put forth and implement a
framework for high-capacity color QR codes equipped with our methods, called
HiQ. To evaluate the performance of HiQ, we collect a challenging large-scale
color QR code dataset, CUHK-CQRC, which consists of 5390 high-density color QR
code samples. The comparison with the baseline method [2] on CUHK-CQRC shows
that HiQ at least outperforms [2] by 188% in decoding success rate and 60% in
bit error rate. Our implementation of HiQ in iOS and Android also demonstrates
the effectiveness of our framework in real-world applications.
",Computer Science,Computer Science
"Walking Through Waypoints   We initiate the study of a fundamental combinatorial problem: Given a
capacitated graph $G=(V,E)$, find a shortest walk (""route"") from a source $s\in
V$ to a destination $t\in V$ that includes all vertices specified by a set
$\mathscr{W}\subseteq V$: the \emph{waypoints}. This waypoint routing problem
finds immediate applications in the context of modern networked distributed
systems. Our main contribution is an exact polynomial-time algorithm for graphs
of bounded treewidth. We also show that if the number of waypoints is
logarithmically bounded, exact polynomial-time algorithms exist even for
general graphs. Our two algorithms provide an almost complete characterization
of what can be solved exactly in polynomial-time: we show that more general
problems (e.g., on grid graphs of maximum degree 3, with slightly more
waypoints) are computationally intractable.
",Computer Science,Computer Science
"Demand Response in the Smart Grid: the Impact of Consumers Temporal Preferences   In Demand Response programs, price incentives might not be sufficient to
modify residential consumers load profile. Here, we consider that each consumer
has a preferred profile and a discomfort cost when deviating from it. Consumers
can value this discomfort at a varying level that we take as a parameter. This
work analyses Demand Response as a game theoretic environment. We study the
equilibria of the game between consumers with preferences within two different
dynamic pricing mechanisms, respectively the daily proportional mechanism
introduced by Mohsenian-Rad et al, and an hourly proportional mechanism. We
give new results about equilibria as functions of the preference level in the
case of quadratic system costs and prove that, whatever the preference level,
system costs are smaller with the hourly mechanism. We simulate the Demand
Response environment using real consumption data from PecanStreet database.
While the Price of Anarchy remains always close to one up to 0.1% with the
hourly mechanism, it can be more than 10% bigger with the daily mechanism.
",Computer Science,Computer Science
"Adaptive Sequential MCMC for Combined State and Parameter Estimation   In the case of a linear state space model, we implement an MCMC sampler with
two phases. In the learning phase, a self-tuning sampler is used to learn the
parameter mean and covariance structure. In the estimation phase, the parameter
mean and covariance structure informs the proposed mechanism and is also used
in a delayed-acceptance algorithm. Information on the resulting state of the
system is given by a Gaussian mixture. In on-line mode, the algorithm is
adaptive and uses a sliding window approach to accelerate sampling speed and to
maintain appropriate acceptance rates. We apply the algorithm to joined state
and parameter estimation in the case of irregularly sampled GPS time series
data.
",Statistics,Computer Science; Statistics
"Goodness-of-fit tests for the functional linear model based on randomly projected empirical processes   We consider marked empirical processes indexed by a randomly projected
functional covariate to construct goodness-of-fit tests for the functional
linear model with scalar response. The test statistics are built from
continuous functionals over the projected process, resulting in computationally
efficient tests that exhibit root-n convergence rates and circumvent the curse
of dimensionality. The weak convergence of the empirical process is obtained
conditionally on a random direction, whilst the almost surely equivalence
between the testing for significance expressed on the original and on the
projected functional covariate is proved. The computation of the test in
practice involves calibration by wild bootstrap resampling and the combination
of several p-values, arising from different projections, by means of the false
discovery rate method. The finite sample properties of the tests are
illustrated in a simulation study for a variety of linear models, underlying
processes, and alternatives. The software provided implements the tests and
allows the replication of simulations and data applications.
",Statistics,Statistics
"Mixtures of Hidden Truncation Hyperbolic Factor Analyzers   The mixture of factor analyzers model was first introduced over 20 years ago
and, in the meantime, has been extended to several non-Gaussian analogues. In
general, these analogues account for situations with heavy tailed and/or skewed
clusters. An approach is introduced that unifies many of these approaches into
one very general model: the mixture of hidden truncation hyperbolic factor
analyzers (MHTHFA) model. In the process of doing this, a hidden truncation
hyperbolic factor analysis model is also introduced. The MHTHFA model is
illustrated for clustering as well as semi-supervised classification using two
real datasets.
",Statistics,Statistics
"Finding influential nodes for integration in brain networks using optimal percolation theory   Global integration of information in the brain results from complex
interactions of segregated brain networks. Identifying the most influential
neuronal populations that efficiently bind these networks is a fundamental
problem of systems neuroscience. Here we apply optimal percolation theory and
pharmacogenetic interventions in-vivo to predict and subsequently target nodes
that are essential for global integration of a memory network in rodents. The
theory predicts that integration in the memory network is mediated by a set of
low-degree nodes located in the nucleus accumbens. This result is confirmed
with pharmacogenetic inactivation of the nucleus accumbens, which eliminates
the formation of the memory network, while inactivations of other brain areas
leave the network intact. Thus, optimal percolation theory predicts essential
nodes in brain networks. This could be used to identify targets of
interventions to modulate brain function.
",Quantitative Biology,Quantitative Biology
"Projected Variational Integrators for Degenerate Lagrangian Systems   We propose and compare several projection methods applied to variational
integrators for degenerate Lagrangian systems, whose Lagrangian is of the form
$L = \vartheta(q) \cdot \dot{q} - H(q)$ and thus linear in velocities. While
previous methods for such systems only work reliably in the case of $\vartheta$
being a linear function of $q$, our methods are long-time stable also for
systems where $\vartheta$ is a nonlinear function of $q$. We analyse the
properties of the resulting algorithms, in particular with respect to the
conservation of energy, momentum maps and symplecticity. In numerical
experiments, we verify the favourable properties of the projected integrators
and demonstrate their excellent long-time fidelity. In particular, we consider
a two-dimensional Lotka-Volterra system, planar point vortices with
position-dependent circulation and guiding centre dynamics.
",Physics,Computer Science; Mathematics
"Measurably entire functions and their growth   In 1997 B. Weiss introduced the notion of measurably entire functions and
proved that they exist on every arbitrary free C- action defined on standard
probability space. In the same paper he asked about the minimal possible growth
of measurably entire functions. In this work we show that for every arbitrary
free C- action defined on a standard probability space there exists a
measurably entire function whose growth does not exceed exp (exp[log^p |z|])
for any p > 3. This complements a recent result by Buhovski, Glücksam,
Logunov, and Sodin (arXiv:1703.08101) who showed that such functions cannot
grow slower than exp (exp[log^p |z|]) for any p < 2.
",Mathematics,Mathematics
"Neural Network Based Speaker Classification and Verification Systems with Enhanced Features   This work presents a novel framework based on feed-forward neural network for
text-independent speaker classification and verification, two related systems
of speaker recognition. With optimized features and model training, it achieves
100% classification rate in classification and less than 6% Equal Error Rate
(ERR), using merely about 1 second and 5 seconds of data respectively. Features
with stricter Voice Active Detection (VAD) than the regular one for speech
recognition ensure extracting stronger voiced portion for speaker recognition,
speaker-level mean and variance normalization helps to eliminate the
discrepancy between samples from the same speaker. Both are proven to improve
the system performance. In building the neural network speaker classifier, the
network structure parameters are optimized with grid search and dynamically
reduced regularization parameters are used to avoid training terminated in
local minimum. It enables the training goes further with lower cost. In speaker
verification, performance is improved with prediction score normalization,
which rewards the speaker identity indices with distinct peaks and penalizes
the weak ones with high scores but more competitors, and speaker-specific
thresholding, which significantly reduces ERR in the ROC curve. TIMIT corpus
with 8K sampling rate is used here. First 200 male speakers are used to train
and test the classification performance. The testing files of them are used as
in-domain registered speakers, while data from the remaining 126 male speakers
are used as out-of-domain speakers, i.e. imposters in speaker verification.
",Computer Science,Computer Science
"Interleaving and Gromov-Hausdorff distance   One of the central notions to emerge from the study of persistent homology is
that of interleaving distance. It has found recent applications in symplectic
and contact geometry, sheaf theory, computational geometry, and phylogenetics.
Here we present a general study of this topic. We define interleaving of
functors with common codomain as solutions to an extension problem. In order to
define interleaving distance in this setting we are led to categorical
generalizations of Hausdorff distance, Gromov-Hausdorff distance, and the space
of metric spaces. We obtain comparisons with previous notions of interleaving
via the study of future equivalences. As an application we recover a definition
of shift equivalences of discrete dynamical systems.
",Mathematics,Mathematics
"Multinomial Sum Formulas of Multiple Zeta Values   For a pair of positive integers $n,k$ with $n\geq 2$, in this paper we prove
that $$ \sum_{r=1}^k\sum_{|\bf\alpha|=k}{k\choose\bf\alpha}
\zeta(n\bf\alpha)=\zeta(n)^k =\sum^k_{r=1}\sum_{|\bf\alpha|=k}
{k\choose\bf\alpha}(-1)^{k-r}\zeta^\star(n\bf\alpha), $$ where
$\bf\alpha=(\alpha_1,\alpha_2,\ldots,\alpha_r)$ is a $r$-tuple of positive
integers. Moreover, we give an application to combinatorics and get the
following identity: $$ \sum^{2k}_{r=1}r!{2k\brace
r}=\sum^k_{p=1}\sum^k_{q=1}{k\brace p}{k\brace q} p!q!D(p,q), $$ where
${k\brace p}$ is the Stirling numbers of the second kind and $D(p,q)$ is the
Delannoy number.
",Mathematics,Mathematics
"Inference-Based Distributed Channel Allocation in Wireless Sensor Networks   Interference-aware resource allocation of time slots and frequency channels
in single-antenna, halfduplex radio wireless sensor networks (WSN) is
challenging. Devising distributed algorithms for such task further complicates
the problem. This work studiesWSN joint time and frequency channel allocation
for a given routing tree, such that: a) allocation is performed in a fully
distributed way, i.e., information exchange is only performed among neighboring
WSN terminals, within communication up to two hops, and b) detection of
potential interfering terminals is simplified and can be practically realized.
The algorithm imprints space, time, frequency and radio hardware constraints
into a loopy factor graph and performs iterative message passing/ loopy belief
propagation (BP) with randomized initial priors. Sufficient conditions for
convergence to a valid solution are offered, for the first time in the
literature, exploiting the structure of the proposed factor graph. Based on
theoretical findings, modifications of BP are devised that i) accelerate
convergence to a valid solution and ii) reduce computation cost. Simulations
reveal promising throughput results of the proposed distributed algorithm, even
though it utilizes simplified interfering terminals set detection. Future work
could modify the constraints such that other disruptive wireless technologies
(e.g., full-duplex radios or network coding) could be accommodated within the
same inference framework.
",Computer Science,Computer Science
"On the risk of convex-constrained least squares estimators under misspecification   We consider the problem of estimating the mean of a noisy vector. When the
mean lies in a convex constraint set, the least squares projection of the
random vector onto the set is a natural estimator. Properties of the risk of
this estimator, such as its asymptotic behavior as the noise tends to zero,
have been well studied. We instead study the behavior of this estimator under
misspecification, that is, without the assumption that the mean lies in the
constraint set. For appropriately defined notions of risk in the misspecified
setting, we prove a generalization of a low noise characterization of the risk
due to Oymak and Hassibi in the case of a polyhedral constraint set. An
interesting consequence of our results is that the risk can be much smaller in
the misspecified setting than in the well-specified setting. We also discuss
consequences of our result for isotonic regression.
",Mathematics; Statistics,Mathematics; Statistics
"The inverse hull of 0-left cancellative semigroups   Given a semigroup S with zero, which is left-cancellative in the sense that
st=sr \neq 0 implies that t=r, we construct an inverse semigroup called the
inverse hull of S, denoted H(S). When S admits least common multiples, in a
precise sense defined below, we study the idempotent semilattice of H(S), with
a focus on its spectrum. When S arises as the language semigroup for a subsift
X on a finite alphabet, we discuss the relationship between H(S) and several
C*-algebras associated to X appearing in the literature.
",Mathematics,Mathematics
"Deep Object Centric Policies for Autonomous Driving   While learning visuomotor skills in an end-to-end manner is appealing, deep
neural networks are often uninterpretable and fail in surprising ways. For
robotics tasks, such as autonomous driving, models that explicitly represent
objects may be more robust to new scenes and provide intuitive visualizations.
We describe a taxonomy of object-centric models which leverage both object
instances and end-to-end learning. In the Grand Theft Auto V simulator, we show
that object centric models outperform object-agnostic methods in scenes with
other vehicles and pedestrians, even with an imperfect detector. We also
demonstrate that our architectures perform well on real world environments by
evaluating on the Berkeley DeepDrive Video dataset.
",Computer Science,Computer Science
"Quench-induced entanglement and relaxation dynamics in Luttinger liquids   We investigate the time evolution towards the asymptotic steady state of a
one dimensional interacting system after a quantum quench. We show that at
finite time the latter induces entanglement between right- and left- moving
density excitations, encoded in their cross-correlators, which vanishes in the
long-time limit. This behavior results in a universal time-decay in system
spectral properties $ \propto t^{-2} $, in addition to non-universal power-law
contributions typical of Luttinger liquids. Importantly, we argue that the
presence of quench-induced entanglement clearly emerges in transport
properties, such as charge and energy currents injected in the system from a
biased probe, and determines their long-time dynamics. In particular, energy
fractionalization phenomenon turns out to be a promising platform to observe
the universal power-law decay $ \propto t^{-2} $ induced by entanglement and
represents a novel way to study the corresponding relaxation mechanism.
",Physics,Physics
"Weak Form of Stokes-Dirac Structures and Geometric Discretization of Port-Hamiltonian Systems   We present the mixed Galerkin discretization of distributed parameter
port-Hamiltonian systems. On the prototypical example of hyperbolic systems of
two conservation laws in arbitrary spatial dimension, we derive the main
contributions: (i) A weak formulation of the underlying geometric
(Stokes-Dirac) structure with a segmented boundary according to the causality
of the boundary ports. (ii) The geometric approximation of the Stokes-Dirac
structure by a finite-dimensional Dirac structure is realized using a mixed
Galerkin approach and power-preserving linear maps, which define minimal
discrete power variables. (iii) With a consistent approximation of the
Hamiltonian, we obtain finite-dimensional port-Hamiltonian state space models.
By the degrees of freedom in the power-preserving maps, the resulting family of
structure-preserving schemes allows for trade-offs between centered
approximations and upwinding. We illustrate the method on the example of
Whitney finite elements on a 2D simplicial triangulation and compare the
eigenvalue approximation in 1D with a related approach.
",Computer Science,Mathematics
"Efficient Attention using a Fixed-Size Memory Representation   The standard content-based attention mechanism typically used in
sequence-to-sequence models is computationally expensive as it requires the
comparison of large encoder and decoder states at each time step. In this work,
we propose an alternative attention mechanism based on a fixed size memory
representation that is more efficient. Our technique predicts a compact set of
K attention contexts during encoding and lets the decoder compute an efficient
lookup that does not need to consult the memory. We show that our approach
performs on-par with the standard attention mechanism while yielding inference
speedups of 20% for real-world translation tasks and more for tasks with longer
sequences. By visualizing attention scores we demonstrate that our models learn
distinct, meaningful alignments.
",Computer Science,Computer Science
"An Ensemble Classification Algorithm Based on Information Entropy for Data Streams   Data stream mining problem has caused widely concerns in the area of machine
learning and data mining. In some recent studies, ensemble classification has
been widely used in concept drift detection, however, most of them regard
classification accuracy as a criterion for judging whether concept drift
happening or not. Information entropy is an important and effective method for
measuring uncertainty. Based on the information entropy theory, a new algorithm
using information entropy to evaluate a classification result is developed. It
uses ensemble classification techniques, and the weight of each classifier is
decided through the entropy of the result produced by an ensemble classifiers
system. When the concept in data streams changing, the classifiers' weight
below a threshold value will be abandoned to adapt to a new concept in one
time. In the experimental analysis section, six databases and four proposed
algorithms are executed. The results show that the proposed method can not only
handle concept drift effectively, but also have a better classification
accuracy and time performance than the contrastive algorithms.
",Computer Science,Computer Science; Statistics
"Multiresolution Tensor Decomposition for Multiple Spatial Passing Networks   This article is motivated by soccer positional passing networks collected
across multiple games. We refer to these data as replicated spatial passing
networks---to accurately model such data it is necessary to take into account
the spatial positions of the passer and receiver for each passing event. This
spatial registration and replicates that occur across games represent key
differences with usual social network data. As a key step before investigating
how the passing dynamics influence team performance, we focus on developing
methods for summarizing different team's passing strategies. Our proposed
approach relies on a novel multiresolution data representation framework and
Poisson nonnegative block term decomposition model, which automatically
produces coarse-to-fine low-rank network motifs. The proposed methods are
applied to detailed passing record data collected from the 2014 FIFA World Cup.
",Computer Science; Statistics,Computer Science
"A giant with feet of clay: on the validity of the data that feed machine learning in medicine   This paper considers the use of Machine Learning (ML) in medicine by focusing
on the main problem that this computational approach has been aimed at solving
or at least minimizing: uncertainty. To this aim, we point out how uncertainty
is so ingrained in medicine that it biases also the representation of clinical
phenomena, that is the very input of ML models, thus undermining the clinical
significance of their output. Recognizing this can motivate both medical
doctors, in taking more responsibility in the development and use of these
decision aids, and the researchers, in pursuing different ways to assess the
value of these systems. In so doing, both designers and users could take this
intrinsic characteristic of medicine more seriously and consider alternative
approaches that do not ""sweep uncertainty under the rug"" within an objectivist
fiction, which everyone can come up by believing as true.
",Computer Science; Statistics,Computer Science; Statistics
"Photometric and radial-velocity time-series of RR Lyrae stars in M3: analysis of single-mode variables   We present the first simultaneous photometric and spectroscopic investigation
of a large set of RR Lyrae variables in a globular cluster. The radial-velocity
data presented comprise the largest sample of RVs of RR Lyrae stars ever
obtained. The target is M3; $BVI_{\mathrm{C}}$ time-series of 111 and $b$ flux
data of further 64 RRab stars, and RV data of 79 RR Lyrae stars are published.
Blazhko modulation of the light curves of 47 percent of the RRab stars are
detected. The mean value of the center-of-mass velocities of RR Lyrae stars is
$-146.8$ km s$^{-1}$ with 4.52 km s$^{-1}$ standard deviation, which is in good
agreement with the results obtained for the red giants of the cluster. The
${\Phi_{21}}^{\mathrm RV}$ phase difference of the RV curves of RRab stars is
found to be uniformly constant both for the M3 and for Galactic field RRab
stars; no period or metallicity dependence of the ${\Phi_{21}}^{\mathrm RV}$ is
detected. The Baade-Wesselink distances of 26 non-Blazhko variables with the
best phase-coverage radial-velocity curves are determined; the corresponding
distance of the cluster, $10480\pm210$ pc, agrees with the previous literature
information. A quadratic formula for the $A_{\mathrm{puls}}-A_V$ relation of
RRab stars is given, which is valid for both OoI and OoII variables. We also
show that the $(V-I)_0$ of RRab stars measured at light minimum is period
dependent, there is at least 0.1 mag difference between the colours at minimum
light of the shortest- and longest-period variables.
",Physics,Physics
"Simplifying branched covering surface-knots by chart moves involving black vertices   A branched covering surface-knot is a surface-knot in the form of a branched
covering over an oriented surface-knot $F$, where we include the case when the
covering has no branch points. A branched covering surface-knot is presented by
a graph called a chart on a surface diagram of $F$. We can simplify a branched
covering surface-knot by an addition of 1-handles with chart loops to a form
such that its chart is the union of free edges and 1-handles with chart loops.
We investigate properties of such simplifications for the case when branched
covering surface-knots have a non-zero number of branch points, using chart
moves involving black vertices.
",Mathematics,Computer Science
"Game Efficiency through Linear Programming Duality   The efficiency of a game is typically quantified by the price of anarchy
(PoA), defined as the worst ratio of the objective function value of an
equilibrium --- solution of the game --- and that of an optimal outcome. Given
the tremendous impact of tools from mathematical programming in the design of
algorithms and the similarity of the price of anarchy and different measures
such as the approximation and competitive ratios, it is intriguing to develop a
duality-based method to characterize the efficiency of games.
In the paper, we present an approach based on linear programming duality to
study the efficiency of games. We show that the approach provides a general
recipe to analyze the efficiency of games and also to derive concepts leading
to improvements. The approach is particularly appropriate to bound the PoA.
Specifically, in our approach the dual programs naturally lead to competitive
PoA bounds that are (almost) optimal for several classes of games. The approach
indeed captures the smoothness framework and also some current non-smooth
techniques/concepts. We show the applicability to the wide variety of games and
environments, from congestion games to Bayesian welfare, from full-information
settings to incomplete-information ones.
",Computer Science,Computer Science
"Non-stationary Stochastic Optimization under $L_{p,q}$-Variation Measures   We consider a non-stationary sequential stochastic optimization problem, in
which the underlying cost functions change over time under a variation budget
constraint. We propose an $L_{p,q}$-variation functional to quantify the
change, which yields less variation for dynamic function sequences whose
changes are constrained to short time periods or small subsets of input domain.
Under the $L_{p,q}$-variation constraint, we derive both upper and matching
lower regret bounds for smooth and strongly convex function sequences, which
generalize previous results in Besbes et al. (2015). Furthermore, we provide an
upper bound for general convex function sequences with noisy gradient feedback,
which matches the optimal rate as $p\to\infty$. Our results reveal some
surprising phenomena under this general variation functional, such as the curse
of dimensionality of the function domain. The key technical novelties in our
analysis include affinity lemmas that characterize the distance of the
minimizers of two convex functions with bounded Lp difference, and a cubic
spline based construction that attains matching lower bounds.
",Computer Science; Statistics,Computer Science; Mathematics
"Repair Strategies for Storage on Mobile Clouds   We study the data reliability problem for a community of devices forming a
mobile cloud storage system. We consider the application of regenerating codes
for file maintenance within a geographically-limited area. Such codes require
lower bandwidth to regenerate lost data fragments compared to file replication
or reconstruction. We investigate threshold-based repair strategies where data
repair is initiated after a threshold number of data fragments have been lost
due to node mobility. We show that at a low departure-to-repair rate regime, a
lazy repair strategy in which repairs are initiated after several nodes have
left the system outperforms eager repair in which repairs are initiated after a
single departure. This optimality is reversed when nodes are highly mobile. We
further compare distributed and centralized repair strategies and derive the
optimal repair threshold for minimizing the average repair cost per unit of
time, as a function of underlying code parameters. In addition, we examine
cooperative repair strategies and show performance improvements compared to
non-cooperative codes. We investigate several models for the time needed for
node repair including a simple fixed time model that allows for the computation
of closed-form expressions and a more realistic model that takes into account
the number of repaired nodes. We derive the conditions under which the former
model approximates the latter. Finally, an extended model where additional
failures are allowed during the repair process is investigated. Overall, our
results establish the joint effect of code design and repair algorithms on the
maintenance cost of distributed storage systems.
",Computer Science,Computer Science
"Origins of bond and spin order in rare-earth nickelate bulk and heterostructures   We analyze the charge- and spin response functions of rare-earth nickelates
RNiO3 and their heterostructures using random-phase approximation in a two-band
Hubbard model. The inter-orbital charge fluctuation is found to be the driving
mechanism for the rock-salt type bond order in bulk RNiO3, and good agreement
of the ordering temperature with experimental values is achieved for all RNiO3
using realistic crystal structures and interaction parameters. We further show
that magnetic ordering in bulk is not driven by the spin fluctuation and should
be instead explained as ordering of localized moments. This picture changes for
low-dimensional heterostructures, where the charge fluctuation is suppressed
and overtaken by the enhanced spin instability, which results in a
spin-density-wave ground state observed in recent experiments. Predictions for
spectroscopy allow for further experimental testing of our claims.
",Physics,Physics
"Delayed pull-in transitions in overdamped MEMS devices   We consider the dynamics of overdamped MEMS devices undergoing the pull-in
instability. Numerous previous experiments and numerical simulations have shown
a significant increase in the pull-in time under DC voltages close to the
pull-in voltage. Here the transient dynamics slow down as the device passes
through a meta-stable or bottleneck phase, but this slowing down is not well
understood quantitatively. Using a lumped parallel-plate model, we perform a
detailed analysis of the pull-in dynamics in this regime. We show that the
bottleneck phenomenon is a type of critical slowing down arising from the
pull-in transition. This allows us to show that the pull-in time obeys an
inverse square-root scaling law as the transition is approached; moreover we
determine an analytical expression for this pull-in time. We then compare our
prediction to a wide range of pull-in time data reported in the literature,
showing that the observed slowing down is well captured by our scaling law,
which appears to be generic for overdamped pull-in under DC loads. This
realization provides a useful design rule with which to tune dynamic response
in applications, including state-of-the-art accelerometers and pressure sensors
that use pull-in time as a sensing mechanism. We also propose a method to
estimate the pull-in voltage based only on data of the pull-in times.
",Physics,Physics
"code2vec: Learning Distributed Representations of Code   We present a neural model for representing snippets of code as continuous
distributed vectors (""code embeddings""). The main idea is to represent a code
snippet as a single fixed-length $\textit{code vector}$, which can be used to
predict semantic properties of the snippet. This is performed by decomposing
code to a collection of paths in its abstract syntax tree, and learning the
atomic representation of each path $\textit{simultaneously}$ with learning how
to aggregate a set of them. We demonstrate the effectiveness of our approach by
using it to predict a method's name from the vector representation of its body.
We evaluate our approach by training a model on a dataset of 14M methods. We
show that code vectors trained on this dataset can predict method names from
files that were completely unobserved during training. Furthermore, we show
that our model learns useful method name vectors that capture semantic
similarities, combinations, and analogies. Comparing previous techniques over
the same data set, our approach obtains a relative improvement of over 75%,
being the first to successfully predict method names based on a large,
cross-project, corpus. Our trained model, visualizations and vector
similarities are available as an interactive online demo at
this http URL. The code, data, and trained models are available at
this https URL.
",Computer Science; Statistics,Computer Science
"Cloaking for a quasi-linear elliptic partial differential equation   In this article we consider cloaking for a quasi-linear elliptic partial
differential equation of divergence type defined on a bounded domain in
$\mathbb{R}^N$ for $N=2,3$. We show that a perfect cloak can be obtained via a
singular change of variables scheme and an approximate cloak can be achieved
via a regular change of variables scheme. These approximate cloaks though
non-degenerate are anisotropic. We also show, within the framework of
homogenization, that it is possible to get isotropic regular approximate
cloaks. This work generalizes to quasi-linear settings previous work on
cloaking in the context of Electrical Impedance Tomography for the conductivity
equation.
",Mathematics,Mathematics
"Uncertainty in Multitask Transfer Learning   Using variational Bayes neural networks, we develop an algorithm capable of
accumulating knowledge into a prior from multiple different tasks. The result
is a rich and meaningful prior capable of few-shot learning on new tasks. The
posterior can go beyond the mean field approximation and yields good
uncertainty on the performed experiments. Analysis on toy tasks shows that it
can learn from significantly different tasks while finding similarities among
them. Experiments of Mini-Imagenet yields the new state of the art with 74.5%
accuracy on 5 shot learning. Finally, we provide experiments showing that other
existing methods can fail to perform well in different benchmarks.
",Statistics,Computer Science; Statistics
"Population of collective modes in light scattering by many atoms   The interaction of light with an atomic sample containing a large number of
particles gives rise to many collective (or cooperative) effects, such as
multiple scattering, superradiance and subradiance, even if the atomic density
is low and the incident optical intensity weak (linear optics regime). Tracing
over the degrees of freedom of the light field, the system can be well
described by an effective atomic Hamiltonian, which contains the light-mediated
dipole-dipole interaction between atoms. This long-range interaction is at the
origin of the various collective effects, or of collective excitation modes of
the system. Even though an analysis of the eigenvalues and eigenfunctions of
these collective modes does allow distinguishing superradiant modes, for
instance, from other collective modes, this is not sufficient to understand the
dynamics of a driven system, as not all collective modes are significantly
populated. Here, we study how the excitation parameters, i.e. the driving
field, determines the population of the collective modes. We investigate in
particular the role of the laser detuning from the atomic transition, and
demonstrate a simple relation between the detuning and the steady-state
population of the modes. This relation allows understanding several properties
of cooperative scattering, such as why superradiance and subradiance become
independent of the detuning at large enough detuning without vanishing, and why
superradiance, but not subradiance, is suppressed near resonance.
",Physics,Physics
"Transient flows in active porous media   Stimuli-responsive materials that modify their shape in response to changes
in environmental conditions -- such as solute concentration, temperature, pH,
and stress -- are widespread in nature and technology. Applications include
micro- and nanoporous materials used in filtration and flow control. The
physiochemical mechanisms that induce internal volume modifications have been
widely studies. The coupling between induced volume changes and solute
transport through porous materials, however, is not well understood. Here, we
consider advective and diffusive transport through a small channel linking two
large reservoirs. A section of stimulus-responsive material regulates the
channel permeability, which is a function of the local solute concentration. We
derive an exact solution to the coupled transport problem and demonstrate the
existence of a flow regime in which the steady state is reached via a damped
oscillation around the equilibrium concentration value. Finally, the
feasibility of an experimental observation of the phenomena is discussed.
Please note that this version of the paper has not been formally peer reviewed,
revised or accepted by a journal.
",Physics,Physics
"The asymptotic behavior of automorphism groups of function fields over finite fields   The purpose of this paper is to investigate the asymptotic behavior of
automorphism groups of function fields when genus tends to infinity.
Motivated by applications in coding and cryptography, we consider the maximum
size of abelian subgroups of the automorphism group
$\mbox{Aut}(F/\mathbb{F}_q)$ in terms of genus ${g_F}$ for a function field $F$
over a finite field $\mathbb{F}_q$. Although the whole group
$\mbox{Aut}(F/\mathbb{F}_q)$ could have size $\Omega({g_F}^4)$, the maximum
size $m_F$ of abelian subgroups of the automorphism group
$\mbox{Aut}(F/\mathbb{F}_q)$ is upper bounded by $4g_F+4$ for $g_F\ge 2$. In
the present paper, we study the asymptotic behavior of $m_F$ by defining
$M_q=\limsup_{{g_F}\rightarrow\infty}\frac{m_F \cdot \log_q m_F}{g_F}$, where
$F$ runs through all function fields over $\mathbb{F}_q$. We show that $M_q$
lies between $2$ and $3$ (or $4$) for odd characteristic (or for even
characteristic, respectively). This means that $m_F$ grows much more slowly
than genus does asymptotically.
The second part of this paper is to study the maximum size $b_F$ of subgroups
of $\mbox{Aut}(F/\mathbb{F}_q)$ whose order is coprime to $q$. The Hurwitz
bound gives an upper bound $b_F\le 84(g_F-1)$ for every function field
$F/\mathbb{F}_q$ of genus $g_F\ge 2$. We investigate the asymptotic behavior of
$b_F$ by defining ${B_q}=\limsup_{{g_F}\rightarrow\infty}\frac{b_F}{g_F}$,
where $F$ runs through all function fields over $\mathbb{F}_q$. Although the
Hurwitz bound shows ${B_q}\le 84$, there are no lower bounds on $B_q$ in
literature. One does not even know if ${B_q}=0$. For the first time, we show
that ${B_q}\ge 2/3$ by explicitly constructing some towers of function fields
in this paper.
",Mathematics,Mathematics
"Training L1-Regularized Models with Orthant-Wise Passive Descent Algorithms   The $L_1$-regularized models are widely used for sparse regression or
classification tasks. In this paper, we propose the orthant-wise passive
descent algorithm (OPDA) for optimizing $L_1$-regularized models, as an
improved substitute of proximal algorithms, which are the standard tools for
optimizing the models nowadays. OPDA uses a stochastic variance-reduced
gradient (SVRG) to initialize the descent direction, then apply a novel
alignment operator to encourage each element keeping the same sign after one
iteration of update, so the parameter remains in the same orthant as before. It
also explicitly suppresses the magnitude of each element to impose sparsity.
The quasi-Newton update can be utilized to incorporate curvature information
and accelerate the speed. We prove a linear convergence rate for OPDA on
general smooth and strongly-convex loss functions. By conducting experiments on
$L_1$-regularized logistic regression and convolutional neural networks, we
show that OPDA outperforms state-of-the-art stochastic proximal algorithms,
implying a wide range of applications in training sparse models.
",Computer Science; Statistics,Computer Science; Statistics
"Secrecy and Robustness for Active Attack in Secure Network Coding and its Application to Network Quantum Key Distribution   In network coding, we discuss the effect of sequential error injection on
information leakage. We show that there is no improvement when the operations
in the network are linear operations. However, when the operations in the
network contains non-linear operations, we find a counterexample to improve
Eve's obtained information. Furthermore, we discuss the asymptotic rate in a
linear network under the secrecy and robustness conditions as well as under the
secrecy condition alone. Finally, we apply our results to network quantum key
distribution, which clarifies the type of network that enables us to realize
secure long distance communication via short distance quantum key distribution.
",Computer Science,Computer Science
"Exact Tensor Completion from Sparsely Corrupted Observations via Convex Optimization   This paper conducts a rigorous analysis for provable estimation of
multidimensional arrays, in particular third-order tensors, from a random
subset of its corrupted entries. Our study rests heavily on a recently proposed
tensor algebraic framework in which we can obtain tensor singular value
decomposition (t-SVD) that is similar to the SVD for matrices, and define a new
notion of tensor rank referred to as the tubal rank. We prove that by simply
solving a convex program, which minimizes a weighted combination of tubal
nuclear norm, a convex surrogate for the tubal rank, and the $\ell_1$-norm, one
can recover an incoherent tensor exactly with overwhelming probability,
provided that its tubal rank is not too large and that the corruptions are
reasonably sparse. Interestingly, our result includes the recovery guarantees
for the problems of tensor completion (TC) and tensor principal component
analysis (TRPCA) under the same algebraic setup as special cases. An
alternating direction method of multipliers (ADMM) algorithm is presented to
solve this optimization problem. Numerical experiments verify our theory and
real-world applications demonstrate the effectiveness of our algorithm.
",Computer Science; Statistics,Computer Science; Mathematics
"Inferring health conditions from fMRI-graph data   Automated classification methods for disease diagnosis are currently in the
limelight, especially for imaging data. Classification does not fully meet a
clinician's needs, however: in order to combine the results of multiple tests
and decide on a course of treatment, a clinician needs the likelihood of a
given health condition rather than binary classification yielded by such
methods. We illustrate how likelihoods can be derived step by step from first
principles and approximations, and how they can be assessed and selected,
illustrating our approach using fMRI data from a publicly available data set
containing schizophrenic and healthy control subjects. We start from the basic
assumption of partial exchangeability, and then the notion of sufficient
statistics and the ""method of translation"" (Edgeworth, 1898) combined with
conjugate priors. This method can be used to construct a likelihood that can be
used to compare different data-reduction algorithms. Despite the
simplifications and possibly unrealistic assumptions used to illustrate the
method, we obtain classification results comparable to previous, more realistic
studies about schizophrenia, whilst yielding likelihoods that can naturally be
combined with the results of other diagnostic tests.
",Statistics; Quantitative Biology,Statistics
"Behavioural Change Support Intelligent Transportation Applications   This workshop invites researchers and practitioners to participate in
exploring behavioral change support intelligent transportation applications. We
welcome submissions that explore intelligent transportation systems (ITS),
which interact with travelers in order to persuade them or nudge them towards
sustainable transportation behaviors and decisions. Emerging opportunities
including the use of data and information generated by ITS and users' mobile
devices in order to render personalized, contextualized and timely transport
behavioral change interventions are in our focus. We invite submissions and
ideas from domains of ITS including, but not limited to, multi-modal journey
planners, advanced traveler information systems and in-vehicle systems. The
expected outcome will be a deeper understanding of the challenges and future
research directions with respect to behavioral change support through ITS.
",Computer Science,Computer Science
"Models of the strongly lensed quasar DES J0408-5354   We present gravitational lens models of the multiply imaged quasar DES
J0408-5354, recently discovered in the Dark Energy Survey (DES) footprint, with
the aim of interpreting its remarkable quad-like configuration. We first model
the DES single-epoch $grizY$ images as a superposition of a lens galaxy and
four point-like objects, obtaining spectral energy distributions (SEDs) and
relative positions for the objects. Three of the point sources (A,B,D) have
SEDs compatible with the discovery quasar spectra, while the faintest
point-like image (G2/C) shows significant reddening and a `grey' dimming of
$\approx0.8$mag. In order to understand the lens configuration, we fit
different models to the relative positions of A,B,D. Models with just a single
deflector predict a fourth image at the location of G2/C but considerably
brighter and bluer. The addition of a small satellite galaxy ($R_{\rm
E}\approx0.2$"") in the lens plane near the position of G2/C suppresses the flux
of the fourth image and can explain both the reddening and grey dimming. All
models predict a main deflector with Einstein radius between $1.7""$ and $2.0"",$
velocity dispersion $267-280$km/s and enclosed mass $\approx
6\times10^{11}M_{\odot},$ even though higher resolution imaging data are needed
to break residual degeneracies in model parameters. The longest time-delay
(B-A) is estimated as $\approx 85$ (resp. $\approx125$) days by models with
(resp. without) a perturber near G2/C. The configuration and predicted
time-delays of J0408-5354 make it an excellent target for follow-up aimed at
understanding the source quasar host galaxy and substructure in the lens, and
measuring cosmological parameters. We also discuss some lessons learnt from
J0408-5354 on lensed quasar finding strategies, due to its chromaticity and
morphology.
",Physics,Physics
"Competitive Equilibrium For almost All Incomes   Competitive equilibrium from equal incomes (CEEI) is a well-known rule for
fair allocation of resources among agents with different preferences. It has
many advantages, among them is the fact that a CEEI allocation is both Pareto
efficient and envy-free. However, when the resources are indivisible, a CEEI
allocation might not exist even when there are two agents and a single item.
In contrast to this discouraging non-existence result, Babaioff, Nisan and
Talgam-Cohen (2017) recently suggested a new and more encouraging approach to
allocation of indivisible items: instead of insisting that the incomes be
equal, they suggest to look at the entire space of possible incomes, and check
whether there exists a competitive equilibrium for almost all income-vectors
(CEFAI) --- all income-space except a subset of measure zero. They show that a
CEFAI exists when there are at most 3 items, or when there are 4 items and two
agents. They also show that when there are 5 items and two agents there might
not exist a CEFAI. They leave open the cases of 4 items with three or four
agents.
This paper presents a new way to implement a CEFAI, as a subgame-perfect
equilibrium of a sequential game. This new implementation allows us both to
offer much simpler solutions to the known cases (at most 3 items, and 4 items
with two agents), and to prove that a CEFAI exists even in the much more
difficult case of 4 items and three agents. Moreover, we prove that a CEFAI
might not exist with 4 items and four agents. When the items to be divided are
bads (chores), CEFAI exists for two agents with at most 4 chores, but does not
exist for two agents with 5 chores or with three agents with 3 or more chores.
Thus, this paper completes the characterization of CEFAI existence for monotone
preferences.
",Computer Science,Computer Science
"Extend of the $\mathbb{Z}_2$-spin liquid phase on the Kagomé-lattice   The $\mathbb{Z}_2$ topological phase in the quantum dimer model on the
Kagomé-lattice is a candidate for the description of the low-energy physics
of the anti-ferromagnetic Heisenberg model on the same lattice. We study the
extend of the topological phase by interpolating between the exactly solvable
parent Hamiltonian of the topological phase and an effective low-energy
description of the Heisenberg model in terms of a quantum-dimer Hamiltonian.
Therefore, we perform a perturbative treatment of the low-energy excitations in
the topological phase including free and interacting quasi-particles. We find a
phase transition out of the topological phase far from the Heisenberg point.
The resulting phase is characterized by a spontaneously broken rotational
symmetry and a unit cell involving six sites.
",Physics,Physics
"Efficient Regret Minimization in Non-Convex Games   We consider regret minimization in repeated games with non-convex loss
functions. Minimizing the standard notion of regret is computationally
intractable. Thus, we define a natural notion of regret which permits efficient
optimization and generalizes offline guarantees for convergence to an
approximate local optimum. We give gradient-based methods that achieve optimal
regret, which in turn guarantee convergence to equilibrium in this framework.
",Computer Science; Statistics,Computer Science
"Topological quantization of energy transport in micro- and nano-mechanical lattices   Topological effects typically discussed in the context of quantum physics are
emerging as one of the central paradigms of physics. Here, we demonstrate the
role of topology in energy transport through dimerized micro- and
nano-mechanical lattices in the classical regime, i.e., essentially ""masses and
springs"". We show that the thermal conductance factorizes into topological and
non-topological components. The former takes on three discrete values and
arises due to the appearance of edge modes that prevent good contact between
the heat reservoirs and the bulk, giving a length-independent reduction of the
conductance. In essence, energy input at the boundary mostly stays there, an
effect robust against disorder and nonlinearity. These results bridge two
seemingly disconnected disciplines of physics, namely topology and thermal
transport, and suggest ways to engineer thermal contacts, opening a direction
to explore the ramifications of topological properties on nanoscale technology.
",Physics,Physics
"Phonetic-attention scoring for deep speaker features in speaker verification   Recent studies have shown that frame-level deep speaker features can be
derived from a deep neural network with the training target set to discriminate
speakers by a short speech segment. By pooling the frame-level features,
utterance-level representations, called d-vectors, can be derived and used in
the automatic speaker verification (ASV) task. This simple average pooling,
however, is inherently sensitive to the phonetic content of the utterance. An
interesting idea borrowed from machine translation is the attention-based
mechanism, where the contribution of an input word to the translation at a
particular time is weighted by an attention score. This score reflects the
relevance of the input word and the present translation. We can use the same
idea to align utterances with different phonetic contents. This paper proposes
a phonetic-attention scoring approach for d-vector systems. By this approach,
an attention score is computed for each frame pair. This score reflects the
similarity of the two frames in phonetic content, and is used to weigh the
contribution of this frame pair in the utterance-based scoring. This new
scoring approach emphasizes the frame pairs with similar phonetic contents,
which essentially provides a soft alignment for utterances with any phonetic
contents. Experimental results show that compared with the naive average
pooling, this phonetic-attention scoring approach can deliver consistent
performance improvement in ASV tasks of both text-dependent and
text-independent.
",Computer Science,Computer Science
"Giant interfacial perpendicular magnetic anisotropy in Fe/CuIn$_{1-x}$Ga$_x$Se$_2$ beyond Fe/MgO   We study interfacial magnetocrystalline anisotropies in various
Fe/semiconductor heterostructures by means of first-principles calculations. We
find that many of those systems show perpendicular magnetic anisotropy (PMA)
with a positive value of the interfacial anisotropy constant $K_{\rm i}$. In
particular, the Fe/CuInSe$_2$ interface has a large $K_{\rm i}$ of $\sim
2.3\,{\rm mJ/m^2}$, which is about 1.6 times larger than that of Fe/MgO known
as a typical system with relatively large PMA. We also find that the values of
$K_{\rm i}$ in almost all the systems studied in this work follow the
well-known Bruno's relation, which indicates that minority-spin states around
the Fermi level provide dominant contributions to the interfacial
magnetocrystalline anisotropies. Detailed analyses of the local density of
states and wave-vector-resolved anisotropy energy clarify that the large
$K_{\rm i}$ in Fe/CuInSe$_2$ is attributed to the preferable $3d$-orbital
configurations around the Fermi level in the minority-spin states of the
interfacial Fe atoms. Moreover, we have shown that the locations of interfacial
Se atoms are the key for such orbital configurations of the interfacial Fe
atoms.
",Physics,Physics
"Solvability regions of affinely parameterized quadratic equations   Quadratic systems of equations appear in several applications. The results in
this paper are motivated by quadratic systems of equations that describe
equilibrium behavior of physical infrastructure networks like the power and gas
grids. The quadratic systems in infrastructure networks are parameterized- the
parameters can represent uncertainty (estimation error in resistance/inductance
of a power transmission line, for example)or controllable decision variables
(power outputs of generators,for example). It is then of interest to understand
conditions on the parameters under which the quadratic system is guaranteed to
have a solution within a specified set (for example, bounds on voltages and
flows in a power grid). Given nominal values of the parameters at which the
quadratic system has a solution and the Jacobian of the quadratic system at the
solution i snon-singular, we develop a general framework to construct convex
regions around the nominal value such that the system is guaranteed to have a
solution within a given distance of the nominal solution. We show that several
results from recen tliterature can be recovered as special cases of our
framework,and demonstrate our approach on several benchmark power systems.
",Computer Science; Mathematics,Mathematics
"Rigorous proof of the Boltzmann-Gibbs distribution of money on connected graphs   Models in econophysics, i.e., the emerging field of statistical physics that
applies the main concepts of traditional physics to economics, typically
consist of large systems of economic agents who are characterized by the amount
of money they have. In the simplest model, at each time step, one agent gives
one dollar to another agent, with both agents being chosen independently and
uniformly at random from the system. Numerical simulations of this model
suggest that, at least when the number of agents and the average amount of
money per agent are large, the distribution of money converges to an
exponential distribution reminiscent of the Boltzmann-Gibbs distribution of
energy in physics. The main objective of this paper is to give a rigorous proof
of this result and show that the convergence to the exponential distribution is
universal in the sense that it holds more generally when the economic agents
are located on the vertices of a connected graph and interact locally with
their neighbors rather than globally with all the other agents. We also study a
closely related model where, at each time step, agents buy with a probability
proportional to the amount of money they have, and prove that in this case the
limiting distribution of money is Poissonian.
",Mathematics,Physics
"Spectroscopic evidence of odd frequency superconducting order   Spin filter superconducting S/I/N tunnel junctions (NbN/GdN/TiN) show a
robust and pronounced zero bias conductance peak at low temperatures, the
magnitude of which is several times the normal state conductance of the
junction. Such a conductance anomaly is representative of unconventional
superconductivity and is interpreted as a direct signature of an odd frequency
superconducting order.
",Physics,Physics
"Cross-lingual Distillation for Text Classification   Cross-lingual text classification(CLTC) is the task of classifying documents
written in different languages into the same taxonomy of categories. This paper
presents a novel approach to CLTC that builds on model distillation, which
adapts and extends a framework originally proposed for model compression. Using
soft probabilistic predictions for the documents in a label-rich language as
the (induced) supervisory labels in a parallel corpus of documents, we train
classifiers successfully for new languages in which labeled training data are
not available. An adversarial feature adaptation technique is also applied
during the model training to reduce distribution mismatch. We conducted
experiments on two benchmark CLTC datasets, treating English as the source
language and German, French, Japan and Chinese as the unlabeled target
languages. The proposed approach had the advantageous or comparable performance
of the other state-of-art methods.
",Computer Science,Computer Science
"On zeros of polynomials in best $L^p$-approximation and inserting mass points   The purpose of this note is to revive in $L^p$ spaces the original A. Markov
ideas to study monotonicity of zeros of orthogonal polynomials. This allows us
to prove and improve in a simple and unified way our previous result [Electron.
Trans. Numer. Anal., 44 (2015), pp. 271-280] concerning the discrete version of
A. Markov's theorem on monotonicity of zeros.
",Mathematics,Mathematics
"SONS: The JCMT legacy survey of debris discs in the submillimetre   Debris discs are evidence of the ongoing destructive collisions between
planetesimals, and their presence around stars also suggests that planets exist
in these systems. In this paper, we present submillimetre images of the thermal
emission from debris discs that formed the SCUBA-2 Observations of Nearby Stars
(SONS) survey, one of seven legacy surveys undertaken on the James Clerk
Maxwell telescope between 2012 and 2015. The overall results of the survey are
presented in the form of 850 microns (and 450 microns, where possible) images
and fluxes for the observed fields. Excess thermal emission, over that expected
from the stellar photosphere, is detected around 49 stars out of the 100
observed fields. The discs are characterised in terms of their flux density,
size (radial distribution of the dust) and derived dust properties from their
spectral energy distributions. The results show discs over a range of sizes,
typically 1-10 times the diameter of the Edgeworth-Kuiper Belt in our Solar
System. The mass of a disc, for particles up to a few millimetres in size, is
uniquely obtainable with submillimetre observations and this quantity is
presented as a function of the host stars' age, showing a tentative decline in
mass with age. Having doubled the number of imaged discs at submillimetre
wavelengths from ground-based, single dish telescope observations, one of the
key legacy products from the SONS survey is to provide a comprehensive target
list to observe at high angular resolution using submillimetre/millimetre
interferometers (e.g., ALMA, SMA).
",Physics,Physics
"Ordered p-median problems with neighborhoods   In this paper, we introduce a new variant of the $p$-median facility location
problem in which it is assumed that the exact location of the potential
facilities is unknown. Instead, each of the facilities must be located in a
region around their initially assigned location (the neighborhood). In this
problem, two main decisions have to be made simultaneously: the determination
of the potential facilities that must be open to serve the demands of the
customers and the location of the open facilities in their neighborhoods, at
global minimum cost. We present several mixed integer non-linear programming
formulations for a wide family of objective functions which are common in
Location Analysis: ordered median functions. We also develop two math-heuristic
approaches for solving the problem. We report the results of extensive
computational experiments.
",Mathematics,Computer Science
"Exponential error rates of SDP for block models: Beyond Grothendieck's inequality   In this paper we consider the cluster estimation problem under the Stochastic
Block Model. We show that the semidefinite programming (SDP) formulation for
this problem achieves an error rate that decays exponentially in the
signal-to-noise ratio. The error bound implies weak recovery in the sparse
graph regime with bounded expected degrees, as well as exact recovery in the
dense regime. An immediate corollary of our results yields error bounds under
the Censored Block Model. Moreover, these error bounds are robust, continuing
to hold under heterogeneous edge probabilities and a form of the so-called
monotone attack.
Significantly, this error rate is achieved by the SDP solution itself without
any further pre- or post-processing, and improves upon existing
polynomially-decaying error bounds proved using the Grothendieck\textquoteright
s inequality. Our analysis has two key ingredients: (i) showing that the graph
has a well-behaved spectrum, even in the sparse regime, after discounting an
exponentially small number of edges, and (ii) an order-statistics argument that
governs the final error rate. Both arguments highlight the implicit
regularization effect of the SDP formulation.
",Computer Science; Mathematics; Statistics,Computer Science; Mathematics
"Virtual retraction and Howson's theorem in pro-$p$ groups   We show that for every finitely generated closed subgroup $K$ of a
non-solvable Demushkin group $G$, there exists an open subgroup $U$ of $G$
containing $K$, and a continuous homomorphism $\tau \colon U \to K$ satisfying
$\tau(k) = k$ for every $k \in K$. We prove that the intersection of a pair of
finitely generated closed subgroups of a Demushkin group is finitely generated
(giving an explicit bound on the number of generators). Furthermore, we show
that these properties of Demushkin groups are preserved under free pro-$p$
products, and deduce that Howson's theorem holds for the Sylow subgroups of the
absolute Galois group of a number field. Finally, we confirm two conjectures of
Ribes, thus classifying the finitely generated pro-$p$ M. Hall groups.
",Mathematics,Mathematics
"Convergence analysis of the information matrix in Gaussian belief propagation   Gaussian belief propagation (BP) has been widely used for distributed
estimation in large-scale networks such as the smart grid, communication
networks, and social networks, where local measurements/observations are
scattered over a wide geographical area. However, the convergence of Gaus- sian
BP is still an open issue. In this paper, we consider the convergence of
Gaussian BP, focusing in particular on the convergence of the information
matrix. We show analytically that the exchanged message information matrix
converges for arbitrary positive semidefinite initial value, and its dis- tance
to the unique positive definite limit matrix decreases exponentially fast.
",Computer Science,Computer Science; Statistics
"Distributed Event-Triggered Control for Global Consensus of Multi-Agent Systems with Input Saturation   We consider the global consensus problem for multi-agent systems with input
saturation over digraphs. Under a mild connectivity condition that the
underlying digraph has a directed spanning tree, we use Lyapunov methods to
show that the widely used distributed consensus protocol, which solves the
consensus problem for the case without input saturation constraints, also
solves the global consensus problem for the case with input saturation
constraints. In order to reduce the overall need of communication and system
updates, we then propose a distributed event-triggered control law. Global
consensus is still realized and Zeno behavior is excluded. Numerical
simulations are provided to illustrate the effectiveness of the theoretical
results.
",Mathematics,Computer Science
"Atomic-scale identification of novel planar defect phases in heteroepitaxial YBa$_2$Cu$_3$O$_{7-δ}$ thin films   We have discovered two novel types of planar defects that appear in
heteroepitaxial YBa$_2$Cu$_3$O$_{7-\delta}$ (YBCO123) thin films, grown by
pulsed-laser deposition (PLD) either with or without a
La$_{2/3}$Ca$_{1/3}$MnO$_3$ (LCMO) overlayer, using the combination of
high-angle annular dark-field scanning transmission electron microscopy
(HAADF-STEM) imaging and electron energy loss spectroscopy (EELS) mapping for
unambiguous identification. These planar lattice defects are based on the
intergrowth of either a BaO plane between two CuO chains or multiple Y-O layers
between two CuO$_2$ planes, resulting in non-stoichiometric layer sequences
that could directly impact the high-$T_c$ superconductivity.
",Physics,Physics
"Tracking Emerges by Colorizing Videos   We use large amounts of unlabeled video to learn models for visual tracking
without manual human supervision. We leverage the natural temporal coherency of
color to create a model that learns to colorize gray-scale videos by copying
colors from a reference frame. Quantitative and qualitative experiments suggest
that this task causes the model to automatically learn to track visual regions.
Although the model is trained without any ground-truth labels, our method
learns to track well enough to outperform the latest methods based on optical
flow. Moreover, our results suggest that failures to track are correlated with
failures to colorize, indicating that advancing video colorization may further
improve self-supervised visual tracking.
",Computer Science,Computer Science
"Directional convexity of harmonic mappings   The convolution properties are discussed for the complex-valued harmonic
functions in the unit disk $\mathbb{D}$ constructed from the harmonic shearing
of the analytic function $\phi(z):=\int_0^z
(1/(1-2\xi\textit{e}^{\textit{i}\mu}\cos\nu+\xi^2\textit{e}^{2\textit{i}\mu}))\textit{d}\xi$,
where $\mu$ and $\nu$ are real numbers. For any real number $\alpha$ and
harmonic function $f=h+\overline{g}$, define an analytic function
$f_{\alpha}:=h+\textit{e}^{-2\textit{i}\alpha}g$. Let $\mu_1$ and $\mu_2$
$(\mu_1+\mu_2=\mu)$ be real numbers, and $f=h+\overline{g}$ and
$F=H+\overline{G}$ be locally-univalent and sense-preserving harmonic functions
such that $f_{\mu_1}*F_{\mu_2}=\phi$. It is shown that the convolution $f*F$ is
univalent and convex in the direction of $-\mu$, provided it is locally
univalent and sense-preserving. Also, local-univalence of the above convolution
$f*F$ is shown for some specific analytic dilatations of $f$ and $F$.
Furthermore, if $g\equiv0$ and both the analytic functions $f_{\mu_1}$ and
$F_{\mu_2}$ are convex, then the convolution $f*F$ is shown to be convex. These
results extends the work done by Dorff \textit{et al.} to a larger class of
functions.
",Mathematics,Mathematics
"Semidefinite Relaxation-Based Optimization of Multiple-Input Wireless Power Transfer Systems   An optimization procedure for multi-transmitter (MISO) wireless power
transfer (WPT) systems based on tight semidefinite relaxation (SDR) is
presented. This method ensures physical realizability of MISO WPT systems
designed via convex optimization -- a robust, semi-analytical and intuitive
route to optimizing such systems. To that end, the nonconvex constraints
requiring that power is fed into rather than drawn from the system via all
transmitter ports are incorporated in a convex semidefinite relaxation, which
is efficiently and reliably solvable by dedicated algorithms. A test of the
solution then confirms that this modified problem is equivalent (tight
relaxation) to the original (nonconvex) one and that the true global optimum
has been found. This is a clear advantage over global optimization methods
(e.g. genetic algorithms), where convergence to the true global optimum cannot
be ensured or tested. Discussions of numerical results yielded by both the
closed-form expressions and the refined technique illustrate the importance and
practicability of the new method. It, is shown that this technique offers a
rigorous optimization framework for a broad range of current and emerging WPT
applications.
",Computer Science; Mathematics,Computer Science
"Shapley effects for sensitivity analysis with correlated inputs: comparisons with Sobol' indices, numerical estimation and applications   The global sensitivity analysis of a numerical model aims to quantify, by
means of sensitivity indices estimate, the contributions of each uncertain
input variable to the model output uncertainty. The so-called Sobol' indices,
which are based on the functional variance analysis, present a difficult
interpretation in the presence of statistical dependence between inputs. The
Shapley effect was recently introduced to overcome this problem as they
allocate the mutual contribution (due to correlation and interaction) of a
group of inputs to each individual input within the group.In this paper, using
several new analytical results, we study the effects of linear correlation
between some Gaussian input variables on Shapley effects, and compare these
effects to classical first-order and total Sobol' indices.This illustrates the
interest, in terms of sensitivity analysis setting and interpretation, of the
Shapley effects in the case of dependent inputs. We also investigate the
numerical convergence of the estimated Shapley effects. For the practical issue
of computationally demanding computer models, we show that the substitution of
the original model by a metamodel (here, kriging) makes it possible to estimate
these indices with precision at a reasonable computational cost.
",Mathematics; Statistics,Mathematics; Statistics
"Sparse Approximation of 3D Meshes using the Spectral Geometry of the Hamiltonian Operator   The discrete Laplace operator is ubiquitous in spectral shape analysis, since
its eigenfunctions are provably optimal in representing smooth functions
defined on the surface of the shape. Indeed, subspaces defined by its
eigenfunctions have been utilized for shape compression, treating the
coordinates as smooth functions defined on the given surface. However, surfaces
of shapes in nature often contain geometric structures for which the general
smoothness assumption may fail to hold. At the other end, some explicit mesh
compression algorithms utilize the order by which vertices that represent the
surface are traversed, a property which has been ignored in spectral
approaches. Here, we incorporate the order of vertices into an operator that
defines a novel spectral domain. We propose a method for representing 3D meshes
using the spectral geometry of the Hamiltonian operator, integrated within a
sparse approximation framework. We adapt the concept of a potential function
from quantum physics and incorporate vertex ordering information into the
potential, yielding a novel data-dependent operator. The potential function
modifies the spectral geometry of the Laplacian to focus on regions with finer
details of the given surface. By sparsely encoding the geometry of the shape
using the proposed data-dependent basis, we improve compression performance
compared to previous results that use the standard Laplacian basis and spectral
graph wavelets.
",Computer Science,Computer Science
"WKB solutions of difference equations and reconstruction by the topological recursion   The purpose of this article is to analyze the connection between
Eynard-Orantin topological recursion and formal WKB solutions of a
$\hbar$-difference equation: $\Psi(x+\hbar)=\left(e^{\hbar\frac{d}{dx}}\right)
\Psi(x)=L(x;\hbar)\Psi(x)$ with $L(x;\hbar)\in GL_2( (\mathbb{C}(x))[\hbar])$.
In particular, we extend the notion of determinantal formulas and topological
type property proposed for formal WKB solutions of $\hbar$-differential systems
to this setting. We apply our results to a specific $\hbar$-difference system
associated to the quantum curve of the Gromov-Witten invariants of
$\mathbb{P}^1$ for which we are able to prove that the correlation functions
are reconstructed from the Eynard-Orantin differentials computed from the
topological recursion applied to the spectral curve $y=\cosh^{-1}\frac{x}{2}$.
Finally, identifying the large $x$ expansion of the correlation functions,
proves a recent conjecture made by B. Dubrovin and D. Yang regarding a new
generating series for Gromov-Witten invariants of $\mathbb{P}^1$.
",Physics; Mathematics,Mathematics
"Fates of the dense cores formed by fragmentation of filaments: do they fragment again or not?   Fragmentation of filaments into dense cores is thought to be an important
step in forming stars. The bar-mode instability of spherically collapsing cores
found in previous linear analysis invokes a possibility of re-fragmentation of
the cores due to their ellipsoidal (prolate or oblate) deformation. To
investigate this possibility, here we perform three-dimensional
self-gravitational hydrodynamics simulations that follow all the way from
filament fragmentation to subsequent core collapse. We assume the gas is
polytropic with index \gamma, which determines the stability of the bar-mode.
For the case that the fragmentation of isolated hydrostatic filaments is
triggered by the most unstable fragmentation mode, we find the bar mode grows
as collapse proceeds if \gamma < 1.1, in agreement with the linear analysis.
However, it takes more than ten orders-of-magnitude increase in the central
density for the distortion to become non-linear. In addition to this fiducial
case, we also study non-fiducial ones such as the fragmentation is triggered by
a fragmentation mode with a longer wavelength and it occurs during radial
collapse of filaments and find the distortion rapidly grows. In most of
astrophysical applications, the effective polytropic index of collapsing gas
exceeds 1.1 before ten orders-of-magnitude increase in the central density.
Thus, supposing the fiducial case of filament fragmentation, re-fragmentation
of dense cores would not be likely and their final mass would be determined
when the filaments fragment.
",Physics,Physics
"A Supervised Approach to Extractive Summarisation of Scientific Papers   Automatic summarisation is a popular approach to reduce a document to its
main arguments. Recent research in the area has focused on neural approaches to
summarisation, which can be very data-hungry. However, few large datasets exist
and none for the traditionally popular domain of scientific publications, which
opens up challenging research avenues centered on encoding large, complex
documents. In this paper, we introduce a new dataset for summarisation of
computer science publications by exploiting a large resource of author provided
summaries and show straightforward ways of extending it further. We develop
models on the dataset making use of both neural sentence encoding and
traditionally used summarisation features and show that models which encode
sentences as well as their local and global context perform best, significantly
outperforming well-established baseline methods.
",Computer Science; Statistics,Computer Science
"Structural Nonrealism and Quantum Information   The article introduces a new concept of structure, defined, echoing J. A.
Wheeler's concept of ""law without law,"" as a ""structure without law,"" and a new
philosophical viewpoint, that of structural nnnrealism, and considers how this
concept and this viewpoint work in quantum theory in general and quantum
information theory in particular. It takes as its historical point of departure
W. Heisenberg's discovery of quantum mechanics, which, the article argues,
could, in retrospect, be considered in quantum-informational terms, while,
conversely, quantum information theory could be seen in Heisenbergian terms.
The article takes advantage of the circumstance that any instance of quantum
information is a ""structure""--an organization of elements, ultimately bits, of
classical information, manifested in measuring instruments. While, however,
this organization can, along with the observed behavior of measuring
instruments, be described by means of classical physics, it cannot be predicted
by means of classical physics, but only, probabilistically or statistically, by
means of quantum mechanics, or in high-energy physics, by means of quantum
field theory (or possibly some alternative theories within each scope). By
contrast, the emergences of this information and of this structure cannot, in
the present view, be described by either classical or quantum theory, or
possibly by any other means, which leads to the concept of ""structure without
law"" and the viewpoint of structural nnnrealism. The article also considers,
from this perspective, some recent work in quantum information theory.
",Physics,Physics
"Hyperbolic Pascal simplex   In this article we introduce a new geometric object called hyperbolic Pascal
simplex. This new object is presented by the regular hypercube mosaic in the
4-dimensional hyperbolic space. The definition of the hyperbolic Pascal
simplex, whose hyperfaces are hyperbolic Pascal pyramids and faces are
hyperbolic Pascals triangles, is a natural generalization of the definition of
the hyperbolic Pascal triangle and pyramid. We describe the growing of the
hyperbolic Pascal simplex considering the numbers and the values of the
elements. Further figures illustrate the stepping from a level to the next one.
",Mathematics,Mathematics
"Inference on Breakdown Frontiers   Given a set of baseline assumptions, a breakdown frontier is the boundary
between the set of assumptions which lead to a specific conclusion and those
which do not. In a potential outcomes model with a binary treatment, we
consider two conclusions: First, that ATE is at least a specific value (e.g.,
nonnegative) and second that the proportion of units who benefit from treatment
is at least a specific value (e.g., at least 50\%). For these conclusions, we
derive the breakdown frontier for two kinds of assumptions: one which indexes
relaxations of the baseline random assignment of treatment assumption, and one
which indexes relaxations of the baseline rank invariance assumption. These
classes of assumptions nest both the point identifying assumptions of random
assignment and rank invariance and the opposite end of no constraints on
treatment selection or the dependence structure between potential outcomes.
This frontier provides a quantitative measure of robustness of conclusions to
relaxations of the baseline point identifying assumptions. We derive
$\sqrt{N}$-consistent sample analog estimators for these frontiers. We then
provide two asymptotically valid bootstrap procedures for constructing lower
uniform confidence bands for the breakdown frontier. As a measure of
robustness, estimated breakdown frontiers and their corresponding confidence
bands can be presented alongside traditional point estimates and confidence
intervals obtained under point identifying assumptions. We illustrate this
approach in an empirical application to the effect of child soldiering on
wages. We find that sufficiently weak conclusions are robust to simultaneous
failures of rank invariance and random assignment, while some stronger
conclusions are fairly robust to failures of rank invariance but not
necessarily to relaxations of random assignment.
",Statistics,Mathematics; Statistics
"Polyteam Semantics   Team semantics is the mathematical framework of modern logics of dependence
and independence in which formulae are interpreted by sets of assignments
(teams) instead of single assignments as in first-order logic. In order to
deepen the fruitful interplay between team semantics and database dependency
theory, we define ""Polyteam Semantics"" in which formulae are evaluated over a
family of teams. We begin by defining a novel polyteam variant of dependence
atoms and give a finite axiomatisation for the associated implication problem.
We also characterise the expressive power of poly-dependence logic by
properties of polyteams that are downward closed and definable in existential
second-order logic (ESO). The analogous result is shown to hold for
poly-independence logic and all ESO-definable properties.
",Computer Science,Computer Science
"A 588 Gbps LDPC Decoder Based on Finite-Alphabet Message Passing   An ultra-high throughput low-density parity check (LDPC) decoder with an
unrolled full-parallel architecture is proposed, which achieves the highest
decoding throughput compared to previously reported LDPC decoders in the
literature. The decoder benefits from a serial message-transfer approach
between the decoding stages to alleviate the well-known routing congestion
problem in parallel LDPC decoders. Furthermore, a finite-alphabet message
passing algorithm is employed to replace the variable node update rule of the
standard min-sum decoder with look-up tables, which are designed in a way that
maximizes the mutual information between decoding messages. The proposed
algorithm results in an architecture with reduced bit-width messages, leading
to a significantly higher decoding throughput and to a lower area as compared
to a min-sum decoder when serial message-transfer is used. The architecture is
placed and routed for the standard min-sum reference decoder and for the
proposed finite-alphabet decoder using a custom pseudo-hierarchical backend
design strategy to further alleviate routing congestions and to handle the
large design. Post-layout results show that the finite-alphabet decoder with
the serial message-transfer architecture achieves a throughput as large as 588
Gbps with an area of 16.2 mm$^2$ and dissipates an average power of 22.7 pJ per
decoded bit in a 28 nm FD-SOI library. Compared to the reference min-sum
decoder, this corresponds to 3.1 times smaller area and 2 times better energy
efficiency.
",Computer Science,Computer Science
"High-order schemes for the Euler equations in cylindrical/spherical coordinates   We consider implementations of high-order finite difference Weighted
Essentially Non-Oscillatory (WENO) schemes for the Euler equations in
cylindrical and spherical coordinate systems with radial dependence only. The
main concern of this work lies in ensuring both high-order accuracy and
conservation. Three different spatial discretizations are assessed: one that is
shown to be high-order accurate but not conservative, one conservative but not
high-order accurate, and a new approach that is both high-order accurate and
conservative. For cylindrical and spherical coordinates, we present convergence
results for the advection equation and the Euler equations with an acoustics
problem; we then use the Sod shock tube and the Sedov point-blast problems in
cylindrical coordinates to verify our analysis and implementations.
",Physics; Mathematics,Physics
"Robust method for finding sparse solutions to linear inverse problems using an L2 regularization   We analyzed the performance of a biologically inspired algorithm called the
Corrected Projections Algorithm (CPA) when a sparseness constraint is required
to unambiguously reconstruct an observed signal using atoms from an
overcomplete dictionary. By changing the geometry of the estimation problem,
CPA gives an analytical expression for a binary variable that indicates the
presence or absence of a dictionary atom using an L2 regularizer. The
regularized solution can be implemented using an efficient real-time
Kalman-filter type of algorithm. The smoother L2 regularization of CPA makes it
very robust to noise, and CPA outperforms other methods in identifying known
atoms in the presence of strong novel atoms in the signal.
",Computer Science; Statistics,Computer Science; Statistics
"Distributed Optimization of Multi-Beam Directional Communication Networks   We formulate an optimization problem for maximizing the data rate of a common
message transmitted from nodes within an airborne network broadcast to a
central station receiver while maintaining a set of intra-network rate demands.
Assuming that the network has full-duplex links with multi-beam directional
capability, we obtain a convex multi-commodity flow problem and use a
distributed augmented Lagrangian algorithm to solve for the optimal flows
associated with each beam in the network. For each augmented Lagrangian
iteration, we propose a scaled gradient projection method to minimize the local
Lagrangian function that incorporates the local topology of each node in the
network. Simulation results show fast convergence of the algorithm in
comparison to simple distributed primal dual methods and highlight performance
gains over standard minimum distance-based routing.
",Mathematics,Computer Science
"Variational Monte Carlo study of spin dynamics in underdoped cuprates   The hour-glass-like dispersion of spin excitations is a common feature of
underdoped cuprates. It was qualitatively explained by the random phase
approximation based on various ordered states with some phenomenological
parameters; however, its origin remains elusive. Here, we present a numerical
study of spin dynamics in the $t$-$J$ model using the variational Monte Carlo
method. This parameter-free method satisfies the no double-occupancy constraint
of the model and thus provides a better evaluation on the spin dynamics with
respect to various mean-field trial states. We conclude that the lower branch
of the hour-glass dispersion is a collective mode and the upper branch is more
likely the consequence of the stripe state than the other candidates.
",Physics,Physics
"Hidden Truncation Hyperbolic Distributions, Finite Mixtures Thereof, and Their Application for Clustering   A hidden truncation hyperbolic (HTH) distribution is introduced and finite
mixtures thereof are applied for clustering. A stochastic representation of the
HTH distribution is given and a density is derived. A hierarchical
representation is described, which aids in parameter estimation. Finite
mixtures of HTH distributions are presented and their identifiability is
proved. The convexity of the HTH distribution is discussed, which is important
in clustering applications, and some theoretical results in this direction are
presented. The relationship between the HTH distribution and other skewed
distributions in the literature is discussed. Illustrations are provided ---
both of the HTH distribution and application of finite mixtures thereof for
clustering.
",Statistics,Mathematics; Statistics
"Design Decisions for Weave: A Real-Time Web-based Collaborative Visualization Framework   There are many web-based visualization systems available to date, each having
its strengths and limitations. The goals these systems set out to accomplish
influence design decisions and determine how reusable and scalable they are.
Weave is a new web-based visualization platform with the broad goal of enabling
visualization of any available data by anyone for any purpose. Our open source
framework supports highly interactive linked visualizations for users of
varying skill levels. What sets Weave apart from other systems is its
consideration for real-time remote collaboration with session history. We
provide a detailed account of the various framework designs we considered with
comparisons to existing state-of-the-art systems.
",Computer Science,Computer Science
"Network Slicing for Ultra-Reliable Low Latency Communication in Industry 4.0 Scenarios   An important novelty of 5G is its role in transforming the industrial
production into Industry 4.0. Specifically, Ultra-Reliable Low Latency
Communications (URLLC) will, in many cases, enable replacement of cables with
wireless connections and bring freedom in designing and operating
interconnected machines, robots, and devices. However, not all industrial links
will be of URLLC type; e.g. some applications will require high data rates.
Furthermore, these industrial networks will be highly heterogeneous, featuring
various communication technologies. We consider network slicing as a mechanism
to handle the diverse set of requirements to the network. We present methods
for slicing deterministic and packet-switched industrial communication
protocols at an abstraction level that is decoupled from the specific
implementation of the underlying technologies. Finally, we show how network
calculus can be used to assess the end-to-end properties of the network slices.
",Computer Science,Computer Science
"Borel class and Cartan involution   In this note we prove that the Borel class of representations of 3-manifold
groups to PGL(n,C) is preserved under Cartan involution up to sign. For
representations to PGL(3,C) this is implied by a more general result of E.
Falbel and Q. Wang, however our proof appears to be much shorter for that
special case.
",Mathematics,Mathematics
"History-aware Autonomous Exploration in Confined Environments using MAVs   Many scenarios require a robot to be able to explore its 3D environment
online without human supervision. This is especially relevant for inspection
tasks and search and rescue missions. To solve this high-dimensional path
planning problem, sampling-based exploration algorithms have proven successful.
However, these do not necessarily scale well to larger environments or spaces
with narrow openings. This paper presents a 3D exploration planner based on the
principles of Next-Best Views (NBVs). In this approach, a Micro-Aerial Vehicle
(MAV) equipped with a limited field-of-view depth sensor randomly samples its
configuration space to find promising future viewpoints. In order to obtain
high sampling efficiency, our planner maintains and uses a history of visited
places, and locally optimizes the robot's orientation with respect to
unobserved space. We evaluate our method in several simulated scenarios, and
compare it against a state-of-the-art exploration algorithm. The experiments
show substantial improvements in exploration time ($2\times$ faster),
computation time, and path length, and advantages in handling difficult
situations such as escaping dead-ends (up to $20\times$ faster). Finally, we
validate the on-line capability of our algorithm on a computational constrained
real world MAV.
",Computer Science,Computer Science
"B-spline-like bases for $C^2$ cubics on the Powell-Sabin 12-split   For spaces of constant, linear, and quadratic splines of maximal smoothness
on the Powell-Sabin 12-split of a triangle, the so-called S-bases were recently
introduced. These are simplex spline bases with B-spline-like properties on the
12-split of a single triangle, which are tied together across triangles in a
Bézier-like manner.
In this paper we give a formal definition of an S-basis in terms of certain
basic properties. We proceed to investigate the existence of S-bases for the
aforementioned spaces and additionally the cubic case, resulting in an
exhaustive list. From their nature as simplex splines, we derive simple
differentiation and recurrence formulas to other S-bases. We establish a
Marsden identity that gives rise to various quasi-interpolants and domain
points forming an intuitive control net, in terms of which conditions for
$C^0$-, $C^1$-, and $C^2$-smoothness are derived.
",Computer Science,Mathematics
"Spin pumping into superconductors: A new probe of spin dynamics in a superconducting thin film   Spin pumping refers to the microwave-driven spin current injection from a
ferromagnet into the adjacent target material. We theoretically investigate the
spin pumping into superconductors by fully taking account of impurity
spin-orbit scattering that is indispensable to describe diffusive spin
transport with finite spin diffusion length. We calculate temperature
dependence of the spin pumping signal and show that a pronounced coherence peak
appears immediately below the superconducting transition temperature Tc, which
survives even in the presence of the spin-orbit scattering. The phenomenon
provides us with a new way of studying the dynamic spin susceptibility in a
superconducting thin film. This is contrasted with the nuclear magnetic
resonance technique used to study a bulk superconductor.
",Physics,Physics
"SuperMinHash - A New Minwise Hashing Algorithm for Jaccard Similarity Estimation   This paper presents a new algorithm for calculating hash signatures of sets
which can be directly used for Jaccard similarity estimation. The new approach
is an improvement over the MinHash algorithm, because it has a better runtime
behavior and the resulting signatures allow a more precise estimation of the
Jaccard index.
",Computer Science,Computer Science
"Learning the Sparse and Low Rank PARAFAC Decomposition via the Elastic Net   In this article, we derive a Bayesian model to learning the sparse and low
rank PARAFAC decomposition for the observed tensor with missing values via the
elastic net, with property to find the true rank and sparse factor matrix which
is robust to the noise. We formulate efficient block coordinate descent
algorithm and admax stochastic block coordinate descent algorithm to solve it,
which can be used to solve the large scale problem. To choose the appropriate
rank and sparsity in PARAFAC decomposition, we will give a solution path by
gradually increasing the regularization to increase the sparsity and decrease
the rank. When we find the sparse structure of the factor matrix, we can fixed
the sparse structure, using a small to regularization to decreasing the
recovery error, and one can choose the proper decomposition from the solution
path with sufficient sparse factor matrix with low recovery error. We test the
power of our algorithm on the simulation data and real data, which show it is
powerful.
",Mathematics; Statistics,Computer Science; Statistics
"Simulated Tempering Method in the Infinite Switch Limit with Adaptive Weight Learning   We investigate the theoretical foundations of the simulated tempering method
and use our findings to design efficient algorithms. Employing a large
deviation argument first used for replica exchange molecular dynamics [Plattner
et al., J. Chem. Phys. 135:134111 (2011)], we demonstrate that the most
efficient approach to simulated tempering is to vary the temperature infinitely
rapidly. In this limit, we can replace the equations of motion for the
temperature and physical variables by averaged equations for the latter alone,
with the forces rescaled according to a position-dependent function defined in
terms of temperature weights. The averaged equations are similar to those used
in Gao's integrated-over-temperature method, except that we show that it is
better to use a continuous rather than a discrete set of temperatures. We give
a theoretical argument for the choice of the temperature weights as the
reciprocal partition function, thereby relating simulated tempering to
Wang-Landau sampling. Finally, we describe a self-consistent algorithm for
simultaneously sampling the canonical ensemble and learning the weights during
simulation. This algorithm is tested on a system of harmonic oscillators as
well as a continuous variant of the Curie-Weiss model, where it is shown to
perform well and to accurately capture the second-order phase transition
observed in this model.
",Statistics,Computer Science; Physics
"Grothendieck rigidity of 3-manifold groups   We show that fundamental groups of compact, orientable, irreducible
3-manifolds with toroidal boundary are Grothendieck rigid.
",Mathematics,Mathematics
"On the Liouville heat kernel for k-coarse MBRW and nonuniversality   We study the Liouville heat kernel (in the $L^2$ phase) associated with a
class of logarithmically correlated Gaussian fields on the two dimensional
torus. We show that for each $\varepsilon>0$ there exists such a field, whose
covariance is a bounded perturbation of that of the two dimensional Gaussian
free field, and such that the associated Liouville heat kernel satisfies the
short time estimates, $$ \exp \left( - t^{ - \frac 1 { 1 + \frac 1 2 \gamma^2 }
- \varepsilon } \right) \le p_t^\gamma (x, y) \le \exp \left( - t^{- \frac 1 {
1 + \frac 1 2 \gamma^2 } + \varepsilon } \right) , $$ for $\gamma<1/2$. In
particular, these are different from predictions, due to Watabiki, concerning
the Liouville heat kernel for the two dimensional Gaussian free field.
",Mathematics,Mathematics
"Asymptotic Goodness-of-Fit Tests for Point Processes Based on Scaled Empirical K-Functions   We study sequences of scaled edge-corrected empirical (generalized)
K-functions (modifying Ripley's K-function) each of them constructed from a
single observation of a $d$-dimensional fourth-order stationary point process
in a sampling window W_n which grows together with some scaling rate
unboundedly as n --> infty. Under some natural assumptions it is shown that the
normalized difference between scaled empirical and scaled theoretical
K-function converges weakly to a mean zero Gaussian process with simple
covariance function. This result suggests discrepancy measures between
empirical and theoretical K-function with known limit distribution which allow
to perform goodness-of-fit tests for checking a hypothesized point process
based only on its intensity and (generalized) K-function. Similar test
statistics are derived for testing the hypothesis that two independent point
processes in W_n have the same distribution without explicit knowledge of their
intensities and K-functions.
",Mathematics; Statistics,Mathematics; Statistics
"Persistence Codebooks for Topological Data Analysis   Topological data analysis, such as persistent homology has shown beneficial
properties for machine learning in many tasks. Topological representations,
such as the persistence diagram (PD), however, have a complex structure
(multiset of intervals) which makes it difficult to combine with typical
machine learning workflows. We present novel compact fixed-size vectorial
representations of PDs based on clustering and bag of words encodings that cope
well with the inherent sparsity of PDs. Our novel representations outperform
state-of-the-art approaches from topological data analysis and are
computationally more efficient.
",Statistics,Computer Science; Statistics
"Flatness results for nonlocal minimal cones and subgraphs   We show that nonlocal minimal cones which are non-singular subgraphs outside
the origin are necessarily halfspaces.
The proof is based on classical ideas of~\cite{DG1} and on the computation of
the linearized nonlocal mean curvature operator, which is proved to satisfy a
suitable maximum principle.
With this, we obtain new, and somehow simpler, proofs of the Bernstein-type
results for nonlocal minimal surfaces which have been recently established
in~\cite{FV}. In addition, we establish a new nonlocal Bernstein-Moser-type
result which classifies Lipschitz nonlocal minimal subgraphs outside a ball.
",Mathematics,Mathematics
"ACtuAL: Actor-Critic Under Adversarial Learning   Generative Adversarial Networks (GANs) are a powerful framework for deep
generative modeling. Posed as a two-player minimax problem, GANs are typically
trained end-to-end on real-valued data and can be used to train a generator of
high-dimensional and realistic images. However, a major limitation of GANs is
that training relies on passing gradients from the discriminator through the
generator via back-propagation. This makes it fundamentally difficult to train
GANs with discrete data, as generation in this case typically involves a
non-differentiable function. These difficulties extend to the reinforcement
learning setting when the action space is composed of discrete decisions. We
address these issues by reframing the GAN framework so that the generator is no
longer trained using gradients through the discriminator, but is instead
trained using a learned critic in the actor-critic framework with a Temporal
Difference (TD) objective. This is a natural fit for sequence modeling and we
use it to achieve improvements on language modeling tasks over the standard
Teacher-Forcing methods.
",Computer Science; Statistics,Computer Science; Statistics
"Determinantal Generalizations of Instrumental Variables   Linear structural equation models relate the components of a random vector
using linear interdependencies and Gaussian noise. Each such model can be
naturally associated with a mixed graph whose vertices correspond to the
components of the random vector. The graph contains directed edges that
represent the linear relationships between components, and bidirected edges
that encode unobserved confounding. We study the problem of generic
identifiability, that is, whether a generic choice of linear and confounding
effects can be uniquely recovered from the joint covariance matrix of the
observed random vector. An existing combinatorial criterion for establishing
generic identifiability is the half-trek criterion (HTC), which uses the
existence of trek systems in the mixed graph to iteratively discover
generically invertible linear equation systems in polynomial time. By focusing
on edges one at a time, we establish new sufficient and necessary conditions
for generic identifiability of edge effects extending those of the HTC. In
particular, we show how edge coefficients can be recovered as quotients of
subdeterminants of the covariance matrix, which constitutes a determinantal
generalization of formulas obtained when using instrumental variables for
identification.
",Mathematics; Statistics,Computer Science; Mathematics
"A Hierarchical Max-infinitely Divisible Process for Extreme Areal Precipitation Over Watersheds   Understanding the spatial extent of extreme precipitation is necessary for
determining flood risk and adequately designing infrastructure (e.g.,
stormwater pipes) to withstand such hazards. While environmental phenomena
typically exhibit weakening spatial dependence at increasingly extreme levels,
limiting max-stable process models for block maxima have a rigid dependence
structure that does not capture this type of behavior. We propose a flexible
Bayesian model from a broader family of max-infinitely divisible processes that
allows for weakening spatial dependence at increasingly extreme levels, and due
to a hierarchical representation of the likelihood in terms of random effects,
our inference approach scales to large datasets. The proposed model is
constructed using flexible random basis functions that are estimated from the
data, allowing for straightforward inspection of the predominant spatial
patterns of extremes. In addition, the described process possesses
max-stability as a special case, making inference on the tail dependence class
possible. We apply our model to extreme precipitation in eastern North America,
and show that the proposed model adequately captures the extremal behavior of
the data.
",Statistics,Statistics
"R-C3D: Region Convolutional 3D Network for Temporal Activity Detection   We address the problem of activity detection in continuous, untrimmed video
streams. This is a difficult task that requires extracting meaningful
spatio-temporal features to capture activities, accurately localizing the start
and end times of each activity. We introduce a new model, Region Convolutional
3D Network (R-C3D), which encodes the video streams using a three-dimensional
fully convolutional network, then generates candidate temporal regions
containing activities, and finally classifies selected regions into specific
activities. Computation is saved due to the sharing of convolutional features
between the proposal and the classification pipelines. The entire model is
trained end-to-end with jointly optimized localization and classification
losses. R-C3D is faster than existing methods (569 frames per second on a
single Titan X Maxwell GPU) and achieves state-of-the-art results on THUMOS'14.
We further demonstrate that our model is a general activity detection framework
that does not rely on assumptions about particular dataset properties by
evaluating our approach on ActivityNet and Charades. Our code is available at
this http URL.
",Computer Science,Computer Science
"SenGen: Sentence Generating Neural Variational Topic Model   We present a new topic model that generates documents by sampling a topic for
one whole sentence at a time, and generating the words in the sentence using an
RNN decoder that is conditioned on the topic of the sentence. We argue that
this novel formalism will help us not only visualize and model the topical
discourse structure in a document better, but also potentially lead to more
interpretable topics since we can now illustrate topics by sampling
representative sentences instead of bag of words or phrases. We present a
variational auto-encoder approach for learning in which we use a factorized
variational encoder that independently models the posterior over topical
mixture vectors of documents using a feed-forward network, and the posterior
over topic assignments to sentences using an RNN. Our preliminary experiments
on two different datasets indicate early promise, but also expose many
challenges that remain to be addressed.
",Computer Science; Statistics,Computer Science; Statistics
"Transfer of magnetic order and anisotropy through epitaxial integration of 3$d$ and 4$f$ spin systems   Resonant x-ray scattering at the Dy $M_5$ and Ni $L_3$ absorption edges was
used to probe the temperature and magnetic field dependence of magnetic order
in epitaxial LaNiO$_3$-DyScO$_3$ superlattices. For superlattices with 2 unit
cell thick LaNiO$_3$ layers, a commensurate spiral state develops in the Ni
spin system below 100 K. Upon cooling below $T_{ind} = 18$ K, Dy-Ni exchange
interactions across the LaNiO$_3$-DyScO$_3$ interfaces induce collinear
magnetic order of interfacial Dy moments as well as a reorientation of the Ni
spins to a direction dictated by the strong magneto-crystalline anisotropy of
Dy. This transition is reversible by an external magnetic field of 3 T.
Tailored exchange interactions between rare-earth and transition-metal ions
thus open up new perspectives for the manipulation of spin structures in
metal-oxide heterostructures and devices.
",Physics,Physics
"Unbiased and Consistent Nested Sampling via Sequential Monte Carlo   We introduce a new class of sequential Monte Carlo methods called Nested
Sampling via Sequential Monte Carlo (NS-SMC), which reframes the Nested
Sampling method of Skilling (2006) in terms of sequential Monte Carlo
techniques. This new framework allows convergence results to be obtained in the
setting when Markov chain Monte Carlo (MCMC) is used to produce new samples. An
additional benefit is that marginal likelihood estimates are unbiased. In
contrast to NS, the analysis of NS-SMC does not require the (unrealistic)
assumption that the simulated samples be independent. As the original NS
algorithm is a special case of NS-SMC, this provides insights as to why NS
seems to produce accurate estimates despite a typical violation of its
assumptions. For applications of NS-SMC, we give advice on tuning MCMC kernels
in an automated manner via a preliminary pilot run, and present a new method
for appropriately choosing the number of MCMC repeats at each iteration.
Finally, a numerical study is conducted where the performance of NS-SMC and
temperature-annealed SMC is compared on several challenging and realistic
problems. MATLAB code for our experiments is made available at
this https URL .
",Statistics,Statistics
"Efficient Charge Collection in Coplanar Grid Radiation Detectors   We have modeled laser-induced transient current waveforms in radiation
coplanar grid detectors. Poisson's equation has been solved by finite element
method and currents induced by photo-generated charge were obtained using
Shockley-Ramo theorem. The spectral response on a radiation flux has been
modeled by Monte-Carlo simulations. We show 10$\times$ improved spectral
resolution of coplanar grid detector using differential signal sensing. We
model the current waveform dependence on doping, depletion width, diffusion and
detector shielding and their mutual dependence is discussed in terms of
detector optimization. The numerical simulations are successfully compared to
experimental data and further model simplifications are proposed. The space
charge below electrodes and a non-homogeneous electric field on a coplanar grid
anode are found to be the dominant contributions to laser-induced transient
current waveforms.
",Physics,Physics
"In-Silico Proportional-Integral Moment Control of Stochastic Reaction Networks with Applications to Gene Expression (with Dimerization)   The problem of controlling the mean and the variance of a species of interest
in a simple gene expression is addressed. It is shown that the protein mean
level can be globally and robustly tracked to any desired value using a simple
PI controller that satisfies certain sufficient conditions. Controlling both
the mean and variance however requires an additional control input, e.g. the
mRNA degradation rate, and local robust tracking of mean and variance is proved
to be achievable using multivariable PI control, provided that the reference
point satisfies necessary conditions imposed by the system. Even more
importantly, it is shown that there exist PI controllers that locally, robustly
and simultaneously stabilize all the equilibrium points inside the admissible
region. The results are then extended to the mean control of a gene expression
with protein dimerization. It is shown that the moment closure problem can be
circumvented without invoking any moment closure technique. Local stabilization
and convergence of the average dimer population to any desired reference value
is ensured using a pure integral control law. Explicit bounds on the controller
gain are provided and shown to be valid for any reference value. As a
byproduct, an explicit upper-bound of the variance of the monomer species,
acting on the system as unknown input due to the moment openness, is obtained.
The results are illustrated by simulation.
",Computer Science; Quantitative Biology,Quantitative Biology
"Discovery of water at high spectral resolution in the atmosphere of 51 Peg b   We report the detection of water absorption features in the dayside spectrum
of the first-known hot Jupiter, 51 Peg b, confirming the star-planet system to
be a double-lined spectroscopic binary. We used high-resolution (R~100,000),
3.2 micron spectra taken with CRIRES/VLT to trace the radial-velocity shift of
the water features in the planet's dayside atmosphere during 4 hours of its
4.23-day orbit after superior conjunction. We detect the signature of molecular
absorption by water at a significance of 5.6 sigma at a systemic velocity of
Vsys=-33+/-2 km/s, coincident with the host star, with a corresponding orbital
velocity Kp = 133^+4.3_-3.5 km/s. This translates directly to a planet mass of
Mp=0.476^+0.032_-0.031MJ, placing it at the transition boundary between Jovian
and Neptunian worlds. We determine upper and lower limits on the orbital
inclination of the system of 70<i (deg)<82.2. We also provide an updated
orbital solution for 51 Peg b, using an extensive set of 639 stellar radial
velocities measured between 1994 and 2013, finding no significant evidence of
an eccentric orbit. We find no evidence of significant absorption or emission
from other major carbon-bearing molecules of the planet, including methane and
carbon dioxide. The atmosphere is non-inverted in the temperature-pressure
region probed by these observations. The deepest absorption lines reach an
observed relative contrast of 0.9x10^-3 with respect to the host star continuum
flux, at an angular separation of 3 milliarcseconds. This work is consistent
with a previous tentative report of K-band molecular absorption for 51 Peg b by
Brogi et al. (2013).
",Physics,Physics
"Frequentist coverage and sup-norm convergence rate in Gaussian process regression   Gaussian process (GP) regression is a powerful interpolation technique due to
its flexibility in capturing non-linearity. In this paper, we provide a general
framework for understanding the frequentist coverage of point-wise and
simultaneous Bayesian credible sets in GP regression. As an intermediate
result, we develop a Bernstein von-Mises type result under supremum norm in
random design GP regression. Identifying both the mean and covariance function
of the posterior distribution of the Gaussian process as regularized
$M$-estimators, we show that the sampling distribution of the posterior mean
function and the centered posterior distribution can be respectively
approximated by two population level GPs. By developing a comparison inequality
between two GPs, we provide exact characterization of frequentist coverage
probabilities of Bayesian point-wise credible intervals and simultaneous
credible bands of the regression function. Our results show that inference
based on GP regression tends to be conservative; when the prior is
under-smoothed, the resulting credible intervals and bands have minimax-optimal
sizes, with their frequentist coverage converging to a non-degenerate value
between their nominal level and one. As a byproduct of our theory, we show that
the GP regression also yields minimax-optimal posterior contraction rate
relative to the supremum norm, which provides a positive evidence to the long
standing problem on optimal supremum norm contraction rate in GP regression.
",Mathematics; Statistics,Mathematics; Statistics
"BFGS convergence to nonsmooth minimizers of convex functions   The popular BFGS quasi-Newton minimization algorithm under reasonable
conditions converges globally on smooth convex functions. This result was
proved by Powell in 1976: we consider its implications for functions that are
not smooth. In particular, an analogous convergence result holds for functions,
like the Euclidean norm, that are nonsmooth at the minimizer.
",Mathematics,Mathematics
"ac properties of short Josephson weak links   The admittance of two types of Josephson weak links is calculated, i.e., of a
one-dimensional superconducting wire with a local suppression of the order
parameter, and the second is a short S-c-S structure, where S denotes a
superconductor and c---a constriction. The systems of the first type are
analyzed on the basis of time-dependent Ginzburg-Landau equations. We show that
the impedance $Z(\Omega)$ has a maximum as a function of the frequency
$\Omega$, and the electric field $E_{\Omega}$ is determined by two
gauge-invariant quantities---the condensate momentum $Q_{\Omega}$ and the
potential $\mu$ related to charge imbalance. The structures of the second type
are studied on the basis of microscopic equations for quasiclassical Green's
functions in the Keldysh technique. For short S-c-S contacts (the Thouless
energy ${E_{\text{Th}} = D/L^{2} \gg \Delta}$) we present a formula for
admittance $Y$ valid at frequencies $\Omega$ and temperatures $T$ less than the
Thouless energy but arbitrary with respect to the energy gap $\Delta$. It is
shown that, at low temperatures, the absorption is absent [${\mathrm{Re}(Y) =
0}$] if the frequency does not exceed the energy gap in the center of the
constriction (${\Omega < \Delta \cos \varphi_{0}}$, where $2 \varphi_{0}$ is
the phase difference between the S reservoirs). The absorption gradually
increases with increasing the difference ${(\Omega - \Delta \cos \varphi_{0})}$
if $2 \varphi_{0}$ is less than the phase difference $2 \varphi_{\text{c}}$
corresponding to the critical Josephson current. In the interval ${2
\varphi_{\text{c}} < 2 \varphi_{0} < \pi}$, the absorption has a maximum. This
interval of the phase difference is achievable in phase-biased Josephson
junctions. Close to $T_{\text{c}}$ the admittance has a maximum at low $\Omega$
which is described by an analytical formula.
",Physics,Physics
"Information Elicitation for Bayesian Auctions   In this paper we design information elicitation mechanisms for Bayesian
auctions. While in Bayesian mechanism design the distributions of the players'
private types are often assumed to be common knowledge, information elicitation
considers the situation where the players know the distributions better than
the decision maker. To weaken the information assumption in Bayesian auctions,
we consider an information structure where the knowledge about the
distributions is arbitrarily scattered among the players. In such an
unstructured information setting, we design mechanisms for unit-demand auctions
and additive auctions that aggregate the players' knowledge, generating revenue
that are constant approximations to the optimal Bayesian mechanisms with a
common prior. Our mechanisms are 2-step dominant-strategy truthful and the
revenue increases gracefully with the amount of knowledge the players
collectively have.
",Computer Science,Computer Science
"A short note on Godbersen's Conjecture   In this short note we improve the best to date bound in Godbersen's
conjecture, and show some implications for unbalanced difference bodies.
",Mathematics,Mathematics
"Third-Person Imitation Learning   Reinforcement learning (RL) makes it possible to train agents capable of
achiev- ing sophisticated goals in complex and uncertain environments. A key
difficulty in reinforcement learning is specifying a reward function for the
agent to optimize. Traditionally, imitation learning in RL has been used to
overcome this problem. Unfortunately, hitherto imitation learning methods tend
to require that demonstra- tions are supplied in the first-person: the agent is
provided with a sequence of states and a specification of the actions that it
should have taken. While powerful, this kind of imitation learning is limited
by the relatively hard problem of collect- ing first-person demonstrations.
Humans address this problem by learning from third-person demonstrations: they
observe other humans perform tasks, infer the task, and accomplish the same
task themselves.
In this paper, we present a method for unsupervised third-person imitation
learn- ing. Here third-person refers to training an agent to correctly achieve
a simple goal in a simple environment when it is provided a demonstration of a
teacher achieving the same goal but from a different viewpoint; and
unsupervised refers to the fact that the agent receives only these third-person
demonstrations, and is not provided a correspondence between teacher states and
student states. Our methods primary insight is that recent advances from domain
confusion can be utilized to yield domain agnostic features which are crucial
during the training process. To validate our approach, we report successful
experiments on learning from third-person demonstrations in a pointmass domain,
a reacher domain, and inverted pendulum.
",Computer Science,Computer Science; Statistics
"Chiral Topological Superconductors Enhanced by Long-Range Interactions   We study the phase diagram and edge states of a two-dimensional p-wave
superconductor with long-range hopping and pairing amplitudes. New topological
phases and quasiparticles different from the usual short-range model are
obtained. When both hopping and pairing terms decay with the same exponent, one
of the topological chiral phases with propagating Majorana edge states gets
significantly enhanced by long-range couplings. On the other hand, when the
long-range pairing amplitude decays more slowly than the hopping, we discover
new topological phases where propagating Majorana fermions at each edge pair
nonlocally and become gapped even in the thermodynamic limit. Remarkably, these
nonlocal edge states are still robust, remain separated from the bulk, and are
localized at both edges at the same time. The inclusion of long-range effects
is potentially applicable to recent experiments with magnetic impurities and
islands in 2D superconductors.
",Physics,Physics
"MACS J0416.1-2403: Impact of line-of-sight structures on strong gravitational lensing modelling of galaxy clusters   Exploiting the powerful tool of strong gravitational lensing by galaxy
clusters to study the highest-redshift Universe and cluster mass distributions
relies on precise lens mass modelling. In this work, we present the first
attempt at modelling line-of-sight mass distribution in addition to that of the
cluster, extending previous modelling techniques that assume mass distributions
to be on a single lens plane. We focus on the Hubble Frontier Field cluster
MACS J0416.1-2403, and our multi-plane model reproduces the observed image
positions with a rms offset of ~0.53"". Starting from this best-fitting model,
we simulate a mock cluster that resembles MACS J0416.1-2403 in order to explore
the effects of line-of-sight structures on cluster mass modelling. By
systematically analysing the mock cluster under different model assumptions, we
find that neglecting the lensing environment has a significant impact on the
reconstruction of image positions (rms ~0.3""); accounting for line-of-sight
galaxies as if they were at the cluster redshift can partially reduce this
offset. Moreover, foreground galaxies are more important to include into the
model than the background ones. While the magnification factors of the lensed
multiple images are recovered within ~10% for ~95% of them, those ~5% that lie
near critical curves can be significantly affected by the exclusion of the
lensing environment in the models (up to a factor of ~200). In addition,
line-of-sight galaxies cannot explain the apparent discrepancy in the
properties of massive subhalos between MACS J0416.1-2403 and N-body simulated
clusters. Since our model of MACS J0416.1-2403 with line-of-sight galaxies only
reduced modestly the rms offset in the image positions, we conclude that
additional complexities, such as more flexible halo shapes, would be needed in
future models of MACS J0416.1-2403.
",Physics,Physics
"A note on integrating products of linear forms over the unit simplex   Integrating a product of linear forms over the unit simplex can be done in
polynomial time if the number of variables n is fixed (V. Baldoni et al.,
2011). In this note, we highlight that this problem is equivalent to obtaining
the normalizing constant of state probabilities for a popular class of Markov
processes used in queueing network theory. In light of this equivalence, we
survey existing computational algorithms developed in queueing theory that can
be used for exact integration. For example, under some regularity conditions,
queueing theory algorithms can exactly integrate a product of linear forms of
total degree N by solving N systems of linear equations.
",Computer Science; Mathematics,Computer Science; Mathematics
"Expansion of percolation critical points for Hamming graphs   The Hamming graph $H(d,n)$ is the Cartesian product of $d$ complete graphs on
$n$ vertices. Let $m=d(n-1)$ be the degree and $V = n^d$ be the number of
vertices of $H(d,n)$. Let $p_c^{(d)}$ be the critical point for bond
percolation on $H(d,n)$. We show that, for $d \in \mathbb N$ fixed and $n \to
\infty$,
\begin{equation*}
p_c^{(d)}= \dfrac{1}{m} + \dfrac{2d^2-1}{2(d-1)^2}\dfrac{1}{m^2}
+ O(m^{-3}) + O(m^{-1}V^{-1/3}),
\end{equation*} which extends the asymptotics found in
\cite{BorChaHofSlaSpe05b} by one order. The term $O(m^{-1}V^{-1/3})$ is the
width of the critical window. For $d=4,5,6$ we have $m^{-3} =
O(m^{-1}V^{-1/3})$, and so the above formula represents the full asymptotic
expansion of $p_c^{(d)}$. In \cite{FedHofHolHul16a} \st{we show that} this
formula is a crucial ingredient in the study of critical bond percolation on
$H(d,n)$ for $d=2,3,4$. The proof uses a lace expansion for the upper bound and
a novel comparison with a branching random walk for the lower bound. The proof
of the lower bound also yields a refined asymptotics for the susceptibility of
a subcritical Erdős-Rényi random graph.
",Mathematics,Mathematics
"On a cross-diffusion system arising in image denosing   We study a generalization of a cross-diffusion problem deduced from a
nonlinear complex-variable diffusion model for signal and image denoising.
We prove the existence of weak solutions of the time-independent problem with
fidelity terms under mild conditions on the data problem. Then, we show that
this translates on the well-posedness of a quasi-steady state approximation of
the evolution problem, and also prove the existence of weak solutions of the
latter under more restrictive hypothesis.
We finally perform some numerical simulations for image denoising, comparing
the performance of the cross-diffusion model and its corresponding scalar
Perona-Malik equation.
",Mathematics,Mathematics; Statistics
"The GTC exoplanet transit spectroscopy survey. VII. Detection of sodium in WASP-52b's cloudy atmosphere   We report the first detection of sodium absorption in the atmosphere of the
hot Jupiter WASP-52b. We observed one transit of WASP-52b with the
low-resolution Optical System for Imaging and low-Intermediate-Resolution
Integrated Spectroscopy (OSIRIS) at the 10.4 m Gran Telescopio Canarias (GTC).
The resulting transmission spectrum, covering the wavelength range from 522 nm
to 903 nm, is flat and featureless, except for the significant narrow
absorption signature at the sodium doublet, which can be explained by an
atmosphere in solar composition with clouds at 1 mbar. A cloud-free atmosphere
is stringently ruled out. By assessing the absorption depths of sodium in
various bin widths, we find that temperature increases towards lower
atmospheric pressure levels, with a positive temperature gradient of 0.88 +/-
0.65 K/km, possibly indicative of upper atmospheric heating and a temperature
inversion.
",Physics,Physics
"Threshold Selection for Multivariate Heavy-Tailed Data   Regular variation is often used as the starting point for modeling
multivariate heavy-tailed data. A random vector is regularly varying if and
only if its radial part $R$ is regularly varying and is asymptotically
independent of the angular part $\Theta$ as $R$ goes to infinity. The
conditional limiting distribution of $\Theta$ given $R$ is large characterizes
the tail dependence of the random vector and hence its estimation is the
primary goal of applications. A typical strategy is to look at the angular
components of the data for which the radial parts exceed some threshold. While
a large class of methods has been proposed to model the angular distribution
from these exceedances, the choice of threshold has been scarcely discussed in
the literature. In this paper, we describe a procedure for choosing the
threshold by formally testing the independence of $R$ and $\Theta$ using a
measure of dependence called distance covariance. We generalize the limit
theorem for distance covariance to our unique setting and propose an algorithm
which selects the threshold for $R$. This algorithm incorporates a subsampling
scheme that is also applicable to weakly dependent data. Moreover, it avoids
the heavy computation in the calculation of the distance covariance, a typical
limitation for this measure. The performance of our method is illustrated on
both simulated and real data.
",Mathematics; Statistics,Statistics
"Entombed: An archaeological examination of an Atari 2600 game   The act and experience of programming is, at its heart, a fundamentally human
activity that results in the production of artifacts. When considering
programming, therefore, it would be a glaring omission to not involve people
who specialize in studying artifacts and the human activity that yields them:
archaeologists. Here we consider this with respect to computer games, the focus
of archaeology's nascent subarea of archaeogaming.
One type of archaeogaming research is digital excavation, a technical
examination of the code and techniques used in old games' implementation. We
apply that in a case study of Entombed, an Atari 2600 game released in 1982 by
US Games. The player in this game is, appropriately, an archaeologist who must
make their way through a zombie-infested maze. Maze generation is a fruitful
area for comparative retrogame archaeology, because a number of early games on
different platforms featured mazes, and their variety of approaches can be
compared. The maze in Entombed is particularly interesting: it is shaped in
part by the extensive real-time constraints of the Atari 2600 platform, and
also had to be generated efficiently and use next to no memory. We reverse
engineered key areas of the game's code to uncover its unusual maze-generation
algorithm, which we have also built a reconstruction of, and analyzed the
mysterious table that drives it. In addition, we discovered what appears to be
a 35-year-old bug in the code, as well as direct evidence of code-reuse
practices amongst game developers.
What further makes this game's development interesting is that, in an era
where video games were typically solo projects, a total of five people were
involved in various ways with Entombed. We piece together some of the backstory
of the game's development and intoxicant-fueled design using interviews to
complement our technical work.
Finally, we contextualize this example in archaeology and lay the groundwork
for a broader interdisciplinary discussion about programming, one that includes
both computer scientists and archaeologists.
",Computer Science,Computer Science
"Stability and elasticity of metastable solid solutions and superlattices in the MoN-TaN system: a first-principles study   Employing ab initio calculations, we discuss chemical, mechanical, and
dynamical stability of MoN-TaN solid solutions together with cubic-like MoN/TaN
superlattices, as another materials design concept. Hexagonal-type structures
based on low-energy modifications of MoN and TaN are the most stable ones over
the whole composition range. Despite being metastable, disordered cubic
polymorphs are energetically significantly preferred over their ordered
counterparts. An in-depth analysis of atomic environments in terms of bond
lengths and angles reveals that the chemical disorder results in (partially)
broken symmetry, i.e., the disordered cubic structure relaxes towards a
hexagonal NiAs-type phase, the ground state of MoN. Surprisingly, also the
superlattice architecture is clearly favored over the ordered cubic solid
solution. We show that the bi-axial coherency stresses in superlattices break
the cubic symmetry beyond simple tetragonal distortions and lead to a new
tetragonal $\zeta$-phase (space group P4/nmm), which exhibits a more negative
formation energy than the symmetry-stabilized cubic structures of MoN and TaN.
Unlike cubic TaN, the $\zeta\text{-TaN}$ is elastically and vibrationally
stable, while $\zeta$-MoN is stabilized only by the superlattice structure. To
map compositional trends in elasticity, we establish mechanical stability of
various Mo$_{1-x}$Ta$_x$N systems and find the closest high-symmetry
approximants of the corresponding elastic tensors. According to the estimated
polycrystalline moduli, the hexagonal polymorphs are predicted to be extremely
hard, however, less ductile than the cubic phases and superlattices. The trends
in stability based on energetics and elasticity are corroborated by density of
electronic states.
",Physics,Physics
"A Generalization of Convolutional Neural Networks to Graph-Structured Data   This paper introduces a generalization of Convolutional Neural Networks
(CNNs) from low-dimensional grid data, such as images, to graph-structured
data. We propose a novel spatial convolution utilizing a random walk to uncover
the relations within the input, analogous to the way the standard convolution
uses the spatial neighborhood of a pixel on the grid. The convolution has an
intuitive interpretation, is efficient and scalable and can also be used on
data with varying graph structure. Furthermore, this generalization can be
applied to many standard regression or classification problems, by learning the
the underlying graph. We empirically demonstrate the performance of the
proposed CNN on MNIST, and challenge the state-of-the-art on Merck molecular
activity data set.
",Computer Science; Statistics,Computer Science; Statistics
"Learning Combinatorial Optimization Algorithms over Graphs   The design of good heuristics or approximation algorithms for NP-hard
combinatorial optimization problems often requires significant specialized
knowledge and trial-and-error. Can we automate this challenging, tedious
process, and learn the algorithms instead? In many real-world applications, it
is typically the case that the same optimization problem is solved again and
again on a regular basis, maintaining the same problem structure but differing
in the data. This provides an opportunity for learning heuristic algorithms
that exploit the structure of such recurring problems. In this paper, we
propose a unique combination of reinforcement learning and graph embedding to
address this challenge. The learned greedy policy behaves like a meta-algorithm
that incrementally constructs a solution, and the action is determined by the
output of a graph embedding network capturing the current state of the
solution. We show that our framework can be applied to a diverse range of
optimization problems over graphs, and learns effective algorithms for the
Minimum Vertex Cover, Maximum Cut and Traveling Salesman problems.
",Computer Science; Statistics,Computer Science; Statistics
"Bayesian Sparsification of Recurrent Neural Networks   Recurrent neural networks show state-of-the-art results in many text analysis
tasks but often require a lot of memory to store their weights. Recently
proposed Sparse Variational Dropout eliminates the majority of the weights in a
feed-forward neural network without significant loss of quality. We apply this
technique to sparsify recurrent neural networks. To account for recurrent
specifics we also rely on Binary Variational Dropout for RNN. We report 99.5%
sparsity level on sentiment analysis task without a quality drop and up to 87%
sparsity level on language modeling task with slight loss of accuracy.
",Computer Science; Statistics,Computer Science; Statistics
"Characteristics of stratified flows of Newtonian/non-Newtonian shear-thinning fluids   Exact solutions for laminar stratified flows of Newtonian/non-Newtonian
shear-thinning fluids in horizontal and inclined channels are presented. An
iterative algorithm is proposed to compute the laminar solution for the general
case of a Carreau non-Newtonian fluid. The exact solution is used to study the
effect of the rheology of the shear-thinning liquid on two-phase flow
characteristics considering both gas/liquid and liquid/liquid systems.
Concurrent and counter-current inclined systems are investigated, including the
mapping of multiple solution boundaries. Aspects relevant to practical
applications are discussed, such as the insitu hold-up, or lubrication effects
achieved by adding a less viscous phase. A characteristic of this family of
systems is that, even if the liquid has a complex rheology (Carreau fluid), the
two-phase stratified flow can behave like the liquid is Newtonian for a wide
range of operational conditions. The capability of the two-fluid model to yield
satisfactory predictions in the presence of shear-thinning liquids is tested,
and an algorithm is proposed to a priori predict if the Newtonian (zero shear
rate viscosity) behaviour arises for a given operational conditions in order to
avoid large errors in the predictions of flow characteristics when the
power-law is considered for modelling the shear-thinning behaviour. Two-fluid
model closures implied by the exact solution and the effect of a turbulent gas
layer are also addressed.
",Physics,Physics
"Interacting Fields and Flows: Magnetic Hot Jupiters   We present Magnetohydrodynamic (MHD) simulations of the magnetic interactions
between a solar type star and short period hot Jupiter exoplanets, using the
publicly available MHD code PLUTO. It has been predicted that emission due to
magnetic interactions such as the electron cyclotron maser instability (ECMI)
will be observable. In our simulations, a planetary outflow, due to UV
evaporation of the exoplanets atmosphere, results in the build-up of
circumplanetary material. We predict the ECMI emission and determine that the
emission is prevented from escaping from the system. This is due to the
evaporated material leading to a high plasma frequency in the vicinity of the
planet, which inhibits the ECMI process.
",Physics,Physics
"F-index of graphs based on four operations related to the lexicographic product   The forgotten topological index or F-index of a graph is defined as the sum
of cubes of the degree of all the vertices of the graph. In this paper we study
the F-index of four operations related to the lexicographic product on graphs
which were introduced by Sarala et al. [D. Sarala, H. Deng, S.K. Ayyaswamya and
S. Balachandrana, The Zagreb indices of graphs based on four new operations
related to the lexicographic product, \textit{Applied Mathematics and
Computation}, 309 (2017) 156--169.].
",Computer Science,Mathematics
"Batched High-dimensional Bayesian Optimization via Structural Kernel Learning   Optimization of high-dimensional black-box functions is an extremely
challenging problem. While Bayesian optimization has emerged as a popular
approach for optimizing black-box functions, its applicability has been limited
to low-dimensional problems due to its computational and statistical challenges
arising from high-dimensional settings. In this paper, we propose to tackle
these challenges by (1) assuming a latent additive structure in the function
and inferring it properly for more efficient and effective BO, and (2)
performing multiple evaluations in parallel to reduce the number of iterations
required by the method. Our novel approach learns the latent structure with
Gibbs sampling and constructs batched queries using determinantal point
processes. Experimental validations on both synthetic and real-world functions
demonstrate that the proposed method outperforms the existing state-of-the-art
approaches.
",Computer Science; Mathematics; Statistics,Statistics
"On Reliability-Aware Server Consolidation in Cloud Datacenters   In the past few years, datacenter (DC) energy consumption has become an
important issue in technology world. Server consolidation using virtualization
and virtual machine (VM) live migration allows cloud DCs to improve resource
utilization and hence energy efficiency. In order to save energy, consolidation
techniques try to turn off the idle servers, while because of workload
fluctuations, these offline servers should be turned on to support the
increased resource demands. These repeated on-off cycles could affect the
hardware reliability and wear-and-tear of servers and as a result, increase the
maintenance and replacement costs. In this paper we propose a holistic
mathematical model for reliability-aware server consolidation with the
objective of minimizing total DC costs including energy and reliability costs.
In fact, we try to minimize the number of active PMs and racks, in a
reliability-aware manner. We formulate the problem as a Mixed Integer Linear
Programming (MILP) model which is in form of NP-complete. Finally, we evaluate
the performance of our approach in different scenarios using extensive
numerical MATLAB simulations.
",Computer Science,Computer Science
"Parametrizations, weights, and optimal prediction: Part 1   We consider the problem of the annual mean temperature prediction. The years
taken into account and the corresponding annual mean temperatures are denoted
by $0,\ldots, n$ and $t_0$, $\ldots$, $t_n$, respectively. We propose to
predict the temperature $t_{n+1}$ using the data $t_0$, $\ldots$, $t_n$. For
each $0\leq l\leq n$ and each parametrization $\Theta^{(l)}$ of the Euclidean
space $\mathbb{R}^{l+1}$ we construct a list of weights for the data
$\{t_0,\ldots, t_l\}$ based on the rows of $\Theta^{(l)}$ which are correlated
with the constant trend. Using these weights we define a list of predictors of
$t_{l+1}$ from the data $t_0$, $\ldots$, $t_l$. We analyse how the
parametrization affects the prediction, and provide three optimality criteria
for the selection of weights and parametrization. We illustrate our results for
the annual mean temperature of France and Morocco.
",Statistics,Mathematics; Statistics
"Non-Generic Unramified Representations in Metaplectic Covering Groups   Let $G^{(r)}$ denote the metaplectic covering group of the linear algebraic
group $G$. In this paper we study conditions on unramified representations of
the group $G^{(r)}$ not to have a nonzero Whittaker function. We state a
general Conjecture about the possible unramified characters $\chi$ such that
the unramified sub-representation of
$Ind_{B^{(r)}}^{G^{(r)}}\chi\delta_B^{1/2}$ will have no nonzero Whittaker
function. We prove this Conjecture for the groups $GL_n^{(r)}$ with $r\ge n-1$,
and for the exceptional groups $G_2^{(r)}$ when $r\ne 2$.
",Mathematics,Mathematics
"Atomic Data Revisions for Transitions Relevant to Observations of Interstellar, Circumgalactic, and Intergalactic Matter   Measurements of element abundances in galaxies from astrophysical
spectroscopy depend sensitively on the atomic data used. With the goal of
making the latest atomic data accessible to the community, we present a
compilation of selected atomic data for resonant absorption lines at
wavelengths longward of 911.753 {\AA} (the \ion{H}{1} Lyman limit), for key
heavy elements (heavier than atomic number 5) of astrophysical interest. In
particular, we focus on the transitions of those ions that have been observed
in the Milky Way interstellar medium (ISM), the circumgalactic medium (CGM) of
the Milky Way and/or other galaxies, and the intergalactic medium (IGM).
We provide wavelengths, oscillator strengths, associated accuracy grades, and
references to the oscillator strength determinations. We also attempt to
compare and assess the recent oscillator strength determinations. For about
22\% of the lines that have updated oscillator strength values, the differences
between the former values and the updated ones are $\gtrsim$~0.1 dex.
Our compilation will be a useful resource for absorption line studies of the
ISM, as well as studies of the CGM and IGM traced by sight lines to quasars and
gamma-ray bursts. Studies (including those enabled by future generations of
extremely large telescopes) of absorption by galaxies against the light of
background galaxies will also benefit from our compilation.
",Physics,Physics
"Bernoulli-Carlitz and Cauchy-Carlitz numbers with Stirling-Carlitz numbers   Recently, the Cauchy-Carlitz number was defined as the counterpart of the
Bernoulli-Carlitz number. Both numbers can be expressed explicitly in terms of
so-called Stirling-Carlitz numbers. In this paper, we study the second analogue
of Stirling-Carlitz numbers and give some general formulae, including Bernoulli
and Cauchy numbers in formal power series with complex coefficients, and
Bernoulli-Carlitz and Cauchy-Carlitz numbers in function fields. We also give
some applications of Hasse-Teichmüller derivative to hypergeometric Bernoulli
and Cauchy numbers in terms of associated Stirling numbers.
",Mathematics,Mathematics
"On (in)stabilities of perturbations in mimetic models with higher derivatives   Usually when applying the mimetic model to the early universe, higher
derivative terms are needed to promote the mimetic field to be dynamical.
However such models suffer from the ghost and/or the gradient instabilities and
simple extensions cannot cure this pathology. We point out in this paper that
it is possible to overcome this difficulty by considering the direct couplings
of the higher derivatives of the mimetic field to the curvature of the
spacetime.
",Physics,Physics
"From optimal transport to generative modeling: the VEGAN cookbook   We study unsupervised generative modeling in terms of the optimal transport
(OT) problem between true (but unknown) data distribution $P_X$ and the latent
variable model distribution $P_G$. We show that the OT problem can be
equivalently written in terms of probabilistic encoders, which are constrained
to match the posterior and prior distributions over the latent space. When
relaxed, this constrained optimization problem leads to a penalized optimal
transport (POT) objective, which can be efficiently minimized using stochastic
gradient descent by sampling from $P_X$ and $P_G$. We show that POT for the
2-Wasserstein distance coincides with the objective heuristically employed in
adversarial auto-encoders (AAE) (Makhzani et al., 2016), which provides the
first theoretical justification for AAEs known to the authors. We also compare
POT to other popular techniques like variational auto-encoders (VAE) (Kingma
and Welling, 2014). Our theoretical results include (a) a better understanding
of the commonly observed blurriness of images generated by VAEs, and (b)
establishing duality between Wasserstein GAN (Arjovsky and Bottou, 2017) and
POT for the 1-Wasserstein distance.
",Statistics,Statistics
"Adaptive Noise Cancellation Using Deep Cerebellar Model Articulation Controller   This paper proposes a deep cerebellar model articulation controller (DCMAC)
for adaptive noise cancellation (ANC). We expand upon the conventional CMAC by
stacking sin-gle-layer CMAC models into multiple layers to form a DCMAC model
and derive a modified backpropagation training algorithm to learn the DCMAC
parameters. Com-pared with conventional CMAC, the DCMAC can characterize
nonlinear transformations more effectively because of its deep structure.
Experimental results confirm that the pro-posed DCMAC model outperforms the
CMAC in terms of residual noise in an ANC task, showing that DCMAC provides
enhanced modeling capability based on channel characteristics.
",Computer Science,Computer Science
"Bound states of the two-dimensional Dirac equation for an energy-dependent hyperbolic Scarf potential   We study the two-dimensional massless Dirac equation for a potential that is
allowed to depend on the energy and on one of the spatial variables. After
determining a modified orthogonality relation and norm for such systems, we
present an application involving an energy-dependent version of the hyperbolic
Scarf potential. We construct closed-form bound state solutions of the
associated Dirac equation.
",Physics,Mathematics
"First measeurements in search for keV-sterile neutrino in tritium beta-decay by Troitsk nu-mass experiment   We present the first measurements of tritium beta-decay spectrum in the
electron energy range 16-18.6 keV. The goal is to find distortions which may
correspond to the presence of a heavy sterile neutrinos. A possible
contribution of this kind would manifest itself as a kink in the spectrum with
a similar shape but with end point shifted by the value of a heavy neutrino
mass. We set a new upper limits to the neutrino mixing matrix element U^2_{e4}
which improve existing limits by a factor from 2 to 5 in the mass range 0.1-2
keV.
",Physics,Physics
"Accelerated Consensus via Min-Sum Splitting   We apply the Min-Sum message-passing protocol to solve the consensus problem
in distributed optimization. We show that while the ordinary Min-Sum algorithm
does not converge, a modified version of it known as Splitting yields
convergence to the problem solution. We prove that a proper choice of the
tuning parameters allows Min-Sum Splitting to yield subdiffusive accelerated
convergence rates, matching the rates obtained by shift-register methods. The
acceleration scheme embodied by Min-Sum Splitting for the consensus problem
bears similarities with lifted Markov chains techniques and with multi-step
first order methods in convex optimization.
",Mathematics,Computer Science; Mathematics
"A Hand-Held Multimedia Translation and Interpretation System with Application to Diet Management   We propose a network independent, hand-held system to translate and
disambiguate foreign restaurant menu items in real-time. The system is based on
the use of a portable multimedia device, such as a smartphones or a PDA. An
accurate and fast translation is obtained using a Machine Translation engine
and a context-specific corpora to which we apply two pre-processing steps,
called translation standardization and $n$-gram consolidation. The phrase-table
generated is orders of magnitude lighter than the ones commonly used in market
applications, thus making translations computationally less expensive, and
decreasing the battery usage. Translation ambiguities are mitigated using
multimedia information including images of dishes and ingredients, along with
ingredient lists. We implemented a prototype of our system on an iPod Touch
Second Generation for English speakers traveling in Spain. Our tests indicate
that our translation method yields higher accuracy than translation engines
such as Google Translate, and does so almost instantaneously. The memory
requirements of the application, including the database of images, are also
well within the limits of the device. By combining it with a database of
nutritional information, our proposed system can be used to help individuals
who follow a medical diet maintain this diet while traveling.
",Statistics,Computer Science
"Asymptotics of Hankel determinants with a one-cut regular potential and Fisher-Hartwig singularities   We obtain asymptotics of large Hankel determinants whose weight depends on a
one-cut regular potential and any number of Fisher-Hartwig singularities. This
generalises two results: 1) a result of Berestycki, Webb and Wong [5] for
root-type singularities, and 2) a result of Its and Krasovsky [37] for a
Gaussian weight with a single jump-type singularity. We show that when we apply
a piecewise constant thinning on the eigenvalues of a random Hermitian matrix
drawn from a one-cut regular ensemble, the gap probability in the thinned
spectrum, as well as correlations of the characteristic polynomial of the
associated conditional point process, can be expressed in terms of these
determinants.
",Mathematics,Mathematics
"Nonparametric Poisson regression from independent and weakly dependent observations by model selection   We consider the non-parametric Poisson regression problem where the integer
valued response $Y$ is the realization of a Poisson random variable with
parameter $\lambda(X)$. The aim is to estimate the functional parameter
$\lambda$ from independent or weakly dependent observations
$(X_1,Y_1),\ldots,(X_n,Y_n)$ in a random design framework.
First we determine upper risk bounds for projection estimators on finite
dimensional subspaces under mild conditions. In the case of Sobolev ellipsoids
the obtained rates of convergence turn out to be optimal.
The main part of the paper is devoted to the construction of adaptive
projection estimators of $\lambda$ via model selection. We proceed in two
steps: first, we assume that an upper bound for $\Vert \lambda \Vert_\infty$ is
known. Under this assumption, we construct an adaptive estimator whose
dimension parameter is defined as the minimizer of a penalized contrast
criterion. Second, we replace the known upper bound on $\Vert \lambda
\Vert_\infty$ by an appropriate plug-in estimator of $\Vert \lambda
\Vert_\infty$. The resulting adaptive estimator is shown to attain the minimax
optimal rate up to an additional logarithmic factor both in the independent and
the weakly dependent setup. Appropriate concentration inequalities for Poisson
point processes turn out to be an important ingredient of the proofs.
We illustrate our theoretical findings by a short simulation study and
conclude by indicating directions of future research.
",Mathematics; Statistics,Mathematics; Statistics
"Stochastic Multi-objective Optimization on a Budget: Application to multi-pass wire drawing with quantified uncertainties   Design optimization of engineering systems with multiple competing objectives
is a painstakingly tedious process especially when the objective functions are
expensive-to-evaluate computer codes with parametric uncertainties. The
effectiveness of the state-of-the-art techniques is greatly diminished because
they require a large number of objective evaluations, which makes them
impractical for problems of the above kind. Bayesian global optimization (BGO),
has managed to deal with these challenges in solving single-objective
optimization problems and has recently been extended to multi-objective
optimization (MOO). BGO models the objectives via probabilistic surrogates and
uses the epistemic uncertainty to define an information acquisition function
(IAF) that quantifies the merit of evaluating the objective at new designs.
This iterative data acquisition process continues until a stopping criterion is
met. The most commonly used IAF for MOO is the expected improvement over the
dominated hypervolume (EIHV) which in its original form is unable to deal with
parametric uncertainties or measurement noise. In this work, we provide a
systematic reformulation of EIHV to deal with stochastic MOO problems. The
primary contribution of this paper lies in being able to filter out the noise
and reformulate the EIHV without having to observe or estimate the stochastic
parameters. An addendum of the probabilistic nature of our methodology is that
it enables us to characterize our confidence about the predicted Pareto front.
We verify and validate the proposed methodology by applying it to synthetic
test problems with known solutions. We demonstrate our approach on an
industrial problem of die pass design for a steel wire drawing process.
",Mathematics,Computer Science
"Caching Meets Millimeter Wave Communications for Enhanced Mobility Management in 5G Networks   One of the most promising approaches to overcome the uncertainty and dynamic
channel variations of millimeter wave (mmW) communications is to deploy
dual-mode base stations that integrate both mmW and microwave ($\mu$W)
frequencies. If properly designed, such dual-mode base stations can enhance
mobility and handover in highly mobile wireless environments. In this paper, a
novel approach for analyzing and managing mobility in joint $\mu$W-mmW networks
is proposed. The proposed approach leverages device-level caching along with
the capabilities of dual-mode base stations to minimize handover failures,
reduce inter-frequency measurement energy consumption, and provide seamless
mobility in emerging dense heterogeneous networks. First, fundamental results
on the caching capabilities, including caching probability and cache duration
are derived for the proposed dual-mode network scenario. Second, the average
achievable rate of caching is derived for mobile users. Third, the proposed
cache-enabled mobility management problem is formulated as a dynamic matching
game between mobile user equipments (MUEs) and small base stations (SBSs). The
goal of this game is to find a distributed handover mechanism that subject to
the network constraints on HOFs and limited cache sizes, allows each MUE to
choose between executing an HO to a target SBS, being connected to the
macrocell base station (MBS), or perform a transparent HO by using the cached
content. The formulated matching game allows capturing the dynamics of the
mobility management problem caused by HOFs. To solve this dynamic matching
problem, a novel algorithm is proposed and its convergence to a two-sided
dynamically stable HO policy is proved. Numerical results corroborate the
analytical derivations and show that the proposed solution will provides
significant reductions in both the HOF and energy consumption by MUEs.
",Computer Science,Computer Science
"Suppression of material transfer at contacting surfaces: The effect of adsorbates on Al/TiN and Cu/diamond interfaces from first-principles calculations   The effect of monolayers of oxygen (O) and hydrogen (H) on the possibility of
material transfer at aluminium/titanium nitride (Al/TiN) and copper/diamond
(Cu/C$_{\text{dia}}$) interfaces, respectively, were investigated within the
framework of density functional theory (DFT). To this end the approach,
contact, and subsequent separation of two atomically flat surfaces consisting
of the aforementioned pairs of materials were simulated. These calculations
were performed for the clean as well as oxygenated and hydrogenated Al and
C$_{\text{dia}}$ surfaces, respectively. Various contact configurations were
considered by studying several lateral arrangements of the involved surfaces at
the interface. Material transfer is typically possible at interfaces between
the investigated clean surfaces; however, the addition of O to the Al and H to
the C$_{\text{dia}}$ surfaces was found to hinder material transfer. This
passivation occurs because of a significant reduction of the adhesion energy at
the examined interfaces, which can be explained by the distinct bonding
situations.
",Physics,Physics
"Pre-Synaptic Pool Modification (PSPM): A Supervised Learning Procedure for Spiking Neural Networks   A central question in neuroscience is how to develop realistic models that
predict output firing behavior based on provided external stimulus. Given a set
of external inputs and a set of output spike trains, the objective is to
discover a network structure which can accomplish the transformation as
accurately as possible. Due to the difficulty of this problem in its most
general form, approximations have been made in previous work. Past
approximations have sacrificed network size, recurrence, allowed spiked count,
or have imposed layered network structure. Here we present a learning rule
without these sacrifices, which produces a weight matrix of a leaky
integrate-and-fire (LIF) network to match the output activity of both
deterministic LIF networks as well as probabilistic integrate-and-fire (PIF)
networks. Inspired by synaptic scaling, our pre-synaptic pool modification
(PSPM) algorithm outputs deterministic, fully recurrent spiking neural networks
that can provide a novel generative model for given spike trains. Similarity in
output spike trains is evaluated with a variety of metrics including a
van-Rossum like measure and a numerical comparison of inter-spike interval
distributions. Application of our algorithm to randomly generated networks
improves similarity to the reference spike trains on both of these stated
measures. In addition, we generated LIF networks that operate near criticality
when trained on critical PIF outputs. Our results establish that learning rules
based on synaptic homeostasis can be used to represent input-output
relationships in fully recurrent spiking neural networks.
",Quantitative Biology,Computer Science; Statistics
"On Nonlinear Dimensionality Reduction, Linear Smoothing and Autoencoding   We develop theory for nonlinear dimensionality reduction (NLDR). A number of
NLDR methods have been developed, but there is limited understanding of how
these methods work and the relationships between them. There is limited basis
for using existing NLDR theory for deriving new algorithms. We provide a novel
framework for analysis of NLDR via a connection to the statistical theory of
linear smoothers. This allows us to both understand existing methods and derive
new ones. We use this connection to smoothing to show that asymptotically,
existing NLDR methods correspond to discrete approximations of the solutions of
sets of differential equations given a boundary condition. In particular, we
can characterize many existing methods in terms of just three limiting
differential operators and boundary conditions. Our theory also provides a way
to assert that one method is preferable to another; indeed, we show Local
Tangent Space Alignment is superior within a class of methods that assume a
global coordinate chart defines an isometric embedding of the manifold.
",Statistics,Computer Science; Mathematics
"Learning an internal representation of the end-effector configuration space   Current machine learning techniques proposed to automatically discover a
robot kinematics usually rely on a priori information about the robot's
structure, sensors properties or end-effector position. This paper proposes a
method to estimate a certain aspect of the forward kinematics model with no
such information. An internal representation of the end-effector configuration
is generated from unstructured proprioceptive and exteroceptive data flow under
very limited assumptions. A mapping from the proprioceptive space to this
representational space can then be used to control the robot.
",Computer Science,Computer Science
"Sample Efficient Feature Selection for Factored MDPs   In reinforcement learning, the state of the real world is often represented
by feature vectors. However, not all of the features may be pertinent for
solving the current task. We propose Feature Selection Explore and Exploit
(FS-EE), an algorithm that automatically selects the necessary features while
learning a Factored Markov Decision Process, and prove that under mild
assumptions, its sample complexity scales with the in-degree of the dynamics of
just the necessary features, rather than the in-degree of all features. This
can result in a much better sample complexity when the in-degree of the
necessary features is smaller than the in-degree of all features.
",Computer Science; Statistics,Computer Science; Statistics
"Generalized Approximate Message-Passing Decoder for Universal Sparse Superposition Codes   Sparse superposition (SS) codes were originally proposed as a
capacity-achieving communication scheme over the additive white Gaussian noise
channel (AWGNC) [1]. Very recently, it was discovered that these codes are
universal, in the sense that they achieve capacity over any memoryless channel
under generalized approximate message-passing (GAMP) decoding [2], although
this decoder has never been stated for SS codes. In this contribution we
introduce the GAMP decoder for SS codes, we confirm empirically the
universality of this communication scheme through its study on various channels
and we provide the main analysis tools: state evolution and potential. We also
compare the performance of GAMP with the Bayes-optimal MMSE decoder. We
empirically illustrate that despite the presence of a phase transition
preventing GAMP to reach the optimal performance, spatial coupling allows to
boost the performance that eventually tends to capacity in a proper limit. We
also prove that, in contrast with the AWGNC case, SS codes for binary input
channels have a vanishing error floor in the limit of large codewords.
Moreover, the performance of Hadamard-based encoders is assessed for practical
implementations.
",Computer Science; Mathematics,Computer Science
"Complete algebraic solution of multidimensional optimization problems in tropical semifield   We consider multidimensional optimization problems that are formulated in the
framework of tropical mathematics to minimize functions defined on vectors over
a tropical semifield (a semiring with idempotent addition and invertible
multiplication). The functions, given by a matrix and calculated through
multiplicative conjugate transposition, are nonlinear in the tropical
mathematics sense. We start with known results on the solution of the problems
with irreducible matrices. To solve the problems in the case of arbitrary
(reducible) matrices, we first derive the minimum value of the objective
function, and find a set of solutions. We show that all solutions of the
problem satisfy a system of vector inequalities, and then use these
inequalities to establish characteristic properties of the solution set.
Furthermore, all solutions of the problem are represented as a family of
subsets, each defined by a matrix that is obtained by using a matrix
sparsification technique. We describe a backtracking procedure that allows one
to reduce the brute-force generation of sparsified matrices by skipping those,
which cannot provide solutions, and thus offers an economical way to obtain all
subsets in the family. Finally, the characteristic properties of the solution
set are used to provide complete solutions in a closed form. We illustrate the
results obtained with simple numerical examples.
",Computer Science; Mathematics,Mathematics
"Urban Vibrancy and Safety in Philadelphia   Statistical analyses of urban environments have been recently improved
through publicly available high resolution data and mapping technologies that
have been adopted across industries. These technologies allow us to create
metrics to empirically investigate urban design principles of the past
half-century. Philadelphia is an interesting case study for this work, with its
rapid urban development and population increase in the last decade. We outline
a data analysis pipeline for exploring the association between safety and local
neighborhood features such as population, economic health and the built
environment. As a particular example of our analysis pipeline, we focus on
quantitative measures of the built environment that serve as proxies for
vibrancy: the amount of human activity in a local area. Historically, vibrancy
has been very challenging to measure empirically. Measures based on land use
zoning are not an adequate description of local vibrancy and so we construct a
database and set of measures of business activity in each neighborhood. We
employ several matching analyses to explore the relationship between
neighborhood vibrancy and safety, such as comparing high crime versus low crime
locations within the same neighborhood. As additional sources of urban data
become available, our analysis pipeline can serve as the template for further
investigations into the relationships between safety, economic factors and the
built environment at the local neighborhood level.
",Statistics,Statistics
"On perpetuities with gamma-like tails   An infinite convergent sum of independent and identically distributed random
variables discounted by a multiplicative random walk is called perpetuity,
because of a possible actuarial application. We give three disjoint groups of
sufficient conditions which ensure that the distribution right tail of a
perpetuity $\mathbb{P}\{X>x\}$ is asymptotic to $ax^ce^{-bx}$ as $x\to\infty$
for some $a,b>0$ and $c\in\mathbb{R}$. Our results complement those of Denisov
and Zwart [J. Appl. Probab. 44 (2007), 1031--1046]. As an auxiliary tool we
provide criteria for the finiteness of the one-sided exponential moments of
perpetuities. Several examples are given in which the distributions of
perpetuities are explicitly identified.
",Mathematics,Mathematics
"Phrase-based Image Captioning with Hierarchical LSTM Model   Automatic generation of caption to describe the content of an image has been
gaining a lot of research interests recently, where most of the existing works
treat the image caption as pure sequential data. Natural language, however
possess a temporal hierarchy structure, with complex dependencies between each
subsequence. In this paper, we propose a phrase-based hierarchical Long
Short-Term Memory (phi-LSTM) model to generate image description. In contrast
to the conventional solutions that generate caption in a pure sequential
manner, our proposed model decodes image caption from phrase to sentence. It
consists of a phrase decoder at the bottom hierarchy to decode noun phrases of
variable length, and an abbreviated sentence decoder at the upper hierarchy to
decode an abbreviated form of the image description. A complete image caption
is formed by combining the generated phrases with sentence during the inference
stage. Empirically, our proposed model shows a better or competitive result on
the Flickr8k, Flickr30k and MS-COCO datasets in comparison to the state-of-the
art models. We also show that our proposed model is able to generate more novel
captions (not seen in the training data) which are richer in word contents in
all these three datasets.
",Computer Science,Computer Science
"Detecting and Explaining Causes From Text For a Time Series Event   Explaining underlying causes or effects about events is a challenging but
valuable task. We define a novel problem of generating explanations of a time
series event by (1) searching cause and effect relationships of the time series
with textual data and (2) constructing a connecting chain between them to
generate an explanation. To detect causal features from text, we propose a
novel method based on the Granger causality of time series between features
extracted from text such as N-grams, topics, sentiments, and their composition.
The generation of the sequence of causal entities requires a commonsense
causative knowledge base with efficient reasoning. To ensure good
interpretability and appropriate lexical usage we combine symbolic and neural
representations, using a neural reasoning algorithm trained on commonsense
causal tuples to predict the next cause step. Our quantitative and human
analysis show empirical evidence that our method successfully extracts
meaningful causality relationships between time series with textual features
and generates appropriate explanation between them.
",Computer Science,Computer Science
"Consistency of Dirichlet Partitions   A Dirichlet $k$-partition of a domain $U \subseteq \mathbb{R}^d$ is a
collection of $k$ pairwise disjoint open subsets such that the sum of their
first Laplace-Dirichlet eigenvalues is minimal. A discrete version of Dirichlet
partitions has been posed on graphs with applications in data analysis. Both
versions admit variational formulations: solutions are characterized by
minimizers of the Dirichlet energy of mappings from $U$ into a singular space
$\Sigma_k \subseteq \mathbb{R}^k$. In this paper, we extend results of N.\
García Trillos and D.\ Slepčev to show that there exist solutions of the
continuum problem arising as limits to solutions of a sequence of discrete
problems. Specifically, a sequence of points $\{x_i\}_{i \in \mathbb{N}}$ from
$U$ is sampled i.i.d.\ with respect to a given probability measure $\nu$ on $U$
and for all $n \in \mathbb{N}$, a geometric graph $G_n$ is constructed from the
first $n$ points $x_1, x_2, \ldots, x_n$ and the pairwise distances between the
points. With probability one with respect to the choice of points $\{x_i\}_{i
\in \mathbb{N}}$, we show that as $n \to \infty$ the discrete Dirichlet
energies for functions $G_n \to \Sigma_k$ $\Gamma$-converge to (a scalar
multiple of) the continuum Dirichlet energy for functions $U \to \Sigma_k$ with
respect to a metric coming from the theory of optimal transport. This, along
with a compactness property for the aforementioned energies that we prove,
implies the convergence of minimizers. When $\nu$ is the uniform distribution,
our results also imply the statistical consistency statement that Dirichlet
partitions of geometric graphs converge to partitions of the sampled space in
the Hausdorff sense.
",Mathematics; Statistics,Mathematics
"A Reactive and Efficient Walking Pattern Generator for Robust Bipedal Locomotion   Available possibilities to prevent a biped robot from falling down in the
presence of severe disturbances are mainly Center of Pressure (CoP) modulation,
step location and timing adjustment, and angular momentum regulation. In this
paper, we aim at designing a walking pattern generator which employs an optimal
combination of these tools to generate robust gaits. In this approach, first,
the next step location and timing are decided consistent with the commanded
walking velocity and based on the Divergent Component of Motion (DCM)
measurement. This stage which is done by a very small-size Quadratic Program
(QP) uses the Linear Inverted Pendulum Model (LIPM) dynamics to adapt the
switching contact location and time. Then, consistent with the first stage, the
LIPM with flywheel dynamics is used to regenerate the DCM and angular momentum
trajectories at each control cycle. This is done by modulating the CoP and
Centroidal Momentum Pivot (CMP) to realize a desired DCM at the end of current
step. Simulation results show the merit of this reactive approach in generating
robust and dynamically consistent walking patterns.
",Computer Science,Computer Science
"The Muon g-2 experiment at Fermilab   The upcoming Fermilab E989 experiment will measure the muon anomalous
magnetic moment $a_{\mu}$ . This measurement is motivated by the previous
measurement performed in 2001 by the BNL E821 experiment that reported a 3-4
standard deviation discrepancy between the measured value and the Standard
Model prediction. The new measurement at Fermilab aims to improve the precision
by a factor of four reducing the total uncertainty from 540 parts per billion
(BNL E821) to 140 parts per billion (Fermilab E989). This paper gives the
status of the experiment.
",Physics,Physics
"Covering and tiling hypergraphs with tight cycles   Given $3 \leq k \leq s$, we say that a $k$-uniform hypergraph $C^k_s$ is a
tight cycle on $s$ vertices if there is a cyclic ordering of the vertices of
$C^k_s$ such that every $k$ consecutive vertices under this ordering form an
edge. We prove that if $k \ge 3$ and $s \ge 2k^2$, then every $k$-uniform
hypergraph on $n$ vertices with minimum codegree at least $(1/2 + o(1))n$ has
the property that every vertex is covered by a copy of $C^k_s$. Our result is
asymptotically best possible for infinitely many pairs of $s$ and $k$, e.g.
when $s$ and $k$ are coprime.
A perfect $C^k_s$-tiling is a spanning collection of vertex-disjoint copies
of $C^k_s$. When $s$ is divisible by $k$, the problem of determining the
minimum codegree that guarantees a perfect $C^k_s$-tiling was solved by a
result of Mycroft. We prove that if $k \ge 3$ and $s \ge 5k^2$ is not divisible
by $k$ and $s$ divides $n$, then every $k$-uniform hypergraph on $n$ vertices
with minimum codegree at least $(1/2 + 1/(2s) + o(1))n$ has a perfect
$C^k_s$-tiling. Again our result is asymptotically best possible for infinitely
many pairs of $s$ and $k$, e.g. when $s$ and $k$ are coprime with $k$ even.
",Mathematics,Computer Science
"QWIRE Practice: Formal Verification of Quantum Circuits in Coq   We describe an embedding of the QWIRE quantum circuit language in the Coq
proof assistant. This allows programmers to write quantum circuits using
high-level abstractions and to prove properties of those circuits using Coq's
theorem proving features. The implementation uses higher-order abstract syntax
to represent variable binding and provides a type-checking algorithm for linear
wire types, ensuring that quantum circuits are well-formed. We formalize a
denotational semantics that interprets QWIRE circuits as superoperators on
density matrices, and prove the correctness of some simple quantum programs.
",Computer Science,Computer Science
"Normal-state Properties of a Unitary Bose-Fermi Mixture: A Combined Strong-coupling Approach with Universal Thermodynamics   We theoretically investigate normal-state properties of a unitary Bose-Fermi
mixture. Including strong hetero-pairing fluctuations, we evaluate the Bose and
Fermi chemical potential, internal energy, pressure, entropy, as well as
specific heat at constant volume $C_V$, within the framework of a combined
strong-coupling theory with exact thermodynamic identities. We show that
hetero-pairing fluctuations at the unitarity cause non-monotonic temperature
dependence of $C_V$, being qualitatively different from the monotonic behavior
of this quantity in the weak- and strong-coupling limit. On the other hand,
such an anomalous behavior is not seen in the other quantities. Our results
indicate that the specific heat $C_V$, which has recently become observable in
cold atom physics, is a useful quantity for understanding strong-coupling
aspects of this quantum system.
",Physics,Physics
"BézierGAN: Automatic Generation of Smooth Curves from Interpretable Low-Dimensional Parameters   Many real-world objects are designed by smooth curves, especially in the
domain of aerospace and ship, where aerodynamic shapes (e.g., airfoils) and
hydrodynamic shapes (e.g., hulls) are designed. To facilitate the design
process of those objects, we propose a deep learning based generative model
that can synthesize smooth curves. The model maps a low-dimensional latent
representation to a sequence of discrete points sampled from a rational
Bézier curve. We demonstrate the performance of our method in completing both
synthetic and real-world generative tasks. Results show that our method can
generate diverse and realistic curves, while preserving consistent shape
variation in the latent space, which is favorable for latent space design
optimization or design space exploration.
",Statistics,Computer Science; Statistics
"Nearest-neighbour Markov point processes on graphs with Euclidean edges   We define nearest-neighbour point processes on graphs with Euclidean edges
and linear networks. They can be seen as the analogues of renewal processes on
the real line. We show that the Delaunay neighbourhood relation on a tree
satisfies the Baddeley--M{\o}ller consistency conditions and provide a
characterisation of Markov functions with respect to this relation. We show
that a modified relation defined in terms of the local geometry of the graph
satisfies the consistency conditions for all graphs with Euclidean edges.
",Mathematics; Statistics,Mathematics
"The Resilience of Life to Astrophysical Events   Much attention has been given in the literature to the effects of
astrophysical events on human and land-based life. However, little has been
discussed on the resilience of life itself. Here we instead explore the
statistics of events that completely sterilise an Earth-like planet with planet
radii in the range $0.5-1.5 R_{Earth}$ and temperatures of $\sim 300 \;
\text{K}$, eradicating all forms of life. We consider the relative likelihood
of complete global sterilisation events from three astrophysical sources --
supernovae, gamma-ray bursts, large asteroid impacts, and passing-by stars. To
assess such probabilities we consider what cataclysmic event could lead to the
annihilation of not just human life, but also extremophiles, through the
boiling of all water in Earth's oceans. Surprisingly we find that although
human life is somewhat fragile to nearby events, the resilience of Ecdysozoa
such as \emph{Milnesium tardigradum} renders global sterilisation an unlikely
event.
",Physics,Physics
"A supernova at 50 pc: Effects on the Earth's atmosphere and biota   Recent 60Fe results have suggested that the estimated distances of supernovae
in the last few million years should be reduced from 100 pc to 50 pc. Two
events or series of events are suggested, one about 2.7 million years to 1.7
million years ago, and another may at 6.5 to 8.7 million years ago. We ask what
effects such supernovae are expected to have on the terrestrial atmosphere and
biota. Assuming that the Local Bubble was formed before the event being
considered, and that the supernova and the Earth were both inside a weak,
disordered magnetic field at that time, TeV-PeV cosmic rays at Earth will
increase by a factor of a few hundred. Tropospheric ionization will increase
proportionately, and the overall muon radiation load on terrestrial organisms
will increase by a factor of 150. All return to pre-burst levels within 10kyr.
In the case of an ordered magnetic field, effects depend strongly on the field
orientation. The upper bound in this case is with a largely coherent field
aligned along the line of sight to the supernova, in which case TeV-PeV cosmic
ray flux increases are 10^4; in the case of a transverse field they are below
current levels. We suggest a substantial increase in the extended effects of
supernovae on Earth and in the lethal distance estimate; more work is
needed.This paper is an explicit followup to Thomas et al. (2016). We also here
provide more detail on the computational procedures used in both works.
",Physics,Physics
"Experimental study of extrinsic spin Hall effect in CuPt alloy   We have experimentally studied the effects on the spin Hall angle due to
systematic addition of Pt into the light metal Cu. We perform spin torque
ferromagnetic resonance measurements on Py/CuPt bilayer and find that as the Pt
concentration increases, the spin Hall angle of CuPt alloy increases. Moreover,
only 28% Pt in CuPt alloy can give rise to a spin Hall angle close to that of
Pt. We further extract the spin Hall resistivity of CuPt alloy for different Pt
concentrations and find that the contribution of skew scattering is larger for
lower Pt concentrations, while the side-jump contribution is larger for higher
Pt concentrations. From technological perspective, since the CuPt alloy can
sustain high processing temperatures and Cu is the most common metallization
element in the Si platform, it would be easier to integrate the CuPt alloy
based spintronic devices into existing Si fabrication technology.
",Physics,Physics
"Weakly- and Semi-Supervised Object Detection with Expectation-Maximization Algorithm   Object detection when provided image-level labels instead of instance-level
labels (i.e., bounding boxes) during training is an important problem in
computer vision, since large scale image datasets with instance-level labels
are extremely costly to obtain. In this paper, we address this challenging
problem by developing an Expectation-Maximization (EM) based object detection
method using deep convolutional neural networks (CNNs). Our method is
applicable to both the weakly-supervised and semi-supervised settings.
Extensive experiments on PASCAL VOC 2007 benchmark show that (1) in the weakly
supervised setting, our method provides significant detection performance
improvement over current state-of-the-art methods, (2) having access to a small
number of strongly (instance-level) annotated images, our method can almost
match the performace of the fully supervised Fast RCNN. We share our source
code at this https URL.
",Computer Science,Computer Science
"A parametric level-set method for partially discrete tomography   This paper introduces a parametric level-set method for tomographic
reconstruction of partially discrete images. Such images consist of a
continuously varying background and an anomaly with a constant (known)
grey-value. We represent the geometry of the anomaly using a level-set
function, which we represent using radial basis functions. We pose the
reconstruction problem as a bi-level optimization problem in terms of the
background and coefficients for the level-set function. To constrain the
background reconstruction we impose smoothness through Tikhonov regularization.
The bi-level optimization problem is solved in an alternating fashion; in each
iteration we first reconstruct the background and consequently update the
level-set function. We test our method on numerical phantoms and show that we
can successfully reconstruct the geometry of the anomaly, even from limited
data. On these phantoms, our method outperforms Total Variation reconstruction,
DART and P-DART.
",Computer Science,Mathematics; Statistics
"Merlin-Arthur with efficient quantum Merlin and quantum supremacy for the second level of the Fourier hierarchy   We introduce a simple sub-universal quantum computing model, which we call
the Hadamard-classical circuit with one-qubit (HC1Q) model. It consists of a
classical reversible circuit sandwiched by two layers of Hadamard gates, and
therefore it is in the second level of the Fourier hierarchy. We show that
output probability distributions of the HC1Q model cannot be classically
efficiently sampled within a multiplicative error unless the polynomial-time
hierarchy collapses to the second level. The proof technique is different from
those used for previous sub-universal models, such as IQP, Boson Sampling, and
DQC1, and therefore the technique itself might be useful for finding other
sub-universal models that are hard to classically simulate. We also study the
classical verification of quantum computing in the second level of the Fourier
hierarchy. To this end, we define a promise problem, which we call the
probability distribution distinguishability with maximum norm (PDD-Max). It is
a promise problem to decide whether output probability distributions of two
quantum circuits are far apart or close. We show that PDD-Max is BQP-complete,
but if the two circuits are restricted to some types in the second level of the
Fourier hierarchy, such as the HC1Q model or the IQP model, PDD-Max has a
Merlin-Arthur system with quantum polynomial-time Merlin and classical
probabilistic polynomial-time Arthur.
",Computer Science,Computer Science; Mathematics
"Supersonic Flow onto Solid Wedges, Multidimensional Shock Waves and Free Boundary Problems   When an upstream steady uniform supersonic flow impinges onto a symmetric
straight-sided wedge, governed by the Euler equations, there are two possible
steady oblique shock configurations if the wedge angle is less than the
detachment angle -- the steady weak shock with supersonic or subsonic
downstream flow (determined by the wedge angle that is less or larger than the
sonic angle) and the steady strong shock with subsonic downstream flow, both of
which satisfy the entropy condition. The fundamental issue -- whether one or
both of the steady weak and strong shocks are physically admissible solutions
-- has been vigorously debated over the past eight decades. In this paper, we
survey some recent developments on the stability analysis of the steady shock
solutions in both the steady and dynamic regimes. For the static stability, we
first show how the stability problem can be formulated as an initial-boundary
value type problem and then reformulate it into a free boundary problem when
the perturbation of both the upstream steady supersonic flow and the wedge
boundary are suitably regular and small, and we finally present some recent
results on the static stability of the steady supersonic and transonic shocks.
For the dynamic stability for potential flow, we first show how the stability
problem can be formulated as an initial-boundary value problem and then use the
self-similarity of the problem to reduce it into a boundary value problem and
further reformulate it into a free boundary problem, and we finally survey some
recent developments in solving this free boundary problem for the existence of
the Prandtl-Meyer configurations that tend to the steady weak supersonic or
transonic oblique shock solutions as time goes to infinity. Some further
developments and mathematical challenges in this direction are also discussed.
",Physics; Mathematics,Physics
"Structure preserving schemes for mean-field equations of collective behavior   In this paper we consider the development of numerical schemes for mean-field
equations describing the collective behavior of a large group of interacting
agents. The schemes are based on a generalization of the classical Chang-Cooper
approach and are capable to preserve the main structural properties of the
systems, namely nonnegativity of the solution, physical conservation laws,
entropy dissipation and stationary solutions. In particular, the methods here
derived are second order accurate in transient regimes whereas they can reach
arbitrary accuracy asymptotically for large times. Several examples are
reported to show the generality of the approach.
",Physics,Mathematics
"Exponential Decay of the lengths of Spectral Gaps for Extended Harper's Model with Liouvillean Frequency   In this paper, we study the non-self dual extended Harper's model with
Liouvillean frequency. By establishing quantitative reducibility results
together with the averaging method, we prove that the lengths of spectral gaps
decay exponentially.
",Mathematics,Mathematics
"Octupolar Tensors for Liquid Crystals   A third-order three-dimensional symmetric traceless tensor, called the
\emph{octupolar} tensor, has been introduced to study tetrahedratic nematic
phases in liquid crystals. The octupolar \emph{potential}, a scalar-valued
function generated on the unit sphere by that tensor, should ideally have four
maxima capturing the most probable molecular orientations (on the vertices of a
tetrahedron), but it was recently found to possess an equally generic variant
with \emph{three} maxima instead of four. It was also shown that the
irreducible admissible region for the octupolar tensor in a three-dimensional
parameter space is bounded by a dome-shaped surface, beneath which is a
\emph{separatrix} surface connecting the two generic octupolar states. The
latter surface, which was obtained through numerical continuation, may be
physically interpreted as marking a possible \emph{intra-octupolar} transition.
In this paper, by using the resultant theory of algebraic geometry and the
E-characteristic polynomial of spectral theory of tensors, we give a
closed-form, algebraic expression for both the dome-shaped surface and the
separatrix surface. This turns the envisaged intra-octupolar transition into a
quantitative, possibly observable prediction. Some other properties of
octupolar tensors are also studied.
",Mathematics,Physics
"The relationship between the number of editorial board members and the scientific output of universities in the chemistry field   Editorial board members, who are considered the gatekeepers of scientific
journals, play an important role in academia, and may directly or indirectly
affect the scientific output of a university. In this article, we used the
quantile regression method among a sample of 1,387 university in chemistry to
characterize the correlation between the number of editorial board members and
the scientific output of their universities. Furthermore, we used time-series
data and the Granger causality test to explore the causal relationship between
the number of editorial board members and the number of articles of some top
universities. Our results suggest that the number of editorial board members is
positively and significantly related to the scientific output (as measured by
the number of articles, total number of citations, citations per paper, and h
index) of their universities. However, the Granger causality test results
suggest that the causal relationship between the number of editorial board
members and the number of articles of some top universities is not obvious.
Combining these findings with the results of qualitative interviews with
editorial board members, we discuss the causal relationship between the number
of editorial board members and the scientific output of their universities.
",Computer Science; Statistics,Computer Science; Physics
"On 2d-4d motivic wall-crossing formulas   In this paper we propose definitions and examples of categorical enhancements
of the data involved in the $2d$-$4d$ wall-crossing formulas which generalize
both Cecotti-Vafa and Kontsevich-Soibelman motivic wall-crossing formulas.
",Mathematics,Mathematics
"Accurate Single Stage Detector Using Recurrent Rolling Convolution   Most of the recent successful methods in accurate object detection and
localization used some variants of R-CNN style two stage Convolutional Neural
Networks (CNN) where plausible regions were proposed in the first stage then
followed by a second stage for decision refinement. Despite the simplicity of
training and the efficiency in deployment, the single stage detection methods
have not been as competitive when evaluated in benchmarks consider mAP for high
IoU thresholds. In this paper, we proposed a novel single stage end-to-end
trainable object detection network to overcome this limitation. We achieved
this by introducing Recurrent Rolling Convolution (RRC) architecture over
multi-scale feature maps to construct object classifiers and bounding box
regressors which are ""deep in context"". We evaluated our method in the
challenging KITTI dataset which measures methods under IoU threshold of 0.7. We
showed that with RRC, a single reduced VGG-16 based model already significantly
outperformed all the previously published results. At the time this paper was
written our models ranked the first in KITTI car detection (the hard level),
the first in cyclist detection and the second in pedestrian detection. These
results were not reached by the previous single stage methods. The code is
publicly available.
",Computer Science,Computer Science
"Composition Properties of Inferential Privacy for Time-Series Data   With the proliferation of mobile devices and the internet of things,
developing principled solutions for privacy in time series applications has
become increasingly important. While differential privacy is the gold standard
for database privacy, many time series applications require a different kind of
guarantee, and a number of recent works have used some form of inferential
privacy to address these situations.
However, a major barrier to using inferential privacy in practice is its lack
of graceful composition -- even if the same or related sensitive data is used
in multiple releases that are safe individually, the combined release may have
poor privacy properties. In this paper, we study composition properties of a
form of inferential privacy called Pufferfish when applied to time-series data.
We show that while general Pufferfish mechanisms may not compose gracefully, a
specific Pufferfish mechanism, called the Markov Quilt Mechanism, which was
recently introduced, has strong composition properties comparable to that of
pure differential privacy when applied to time series data.
",Computer Science; Statistics,Computer Science
"Typed Closure Conversion for the Calculus of Constructions   Dependently typed languages such as Coq are used to specify and verify the
full functional correctness of source programs. Type-preserving compilation can
be used to preserve these specifications and proofs of correctness through
compilation into the generated target-language programs. Unfortunately,
type-preserving compilation of dependent types is hard. In essence, the problem
is that dependent type systems are designed around high-level compositional
abstractions to decide type checking, but compilation interferes with the
type-system rules for reasoning about run-time terms.
We develop a type-preserving closure-conversion translation from the Calculus
of Constructions (CC) with strong dependent pairs ($\Sigma$ types)---a subset
of the core language of Coq---to a type-safe, dependently typed compiler
intermediate language named CC-CC. The central challenge in this work is how to
translate the source type-system rules for reasoning about functions into
target type-system rules for reasoning about closures. To justify these rules,
we prove soundness of CC-CC by giving a model in CC. In addition to type
preservation, we prove correctness of separate compilation.
",Computer Science,Computer Science
"Pseudo asymptotically periodic solutions for fractional integro-differential neutral equations   In this paper, we study the existence and uniqueness of pseudo
$S$-asymptotically $\omega$-periodic mild solutions of class $r$ for fractional
integro-differential neutral equations. An example is presented to illustrate
the application of the abstract results.
",Mathematics,Mathematics
"Benford analysis of quantum critical phenomena: First digit provides high finite-size scaling exponent while first two and further are not much better   Benford's law is an empirical edict stating that the lower digits appear more
often than higher ones as the first few significant digits in statistics of
natural phenomena and mathematical tables. A marked proportion of such analyses
is restricted to the first significant digit. We employ violation of Benford's
law, up to the first four significant digits, for investigating magnetization
and correlation data of paradigmatic quantum many-body systems to detect
cooperative phenomena, focusing on the finite-size scaling exponents thereof.
We find that for the transverse field quantum XY model, behavior of the very
first significant digit of an observable, at an arbitrary point of the
parameter space, is enough to capture the quantum phase transition in the model
with a relatively high scaling exponent. A higher number of significant digits
do not provide an appreciable further advantage, in particular, in terms of an
increase in scaling exponents. Since the first significant digit of a physical
quantity is relatively simple to obtain in experiments, the results have
potential implications for laboratory observations in noisy environments.
",Physics,Physics
"Security Analysis of Cache Replacement Policies   Modern computer architectures share physical resources between different
programs in order to increase area-, energy-, and cost-efficiency.
Unfortunately, sharing often gives rise to side channels that can be exploited
for extracting or transmitting sensitive information. We currently lack
techniques for systematic reasoning about this interplay between security and
efficiency. In particular, there is no established way for quantifying security
properties of shared caches.
In this paper, we propose a novel model that enables us to characterize
important security properties of caches. Our model encompasses two aspects: (1)
The amount of information that can be absorbed by a cache, and (2) the amount
of information that can effectively be extracted from the cache by an
adversary. We use our model to compute both quantities for common cache
replacement policies (FIFO, LRU, and PLRU) and to compare their isolation
properties. We further show how our model for information extraction leads to
an algorithm that can be used to improve the bounds delivered by the CacheAudit
static analyzer.
",Computer Science,Computer Science
"SGD Learns the Conjugate Kernel Class of the Network   We show that the standard stochastic gradient decent (SGD) algorithm is
guaranteed to learn, in polynomial time, a function that is competitive with
the best function in the conjugate kernel space of the network, as defined in
Daniely, Frostig and Singer. The result holds for log-depth networks from a
rich family of architectures. To the best of our knowledge, it is the first
polynomial-time guarantee for the standard neural network learning algorithm
for networks of depth more that two.
As corollaries, it follows that for neural networks of any depth between $2$
and $\log(n)$, SGD is guaranteed to learn, in polynomial time, constant degree
polynomials with polynomially bounded coefficients. Likewise, it follows that
SGD on large enough networks can learn any continuous function (not in
polynomial time), complementing classical expressivity results.
",Computer Science; Statistics,Computer Science; Statistics
"A tutorial on the synthesis and validation of a closed-loop wind farm controller using a steady-state surrogate model   In wind farms, wake interaction leads to losses in power capture and
accelerated structural degradation when compared to freestanding turbines. One
method to reduce wake losses is by misaligning the rotor with the incoming flow
using its yaw actuator, thereby laterally deflecting the wake away from
downstream turbines. However, this demands an accurate and computationally
tractable model of the wind farm dynamics. This problem calls for a closed-loop
solution. This tutorial paper fills the scientific gap by demonstrating the
full closed-loop controller synthesis cycle using a steady-state surrogate
model. Furthermore, a novel, computationally efficient and modular
communication interface is presented that enables researchers to
straight-forwardly test their control algorithms in large-eddy simulations.
High-fidelity simulations of a 9-turbine farm show a power production increase
of up to 11% using the proposed closed-loop controller compared to traditional,
greedy wind farm operation.
",Computer Science,Physics
"Identification of Voice Utterance with Aging Factor Using the Method of MFCC Multichannel   This research was conducted to develop a method to identify voice utterance.
For voice utterance that encounters change caused by aging factor, with the
interval of 10 to 25 years. The change of voice utterance influenced by aging
factor might be extracted by MFCC (Mel Frequency Cepstrum Coefficient).
However, the level of the compatibility of the feature may be dropped down to
55%. While the ones which do not encounter it may reach 95%. To improve the
compatibility of the changing voice feature influenced by aging factor, then
the method of the more specific feature extraction is developed: which is by
separating the voice into several channels, suggested as MFCC multichannel,
consisting of multichannel 5 filterbank (M5FB), multichannel 2 filterbank
(M2FB) and multichannel 1 filterbank (M1FB). The result of the test shows that
for model M5FB and M2FB have the highest score in the level of compatibility
with 85% and 82% with 25 years interval. While model M5FB gets the highest
score of 86% for 10 years time interval.
",Computer Science,Computer Science
"Deep Architectures for Neural Machine Translation   It has been shown that increasing model depth improves the quality of neural
machine translation. However, different architectural variants to increase
model depth have been proposed, and so far, there has been no thorough
comparative study.
In this work, we describe and evaluate several existing approaches to
introduce depth in neural machine translation. Additionally, we explore novel
architectural variants, including deep transition RNNs, and we vary how
attention is used in the deep decoder. We introduce a novel ""BiDeep"" RNN
architecture that combines deep transition RNNs and stacked RNNs.
Our evaluation is carried out on the English to German WMT news translation
dataset, using a single-GPU machine for both training and inference. We find
that several of our proposed architectures improve upon existing approaches in
terms of speed and translation quality. We obtain best improvements with a
BiDeep RNN of combined depth 8, obtaining an average improvement of 1.5 BLEU
over a strong shallow baseline.
We release our code for ease of adoption.
",Computer Science,Statistics
"Dynamics of Relaxed Inflation   The cosmological relaxation of the electroweak scale has been proposed as a
mechanism to address the hierarchy problem of the Standard Model. A field, the
relaxion, rolls down its potential and, in doing so, scans the squared mass
parameter of the Higgs, relaxing it to a parametrically small value. In this
work, we promote the relaxion to an inflaton. We couple it to Abelian gauge
bosons, thereby introducing the necessary dissipation mechanism which slows
down the field in the last stages. We describe a novel reheating mechanism,
which relies on the gauge-boson production leading to strong electromagnetic
fields, and proceeds via the vacuum production of electron-positron pairs
through the Schwinger effect. We refer to this mechanism as Schwinger
reheating. We discuss the cosmological dynamics of the model and the
phenomenological constraints from CMB and other experiments. We find that a
cutoff close to the Planck scale may be achieved. In its minimal form, the
model does not generate sufficient curvature perturbations and additional
ingredients, such as a curvaton field, are needed.
",Physics,Physics
"Borel subsets of the real line and continuous reducibility   We study classes of Borel subsets of the real line $\mathbb{R}$ such as
levels of the Borel hierarchy and the class of sets that are reducible to the
set $\mathbb{Q}$ of rationals, endowed with the Wadge quasi-order of
reducibility with respect to continuous functions on $\mathbb{R}$. Notably, we
explore several structural properties of Borel subsets of $\mathbb{R}$ that
diverge from those of Polish spaces with dimension zero. Our first main result
is on the existence of embeddings of several posets into the restriction of
this quasi-order to any Borel class that is strictly above the classes of open
and closed sets, for instance the linear order $\omega_1$, its reverse
$\omega_1^\star$ and the poset $\mathcal{P}(\omega)/\mathsf{fin}$ of inclusion
modulo finite error. As a consequence of its proof, it is shown that there are
no complete sets for these classes. We further extend the previous theorem to
targets that are reducible to $\mathbb{Q}$. These non-structure results
motivate the study of further restrictions of the Wadge quasi-order. In our
second main theorem, we introduce a combinatorial property that is shown to
characterize those $F_\sigma$ sets that are reducible to $\mathbb{Q}$. This is
applied to construct a minimal set below $\mathbb{Q}$ and prove its uniqueness
up to Wadge equivalence. We finally prove several results concerning gaps and
cardinal characteristics of the Wadge quasi-order and thereby answer questions
of Brendle and Geschke.
",Mathematics,Mathematics
"Learning to Detect and Mitigate Cross-layer Attacks in Wireless Networks: Framework and Applications   Security threats such as jamming and route manipulation can have significant
consequences on the performance of modern wireless networks. To increase the
efficacy and stealthiness of such threats, a number of extremely challenging,
cross-layer attacks have been recently unveiled. Although existing research has
thoroughly addressed many single-layer attacks, the problem of detecting and
mitigating cross-layer attacks still remains unsolved. For this reason, in this
paper we propose a novel framework to analyze and address cross-layer attacks
in wireless networks. Specifically, our framework consists of a detection and a
mitigation component. The attack detection component is based on a Bayesian
learning detection scheme that constructs a model of observed evidence to
identify stealthy attack activities. The mitigation component comprises a
scheme that achieves the desired trade-off between security and performance. We
specialize and evaluate the proposed framework by considering a specific
cross-layer attack that uses jamming as an auxiliary tool to achieve route
manipulation. Simulations and experimental results obtained with a test-bed
made up by USRP software-defined radios demonstrate the effectiveness of the
proposed methodology.
",Computer Science,Computer Science
"Space of initial conditions for a cubic Hamiltonian system   In this paper we perform the analysis that leads to the space of initial
conditions for the Hamiltonian system $q' = p^2 + zq + \alpha$, $p' = -q^2 - zp
- \beta$, studied by the author in an earlier article. By compactifying the
phase space of the system from $\mathbb{C}^2$ to $\mathbb{CP}^2$ three base
points arise in the standard coordinate charts covering the complex projective
space. Each of these is removed by a sequence of three blow-ups, a construction
to regularise the system at these points. The resulting space, where the
exceptional curves introduced after the first and second blow-up are removed,
is the so-called Okamoto's space of initial conditions for this system which,
at every point, defines a regular initial value problem in some coordinate
chart of the space.
",Physics; Mathematics,Mathematics
"Trajectory Tracking Control of a Flexible Spine Robot, With and Without a Reference Input   The Underactuated Lightweight Tensegrity Robotic Assistive Spine (ULTRA
Spine) project is an ongoing effort to develop a flexible, actuated backbone
for quadruped robots. In this work, model-predictive control is used to track a
trajectory in the robot's state space, in simulation. The state trajectory used
here corresponds to a bending motion of the spine, with translations and
rotations of the moving vertebrae. Two different controllers are presented in
this work: one that does not use a reference input but includes smoothing
constrants, and a second one that uses a reference input without smoothing. For
the smoothing controller, without reference inputs, the error converges to
zero, while the simpler-to-tune controller with an input reference shows small
errors but not complete convergence. It is expected that this controller will
converge as it is improved further.
",Computer Science,Computer Science
"A Survey on Mobile Edge Computing: The Communication Perspective   Driven by the visions of Internet of Things and 5G communications, recent
years have seen a paradigm shift in mobile computing, from the centralized
Mobile Cloud Computing towards Mobile Edge Computing (MEC). The main feature of
MEC is to push mobile computing, network control and storage to the network
edges (e.g., base stations and access points) so as to enable
computation-intensive and latency-critical applications at the resource-limited
mobile devices. MEC promises dramatic reduction in latency and mobile energy
consumption, tackling the key challenges for materializing 5G vision. The
promised gains of MEC have motivated extensive efforts in both academia and
industry on developing the technology. A main thrust of MEC research is to
seamlessly merge the two disciplines of wireless communications and mobile
computing, resulting in a wide-range of new designs ranging from techniques for
computation offloading to network architectures. This paper provides a
comprehensive survey of the state-of-the-art MEC research with a focus on joint
radio-and-computational resource management. We also present a research outlook
consisting of a set of promising directions for MEC research, including MEC
system deployment, cache-enabled MEC, mobility management for MEC, green MEC,
as well as privacy-aware MEC. Advancements in these directions will facilitate
the transformation of MEC from theory to practice. Finally, we introduce recent
standardization efforts on MEC as well as some typical MEC application
scenarios.
",Computer Science; Mathematics,Computer Science
"A Generalized Function defined by the Euler first kind integral and its connection with the Dirac delta function   We have shown that in some region where the Euler integral of the first kind
diverges, the Euler formula defines a generalized function. The connected of
this generalized function with the Dirac delta function is found.
",Mathematics,Mathematics
"Well quasi-orders and the functional interpretation   The purpose of this article is to study the role of Gödel's functional
interpretation in the extraction of programs from proofs in well quasi-order
theory. The main focus is on the interpretation of Nash-Williams' famous
minimal bad sequence construction, and the exploration of a number of much
broader problems which are related to this, particularly the question of the
constructive meaning of Zorn's lemma and the notion of recursion over the
non-wellfounded lexicographic ordering on infinite sequences.
",Computer Science; Mathematics,Mathematics
"Optical Music Recognition with Convolutional Sequence-to-Sequence Models   Optical Music Recognition (OMR) is an important technology within Music
Information Retrieval. Deep learning models show promising results on OMR
tasks, but symbol-level annotated data sets of sufficient size to train such
models are not available and difficult to develop. We present a deep learning
architecture called a Convolutional Sequence-to-Sequence model to both move
towards an end-to-end trainable OMR pipeline, and apply a learning process that
trains on full sentences of sheet music instead of individually labeled
symbols. The model is trained and evaluated on a human generated data set, with
various image augmentations based on real-world scenarios. This data set is the
first publicly available set in OMR research with sufficient size to train and
evaluate deep learning models. With the introduced augmentations a pitch
recognition accuracy of 81% and a duration accuracy of 94% is achieved,
resulting in a note level accuracy of 80%. Finally, the model is compared to
commercially available methods, showing a large improvements over these
applications.
",Computer Science,Computer Science
"Scene Graph Generation by Iterative Message Passing   Understanding a visual scene goes beyond recognizing individual objects in
isolation. Relationships between objects also constitute rich semantic
information about the scene. In this work, we explicitly model the objects and
their relationships using scene graphs, a visually-grounded graphical structure
of an image. We propose a novel end-to-end model that generates such structured
scene representation from an input image. The model solves the scene graph
inference problem using standard RNNs and learns to iteratively improves its
predictions via message passing. Our joint inference model can take advantage
of contextual cues to make better predictions on objects and their
relationships. The experiments show that our model significantly outperforms
previous methods for generating scene graphs using Visual Genome dataset and
inferring support relations with NYU Depth v2 dataset.
",Computer Science,Computer Science
"Weak quadrupole moments   Collective effects in deformed atomic nuclei present possible avenues of
study on the non-spherical distribution of neutrons and the violation of the
local Lorentz invariance. We introduce the weak quadrupole moment of nuclei,
related to the quadrupole distribution of the weak charge in the nucleus. The
weak quadrupole moment produces tensor weak interaction between the nucleus and
electrons and can be observed in atomic and molecular experiments measuring
parity nonconservation. The dominating contribution to the weak quadrupole is
given by the quadrupole moment of the neutron distribution, therefore,
corresponding experiments should allow one to measure the neutron quadrupoles.
Using the deformed oscillator model and the Schmidt model we calculate the
quadrupole distributions of neutrons, $Q_{n}$, the weak quadrupole moments
,$Q_{W}^{(2)}$, and the Lorentz Innvariance violating energy shifts in
$^{9}$Be, $^{21}$Ne , $^{27}$Al, $^{131}$Xe, $^{133}$Cs, $^{151}$Eu,
$^{153}$Eu, $^{163}$Dy, $^{167}$Er, $^{173}$Yb, $^{177}$Hf, $^{179}$Hf,
$^{181}$Ta, $^{201}$Hg and $^{229}$Th.
",Physics,Physics
"Most Complex Non-Returning Regular Languages   A regular language $L$ is non-returning if in the minimal deterministic
finite automaton accepting it there are no transitions into the initial state.
Eom, Han and Jirásková derived upper bounds on the state complexity of
boolean operations and Kleene star, and proved that these bounds are tight
using two different binary witnesses. They derived upper bounds for
concatenation and reversal using three different ternary witnesses. These five
witnesses use a total of six different transformations. We show that for each
$n\ge 4$ there exists a ternary witness of state complexity $n$ that meets the
bound for reversal and that at least three letters are needed to meet this
bound. Moreover, the restrictions of this witness to binary alphabets meet the
bounds for product, star, and boolean operations. We also derive tight upper
bounds on the state complexity of binary operations that take arguments with
different alphabets. We prove that the maximal syntactic semigroup of a
non-returning language has $(n-1)^n$ elements and requires at least
$\binom{n}{2}$ generators. We find the maximal state complexities of atoms of
non-returning languages. Finally, we show that there exists a most complex
non-returning language that meets the bounds for all these complexity measures.
",Computer Science,Computer Science
"Generalized Internal Boundaries (GIB)   Representing large-scale motions and topological changes in the finite volume
(FV) framework, while at the same time preserving the accuracy of the numerical
solution, is difficult. In this paper, we present a robust, highly efficient
method designed to achieve this capability. The proposed approach conceptually
shares many of the characteristics of the cut-cell interface tracking method,
but without the need for complex cell splitting/merging operations. The heart
of the new technique is to align existing mesh facets with the geometry to be
represented. We then modify the matrix contributions from these facets such
that they are represented in an identical fashion to traditional boundary
conditions. The collection of such faces is named a Generalised Internal
Boundary (GIB). In order to introduce motion into the system, we rely on the
classical ALE (Arbitrary Lagrangian-Eulerian) approach, but with the caveat
that the non-time-dependent motion of elements instantaneously crossing the
interface is handled separately from the time dependent component. The new
methodology is validated through comparison with: a) a body-fitted grid
simulation of an oscillating two dimensional cylinder and b) experimental
results of a butterfly valve.
",Computer Science; Physics,Physics
"On a family of Caldero-Chapoton algebras that have the Laurent phenomenon   We realize a family of generalized cluster algebras as Caldero-Chapoton
algebras of quivers with relations. Each member of this family arises from an
unpunctured polygon with one orbifold point of order 3, and is realized as a
Caldero-Chapoton algebra of a quiver with relations naturally associated to any
triangulation of the alluded polygon. The realization is done by defining for
every arc $j$ on the polygon with orbifold point a representation $M(j)$ of the
referred quiver with relations, and by proving that for every triangulation
$\tau$ and every arc $j\in\tau$, the product of the Caldero-Chapoton functions
of $M(j)$ and $M(j')$, where $j'$ is the arc that replaces $j$ when we flip $j$
in $\tau$, equals the corresponding exchange polynomial of Chekhov-Shapiro in
the generalized cluster algebra. Furthermore, we show that there is a bijection
between the set of generalized cluster variables and the isomorphism classes of
$E$-rigid indecomposable decorated representations of $\Lambda$.
",Mathematics,Mathematics
"On solving a restricted linear congruence using generalized Ramanujan sums   Consider the linear congruence equation $x_1+\ldots+x_k \equiv b\,(\text{mod
} n)$ for $b,n\in\mathbb{Z}$. By $(a,b)_s$, we mean the largest
$l^s\in\mathbb{N}$ which divides $a$ and $b$ simultaneously. For each $d_j|n$,
define $\mathcal{C}_{j,s} = \{1\leq x\leq n^s | (x,n^s)_s = d^s_j\}$. Bibak et
al. gave a formula using Ramanujan sums for the number of solutions of the
above congruence equation with some gcd restrictions on $x_i$. We generalize
their result with generalized gcd restrictions on $x_i$ by proving that for the
above linear congruence, the number of solutions is
$$\frac{1}{n^s}\sum\limits_{d|n}c_{d,s}(b)\prod\limits_{j=1}^{\tau(n)}\left(c_{\frac{n}{d_j},s}(\frac{n^s}{d^s})\right)^{g_j}$$
where $g_j = |\{x_1,\ldots, x_k\}\cap \mathcal{C}_{j,s}|$ for $j=1,\ldots
\tau(n)$ and $c_{d,s}$ denote the generalized ramanujan sum defined by E.
Cohen.
",Mathematics,Mathematics
"Tensors Come of Age: Why the AI Revolution will help HPC   This article discusses how the automation of tensor algorithms, based on A
Mathematics of Arrays and Psi Calculus, and a new way to represent numbers,
Unum Arithmetic, enables mechanically provable, scalable, portable, and more
numerically accurate software.
",Computer Science,Computer Science
"A Capillary Surface with No Radial Limits   In 1996, Kirk Lancaster and David Siegel investigated the existence and
behavior of radial limits at a corner of the boundary of the domain of
solutions of capillary and other prescribed mean curvature problems with
contact angle boundary data. In Theorem 3, they provide an example of a
capillary surface in a unit disk $D$ which has no radial limits at
$(0,0)\in\partial D.$ In their example, the contact angle ($\gamma$) cannot be
bounded away from zero and $\pi.$
Here we consider a domain $\Omega$ with a convex corner at $(0,0)$ and find a
capillary surface $z=f(x,y)$ in $\Omega\times\mathbb{R}$ which has no radial
limits at $(0,0)\in\partial\Omega$ such that $\gamma$ is bounded away from $0$
and $\pi.$
",Mathematics,Mathematics
"On Robust Tie-line Scheduling in Multi-Area Power Systems   The tie-line scheduling problem in a multi-area power system seeks to
optimize tie-line power flows across areas that are independently operated by
different system operators (SOs). In this paper, we leverage the theory of
multi-parametric linear programming to propose algorithms for optimal tie-line
scheduling within a deterministic and a robust optimization framework. Through
a coordinator, the proposed algorithms are proved to converge to the optimal
schedule within a finite number of iterations. A key feature of the proposed
algorithms, besides their finite step convergence, is the privacy of the
information exchanges; the SO in an area does not need to reveal its dispatch
cost structure, network constraints, or the nature of the uncertainty set to
the coordinator. The performance of the algorithms is evaluated using several
power system examples.
",Mathematics,Computer Science
"INtERAcT: Interaction Network Inference from Vector Representations of Words   In recent years, the number of biomedical publications has steadfastly grown,
resulting in a rich source of untapped new knowledge. Most biomedical facts are
however not readily available, but buried in the form of unstructured text, and
hence their exploitation requires the time-consuming manual curation of
published articles. Here we present INtERAcT, a novel approach to extract
protein-protein interactions from a corpus of biomedical articles related to a
broad range of scientific domains in a completely unsupervised way. INtERAcT
exploits vector representation of words, computed on a corpus of domain
specific knowledge, and implements a new metric that estimates an interaction
score between two molecules in the space where the corresponding words are
embedded. We demonstrate the power of INtERAcT by reconstructing the molecular
pathways associated to 10 different cancer types using a corpus of
disease-specific articles for each cancer type. We evaluate INtERAcT using
STRING database as a benchmark, and show that our metric outperforms currently
adopted approaches for similarity computation at the task of identifying known
molecular interactions in all studied cancer types. Furthermore, our approach
does not require text annotation, manual curation or the definition of semantic
rules based on expert knowledge, and hence it can be easily and efficiently
applied to different scientific domains. Our findings suggest that INtERAcT may
increase our capability to summarize the understanding of a specific disease
using the published literature in an automated and completely unsupervised
fashion.
",Quantitative Biology,Computer Science
"Non-Hamiltonian isotopic Lagrangians on the one-point blow-up of CP^2   We show that two Hamiltonian isotopic Lagrangians in
(CP^2,\omega_\textup{FS}) induce two Lagrangian submanifolds in the one-point
blow-up (\widetilde{CP}^2,\widetilde{\omega}_\rho) that are not Hamiltonian
isotopic. Furthermore, we show that for any integer k>1 there are k Hamiltonian
isotopic Lagrangians in (CP^2,\omega_\textup{FS}) that induce k Lagrangian
submanifolds in the one-point blow-up such that no two of them are Hamiltonian
isotopic.
",Mathematics,Mathematics
"Large Scale Automated Forecasting for Monitoring Network Safety and Security   Real time large scale streaming data pose major challenges to forecasting, in
particular defying the presence of human experts to perform the corresponding
analysis. We present here a class of models and methods used to develop an
automated, scalable and versatile system for large scale forecasting oriented
towards safety and security monitoring. Our system provides short and long term
forecasts and uses them to detect safety and security issues in relation with
multiple internet connected devices well in advance they might take place.
",Statistics,Computer Science
"Neurofeedback: principles, appraisal and outstanding issues   Neurofeedback is a form of brain training in which subjects are fed back
information about some measure of their brain activity which they are
instructed to modify in a way thought to be functionally advantageous. Over the
last twenty years, NF has been used to treat various neurological and
psychiatric conditions, and to improve cognitive function in various contexts.
However, despite its growing popularity, each of the main steps in NF comes
with its own set of often covert assumptions. Here we critically examine some
conceptual and methodological issues associated with the way general objectives
and neural targets of NF are defined, and review the neural mechanisms through
which NF may act, and the way its efficacy is gauged. The NF process is
characterised in terms of functional dynamics, and possible ways in which it
may be controlled are discussed. Finally, it is proposed that improving NF will
require better understanding of various fundamental aspects of brain dynamics
and a more precise definition of functional brain activity and brain-behaviour
relationships.
",Quantitative Biology,Quantitative Biology
"Meta Networks   Neural networks have been successfully applied in applications with a large
amount of labeled data. However, the task of rapid generalization on new
concepts with small training data while preserving performances on previously
learned ones still presents a significant challenge to neural network models.
In this work, we introduce a novel meta learning method, Meta Networks
(MetaNet), that learns a meta-level knowledge across tasks and shifts its
inductive biases via fast parameterization for rapid generalization. When
evaluated on Omniglot and Mini-ImageNet benchmarks, our MetaNet models achieve
a near human-level performance and outperform the baseline approaches by up to
6% accuracy. We demonstrate several appealing properties of MetaNet relating to
generalization and continual learning.
",Computer Science; Statistics,Statistics
"Playing Pairs with Pepper   As robots become increasingly prevalent in almost all areas of society, the
factors affecting humans trust in those robots becomes increasingly important.
This paper is intended to investigate the factor of robot attributes, looking
specifically at the relationship between anthropomorphism and human development
of trust. To achieve this, an interaction game, Matching the Pairs, was
designed and implemented on two robots of varying levels of anthropomorphism,
Pepper and Husky. Participants completed both pre- and post-test questionnaires
that were compared and analyzed predominantly with the use of quantitative
methods, such as paired sample t-tests. Post-test analyses suggested a positive
relationship between trust and anthropomorphism with $80\%$ of participants
confirming that the robots' adoption of facial features assisted in
establishing trust. The results also indicated a positive relationship between
interaction and trust with $90\%$ of participants confirming this for both
robots post-test
",Computer Science,Computer Science
"Program Synthesis from Visual Specification   Program synthesis is the process of automatically translating a specification
into computer code. Traditional synthesis settings require a formal, precise
specification. Motivated by computer education applications where a student
learns to code simple turtle-style drawing programs, we study a novel synthesis
setting where only a noisy user-intention drawing is specified. This allows
students to sketch their intended output, optionally together with their own
incomplete program, to automatically produce a completed program. We formulate
this synthesis problem as search in the space of programs, with the score of a
state being the Hausdorff distance between the program output and the user
drawing. We compare several search algorithms on a corpus consisting of real
user drawings and the corresponding programs, and demonstrate that our
algorithms can synthesize programs optimally satisfying the specification.
",Computer Science,Computer Science
"The cavity approach for Steiner trees packing problems   The Belief Propagation approximation, or cavity method, has been recently
applied to several combinatorial optimization problems in its zero-temperature
implementation, the max-sum algorithm. In particular, recent developments to
solve the edge-disjoint paths problem and the prize-collecting Steiner tree
problem on graphs have shown remarkable results for several classes of graphs
and for benchmark instances. Here we propose a generalization of these
techniques for two variants of the Steiner trees packing problem where multiple
""interacting"" trees have to be sought within a given graph. Depending on the
interaction among trees we distinguish the vertex-disjoint Steiner trees
problem, where trees cannot share nodes, from the edge-disjoint Steiner trees
problem, where edges cannot be shared by trees but nodes can be members of
multiple trees. Several practical problems of huge interest in network design
can be mapped into these two variants, for instance, the physical design of
Very Large Scale Integration (VLSI) chips. The formalism described here relies
on two components edge-variables that allows us to formulate a massage-passing
algorithm for the V-DStP and two algorithms for the E-DStP differing in the
scaling of the computational time with respect to some relevant parameters. We
will show that one of the two formalisms used for the edge-disjoint variant
allow us to map the max-sum update equations into a weighted maximum matching
problem over proper bipartite graphs. We developed a heuristic procedure based
on the max-sum equations that shows excellent performance in synthetic networks
(in particular outperforming standard multi-step greedy procedures by large
margins) and on large benchmark instances of VLSI for which the optimal
solution is known, on which the algorithm found the optimum in two cases and
the gap to optimality was never larger than 4 %.
",Computer Science,Computer Science
"Soft Weight-Sharing for Neural Network Compression   The success of deep learning in numerous application domains created the de-
sire to run and train them on mobile devices. This however, conflicts with
their computationally, memory and energy intense nature, leading to a growing
interest in compression. Recent work by Han et al. (2015a) propose a pipeline
that involves retraining, pruning and quantization of neural network weights,
obtaining state-of-the-art compression rates. In this paper, we show that
competitive compression rates can be achieved by using a version of soft
weight-sharing (Nowlan & Hinton, 1992). Our method achieves both quantization
and pruning in one simple (re-)training procedure. This point of view also
exposes the relation between compression and the minimum description length
(MDL) principle.
",Statistics,Computer Science; Statistics
"Structural subnetwork evolution across the life-span: rich-club, feeder, seeder   The impact of developmental and aging processes on brain connectivity and the
connectome has been widely studied. Network theoretical measures and certain
topological principles are computed from the entire brain, however there is a
need to separate and understand the underlying subnetworks which contribute
towards these observed holistic connectomic alterations. One organizational
principle is the rich-club - a core subnetwork of brain regions that are
strongly connected, forming a high-cost, high-capacity backbone that is
critical for effective communication in the network. Investigations primarily
focus on its alterations with disease and age. Here, we present a systematic
analysis of not only the rich-club, but also other subnetworks derived from
this backbone - namely feeder and seeder subnetworks. Our analysis is applied
to structural connectomes in a normal cohort from a large, publicly available
lifespan study. We demonstrate changes in rich-club membership with age
alongside a shift in importance from 'peripheral' seeder to feeder subnetworks.
Our results show a refinement within the rich-club structure (increase in
transitivity and betweenness centrality), as well as increased efficiency in
the feeder subnetwork and decreased measures of network integration and
segregation in the seeder subnetwork. These results demonstrate the different
developmental patterns when analyzing the connectome stratified according to
its rich-club and the potential of utilizing this subnetwork analysis to reveal
the evolution of brain architectural alterations across the life-span.
",Quantitative Biology,Quantitative Biology
"Supervised Speech Separation Based on Deep Learning: An Overview   Speech separation is the task of separating target speech from background
interference. Traditionally, speech separation is studied as a signal
processing problem. A more recent approach formulates speech separation as a
supervised learning problem, where the discriminative patterns of speech,
speakers, and background noise are learned from training data. Over the past
decade, many supervised separation algorithms have been put forward. In
particular, the recent introduction of deep learning to supervised speech
separation has dramatically accelerated progress and boosted separation
performance. This article provides a comprehensive overview of the research on
deep learning based supervised speech separation in the last several years. We
first introduce the background of speech separation and the formulation of
supervised separation. Then we discuss three main components of supervised
separation: learning machines, training targets, and acoustic features. Much of
the overview is on separation algorithms where we review monaural methods,
including speech enhancement (speech-nonspeech separation), speaker separation
(multi-talker separation), and speech dereverberation, as well as
multi-microphone techniques. The important issue of generalization, unique to
supervised learning, is discussed. This overview provides a historical
perspective on how advances are made. In addition, we discuss a number of
conceptual issues, including what constitutes the target source.
",Computer Science,Computer Science
"Using Big Data Technologies for HEP Analysis   The HEP community is approaching an era were the excellent performances of
the particle accelerators in delivering collision at high rate will force the
experiments to record a large amount of information. The growing size of the
datasets could potentially become a limiting factor in the capability to
produce scientific results timely and efficiently. Recently, new technologies
and new approaches have been developed in industry to answer to the necessity
to retrieve information as quickly as possible to analyze PB and EB datasets.
Providing the scientists with these modern computing tools will lead to
rethinking the principles of data analysis in HEP, making the overall
scientific process faster and smoother.
In this paper, we are presenting the latest developments and the most recent
results on the usage of Apache Spark for HEP analysis. The study aims at
evaluating the efficiency of the application of the new tools both
quantitatively, by measuring the performances, and qualitatively, focusing on
the user experience. The first goal is achieved by developing a data reduction
facility: working together with CERN Openlab and Intel, CMS replicates a real
physics search using Spark-based technologies, with the ambition of reducing 1
PB of public data in 5 hours, collected by the CMS experiment, to 1 TB of data
in a format suitable for physics analysis.
The second goal is achieved by implementing multiple physics use-cases in
Apache Spark using as input preprocessed datasets derived from official CMS
data and simulation. By performing different end-analyses up to the publication
plots on different hardware, feasibility, usability and portability are
compared to the ones of a traditional ROOT-based workflow.
",Computer Science,Computer Science
"Current induced magnetization switching in PtCoCr structures with enhanced perpendicular magnetic anisotropy and spin-orbit torques   Magnetic trilayers having large perpendicular magnetic anisotropy (PMA) and
high spin-orbit torques (SOTs) efficiency are the key to fabricate nonvolatile
magnetic memory and logic devices. In this work, PMA and SOTs are
systematically studied in Pt/Co/Cr stacks as a function of Cr thickness. An
enhanced perpendicular anisotropy field around 10189 Oe is obtained and is
related to the interface between Co and Cr layers. In addition, an effective
spin Hall angle up to 0.19 is observed due to the improved antidamping-like
torque by employing dissimilar metals Pt and Cr with opposite signs of spin
Hall angles on opposite sides of Co layer. Finally, we observed a nearly linear
dependence between spin Hall angle and longitudinal resistivity from their
temperature dependent properties, suggesting that the spin Hall effect may
arise from extrinsic skew scattering mechanism. Our results indicate that 3d
transition metal Cr with a large negative spin Hall angle could be used to
engineer the interfaces of trilayers to enhance PMA and SOTs.
",Physics,Physics
"Comparative Benchmarking of Causal Discovery Techniques   In this paper we present a comprehensive view of prominent causal discovery
algorithms, categorized into two main categories (1) assuming acyclic and no
latent variables, and (2) allowing both cycles and latent variables, along with
experimental results comparing them from three perspectives: (a) structural
accuracy, (b) standard predictive accuracy, and (c) accuracy of counterfactual
inference. For (b) and (c) we train causal Bayesian networks with structures as
predicted by each causal discovery technique to carry out counterfactual or
standard predictive inference. We compare causal algorithms on two pub- licly
available and one simulated datasets having different sample sizes: small,
medium and large. Experiments show that structural accuracy of a technique does
not necessarily correlate with higher accuracy of inferencing tasks. Fur- ther,
surveyed structure learning algorithms do not perform well in terms of
structural accuracy in case of datasets having large number of variables.
",Computer Science; Statistics,Statistics
"Structure of martingale transports in finite dimensions   We study the structure of martingale transports in finite dimensions. We
consider the family $\mathcal{M}(\mu,\nu) $ of martingale measures on
$\mathbb{R}^N \times \mathbb{R}^N$ with given marginals $\mu,\nu$, and
construct a family of relatively open convex sets $\{C_x:x\in \mathbb{R}^N \}$,
which forms a partition of $\mathbb{R}^N$, and such that any martingale
transport in $\mathcal{M}(\mu,\nu) $ sends mass from $x$ to within
$\overline{C_x}$, $\mu(dx)$--a.e. Our results extend the analogous
one-dimensional results of M. Beiglböck and N. Juillet (2016) and M.
Beiglböck, M. Nutz, and N. Touzi (2015). We conjecture that the decomposition
is canonical and minimal in the sense that it allows to characterise the
martingale polar sets, i.e. the sets which have zero mass under all measures in
$\mathcal{M}(\mu,\nu)$, and offers the martingale analogue of the
characterisation of transport polar sets proved in M. Beiglböck, M.
Goldstern, G. Maresch, and W. Schachermayer (2009).
",Mathematics,Mathematics
"On Gallai's and Hajós' Conjectures for graphs with treewidth at most 3   A path (resp. cycle) decomposition of a graph $G$ is a set of edge-disjoint
paths (resp. cycles) of $G$ that covers the edge set of $G$. Gallai (1966)
conjectured that every graph on $n$ vertices admits a path decomposition of
size at most $\lfloor (n+1)/2\rfloor$, and Hajós (1968) conjectured that
every Eulerian graph on $n$ vertices admits a cycle decomposition of size at
most $\lfloor (n-1)/2\rfloor$. Gallai's Conjecture was verified for many
classes of graphs. In particular, Lovász (1968) verified this conjecture for
graphs with at most one vertex of even degree, and Pyber (1996) verified it for
graphs in which every cycle contains a vertex of odd degree. Hajós'
Conjecture, on the other hand, was verified only for graphs with maximum degree
$4$ and for planar graphs. In this paper, we verify Gallai's and Hajós'
Conjectures for graphs with treewidth at most $3$. Moreover, we show that the
only graphs with treewidth at most $3$ that do not admit a path decomposition
of size at most $\lfloor n/2\rfloor$ are isomorphic to $K_3$ or $K_5-e$.
Finally, we use the technique developed in this paper to present new proofs for
Gallai's and Hajós' Conjectures for graphs with maximum degree at most $4$,
and for planar graphs with girth at least $6$.
",Computer Science,Mathematics
"Application of Convolutional Neural Network to Predict Airfoil Lift Coefficient   The adaptability of the convolutional neural network (CNN) technique for
aerodynamic meta-modeling tasks is probed in this work. The primary objective
is to develop suitable CNN architecture for variable flow conditions and object
geometry, in addition to identifying a sufficient data preparation process.
Multiple CNN structures were trained to learn the lift coefficients of the
airfoils with a variety of shapes in multiple flow Mach numbers, Reynolds
numbers, and diverse angles of attack. This is conducted to illustrate the
concept of the technique. A multi-layered perceptron (MLP) is also used for the
training sets. The MLP results are compared with that of the CNN results. The
newly proposed meta-modeling concept has been found to be comparable with the
MLP in learning capability; and more importantly, our CNN model exhibits a
competitive prediction accuracy with minimal constraints in a geometric
representation.
",Computer Science; Statistics,Computer Science; Statistics
"An Uncertainty Principle for Estimates of Floquet Multipliers   We derive a Cramér-Rao lower bound for the variance of Floquet multiplier
estimates that have been constructed from stable limit cycles perturbed by
noise. To do so, we consider perturbed periodic orbits in the plane. We use a
periodic autoregressive process to model the intersections of these orbits with
cross sections, then passing to the limit of a continuum of sections to obtain
a bound that depends on the continuous flow restricted to the (nontrivial)
Floquet mode. We compare our bound against the empirical variance of estimates
constructed using several cross sections. The section-based estimates are close
to being optimal. We posit that the utility of our bound persists in higher
dimensions when computed along Floquet modes for real and distinct multipliers.
Our bound elucidates some of the empirical observations noted in the
literature; e.g., (a) it is the number of cycles (as opposed to the frequency
of observations) that drives the variance of estimates to zero, and (b) the
estimator variance has a positive lower bound as the noise amplitude tends to
zero.
",Statistics,Mathematics; Statistics
"Bistability of Cavity Magnon Polaritons   We report the first observation of the magnon-polariton bistability in a
cavity magnonics system consisting of cavity photons strongly interacting with
the magnons in a small yttrium iron garnet (YIG) sphere. The bistable behaviors
are emerged as sharp frequency switchings of the cavity magnon-polaritons
(CMPs) and related to the transition between states with large and small number
of polaritons. In our experiment, we align, respectively, the [100] and [110]
crystallographic axes of the YIG sphere parallel to the static magnetic field
and find very different bistable behaviors (e.g., clockwise and
counter-clockwise hysteresis loops) in these two cases. The experimental
results are well fitted and explained as being due to the Kerr nonlinearity
with either positive or negative coefficient. Moreover, when the magnetic field
is tuned away from the anticrossing point of CMPs, we observe simultaneous
bistability of both magnons and cavity photons by applying a drive field on the
lower branch.
",Physics,Physics
"Continuity properties for Born-Jordan operators with symbols in Hörmander classes and modulation spaces   We show that the Weyl symbol of a Born-Jordan operator is in the same class
as the Born-Jordan symbol, when Hörmander symbols and certain types of
modulation spaces are used as symbol classes. We use these properties to carry
over continuity and Schatten-von Neumann properties to the Born-Jordan
calculus.
",Mathematics,Mathematics
"Limitations on Variance-Reduction and Acceleration Schemes for Finite Sum Optimization   We study the conditions under which one is able to efficiently apply
variance-reduction and acceleration schemes on finite sum optimization
problems. First, we show that, perhaps surprisingly, the finite sum structure
by itself, is not sufficient for obtaining a complexity bound of
$\tilde{\cO}((n+L/\mu)\ln(1/\epsilon))$ for $L$-smooth and $\mu$-strongly
convex individual functions - one must also know which individual function is
being referred to by the oracle at each iteration. Next, we show that for a
broad class of first-order and coordinate-descent finite sum algorithms
(including, e.g., SDCA, SVRG, SAG), it is not possible to get an `accelerated'
complexity bound of $\tilde{\cO}((n+\sqrt{n L/\mu})\ln(1/\epsilon))$, unless
the strong convexity parameter is given explicitly. Lastly, we show that when
this class of algorithms is used for minimizing $L$-smooth and convex finite
sums, the optimal complexity bound is $\tilde{\cO}(n+L/\epsilon)$, assuming
that (on average) the same update rule is used in every iteration, and
$\tilde{\cO}(n+\sqrt{nL/\epsilon})$, otherwise.
",Computer Science; Mathematics; Statistics,Computer Science; Mathematics
"Predicting Pulsar Scintillation from Refractive Plasma Sheets   The dynamic and secondary spectra of many pulsars show evidence for
long-lived, aligned images of the pulsar that are stationary on a thin
scattering sheet. One explanation for this phenomenon considers the effects of
wave crests along sheets in the ionized interstellar medium, such as those due
to Alfvén waves propagating along current sheets. If these sheets are closely
aligned to our line-of-sight to the pulsar, high bending angles arise at the
wave crests and a selection effect causes alignment of images produced at
different crests, similar to grazing reflection off of a lake. Using geometric
optics, we develop a simple parameterized model of these corrugated sheets that
can be constrained with a single observation and that makes observable
predictions for variations in the scintillation of the pulsar over time and
frequency. This model reveals qualitative differences between lensing from
overdense and underdense corrugated sheets: Only if the sheet is overdense
compared to the surrounding interstellar medium can the lensed images be
brighter than the line-of-sight image to the pulsar, and the faint lensed
images are closer to the pulsar at higher frequencies if the sheet is
underdense, but at lower frequencies if the sheet is overdense.
",Physics,Physics
"Exceptional points in two simple textbook examples   We propose to introduce the concept of exceptional points in intermediate
courses on mathematics and classical mechanics by means of simple textbook
examples. The first one is an ordinary second-order differential equation with
constant coefficients. The second one is the well known damped harmonic
oscillator. They enable one to connect the occurrence of linearly dependent
exponential solutions with a defective matrix that cannot be diagonalized but
can be transformed into a Jordan canonical form.
",Physics,Mathematics
"Inflationary Features and Shifts in Cosmological Parameters from Planck 2015 Data   We explore the relationship between features in the Planck 2015 temperature
and polarization data, shifts in the cosmological parameters, and features from
inflation. Residuals in the temperature data at low multipole $\ell$, which are
responsible for the high $H_0\approx 70$ km s$^{-1}$Mpc$^{-1}$ and low
$\sigma_8\Omega_m^{1/2}$ values from $\ell<1000$ in power-law $\Lambda$CDM
models, are better fit to inflationary features with a $1.9\sigma$ preference
for running of the running of the tilt or a stronger $99\%$ CL local
significance preference for a sharp drop in power around $k=0.004$ Mpc$^{-1}$
in generalized slow roll and a lower $H_0\approx 67$ km s$^{-1}$Mpc$^{-1}$. The
same in-phase acoustic residuals at $\ell>1000$ that drive the global $H_0$
constraints and appear as a lensing anomaly also favor running parameters which
allow even lower $H_0$, but not once lensing reconstruction is considered.
Polarization spectra are intrinsically highly sensitive to these parameter
shifts, and even more so in the Planck 2015 TE data due to an outlier at $\ell
\approx 165$, which disfavors the best fit $H_0$ $\Lambda$CDM solution by more
than $2\sigma$, and high $H_0$ value at almost $3\sigma$. Current polarization
data also slightly enhance the significance of a sharp suppression of
large-scale power but leave room for large improvements in the future with
cosmic variance limited $E$-mode measurements.
",Physics,Physics
"Universality of group embeddability   Working in the framework of Borel reducibility, we study various notions of
embeddability between groups. We prove that the embeddability between countable
groups, the topological embeddability between (discrete) Polish groups, and the
isometric embeddability between separable groups with a bounded bi-invariant
complete metric are all invariantly universal analytic quasi-orders. This
strengthens some results from [Wil14] and [FLR09].
",Mathematics,Mathematics
"Detection of virial shocks in stacked Fermi-LAT clusters   Galaxy clusters are thought to grow by accreting mass through large-scale,
strong, yet elusive, virial shocks. Such a shock is expected to accelerate
relativistic electrons, thus generating a spectrally-flat leptonic virial-ring.
However, until now, only the nearby Coma cluster has shown evidence for a
$\gamma$-ray virial ring. We stack Fermi-LAT data for the 112 most massive,
high latitude, extended clusters, enhancing the ring sensitivity by rescaling
clusters to their virial radii and utilizing the expected flat energy spectrum.
In addition to a central unresolved, hard signal (detected at the $\sim
5.8\sigma$ confidence level), probably dominated by AGN, we identify (at the
$5.8\sigma$ confidence level) a bright, spectrally-flat $\gamma$-ray ring at
the expected virial shock position. The ring signal implies that the shock
deposits $\sim 0.6\%$ (with an interpretation uncertainty factor $\sim2$) of
the thermal energy in relativistic electrons over a Hubble time. This result,
consistent with the Coma signal, validates and calibrates the virial shock
model, and indicates that the cumulative emission from such shocks
significantly contributes to the diffuse extragalactic $\gamma$-ray and
low-frequency radio backgrounds.
",Physics,Physics
"Generalized $k$-core pruning process on directed networks   The resilience of a complex interconnected system concerns the size of the
macroscopic functioning node clusters after external perturbations based on a
random or designed scheme. For a representation of the interconnected systems
with directional or asymmetrical interactions among constituents, the directed
network is a convenient choice. Yet how the interaction directions affect the
network resilience still lacks thorough exploration. Here, we study the
resilience of directed networks with a generalized $k$-core pruning process as
a simple failure procedure based on both the in- and out-degrees of nodes, in
which any node with an in-degree $< k_{in}$ or an out-degree $< k_{ou}$ is
removed iteratively. With an explicitly analytical framework, we can predict
the relative sizes of residual node clusters on uncorrelated directed random
graphs. We show that the discontinuous transitions rise for cases with $k_{in}
\geq 2$ or $k_{ou} \geq 2$, and the unidirectional interactions among nodes
drive the networks more vulnerable against perturbations based on in- and
out-degrees separately.
",Physics,Computer Science; Physics
"Magnetic polarons in a nonequilibrium polariton condensate   We consider a condensate of exciton-polaritons in a diluted magnetic
semiconductor microcavity. Such system may exhibit magnetic self-trapping in
the case of sufficiently strong coupling between polaritons and magnetic ions
embedded in the semiconductor. We investigate the effect of the nonequilibrium
nature of exciton-polaritons on the physics of the resulting self-trapped
magnetic polarons. We find that multiple polarons can exist at the same time,
and derive a critical condition for self-trapping which is different to the one
predicted previously in the equilibrium case. Using the Bogoliubov-de Gennes
approximation, we calculate the excitation spectrum and provide a physical
explanation in terms of the effective magnetic attraction between polaritons,
mediated by the ion subsystem.
",Physics,Physics
"Periodic Airy process and equilibrium dynamics of edge fermions in a trap   We establish an exact mapping between (i) the equilibrium (imaginary time)
dynamics of non-interacting fermions trapped in a harmonic potential at
temperature $T=1/\beta$ and (ii) non-intersecting Ornstein-Uhlenbeck (OU)
particles constrained to return to their initial positions after time $\beta$.
Exploiting the determinantal structure of the process we compute the universal
correlation functions both in the bulk and at the edge of the trapped Fermi
gas. The latter corresponds to the top path of the non-intersecting OU
particles, and leads us to introduce and study the time-periodic Airy$_2$
process, ${\cal A}^b_2(u)$, depending on a single parameter, the period $b$.
The standard Airy$_2$ process is recovered for $b=+\infty$. We discuss
applications of our results to the real time quantum dynamics of trapped
fermions.
",Physics; Mathematics,Physics
"Rule Formats for Nominal Process Calculi   The nominal transition systems (NTSs) of Parrow et al. describe the
operational semantics of nominal process calculi. We study NTSs in terms of the
nominal residual transition systems (NRTSs) that we introduce. We provide rule
formats for the specifications of NRTSs that ensure that the associated NRTS is
an NTS and apply them to the operational specifications of the early and late
pi-calculus. We also explore alternative specifications of the NTSs in which we
allow residuals of abstraction sort, and introduce translations between the
systems with and without residuals of abstraction sort. Our study stems from
the Nominal SOS of Cimini et al. and from earlier works in nominal sets and
nominal logic by Gabbay, Pitts and their collaborators.
",Computer Science,Computer Science
"A bound for the shortest reset words for semisimple synchronizing automata via the packing number   We show that if a semisimple synchronizing automaton with $n$ states has a
minimal reachable non-unary subset of cardinality $r\ge 2$, then there is a
reset word of length at most $(n-1)D(2,r,n)$, where $D(2,r,n)$ is the
$2$-packing number for families of $r$-subsets of $[1,n]$.
",Computer Science; Mathematics,Computer Science
"Renormalization group theory for percolation in time-varying networks   Motivated by multi-hop communication in unreliable wireless networks, we
present a percolation theory for time-varying networks. We develop a
renormalization group theory for a prototypical network on a regular grid,
where individual links switch stochastically between active and inactive
states. The question whether a given source node can communicate with a
destination node along paths of active links is equivalent to a percolation
problem. Our theory maps the temporal existence of multi-hop paths on an
effective two-state Markov process. We show analytically how this Markov
process converges towards a memory-less Bernoulli process as the hop distance
between source and destination node increases. Our work extends classical
percolation theory to the dynamic case and elucidates temporal correlations of
message losses. Quantification of temporal correlations has implications for
the design of wireless communication and control protocols, e.g. in
cyber-physical systems such as self-organized swarms of drones or smart traffic
networks.
",Computer Science; Physics,Computer Science
"From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood   Our goal is to learn a semantic parser that maps natural language utterances
into executable programs when only indirect supervision is available: examples
are labeled with the correct execution result, but not the program itself.
Consequently, we must search the space of programs for those that output the
correct result, while not being misled by spurious programs: incorrect programs
that coincidentally output the correct result. We connect two common learning
paradigms, reinforcement learning (RL) and maximum marginal likelihood (MML),
and then present a new learning algorithm that combines the strengths of both.
The new algorithm guards against spurious programs by combining the systematic
search traditionally employed in MML with the randomized exploration of RL, and
by updating parameters such that probability is spread more evenly across
consistent programs. We apply our learning algorithm to a new neural semantic
parser and show significant gains over existing state-of-the-art results on a
recent context-dependent semantic parsing task.
",Computer Science; Statistics,Computer Science
"Evaluation of Trace Alignment Quality and its Application in Medical Process Mining   Trace alignment algorithms have been used in process mining for discovering
the consensus treatment procedures and process deviations. Different alignment
algorithms, however, may produce very different results. No widely-adopted
method exists for evaluating the results of trace alignment. Existing
reference-free evaluation methods cannot adequately and comprehensively assess
the alignment quality. We analyzed and compared the existing evaluation
methods, identifying their limitations, and introduced improvements in two
reference-free evaluation methods. Our approach assesses the alignment result
globally instead of locally, and therefore helps the algorithm to optimize
overall alignment quality. We also introduced a novel metric to measure the
alignment complexity, which can be used as a constraint on alignment algorithm
optimization. We tested our evaluation methods on a trauma resuscitation
dataset and provided the medical explanation of the activities and patterns
identified as deviations using our proposed evaluation methods.
",Computer Science,Computer Science; Statistics
"Band structure engineered layered metals for low-loss plasmonics   Plasmonics currently faces the problem of seemingly inevitable optical losses
occurring in the metallic components that challenges the implementation of
essentially any application. In this work we show that Ohmic losses are reduced
in certain layered metals, such as the transition metal dichalcogenide TaS$_2$,
due to an extraordinarily small density of states for scattering in the near-IR
originating from their special electronic band structure. Based on this
observation we propose a new class of band structure engineered van der Waals
layered metals composed of hexagonal transition metal chalcogenide-halide
layers with greatly suppressed intrinsic losses. Using first-principles
calculations we show that the suppression of optical losses lead to improved
performance for thin film waveguiding and transformation optics.
",Physics,Physics
"Device-Aware Routing and Scheduling in Multi-Hop Device-to-Device Networks   The dramatic increase in data and connectivity demand, in addition to
heterogeneous device capabilities, poses a challenge for future wireless
networks. One of the promising solutions is Device-to-Device (D2D) networking.
D2D networking, advocating the idea of connecting two or more devices directly
without traversing the core network, is promising to address the increasing
data and connectivity demand. In this paper, we consider D2D networks, where
devices with heterogeneous capabilities including computing power, energy
limitations, and incentives participate in D2D activities heterogeneously. We
develop (i) a device-aware routing and scheduling algorithm (DARS) by taking
into account device capabilities, and (ii) a multi-hop D2D testbed using
Android-based smartphones and tablets by exploiting Wi-Fi Direct and legacy
Wi-Fi connections. We show that DARS significantly improves throughput in our
testbed as compared to state-of-the-art.
",Computer Science,Computer Science
"On a property of the nodal set of least energy sign-changing solutions for quasilinear elliptic equations   In this note we prove the Payne-type conjecture about the behaviour of the
nodal set of least energy sign-changing solutions for the equation $-\Delta_p u
= f(u)$ in bounded Steiner symmetric domains $\Omega \subset \mathbb{R}^N$
under the zero Dirichlet boundary conditions. The nonlinearity $f$ is assumed
to be either superlinear or resonant. In the latter case, least energy
sign-changing solutions are second eigenfunctions of the zero Dirichlet
$p$-Laplacian in $\Omega$. We show that the nodal set of any least energy
sign-changing solution intersects the boundary of $\Omega$. The proof is based
on a moving polarization argument.
",Mathematics,Mathematics
"Anisotropic functional Laplace deconvolution   In the present paper we consider the problem of estimating a
three-dimensional function $f$ based on observations from its noisy Laplace
convolution. Our study is motivated by the analysis of Dynamic Contrast
Enhanced (DCE) imaging data. We construct an adaptive wavelet-Laguerre
estimator of $f$, derive minimax lower bounds for the $L^2$-risk when $f$
belongs to a three-dimensional Laguerre-Sobolev ball and demonstrate that the
wavelet-Laguerre estimator is adaptive and asymptotically near-optimal in a
wide range of Laguerre-Sobolev spaces. We carry out a limited simulations study
and show that the estimator performs well in a finite sample setting. Finally,
we use the technique for the solution of the Laplace deconvolution problem on
the basis of DCE Computerized Tomography data.
",Statistics,Mathematics; Statistics
"Molecular Beam Epitaxy Growth of [CrGe/MnGe/FeGe] Superlattices: Toward Artificial B20 Skyrmion Materials with Tunable Interactions   Skyrmions are localized magnetic spin textures whose stability has been shown
theoretically to depend on material parameters including bulk Dresselhaus spin
orbit coupling (SOC), interfacial Rashba SOC, and magnetic anisotropy. Here, we
establish the growth of a new class of artificial skyrmion materials, namely
B20 superlattices, where these parameters could be systematically tuned.
Specifically, we report the successful growth of B20 superlattices comprised of
single crystal thin films of FeGe, MnGe, and CrGe on Si(111) substrates. Thin
films and superlattices are grown by molecular beam epitaxy and are
characterized through a combination of reflection high energy electron
diffraction, x-ray diffraction, and cross-sectional scanning transmission
electron microscopy (STEM). X-ray energy dispersive spectroscopy (XEDS)
distinguishes layers by elemental mapping and indicates good interface quality
with relatively low levels of intermixing in the [CrGe/MnGe/FeGe] superlattice.
This demonstration of epitaxial, single-crystalline B20 superlattices is a
significant advance toward tunable skyrmion systems for fundamental scientific
studies and applications in magnetic storage and logic.
",Physics,Physics
"Applying Gromov's Amenable Localization to Geodesic Flows   Let $M$ be a compact connected smooth Riemannian $n$-manifold with boundary.
We combine Gromov's amenable localization technique with the Poincaré
duality to study the traversally generic geodesic flows on $SM$, the space of
the spherical tangent bundle. Such flows generate stratifications of $SM$,
governed by rich universal combinatorics. The stratification reflects the ways
in which the geodesic flow trajectories interact with the boundary $\d(SM)$.
Specifically, we get lower estimates of the numbers of connected components of
these flow-generated strata of any given codimension $k$. These universal
bounds are expressed in terms of the normed homology $H_k(M; \R)$ and $H_k(DM;
\R)$, where $DM = M\cup_{\d M} M$ denotes the double of $M$. The norms here are
the Gromov simplicial semi-norms in homology. The more complex the metric on
$M$ is, the more numerous the strata of $SM$ and $S(DM)$ are. So one may regard
our estimates as analogues of the Morse inequalities for the geodesics on
manifolds with boundary.
It turns out that some close relatives of the normed homology spaces form
obstructions to the existence of globally $k$-convex traversally generic
metrics on $M$.
",Mathematics,Mathematics
"Knowledge Transfer for Out-of-Knowledge-Base Entities: A Graph Neural Network Approach   Knowledge base completion (KBC) aims to predict missing information in a
knowledge base.In this paper, we address the out-of-knowledge-base (OOKB)
entity problem in KBC:how to answer queries concerning test entities not
observed at training time. Existing embedding-based KBC models assume that all
test entities are available at training time, making it unclear how to obtain
embeddings for new entities without costly retraining. To solve the OOKB entity
problem without retraining, we use graph neural networks (Graph-NNs) to compute
the embeddings of OOKB entities, exploiting the limited auxiliary knowledge
provided at test time.The experimental results show the effectiveness of our
proposed model in the OOKB setting.Additionally, in the standard KBC setting in
which OOKB entities are not involved, our model achieves state-of-the-art
performance on the WordNet dataset. The code and dataset are available at
this https URL
",Computer Science,Computer Science
"Visual analytics for loan guarantee network risk management   Groups of enterprises guarantee each other and form complex guarantee
networks when they try to obtain loans from banks. Such secured loan can
enhance the solvency and promote the rapid growth in the economic upturn
period. However, potential systemic risk may happen within the risk binding
community. Especially, during the economic down period, the crisis may spread
in the guarantee network like a domino. Monitoring the financial status,
preventing or reducing systematic risk when crisis happens is highly concerned
by the regulatory commission and banks. We propose visual analytics approach
for loan guarantee network risk management, and consolidate the five analysis
tasks with financial experts: i) visual analytics for enterprises default risk,
whereby a hybrid representation is devised to predict the default risk and
developed an interface to visualize key indicators; ii) visual analytics for
high default groups, whereby a community detection based interactive approach
is presented; iii) visual analytics for high defaults pattern, whereby a motif
detection based interactive approach is described, and we adopt a Shneiderman
Mantra strategy to reduce the computation complexity. iv) visual analytics for
evolving guarantee network, whereby animation is used to help understanding the
guarantee dynamic; v) visual analytics approach and interface for default
diffusion path. The temporal diffusion path analysis can be useful for the
government and bank to monitor the default spread status. It also provides
insight for taking precautionary measures to prevent and dissolve systemic
financial risk. We implement the system with case studies on a real-world
guarantee network. Two financial experts are consulted with endorsement on the
developed tool. To the best of our knowledge, this is the first visual
analytics tool to explore the guarantee network risks in a systematic manner.
",Computer Science,Computer Science
"An infinite class of unsaturated rooted trees corresponding to designable RNA secondary structures   An RNA secondary structure is designable if there is an RNA sequence which
can attain its maximum number of base pairs only by adopting that structure.
The combinatorial RNA design problem, introduced by Haleš et al. in 2016,
is to determine whether or not a given RNA secondary structure is designable.
Haleš et al. identified certain classes of designable and non-designable
secondary structures by reference to their corresponding rooted trees. We
introduce an infinite class of rooted trees containing unpaired nucleotides at
the greatest height, and prove constructively that their corresponding
secondary structures are designable. This complements previous results for the
combinatorial RNA design problem.
",Computer Science,Computer Science; Mathematics
"Analyzing biological and artificial neural networks: challenges with opportunities for synergy?   Deep neural networks (DNNs) transform stimuli across multiple processing
stages to produce representations that can be used to solve complex tasks, such
as object recognition in images. However, a full understanding of how they
achieve this remains elusive. The complexity of biological neural networks
substantially exceeds the complexity of DNNs, making it even more challenging
to understand the representations that they learn. Thus, both machine learning
and computational neuroscience are faced with a shared challenge: how can we
analyze their representations in order to understand how they solve complex
tasks?
We review how data-analysis concepts and techniques developed by
computational neuroscientists can be useful for analyzing representations in
DNNs, and in turn, how recently developed techniques for analysis of DNNs can
be useful for understanding representations in biological neural networks. We
explore opportunities for synergy between the two fields, such as the use of
DNNs as in-silico model systems for neuroscience, and how this synergy can lead
to new hypotheses about the operating principles of biological neural networks.
",Quantitative Biology,Computer Science; Statistics
"Bipartite Envy-Free Matching   Bipartite Envy-Free Matching (BEFM) is a relaxation of perfect matching. In a
bipartite graph with parts X and Y, a BEFM is a matching of some vertices in X
to some vertices in Y, such that each unmatched vertex in X is not adjacent to
any matched vertex in Y (so the unmatched vertices do not ""envy"" the matched
ones). The empty matching is always a BEFM. This paper presents sufficient and
necessary conditions for the existence of a non-empty BEFM. These conditions
are based on cardinality of neighbor-sets, similarly to Hall's condition for
the existence of a perfect matching. The conditions can be verified in
polynomial time, and in case they are satisfied, a non-empty BEFM can be found
by a polynomial-time algorithm. The paper presents some applications of BEFM as
a subroutine in fair division algorithms.
",Computer Science,Computer Science
"Binary Evolution and the Progenitor of SN 1987A   Since the majority of massive stars are members of binary systems, an
understanding of the intricacies of binary interactions is essential for
understanding the large variety of supernova types and sub-types. I therefore
briefly review the basic elements of binary evolution theory and discuss how
binary interactions affect the presupernova structure of massive stars and the
resulting supernovae.
SN 1987A was a highly anomalous supernova, almost certainly because of a
previous binary interaction. The most likely scenario at present is that the
progenitor was a member of a massive close binary that experienced dynamical
mass transfer during its second red-supergiant phase and merged completely with
its companion as a consequence. This can naturally explain the three main
anomalies of SN 1987A: the blue color of the progenitor, the chemical anomalies
and the complex triple-ring nebula.
",Physics,Physics
"A 3pi Search for Planet Nine at 3.4 microns with WISE and NEOWISE   The recent 'Planet Nine' hypothesis has led to many observational and
archival searches for this giant planet proposed to orbit the Sun at hundreds
of astronomical units. While trans-Neptunian object searches are typically
conducted in the optical, models suggest Planet Nine could be self-luminous and
potentially bright enough at ~3-5 microns to be detected by the Wide-field
Infrared Survey Explorer (WISE). We have previously demonstrated a Planet Nine
search methodology based on time-resolved WISE coadds, allowing us to detect
moving objects much fainter than would be possible using single-frame
extractions. In the present work, we extend our 3.4 micron (W1) search to cover
more than three quarters of the sky and incorporate four years of WISE
observations spanning a seven year time period. This represents the deepest and
widest-area WISE search for Planet Nine to date. We characterize the spatial
variation of our survey's sensitivity and rule out the presence of Planet Nine
in the parameter space searched at W1 < 16.7 in high Galactic latitude regions
(90% completeness).
",Physics,Physics
"A Recent Survey on the Applications of Genetic Programming in Image Processing   During the last two decades, Genetic Programming (GP) has been largely used
to tackle optimization, classification, and automatic features selection
related tasks. The widespread use of GP is mainly due to its flexible and
comprehensible tree-type structure. Similarly, research is also gaining
momentum in the field of Image Processing (IP) because of its promising results
over wide areas of applications ranging from medical IP to multispectral
imaging. IP is mainly involved in applications such as computer vision, pattern
recognition, image compression, storage and transmission, and medical
diagnostics. This prevailing nature of images and their associated algorithm
i.e complexities gave an impetus to the exploration of GP. GP has thus been
used in different ways for IP since its inception. Many interesting GP
techniques have been developed and employed in the field of IP. To give the
research community an extensive view of these techniques, this paper presents
the diverse applications of GP in IP and provides useful resources for further
research. Also, comparison of different parameters used in ten different
applications of IP are summarized in tabular form. Moreover, analysis of
different parameters used in IP related tasks is carried-out to save the time
needed in future for evaluating the parameters of GP. As more advancement is
made in GP methodologies, its success in solving complex tasks not only related
to IP but also in other fields will increase. Additionally, guidelines are
provided for applying GP in IP related tasks, pros and cons of GP techniques
are discussed, and some future directions are also set.
",Computer Science,Computer Science
"Up-down colorings of virtual-link diagrams and the necessity of Reidemeister moves of type II   We introduce an up-down coloring of a virtual-link diagram. The
colorabilities give a lower bound of the minimum number of Reidemeister moves
of type II which are needed between two 2-component virtual-link diagrams. By
using the notion of a quandle cocycle invariant, we determine the necessity of
Reidemeister moves of type II for a pair of diagrams of the trivial
virtual-knot. This implies that for any virtual-knot diagram $D$, there exists
a diagram $D'$ representing the same virtual-knot such that any sequence of
generalized Reidemeister moves between them includes at least one Reidemeister
move of type II.
",Mathematics,Mathematics
"Supervised Typing of Big Graphs using Semantic Embeddings   We propose a supervised algorithm for generating type embeddings in the same
semantic vector space as a given set of entity embeddings. The algorithm is
agnostic to the derivation of the underlying entity embeddings. It does not
require any manual feature engineering, generalizes well to hundreds of types
and achieves near-linear scaling on Big Graphs containing many millions of
triples and instances by virtue of an incremental execution. We demonstrate the
utility of the embeddings on a type recommendation task, outperforming a
non-parametric feature-agnostic baseline while achieving 15x speedup and
near-constant memory usage on a full partition of DBpedia. Using
state-of-the-art visualization, we illustrate the agreement of our
extensionally derived DBpedia type embeddings with the manually curated domain
ontology. Finally, we use the embeddings to probabilistically cluster about 4
million DBpedia instances into 415 types in the DBpedia ontology.
",Computer Science,Computer Science
"Exact relativistic Toda chain eigenfunctions from Separation of Variables and gauge theory   We provide a proposal, motivated by Separation of Variables and gauge theory
arguments, for constructing exact solutions to the quantum Baxter equation
associated to the $N$-particle relativistic Toda chain and test our proposal
against numerical results. Quantum Mechanical non-perturbative corrections,
essential in order to obtain a sensible solution, are taken into account in our
gauge theory approach by considering codimension two defects on curved
backgrounds (squashed $S^5$ and degenerate limits) rather than flat space; this
setting also naturally incorporates exact quantization conditions and energy
spectrum of the relativistic Toda chain as well as its modular dual structure.
",Physics,Physics
"Ordering Garside groups   We introduce a condition on Garside groups that we call Dehornoy structure.
An iteration of such a structure leads to a left order on the group. We show
conditions for a Garside group to admit a Dehornoy structure, and we apply
these criteria to prove that the Artin groups of type A and I 2 (m), m $\ge$ 4,
have Dehornoy structures. We show that the left orders on the Artin groups of
type A obtained from their Dehornoy structures are the Dehornoy orders. In the
case of the Artin groups of type I 2 (m), m $\ge$ 4, we show that the left
orders derived from their Dehornoy structures coincide with the orders obtained
from embeddings of the groups into braid groups. 20F36
",Mathematics,Mathematics
"Convergence Results for Neural Networks via Electrodynamics   We study whether a depth two neural network can learn another depth two
network using gradient descent. Assuming a linear output node, we show that the
question of whether gradient descent converges to the target function is
equivalent to the following question in electrodynamics: Given $k$ fixed
protons in $\mathbb{R}^d,$ and $k$ electrons, each moving due to the attractive
force from the protons and repulsive force from the remaining electrons,
whether at equilibrium all the electrons will be matched up with the protons,
up to a permutation. Under the standard electrical force, this follows from the
classic Earnshaw's theorem. In our setting, the force is determined by the
activation function and the input distribution. Building on this equivalence,
we prove the existence of an activation function such that gradient descent
learns at least one of the hidden nodes in the target network. Iterating, we
show that gradient descent can be used to learn the entire network one node at
a time.
",Computer Science; Physics,Computer Science; Statistics
"Dimensional crossover of effective orbital dynamics in polar distorted 3He-A: Transitions to anti-spacetime   Topologically protected superfluid phases of $^3$He allow one to simulate
many important aspects of relativistic quantum field theories and quantum
gravity in condensed matter. Here we discuss a topological Lifshitz transition
of the effective quantum vacuum in which the determinant of the tetrad field
changes sign through a crossing to a vacuum state with a degenerate fermionic
metric. Such a transition is realized in polar distorted superfluid $^3$He-A in
terms of the effective tetrad fields emerging in the vicinity of the superfluid
gap nodes: the tetrads of the Weyl points in the chiral A-phase of $^3$He and
the degenerate tetrad in the vicinity of a Dirac nodal line in the polar phase
of $^3$He. The continuous phase transition from the $A$-phase to the polar
phase, i.e. in the transition from the Weyl nodes to the Dirac nodal line and
back, allows one to follow the behavior of the fermionic and bosonic effective
actions when the sign of the tetrad determinant changes, and the effective
chiral space-time transforms to anti-chiral ""anti-spacetime"". This condensed
matter realization demonstrates that while the original fermionic action is
analytic across the transition, the effective action for the orbital degrees of
freedom (pseudo-EM) fields and gravity have non-analytic behavior. In
particular, the action for the pseudo-EM field in the vacuum with Weyl fermions
(A-phase) contains the modulus of the tetrad determinant. In the vacuum with
the degenerate metric (polar phase) the nodal line is effectively a family of
$2+1$d Dirac fermion patches, which leads to a non-analytic $(B^2-E^2)^{3/4}$
QED action in the vicinity of the Dirac line.
",Physics,Physics
"Using PCA and Factor Analysis for Dimensionality Reduction of Bio-informatics Data   Large volume of Genomics data is produced on daily basis due to the
advancement in sequencing technology. This data is of no value if it is not
properly analysed. Different kinds of analytics are required to extract useful
information from this raw data. Classification, Prediction, Clustering and
Pattern Extraction are useful techniques of data mining. These techniques
require appropriate selection of attributes of data for getting accurate
results. However, Bioinformatics data is high dimensional, usually having
hundreds of attributes. Such large a number of attributes affect the
performance of machine learning algorithms used for classification/prediction.
So, dimensionality reduction techniques are required to reduce the number of
attributes that can be further used for analysis. In this paper, Principal
Component Analysis and Factor Analysis are used for dimensionality reduction of
Bioinformatics data. These techniques were applied on Leukaemia data set and
the number of attributes was reduced from to.
",Computer Science,Computer Science; Statistics
"Superconvergence analysis of linear FEM based on the polynomial preserving recovery and Richardson extrapolation for Helmholtz equation with high wave number   We study superconvergence property of the linear finite element method with
the polynomial preserving recovery (PPR) and Richardson extrapolation for the
two dimensional Helmholtz equation. The $H^1$-error estimate with explicit
dependence on the wave number $k$ {is} derived.
First, we prove that under the assumption $k(kh)^2\leq C_0$ ($h$ is the mesh
size) and certain mesh condition, the estimate between the finite element
solution and the linear interpolation of the exact solution is superconvergent
under the $H^1$-seminorm, although the pollution error still exists. Second, we
prove a similar result for the recovered gradient by PPR and found that the PPR
can only improve the interpolation error and has no effect on the pollution
error. Furthermore, we estimate the error between the finite element gradient
and recovered gradient and discovered that the pollution error is canceled
between these two quantities. Finally, we apply the Richardson extrapolation to
recovered gradient and demonstrate numerically that PPR combined with the
Richardson extrapolation can reduce the interpolation and pollution errors
simultaneously, and therefore, leads to an asymptotically exact {\it a
posteriori} error estimator. All theoretical findings are verified by numerical
tests.
",Mathematics,Mathematics
"Simultaneous Block-Sparse Signal Recovery Using Pattern-Coupled Sparse Bayesian Learning   In this paper, we consider the block-sparse signals recovery problem in the
context of multiple measurement vectors (MMV) with common row sparsity
patterns. We develop a new method for recovery of common row sparsity MMV
signals, where a pattern-coupled hierarchical Gaussian prior model is
introduced to characterize both the block-sparsity of the coefficients and the
statistical dependency between neighboring coefficients of the common row
sparsity MMV signals. Unlike many other methods, the proposed method is able to
automatically capture the block sparse structure of the unknown signal. Our
method is developed using an expectation-maximization (EM) framework.
Simulation results show that our proposed method offers competitive performance
in recovering block-sparse common row sparsity pattern MMV signals.
",Computer Science; Statistics,Statistics
"Sun/Moon photometer for the Cherenkov Telescope Array - first results   Determination of the energy and flux of the gamma photons by Imaging
Atmospheric Cherenkov Technique is strongly dependent on optical properties of
the atmosphere. Therefore, atmospheric monitoring during the future
observations of the Cherenkov Telescope Array (CTA) as well as anticipated
long-term monitoring in order to characterize overal properties and annual
variation of atmospheric conditions are very important. Several instruments are
already installed at the CTA sites in order to monitor atmospheric conditions
on long-term. One of them is a Sun/Moon photometer CE318-T, installed at the
Southern CTA site. Since the photometer is installed at a place with very
stable atmospheric conditions, it can be also used for characterization of its
performance and testing of new methods of aerosol optical depth (AOD)
retrieval, cloud-screening and calibration. In this work, we describe our
calibration method for nocturnal measurements and the modification of
cloud-screening for purposes of nocturnal AOD retrieval. We applied these
methods on two months of observations and present the distribution of AODs in
four photometric passbands together with their uncertainties.
",Physics,Physics
"Online Learning with an Almost Perfect Expert   We study the multiclass online learning problem where a forecaster makes a
sequence of predictions using the advice of $n$ experts. Our main contribution
is to analyze the regime where the best expert makes at most $b$ mistakes and
to show that when $b = o(\log_4{n})$, the expected number of mistakes made by
the optimal forecaster is at most $\log_4{n} + o(\log_4{n})$. We also describe
an adversary strategy showing that this bound is tight and that the worst case
is attained for binary prediction.
",Statistics,Computer Science; Statistics
"A Projection-Based Reformulation and Decomposition Algorithm for Global Optimization of a Class of Mixed Integer Bilevel Linear Programs   We propose an extended variant of the reformulation and decomposition
algorithm for solving a special class of mixed-integer bilevel linear programs
(MIBLPs) where continuous and integer variables are involved in both upper- and
lower-level problems. In particular, we consider MIBLPs with upper-level
constraints that involve lower-level variables. We assume that the inducible
region is nonempty and all variables are bounded. By using the reformulation
and decomposition scheme, an MIBLP is first converted into its equivalent
single-level formulation, then computed by a column-and-constraint generation
based decomposition algorithm. The solution procedure is enhanced by a
projection strategy that does not require the relatively complete response
property. To ensure its performance, we prove that our new method converges to
the global optimal solution in a finite number of iterations. A large-scale
computational study on random instances and instances of hierarchical supply
chain planning are presented to demonstrate the effectiveness of the algorithm.
",Mathematics,Computer Science
"Persistent Spread Measurement for Big Network Data Based on Register Intersection   Persistent spread measurement is to count the number of distinct elements
that persist in each network flow for predefined time periods. It has many
practical applications, including detecting long-term stealthy network
activities in the background of normal-user activities, such as stealthy DDoS
attack, stealthy network scan, or faked network trend, which cannot be detected
by traditional flow cardinality measurement. With big network data, one
challenge is to measure the persistent spreads of a massive number of flows
without incurring too much memory overhead as such measurement may be performed
at the line speed by network processors with fast but small on-chip memory. We
propose a highly compact Virtual Intersection HyperLogLog (VI-HLL) architecture
for this purpose. It achieves far better memory efficiency than the best prior
work of V-Bitmap, and in the meantime drastically extends the measurement
range. Theoretical analysis and extensive experiments demonstrate that VI-HLL
provides good measurement accuracy even in very tight memory space of less than
1 bit per flow.
",Computer Science,Computer Science
"A Lagrangian Model to Predict Microscallop Motion in non Newtonian Fluids   The need to develop models to predict the motion of microrobots, or robots of
a much smaller scale, moving in fluids in a low Reynolds number regime, and in
particular, in non Newtonian fluids, cannot be understated. The article
develops a Lagrangian based model for one such mechanism - a two-link mechanism
termed a microscallop, moving in a low Reynolds number environment in a non
Newtonian fluid. The modelling proceeds through the conventional Lagrangian
construction for a two-link mechanism and then goes on to model the external
fluid forces using empirically based models for viscosity to complete the
dynamic model. The derived model is then simulated for different initial
conditions and key parameters of the non Newtonian fluid, and the results are
corroborated with a few existing experimental results on a similar mechanism
under identical conditions.
",Computer Science,Computer Science
"Test Case Prioritization Techniques for Model-Based Testing: A Replicated Study   Recently, several Test Case Prioritization (TCP) techniques have been
proposed to order test cases for achieving a goal during test execution,
particularly, revealing faults sooner. In the Model-Based Testing (MBT)
context, such techniques are usually based on heuristics related to structural
elements of the model and derived test cases. In this sense, techniques'
performance may vary due to a number of factors. While empirical studies
comparing the performance of TCP techniques have already been presented in
literature, there is still little knowledge, particularly in the MBT context,
about which factors may influence the outcomes suggested by a TCP technique. In
a previous family of empirical studies focusing on labeled transition systems,
we identified that the model layout, i.e. amount of branches, joins, and loops
in the model, alone may have little influence on the performance of TCP
techniques investigated, whereas characteristics of test cases that actually
fail definitely influences their performance. However, we considered only
synthetic artifacts in the study, which reduced the ability of representing
properly the reality. In this paper, we present a replication of one of these
studies, now with a larger and more representative selection of techniques and
considering test suites from industrial applications as experimental objects.
Our objective is to find out whether the results remain while increasing the
validity in comparison to the original study. Results reinforce that there is
no best performer among the investigated techniques and characteristics of test
cases that fail represent an important factor, although adaptive random based
techniques are less affected by it.
",Computer Science,Computer Science; Physics
"Integrable Trotterization: Local Conservation Laws and Boundary Driving   We discuss a general procedure to construct an integrable real-time
trotterization of interacting lattice models. As an illustrative example we
consider a spin-$1/2$ chain, with continuous time dynamics described by the
isotropic ($XXX$) Heisenberg Hamiltonian. For periodic boundary conditions
local conservation laws are derived from an inhomogeneous transfer matrix and a
boost operator is constructed. In the continuous time limit these local charges
reduce to the known integrals of motion of the Heisenberg chain. In a simple
Kraus representation we also examine the nonequilibrium setting, where our
integrable cellular automaton is driven by stochastic processes at the
boundaries. We show explicitly, how an exact nonequilibrium steady state
density matrix can be written in terms of a staggered matrix product ansatz.
This simple trotterization scheme, in particular in the open system framework,
could prove to be a useful tool for experimental simulations of the lattice
models in terms of trapped ion and atom optics setups.
",Physics,Physics
"Review of image quality measures for solar imaging   The observations of solar photosphere from the ground encounter significant
problems due to the presence of Earth's turbulent atmosphere. Prior to applying
image reconstruction techniques, the frames obtained in most favorable
atmospheric conditions (so-called lucky frames) have to be carefully selected.
However, the estimation of the quality of images containing complex
photospheric structures is not a trivial task and the standard routines applied
in night-time Lucky Imaging observations are not applicable. In this paper we
evaluate 36 methods dedicated for the assessment of image quality which were
presented in the rich literature over last 40 years. We compare their
effectiveness on simulated solar observations of both active regions and
granulation patches, using reference data obtained by the Solar Optical
Telescope on the Hindoe satellite. To create the images affected by a known
degree of atmospheric degradation, we employ the Random Wave Vector method
which faithfully models all the seeing characteristics. The results provide
useful information about the methods performance depending on the average
seeing conditions expressed by the ratio of the telescope's aperture to the
Fried parameter, $D/r_0$. The comparison identifies three methods for
consideration by observers: Helmli and Scherer's Mean, Median Filter Gradient
Similarity, and Discrete Cosine Transform Energy Ratio. While the first one
requires less computational effort and can be used effectively virtually in any
atmospherics conditions, the second one shows its superiority at good seeing
($D/r_0<4$). The last one should be considered mainly for the post-processing
of strongly blurred images.
",Physics,Physics
"On Alzer's inequality   Extensions and generalizations of Alzer's inequality; which is of Wirtinger
type are proved. As applications, sharp trapezoid type inequality and sharp
bound for the geometric mean are deduced.
",Mathematics,Mathematics
"Avoiding a Tragedy of the Commons in the Peer Review Process   Peer review is the foundation of scientific publication, and the task of
reviewing has long been seen as a cornerstone of professional service. However,
the massive growth in the field of machine learning has put this community
benefit under stress, threatening both the sustainability of an effective
review process and the overall progress of the field. In this position paper,
we argue that a tragedy of the commons outcome may be avoided by emphasizing
the professional aspects of this service. In particular, we propose a rubric to
hold reviewers to an objective standard for review quality. In turn, we also
propose that reviewers be given appropriate incentive. As one possible such
incentive, we explore the idea of financial compensation on a per-review basis.
We suggest reasonable funding models and thoughts on long term effects.
",Computer Science,Computer Science; Statistics
"Deep Affordance-grounded Sensorimotor Object Recognition   It is well-established by cognitive neuroscience that human perception of
objects constitutes a complex process, where object appearance information is
combined with evidence about the so-called object ""affordances"", namely the
types of actions that humans typically perform when interacting with them. This
fact has recently motivated the ""sensorimotor"" approach to the challenging task
of automatic object recognition, where both information sources are fused to
improve robustness. In this work, the aforementioned paradigm is adopted,
surpassing current limitations of sensorimotor object recognition research.
Specifically, the deep learning paradigm is introduced to the problem for the
first time, developing a number of novel neuro-biologically and
neuro-physiologically inspired architectures that utilize state-of-the-art
neural networks for fusing the available information sources in multiple ways.
The proposed methods are evaluated using a large RGB-D corpus, which is
specifically collected for the task of sensorimotor object recognition and is
made publicly available. Experimental results demonstrate the utility of
affordance information to object recognition, achieving an up to 29% relative
error reduction by its inclusion.
",Computer Science,Computer Science
"Fairness in representation: quantifying stereotyping as a representational harm   While harms of allocation have been increasingly studied as part of the
subfield of algorithmic fairness, harms of representation have received
considerably less attention. In this paper, we formalize two notions of
stereotyping and show how they manifest in later allocative harms within the
machine learning pipeline. We also propose mitigation strategies and
demonstrate their effectiveness on synthetic datasets.
",Computer Science; Statistics,Computer Science; Statistics
"Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis   Optimization algorithms that leverage gradient covariance information, such
as variants of natural gradient descent (Amari, 1998), offer the prospect of
yielding more effective descent directions. For models with many parameters,
the covariance matrix they are based on becomes gigantic, making them
inapplicable in their original form. This has motivated research into both
simple diagonal approximations and more sophisticated factored approximations
such as KFAC (Heskes, 2000; Martens & Grosse, 2015; Grosse & Martens, 2016). In
the present work we draw inspiration from both to propose a novel approximation
that is provably better than KFAC and amendable to cheap partial updates. It
consists in tracking a diagonal variance, not in parameter coordinates, but in
a Kronecker-factored eigenbasis, in which the diagonal approximation is likely
to be more effective. Experiments show improvements over KFAC in optimization
speed for several deep network architectures.
",Statistics,Computer Science; Statistics
"PROBE: Predictive Robust Estimation for Visual-Inertial Navigation   Navigation in unknown, chaotic environments continues to present a
significant challenge for the robotics community. Lighting changes,
self-similar textures, motion blur, and moving objects are all considerable
stumbling blocks for state-of-the-art vision-based navigation algorithms. In
this paper we present a novel technique for improving localization accuracy
within a visual-inertial navigation system (VINS). We make use of training data
to learn a model for the quality of visual features with respect to
localization error in a given environment. This model maps each visual
observation from a predefined prediction space of visual-inertial predictors
onto a scalar weight, which is then used to scale the observation covariance
matrix. In this way, our model can adjust the influence of each observation
according to its quality. We discuss our choice of predictors and report
substantial reductions in localization error on 4 km of data from the KITTI
dataset, as well as on experimental datasets consisting of 700 m of indoor and
outdoor driving on a small ground rover equipped with a Skybotix VI-Sensor.
",Computer Science,Computer Science
"Large Area X-ray Proportional Counter (LAXPC) Instrument on AstroSat   Large Area X-ray Proportional Counter (LAXPC) is one of the major AstroSat
payloads. LAXPC instrument will provide high time resolution X-ray observations
in 3 to 80 keV energy band with moderate energy resolution. A cluster of three
co-aligned identical LAXPC detectors is used in AstroSat to provide large
collection area of more than 6000 cm2 . The large detection volume (15 cm
depth) filled with xenon gas at about 2 atmosphere pressure, results in
detection efficiency greater than 50%, above 30 keV. With its broad energy
range and fine time resolution (10 microsecond), LAXPC instrument is well
suited for timing and spectral studies of a wide variety of known and transient
X-ray sources in the sky. We have done extensive calibration of all LAXPC
detectors using radioactive sources as well as GEANT4 simulation of LAXPC
detectors. We describe in brief some of the results obtained during the payload
verification phase along with LXAPC capabilities.
",Physics,Physics
"Ridesourcing Car Detection by Transfer Learning   Ridesourcing platforms like Uber and Didi are getting more and more popular
around the world. However, unauthorized ridesourcing activities taking
advantages of the sharing economy can greatly impair the healthy development of
this emerging industry. As the first step to regulate on-demand ride services
and eliminate black market, we design a method to detect ridesourcing cars from
a pool of cars based on their trajectories. Since licensed ridesourcing car
traces are not openly available and may be completely missing in some cities
due to legal issues, we turn to transferring knowledge from public transport
open data, i.e, taxis and buses, to ridesourcing detection among ordinary
vehicles. We propose a two-stage transfer learning framework. In Stage 1, we
take taxi and bus data as input to learn a random forest (RF) classifier using
trajectory features shared by taxis/buses and ridesourcing/other cars. Then, we
use the RF to label all the candidate cars. In Stage 2, leveraging the subset
of high confident labels from the previous stage as input, we further learn a
convolutional neural network (CNN) classifier for ridesourcing detection, and
iteratively refine RF and CNN, as well as the feature set, via a co-training
process. Finally, we use the resulting ensemble of RF and CNN to identify the
ridesourcing cars in the candidate pool. Experiments on real car, taxi and bus
traces show that our transfer learning framework, with no need of a pre-labeled
ridesourcing dataset, can achieve similar accuracy as the supervised learning
methods.
",Computer Science; Statistics,Computer Science
"Fast readout algorithm for cylindrical beam position monitors providing good accuracy for particle bunches with large offsets   A simple, analytically correct algorithm is developed for calculating pencil
beam coordinates using the signals from an ideal cylindrical particle beam
position monitor (BPM) with four pickup electrodes (PUEs) of infinitesimal
widths. The algorithm is then applied to simulations of realistic BPMs with
finite width PUEs. Surprisingly small deviations are found. Simple empirically
determined correction terms reduce the deviations even further. The algorithm
is then used to study the impact of beam-size upon the precision of BPMs in the
non-linear region. As an example of the data acquisition speed advantage, a
FPGA-based BPM readout implementation of the new algorithm has been developed
and characterized. Finally,the algorithm is tested with BPM data from the
Cornell Preinjector.
",Physics,Physics
"Driving Interactive Graph Exploration Using 0-Dimensional Persistent Homology Features   Graphs are commonly used to encode relationships among entities, yet, their
abstractness makes them incredibly difficult to analyze. Node-link diagrams are
a popular method for drawing graphs. Classical techniques for the node-link
diagrams include various layout methods that rely on derived information to
position points, which often lack interactive exploration functionalities; and
force-directed layouts, which ignore global structures of the graph. This paper
addresses the graph drawing challenge by leveraging topological features of a
graph as derived information for interactive graph drawing. We first discuss
extracting topological features from a graph using persistent homology. We then
introduce an interactive persistence barcodes to study the substructures of a
force-directed graph layout; in particular, we add contracting and repulsing
forces guided by the 0-dimensional persistent homology features. Finally, we
demonstrate the utility of our approach across three datasets.
",Computer Science,Computer Science
"Time crystal platform: from quasi-crystal structures in time to systems with exotic interactions   Time crystals are quantum many-body systems which, due to interactions
between particles, are able to spontaneously self-organize their motion in a
periodic way in time by analogy with the formation of crystalline structures in
space in condensed matter physics. In solid state physics properties of space
crystals are often investigated with the help of external potentials that are
spatially periodic and reflect various crystalline structures. A similar
approach can be applied for time crystals, as periodically driven systems
constitute counterparts of spatially periodic systems, but in the time domain.
Here we show that condensed matter problems ranging from single particles in
potentials of quasi-crystal structure to many-body systems with exotic
long-range interactions can be realized in the time domain with an appropriate
periodic driving. Moreover, it is possible to create molecules where atoms are
bound together due to destructive interference if the atomic scattering length
is modulated in time.
",Physics,Physics
"Gallucci's axiom revisited   In this paper we propose a well-justified synthetic approach of the
projective space. We define the concepts of plane and space of incidence and
also the Gallucci's axiom as an axiom to our classical projective space. To
this purpose we prove from our space axioms, the theorems of Desargues, Pappus,
the fundamental theorem of projectivities, and the fundamental theorem of
central-axial collinearities, respectively. Our building up do not use any
information on analytical projective geometry, as the concept of cross-ratio
and the homogeneous coordinates of points.
",Mathematics,Mathematics
"Structural, elastic, electronic, and bonding properties of intermetallic Nb3Pt and Nb3Os compounds: a DFT study   Theoretical investigation of structural, elastic, electronic and bonding
properties of A-15 Nb-based intermetallic compounds Nb3B (B = Pt, Os) have been
performed using first principles calculations based on the density functional
theory (DFT). Optimized cell parameters are found to be in good agreement with
available experimental and theoretical results. The elastic constants at zero
pressure and temperature are calculated and the anisotropic behaviors of the
compounds are studied. Both the compounds are mechanically stable and ductile
in nature. Other elastic properties such as Pugh's ratio, Cauchy pressure,
machinability index are derived for the first time. Nb3Os is expected to have
good lubricating properties compared to Nb3Pt. The electronic band structure
and energy density of states (DOS) have been studied with and without
spin-orbit coupling (SOC). The band structures of both the compounds are spin
symmetric. Electronic band structure and DOS reveal that both the compounds are
metallic and the conductivity mainly arise from the Nb 4d states. The Fermi
surface features have been studied for the first time. The Fermi surfaces of
Nb3B contain both hole- and electron-like sheets which change as one replaces
Pt with Os. The electronic charge density distribution shows that Nb3Pt and
Nb3Os both have a mixture of ionic and covalent bonding. The charge transfer
between atomic species in these compounds has been explained by the Mulliken
bond population analysis.
",Physics,Physics
"Existence and symmetry of solutions for critical fractional Schrödinger equations with bounded potentials   This paper is concerned with the following fractional Schrödinger
equations involving critical exponents: \begin{eqnarray*}
(-\Delta)^{\alpha}u+V(x)u=k(x)f(u)+\lambda|u|^{2_{\alpha}^{*}-2}u\quad\quad
\mbox{in}\ \mathbb{R}^{N}, \end{eqnarray*} where $(-\Delta)^{\alpha}$ is the
fractional Laplacian operator with $\alpha\in(0,1)$, $N\geq2$, $\lambda$ is a
positive real parameter and $2_{\alpha}^{*}=2N/(N-2\alpha)$ is the critical
Sobolev exponent, $V(x)$ and $k(x)$ are positive and bounded functions
satisfying some extra hypotheses. Based on the principle of concentration
compactness in the fractional Sobolev space and the minimax arguments, we
obtain the existence of a nontrivial radially symmetric weak solution for the
above-mentioned equations without assuming the Ambrosetti-Rabinowitz condition
on the subcritical nonlinearity.
",Mathematics,Mathematics
"Bayesian Probabilistic Numerical Methods   The emergent field of probabilistic numerics has thus far lacked clear
statistical principals. This paper establishes Bayesian probabilistic numerical
methods as those which can be cast as solutions to certain inverse problems
within the Bayesian framework. This allows us to establish general conditions
under which Bayesian probabilistic numerical methods are well-defined,
encompassing both non-linear and non-Gaussian models. For general computation,
a numerical approximation scheme is proposed and its asymptotic convergence
established. The theoretical development is then extended to pipelines of
computation, wherein probabilistic numerical methods are composed to solve more
challenging numerical tasks. The contribution highlights an important research
frontier at the interface of numerical analysis and uncertainty quantification,
with a challenging industrial application presented.
",Computer Science; Mathematics; Statistics,Mathematics; Statistics
"Upper-Bounding the Regularization Constant for Convex Sparse Signal Reconstruction   Consider reconstructing a signal $x$ by minimizing a weighted sum of a convex
differentiable negative log-likelihood (NLL) (data-fidelity) term and a convex
regularization term that imposes a convex-set constraint on $x$ and enforces
its sparsity using $\ell_1$-norm analysis regularization. We compute upper
bounds on the regularization tuning constant beyond which the regularization
term overwhelmingly dominates the NLL term so that the set of minimum points of
the objective function does not change. Necessary and sufficient conditions for
irrelevance of sparse signal regularization and a condition for the existence
of finite upper bounds are established. We formulate an optimization problem
for finding these bounds when the regularization term can be globally minimized
by a feasible $x$ and also develop an alternating direction method of
multipliers (ADMM) type method for their computation. Simulation examples show
that the derived and empirical bounds match.
",Mathematics; Statistics,Computer Science; Mathematics
"Higher order mobile coverage control with application to localization   Most current results on coverage control using mobile sensors require that
one partitioned cell is associated with precisely one sensor. In this paper, we
consider a class of coverage control problems involving higher order Voronoi
partitions, motivated by applications where more than one sensor is required to
monitor and cover one cell. Such applications are frequent in scenarios
requiring the sensors to localize targets. We introduce a framework depending
on a coverage performance function incorporating higher order Voronoi cells and
then design a gradient-based controller which allows the multi-sensor system to
achieve a local equilibrium in a distributed manner. The convergence properties
are studied and related to Lloyd algorithm. We study also the extension to
coverage of a discrete set of points. In addition, we provide a number of real
world scenarios where our framework can be applied. Simulation results are also
provided to show the controller performance.
",Computer Science,Computer Science
"Simple Classification using Binary Data   Binary, or one-bit, representations of data arise naturally in many
applications, and are appealing in both hardware implementations and algorithm
design. In this work, we study the problem of data classification from binary
data and propose a framework with low computation and resource costs. We
illustrate the utility of the proposed approach through stylized and realistic
numerical experiments, and provide a theoretical analysis for a simple case. We
hope that our framework and analysis will serve as a foundation for studying
similar types of approaches.
",Computer Science; Statistics,Computer Science; Statistics
"On the Statistical Challenges of Echo State Networks and Some Potential Remedies   Echo state networks are powerful recurrent neural networks. However, they are
often unstable and shaky, making the process of finding an good ESN for a
specific dataset quite hard. Obtaining a superb accuracy by using the Echo
State Network is a challenging task. We create, develop and implement a family
of predictably optimal robust and stable ensemble of Echo State Networks via
regularizing the training and perturbing the input. Furthermore, several
distributions of weights have been tried based on the shape to see if the shape
of the distribution has the impact for reducing the error. We found ESN can
track in short term for most dataset, but it collapses in the long run.
Short-term tracking with large size reservoir enables ESN to perform strikingly
with superior prediction. Based on this scenario, we go a further step to
aggregate many of ESNs into an ensemble to lower the variance and stabilize the
system by stochastic replications and bootstrapping of input data.
",Statistics,Statistics
"Comparison of multi-task convolutional neural network (MT-CNN) and a few other methods for toxicity prediction   Toxicity analysis and prediction are of paramount importance to human health
and environmental protection. Existing computational methods are built from a
wide variety of descriptors and regressors, which makes their performance
analysis difficult. For example, deep neural network (DNN), a successful
approach in many occasions, acts like a black box and offers little conceptual
elegance or physical understanding. The present work constructs a common set of
microscopic descriptors based on established physical models for charges,
surface areas and free energies to assess the performance of multi-task
convolutional neural network (MT-CNN) architectures and a few other approaches,
including random forest (RF) and gradient boosting decision tree (GBDT), on an
equal footing. Comparison is also given to convolutional neural network (CNN)
and non-convolutional deep neural network (DNN) algorithms. Four benchmark
toxicity data sets (i.e., endpoints) are used to evaluate various approaches.
Extensive numerical studies indicate that the present MT-CNN architecture is
able to outperform the state-of-the-art methods.
",Computer Science; Statistics,Computer Science; Statistics
"Service adoption spreading in online social networks   The collective behaviour of people adopting an innovation, product or online
service is commonly interpreted as a spreading phenomenon throughout the fabric
of society. This process is arguably driven by social influence, social
learning and by external effects like media. Observations of such processes
date back to the seminal studies by Rogers and Bass, and their mathematical
modelling has taken two directions: One paradigm, called simple contagion,
identifies adoption spreading with an epidemic process. The other one, named
complex contagion, is concerned with behavioural thresholds and successfully
explains the emergence of large cascades of adoption resulting in a rapid
spreading often seen in empirical data. The observation of real world adoption
processes has become easier lately due to the availability of large digital
social network and behavioural datasets. This has allowed simultaneous study of
network structures and dynamics of online service adoption, shedding light on
the mechanisms and external effects that influence the temporal evolution of
behavioural or innovation adoption. These advancements have induced the
development of more realistic models of social spreading phenomena, which in
turn have provided remarkably good predictions of various empirical adoption
processes. In this chapter we review recent data-driven studies addressing
real-world service adoption processes. Our studies provide the first detailed
empirical evidence of a heterogeneous threshold distribution in adoption. We
also describe the modelling of such phenomena with formal methods and
data-driven simulations. Our objective is to understand the effects of
identified social mechanisms on service adoption spreading, and to provide
potential new directions and open questions for future research.
",Computer Science; Physics,Computer Science; Physics
"Maximally rotating waves in AdS and on spheres   We study the cubic wave equation in AdS_(d+1) (and a closely related cubic
wave equation on S^3) in a weakly nonlinear regime. Via time-averaging, these
systems are accurately described by simplified infinite-dimensional quartic
Hamiltonian systems, whose structure is mandated by the fully resonant spectrum
of linearized perturbations. The maximally rotating sector, comprising only the
modes of maximal angular momentum at each frequency level, consistently
decouples in the weakly nonlinear regime. The Hamiltonian systems obtained by
this decoupling display remarkable periodic return behaviors closely analogous
to what has been demonstrated in recent literature for a few other related
equations (the cubic Szego equation, the conformal flow, the LLL equation).
This suggests a powerful underlying analytic structure, such as integrability.
We comment on the connection of our considerations to the Gross-Pitaevskii
equation for harmonically trapped Bose-Einstein condensates.
",Physics; Mathematics,Physics
"Pressure-induced Superconductivity in the Three-component Fermion Topological Semimetal Molybdenum Phosphide   Topological semimetal, a novel state of quantum matter hosting exotic
emergent quantum phenomena dictated by the non-trivial band topology, has
emerged as a new frontier in condensed-matter physics. Very recently, a
coexistence of triply degenerate points of band crossing and Weyl points near
the Fermi level was theoretically predicted and immediately experimentally
verified in single crystalline molybdenum phosphide (MoP). Here we show in this
material the high-pressure electronic transport and synchrotron X-ray
diffraction (XRD) measurements, combined with density functional theory (DFT)
calculations. We report the emergence of pressure-induced superconductivity in
MoP with a critical temperature Tc of about 2 K at 27.6 GPa, rising to 3.7 K at
the highest pressure of 95.0 GPa studied. No structural phase transitions is
detected up to 60.6 GPa from the XRD. Meanwhile, the Weyl points and triply
degenerate points topologically protected by the crystal symmetry are retained
at high pressure as revealed by our DFT calculations. The coexistence of
three-component fermion and superconductivity in heavily pressurized MoP offers
an excellent platform to study the interplay between topological phase of
matter and superconductivity.
",Physics,Physics
"Implementing GraphQL as a Query Language for Deductive Databases in SWI-Prolog Using DCGs, Quasi Quotations, and Dicts   The methods to access large relational databases in a distributed system are
well established: the relational query language SQL often serves as a language
for data access and manipulation, and in addition public interfaces are exposed
using communication protocols like REST. Similarly to REST, GraphQL is the
query protocol of an application layer developed by Facebook. It provides a
unified interface between the client and the server for data fetching and
manipulation. Using GraphQL's type system, it is possible to specify data
handling of various sources and to combine, e.g., relational with NoSQL
databases. In contrast to REST, GraphQL provides a single API endpoint and
supports flexible queries over linked data.
GraphQL can also be used as an interface for deductive databases. In this
paper, we give an introduction of GraphQL and a comparison to REST. Using
language features recently added to SWI-Prolog 7, we have developed the Prolog
library GraphQL.pl, which implements the GraphQL type system and query syntax
as a domain-specific language with the help of definite clause grammars (DCG),
quasi quotations, and dicts. Using our library, the type system created for a
deductive database can be validated, while the query system provides a unified
interface for data access and introspection.
",Computer Science,Computer Science
"A second-order stochastic maximum principle for generalized mean-field control problem   In this paper, we study the generalized mean-field stochastic control problem
when the usual stochastic maximum principle (SMP) is not applicable due to the
singularity of the Hamiltonian function. In this case, we derive a second order
SMP. We introduce the adjoint process by the generalized mean-field backward
stochastic differential equation. The keys in the proofs are the expansion of
the cost functional in terms of a perturbation parameter, and the use of the
range theorem for vector-valued measures.
",Mathematics,Mathematics
"Double spend races   We correct the double spend race analysis given in Nakamoto's foundational
Bitcoin article and give a closed-form formula for the probability of success
of a double spend attack using the Regularized Incomplete Beta Function. We
give a proof of the exponential decay on the number of confirmations, often
cited in the literature, and find an asymptotic formula. Larger number of
confirmations are necessary compared to those given by Nakamoto. We also
compute the probability conditional to the known validation time of the blocks.
This provides a finer risk analysis than the classical one.
",Computer Science; Mathematics,Computer Science; Mathematics
"Tilings of convex sets by mutually incongruent equilateral triangles contain arbitrarily small tiles   We show that every tiling of a convex set in the Euclidean plane
$\mathbb{R}^2$ by equilateral triangles of mutually different sizes contains
arbitrarily small tiles. The proof is purely elementary up to the discussion of
one family of tilings of the full plane $\mathbb{R}^2$, which is based on a
surprising connection to a random walk on a directed graph.
",Mathematics,Mathematics
"Search for Food of Birds, Fish and Insects   This book chapter introduces to the problem to which extent search strategies
of foraging biological organisms can be identified by statistical data analysis
and mathematical modeling. A famous paradigm in this field is the Levy Flight
Hypothesis: It states that under certain mathematical conditions Levy flights,
which are a key concept in the theory of anomalous stochastic processes,
provide an optimal search strategy. This hypothesis may be understood
biologically as the claim that Levy flights represent an evolutionary adaptive
optimal search strategy for foraging organisms. Another interpretation,
however, is that Levy flights emerge from the interaction between a forager and
a given (scale-free) distribution of food sources. These hypotheses are
discussed controversially in the current literature. We give examples and
counterexamples of experimental data and their analyses supporting and
challenging them.
",Quantitative Biology,Quantitative Biology
"Galerkin Least-Squares Stabilization in Ice Sheet Modeling - Accuracy, Robustness, and Comparison to other Techniques   We investigate the accuracy and robustness of one of the most common methods
used in glaciology for the discretization of the $\mathfrak{p}$-Stokes
equations: equal order finite elements with Galerkin Least-Squares (GLS)
stabilization. Furthermore we compare the results to other stabilized methods.
We find that the vertical velocity component is more sensitive to the choice of
GLS stabilization parameter than horizontal velocity. Additionally, the
accuracy of the vertical velocity component is especially important since
errors in this component can cause ice surface instabilities and propagate into
future ice volume predictions. If the element cell size is set to the minimum
edge length and the stabilization parameter is allowed to vary non-linearly
with viscosity, the GLS stabilization parameter found in literature is a good
choice on simple domains. However, near ice margins the standard parameter
choice may result in significant oscillations in the vertical component of the
surface velocity. For these cases, other stabilization techniques, such as the
interior penalty method, result in better accuracy and are less sensitive to
the choice of the stabilization parameter. During this work we also discovered
that the manufactured solutions often used to evaluate errors in glaciology are
not reliable due to high artificial surface forces at singularities. We perform
our numerical experiments in both FEniCS and Elmer/Ice.
",Physics,Physics
"Incremental Principal Component Analysis Exact implementation and continuity corrections   This paper describes some applications of an incremental implementation of
the principal component analysis (PCA). The algorithm updates the
transformation coefficients matrix on-line for each new sample, without the
need to keep all the samples in memory. The algorithm is formally equivalent to
the usual batch version, in the sense that given a sample set the
transformation coefficients at the end of the process are the same. The
implications of applying the PCA in real time are discussed with the help of
data analysis examples. In particular we focus on the problem of the continuity
of the PCs during an on-line analysis.
",Computer Science; Statistics,Computer Science; Statistics
"On semi-supervised learning   Semi-supervised learning deals with the problem of how, if possible, to take
advantage of a huge amount of unclassified data, to perform a classification in
situations when, typically, there is little labeled data. Even though this is
not always possible (it depends on how useful, for inferring the labels, it
would be to know the distribution of the unlabeled data), several algorithm
have been proposed recently. %but in general they are not proved to outperform
A new algorithm is proposed, that under almost necessary conditions, %and it is
proved that it attains asymptotically the performance of the best theoretical
rule as the amount of unlabeled data tends to infinity. The set of necessary
assumptions, although reasonable, show that semi-supervised classification only
works for very well conditioned problems. The focus is on understanding when
and why semi-supervised learning works when the size of the initial training
sample remains fixed and the asymptotic is on the size of the unlabeled data.
The performance of the algorithm is assessed in the well known ""Isolet""
real-data of phonemes, where a strong dependence on the choice of the initial
training sample is shown.
",Statistics,Computer Science; Statistics
"Maximum Number of Modes of Gaussian Mixtures   Gaussian mixture models are widely used in Statistics. A fundamental aspect
of these distributions is the study of the local maxima of the density, or
modes. In particular, it is not known how many modes a mixture of $k$ Gaussians
in $d$ dimensions can have. We give a brief account of this problem's history.
Then, we give improved lower bounds and the first upper bound on the maximum
number of modes, provided it is finite.
",Mathematics; Statistics,Mathematics; Statistics
"Some properties of nested Kriging predictors   Kriging is a widely employed technique, in particular for computer
experiments, in machine learning or in geostatistics. An important challenge
for Kriging is the computational burden when the data set is large. We focus on
a class of methods aiming at decreasing this computational cost, consisting in
aggregating Kriging predictors based on smaller data subsets. We prove that
aggregations based solely on the conditional variances provided by the
different Kriging predictors can yield an inconsistent final Kriging
prediction. In contrasts, we study theoretically the recent proposal by
[Rulli{è}re et al., 2017] and obtain additional attractive properties for it.
We prove that this predictor is consistent, we show that it can be interpreted
as an exact conditional distribution for a modified process and we provide
error bounds for it.
",Mathematics; Statistics,Statistics
"Efficient Pricing of Barrier Options on High Volatility Assets using Subset Simulation   Barrier options are one of the most widely traded exotic options on stock
exchanges. In this paper, we develop a new stochastic simulation method for
pricing barrier options and estimating the corresponding execution
probabilities. We show that the proposed method always outperforms the standard
Monte Carlo approach and becomes substantially more efficient when the
underlying asset has high volatility, while it performs better than multilevel
Monte Carlo for special cases of barrier options and underlying assets. These
theoretical findings are confirmed by numerous simulation results.
",Statistics; Quantitative Finance,Quantitative Finance
"Interplay between social influence and competitive strategical games in multiplex networks   We present a model that takes into account the coupling between evolutionary
game dynamics and social influence. Importantly, social influence and game
dynamics take place in different domains, which we model as different layers of
a multiplex network. We show that the coupling between these dynamical
processes can lead to cooperation in scenarios where the pure game dynamics
predicts defection. In addition, we show that the structure of the network
layers and the relation between them can further increase cooperation.
Remarkably, if the layers are related in a certain way, the system can reach a
polarized metastable state.These findings could explain the prevalence of
polarization observed in many social dilemmas.
",Computer Science; Physics,Computer Science; Physics
"Demonstration of an efficient, photonic-based astronomical spectrograph on an 8-m telescope   We demonstrate for the first time an efficient, photonic-based astronomical
spectrograph on the 8-m Subaru Telescope. An extreme adaptive optics system is
combined with pupil apodiziation optics to efficiently inject light directly
into a single-mode fiber, which feeds a compact cross-dispersed spectrograph
based on array waveguide grating technology. The instrument currently offers a
throughput of 5% from sky-to-detector which we outline could easily be upgraded
to ~13% (assuming a coupling efficiency of 50%). The isolated spectrograph
throughput from the single-mode fiber to detector was 42% at 1550 nm. The
coupling efficiency into the single-mode fiber was limited by the achievable
Strehl ratio on a given night. A coupling efficiency of 47% has been achieved
with ~60% Strehl ratio on-sky to date. Improvements to the adaptive optics
system will enable 90% Strehl ratio and a coupling of up to 67% eventually.
This work demonstrates that the unique combination of advanced technologies
enables the realization of a compact and highly efficient spectrograph, setting
a precedent for future instrument design on very-large and extremely-large
telescopes.
",Physics,Physics
"Formalization of Transform Methods using HOL Light   Transform methods, like Laplace and Fourier, are frequently used for
analyzing the dynamical behaviour of engineering and physical systems, based on
their transfer function, and frequency response or the solutions of their
corresponding differential equations. In this paper, we present an ongoing
project, which focuses on the higher-order logic formalization of transform
methods using HOL Light theorem prover. In particular, we present the
motivation of the formalization, which is followed by the related work. Next,
we present the task completed so far while highlighting some of the challenges
faced during the formalization. Finally, we present a roadmap to achieve our
objectives, the current status and the future goals for this project.
",Computer Science,Computer Science
"Prior-aware Dual Decomposition: Document-specific Topic Inference for Spectral Topic Models   Spectral topic modeling algorithms operate on matrices/tensors of word
co-occurrence statistics to learn topic-specific word distributions. This
approach removes the dependence on the original documents and produces
substantial gains in efficiency and provable topic inference, but at a cost:
the model can no longer provide information about the topic composition of
individual documents. Recently Thresholded Linear Inverse (TLI) is proposed to
map the observed words of each document back to its topic composition. However,
its linear characteristics limit the inference quality without considering the
important prior information over topics. In this paper, we evaluate Simple
Probabilistic Inverse (SPI) method and novel Prior-aware Dual Decomposition
(PADD) that is capable of learning document-specific topic compositions in
parallel. Experiments show that PADD successfully leverages topic correlations
as a prior, notably outperforming TLI and learning quality topic compositions
comparable to Gibbs sampling on various data.
",Computer Science,Statistics
"Procedural Content Generation via Machine Learning (PCGML)   This survey explores Procedural Content Generation via Machine Learning
(PCGML), defined as the generation of game content using machine learning
models trained on existing content. As the importance of PCG for game
development increases, researchers explore new avenues for generating
high-quality content with or without human involvement; this paper addresses
the relatively new paradigm of using machine learning (in contrast with
search-based, solver-based, and constructive methods). We focus on what is most
often considered functional game content such as platformer levels, game maps,
interactive fiction stories, and cards in collectible card games, as opposed to
cosmetic content such as sprites and sound effects. In addition to using PCG
for autonomous generation, co-creativity, mixed-initiative design, and
compression, PCGML is suited for repair, critique, and content analysis because
of its focus on modeling existing content. We discuss various data sources and
representations that affect the resulting generated content. Multiple PCGML
methods are covered, including neural networks, long short-term memory (LSTM)
networks, autoencoders, and deep convolutional networks; Markov models,
$n$-grams, and multi-dimensional Markov chains; clustering; and matrix
factorization. Finally, we discuss open problems in the application of PCGML,
including learning from small datasets, lack of training data, multi-layered
learning, style-transfer, parameter tuning, and PCG as a game mechanic.
",Computer Science,Computer Science; Statistics
"Q-Learning Algorithm for VoLTE Closed-Loop Power Control in Indoor Small Cells   We propose a reinforcement learning (RL) based closed loop power control
algorithm for the downlink of the voice over LTE (VoLTE) radio bearer for an
indoor environment served by small cells. The main contributions of our paper
are to 1) use RL to solve performance tuning problems in an indoor cellular
network for voice bearers and 2) show that our derived lower bound loss in
effective signal to interference plus noise ratio due to neighboring cell
failure is sufficient for VoLTE power control purposes in practical cellular
networks. In our simulation, the proposed RL-based power control algorithm
significantly improves both voice retainability and mean opinion score compared
to current industry standards. The improvement is due to maintaining an
effective downlink signal to interference plus noise ratio against adverse
network operational issues and faults.
",Computer Science; Statistics,Computer Science
"Human-Centered Autonomous Vehicle Systems: Principles of Effective Shared Autonomy   Building effective, enjoyable, and safe autonomous vehicles is a lot harder
than has historically been considered. The reason is that, simply put, an
autonomous vehicle must interact with human beings. This interaction is not a
robotics problem nor a machine learning problem nor a psychology problem nor an
economics problem nor a policy problem. It is all of these problems put into
one. It challenges our assumptions about the limitations of human beings at
their worst and the capabilities of artificial intelligence systems at their
best. This work proposes a set of principles for designing and building
autonomous vehicles in a human-centered way that does not run away from the
complexity of human nature but instead embraces it. We describe our development
of the Human-Centered Autonomous Vehicle (HCAV) as an illustrative case study
of implementing these principles in practice.
",Computer Science,Computer Science
"Superlinear scaling in the urban system of England of Wales. A comparison with US cities   According to the theory of urban scaling, urban indicators scale with city
size in a predictable fashion. In particular, indicators of social and economic
productivity are expected to have a superlinear relation. This behavior was
verified for many urban systems, but recent findings suggest that this pattern
may not be valid for England and Wales (E&W), where income has a linear
relation with city size. This finding raises the question of whether the cities
of E&W exhibit any superlinear relation with respect to quantities such as the
level of education and occupational groups. In this paper, we evaluate the
scaling of educational and occupational groups of E&W to see if we can detect
superlinear relations in the number of educated and better-paid persons. As E&W
may be unique in its linear scaling of income, we complement our analysis by
comparing it to the urban system of the United States (US), a country for which
superlinear scaling of income has already been demonstrated. To make the two
urban systems comparable, we define the urban systems of both countries using
the same method and test the sensitivity of our results to changes in the
boundaries of cities. We find that cities of E&W exhibit patterns of
superlinear scaling with respect to education and certain categories of
better-paid occupations. However, the tendency of such groups to have
superlinear scaling seems to be more consistent in the US. We show that while
the educational and occupational distributions of US cities can partly explain
the superlinear scaling of earnings, the distribution leads to a linear scaling
of earnings in E&W.
",Physics,Computer Science; Physics
"Online Boosting Algorithms for Multi-label Ranking   We consider the multi-label ranking approach to multi-label learning.
Boosting is a natural method for multi-label ranking as it aggregates weak
predictions through majority votes, which can be directly used as scores to
produce a ranking of the labels. We design online boosting algorithms with
provable loss bounds for multi-label ranking. We show that our first algorithm
is optimal in terms of the number of learners required to attain a desired
accuracy, but it requires knowledge of the edge of the weak learners. We also
design an adaptive algorithm that does not require this knowledge and is hence
more practical. Experimental results on real data sets demonstrate that our
algorithms are at least as good as existing batch boosting algorithms.
",Computer Science; Statistics,Computer Science; Statistics
"Posterior Asymptotic Normality for an Individual Coordinate in High-dimensional Linear Regression   We consider the sparse high-dimensional linear regression model
$Y=Xb+\epsilon$ where $b$ is a sparse vector. For the Bayesian approach to this
problem, many authors have considered the behavior of the posterior
distribution when, in truth, $Y=X\beta+\epsilon$ for some given $\beta$. There
have been numerous results about the rate at which the posterior distribution
concentrates around $\beta$, but few results about the shape of that posterior
distribution. We propose a prior distribution for $b$ such that the marginal
posterior distribution of an individual coordinate $b_i$ is asymptotically
normal centered around an asymptotically efficient estimator, under the truth.
Such a result gives Bayesian credible intervals that match with the confidence
intervals obtained from an asymptotically efficient estimator for $b_i$. We
also discuss ways of obtaining such asymptotically efficient estimators on
individual coordinates. We compare the two-step procedure proposed by Zhang and
Zhang (2014) and a one-step modified penalization method.
",Mathematics; Statistics,Mathematics; Statistics
"Second differentials in the Quillen spectral sequence   For an algebraic variety $X$ we introduce generalized first Chern classes,
which are defined for coherent sheaves on $X$ with support in codimension $p$
and take values in $CH^p(X)$. We use them to provide an explicit formula for
the differentials ${d_2^p: E_2^{p,-p-1} \to E_2^{p+2, -p-2} \cong CH^{p+2}(X)}$
in the Quillen spectral sequence.
",Mathematics,Mathematics
"Modelling and prediction of financial trading networks: An application to the NYMEX natural gas futures market   Over the last few years there has been a growing interest in using financial
trading networks to understand the microstructure of financial markets. Most of
the methodologies developed so far for this purpose have been based on the
study of descriptive summaries of the networks such as the average node degree
and the clustering coefficient. In contrast, this paper develops novel
statistical methods for modeling sequences of financial trading networks. Our
approach uses a stochastic blockmodel to describe the structure of the network
during each period, and then links multiple time periods using a hidden Markov
model. This structure allows us to identify events that affect the structure of
the market and make accurate short-term prediction of future transactions. The
methodology is illustrated using data from the NYMEX natural gas futures market
from January 2005 to December 2008.
",Statistics,Quantitative Finance
"Superconductivity in quantum wires: A symmetry analysis   We study properties of quantim wires with spin-orbit coupling and time
reversal symmetry breaking, in normal and superconducting states. Electronic
band structures are classified according to quasi-one-dimensional magnetic
point groups, or magnetic classes. The latter belong to one of three distinct
types, depending on the way the time reversal operation appears in the group
elements. The superconducting gap functions are constructed using antiunitary
operations and have different symmetry properties depending on the type of the
magnetic point group. We obtain the spectrum of the Andreev boundary modes near
the end of the wire in a model-independent way, using the semiclassical
approach with the boundary conditions described by a phenomenological
scattering matrix. Explicit expressions for the bulk topological invariants
controlling the number of the boundary zero modes are presented in the general
multiband case for two types of the magnetic point groups, corresponding to
DIII and BDI symmetry classes.
",Physics,Physics
"Using English as Pivot to Extract Persian-Italian Parallel Sentences from Non-Parallel Corpora   The effectiveness of a statistical machine translation system (SMT) is very
dependent upon the amount of parallel corpus used in the training phase. For
low-resource language pairs there are not enough parallel corpora to build an
accurate SMT. In this paper, a novel approach is presented to extract bilingual
Persian-Italian parallel sentences from a non-parallel (comparable) corpus. In
this study, English is used as the pivot language to compute the matching
scores between source and target sentences and candidate selection phase.
Additionally, a new monolingual sentence similarity metric, Normalized Google
Distance (NGD) is proposed to improve the matching process. Moreover, some
extensions of the baseline system are applied to improve the quality of
extracted sentences measured with BLEU. Experimental results show that using
the new pivot based extraction can increase the quality of bilingual corpus
significantly and consequently improves the performance of the Persian-Italian
SMT system.
",Computer Science,Computer Science
"Link colorings and the Goeritz matrix   We discuss the connection between colorings of a link diagram and the Goeritz
matrix.
",Mathematics,Mathematics
"Improving Trajectory Optimization using a Roadmap Framework   We present an evaluation of several representative sampling-based and
optimization-based motion planners, and then introduce an integrated motion
planning system which incorporates recent advances in trajectory optimization
into a sparse roadmap framework. Through experiments in 4 common application
scenarios with 5000 test cases each, we show that optimization-based or
sampling-based planners alone are not effective for realistic problems where
fast planning times are required. To the best of our knowledge, this is the
first work that presents such a systematic and comprehensive evaluation of
state-of-the-art motion planners, which are based on a significant amount of
experiments. We then combine different stand-alone planners with trajectory
optimization. The results show that the combination of our sparse roadmap and
trajectory optimization provides superior performance over other standard
sampling-based planners combinations. By using a multi-query roadmap instead of
generating completely new trajectories for each planning problem, our approach
allows for extensions such as persistent control policy information associated
with a trajectory across planning problems. Also, the sub-optimality resulting
from the sparsity of roadmap, as well as the unexpected disturbances from the
environment, can both be overcome by the real-time trajectory optimization
process.
",Computer Science,Computer Science
"Full-angle Negative Reflection with An Ultrathin Acoustic Gradient Metasurface: Floquet-Bloch Modes Perspective and Experimental Verification   Metasurface with gradient phase response offers new alternative for steering
the propagation of waves. Conventional Snell's law has been revised by taking
the contribution of local phase gradient into account. However, the requirement
of momentum matching along the metasurface sets its nontrivial beam
manipulation functionality within a limited-angle incidence. In this work, we
theoretically and experimentally demonstrate that the acoustic gradient
metasurface supports the negative reflection for full-angle incidence. The mode
expansion theory is developed to help understand how the gradient metasurface
tailors the incident beams, and the full-angle negative reflection occurs when
the first negative order Floquet-Bloch mode dominates. The coiling-up space
structures are utilized to build desired acoustic gradient metasurface and the
full-angle negative reflections have been perfectly verified by experimental
measurements. Our work offers the Floquet-Bloch modes perspective for
qualitatively understanding the reflection behaviors of the acoustic gradient
metasurface and enables a new degree of the acoustic wave manipulating.
",Physics,Physics
"Transverse spinning of light with globally unique handedness   Access to the transverse spin of light has unlocked new regimes in
topological photonics and optomechanics. To achieve the transverse spin of
nonzero longitudinal fields, various platforms that derive transversely
confined waves based on focusing, interference, or evanescent waves have been
suggested. Nonetheless, because of the transverse confinement inherently
accompanying sign reversal of the field derivative, the resulting transverse
spin handedness experiences spatial inversion, which leads to a mismatch
between the densities of the wavefunction and its spin component and hinders
the global observation of the transverse spin. Here, we reveal a globally pure
transverse spin in which the wavefunction density signifies the spin
distribution, by employing inverse molding of the eigenmode in the spin basis.
Starting from the target spin profile, we analytically obtain the potential
landscape and then show that the elliptic-hyperbolic transition around the
epsilon-near-zero permittivity allows for the global conservation of transverse
spin handedness across the topological interface between anisotropic
metamaterials. Extending to the non-Hermitian regime, we also develop
annihilated transverse spin modes to cover the entire Poincare sphere of the
meridional plane. Our results enable the complete transfer of optical energy to
transverse spinning motions and realize the classical analogy of 3-dimensional
quantum spin states.
",Physics,Physics
"Strong coupling Bose polarons in a BEC   We use a non-perturbative renormalization group approach to develop a unified
picture of the Bose polaron problem, where a mobile impurity is strongly
interacting with a surrounding Bose-Einstein condensate (BEC). A detailed
theoretical analysis of the phase diagram is presented and the
polaron-to-molecule transition is discussed. For attractive polarons we argue
that a description in terms of an effective Fröhlich Hamiltonian with
renormalized parameters is possible. Its strong coupling regime is realized
close to a Feshbach resonance, where we predict a sharp increase of the
effective mass. Already for weaker interactions, before the polaron mass
diverges, we predict a transition to a regime where states exist below the
polaron energy and the attractive polaron is no longer the ground state. On the
repulsive side of the Feshbach resonance we recover the repulsive polaron,
which has a finite lifetime because it can decay into low-lying molecular
states. We show for the entire range of couplings that the polaron energy has
logarithmic corrections in comparison with predictions by the mean-field
approach. We demonstrate that they are a consequence of the polaronic mass
renormalization which is due to quantum fluctuations of correlated phonons in
the polaron cloud.
",Physics,Physics
"Optimal lower exponent for the higher gradient integrability of solutions to two-phase elliptic equations in two dimensions   We study the higher gradient integrability of distributional solutions $u$ to
the equation $div(\sigma \nabla u) = 0$ in dimension two, in the case when the
essential range of $\sigma$ consists of only two elliptic matrices, i.e.,
$\sigma\in\{\sigma_1, \sigma_2\}$ a.e. in $\Omega$.
In [4], for every pair of elliptic matrices $\sigma_1$ and $\sigma_2$,
exponents $p_{\sigma_1,\sigma_2}\in(2,+\infty)$ and $q_{\sigma_1,\sigma_2}\in
(1,2)$ have been characterised so that if $u\in
W^{1,q_{\sigma_1,\sigma_2}}(\Omega)$ is solution to the elliptic equation then
$\nabla u\in L^{p_{\sigma_1,\sigma_2}}_{\rm weak}(\Omega)$ and the optimality
of the upper exponent $p_{\sigma_1,\sigma_2}$ has been proved. In this paper we
complement the above result by proving the optimality of the lower exponent
$q_{\sigma_1,\sigma_2}$. Precisely, we show that for every arbitrarily small
$\delta$, one can find a particular microgeometry, i.e., an arrangement of the
sets $\sigma^{-1}(\sigma_1)$ and $\sigma^{-1}(\sigma_2)$, for which there
exists a solution $u$ to the corresponding elliptic equation such that $\nabla
u \in L^{q_{\sigma_1,\sigma_2}-\delta}$, but $\nabla u \notin
L^{q_{\sigma_1,\sigma_2}}.$ The existence of such optimal microgeometries is
achieved by convex integration methods, adapting to the present setting the
geometric constructions provided in [2] for the isotropic case.
",Mathematics,Mathematics
"Large odd order character sums and improvements of the Pólya-Vinogradov inequality   For a primitive Dirichlet character $\chi$ modulo $q$, we define
$M(\chi)=\max_{t } |\sum_{n \leq t} \chi(n)|$. In this paper, we study this
quantity for characters of a fixed odd order $g\geq 3$. Our main result
provides a further improvement of the classical Pólya-Vinogradov inequality
in this case. More specifically, we show that for any such character $\chi$ we
have $$M(\chi)\ll_{\varepsilon} \sqrt{q}(\log q)^{1-\delta_g}(\log\log
q)^{-1/4+\varepsilon},$$ where $\delta_g := 1-\frac{g}{\pi}\sin(\pi/g)$. This
improves upon the works of Granville and Soundararajan and of Goldmakher.
Furthermore, assuming the Generalized Riemann hypothesis (GRH) we prove that $$
M(\chi) \ll \sqrt{q} \left(\log_2 q\right)^{1-\delta_g} \left(\log_3
q\right)^{-\frac{1}{4}}\left(\log_4 q\right)^{O(1)}, $$ where $\log_j$ is the
$j$-th iterated logarithm. We also show unconditionally that this bound is best
possible (up to a power of $\log_4 q$). One of the key ingredients in the proof
of the upper bounds is a new Halász-type inequality for logarithmic mean
values of completely multiplicative functions, which might be of independent
interest.
",Mathematics,Mathematics
"Abrupt disappearance and reemergence of the SU(2) and SU(4) Kondo effects due to population inversion   The interplay of almost degenerate levels in quantum dots and molecular
junctions with possibly different couplings to the reservoirs has lead to many
observable phenomena, such as the Fano effect, transmission phase slips and the
SU(4) Kondo effect. Here we predict a dramatic repeated disappearance and
reemergence of the SU(4) and anomalous SU(2) Kondo effects with increasing gate
voltage. This phenomenon is attributed to the level occupation switching which
has been previously invoked to explain the universal transmission phase slips
in the conductance through a quantum dot. We use analytical arguments and
numerical renormalization group calculations to explain the observations and
discuss their experimental relevance and dependence on the physical parameters.
",Physics,Physics
"Riesz sequences and generalized arithmetic progressions   The purpose of this note is to verify that the results attained in [6] admit
an extension to the multidimensional setting. Namely, for subsets of the two
dimensional torus we find the sharp growth rate of the step(s) of a generalized
arithmetic progression in terms of its size which may be found in an
exponential systems satisfying the Riesz sequence property.
",Mathematics,Mathematics
"Incremental Skip-gram Model with Negative Sampling   This paper explores an incremental training strategy for the skip-gram model
with negative sampling (SGNS) from both empirical and theoretical perspectives.
Existing methods of neural word embeddings, including SGNS, are multi-pass
algorithms and thus cannot perform incremental model update. To address this
problem, we present a simple incremental extension of SGNS and provide a
thorough theoretical analysis to demonstrate its validity. Empirical
experiments demonstrated the correctness of the theoretical analysis as well as
the practical usefulness of the incremental algorithm.
",Computer Science,Computer Science; Statistics
"Towards Object Life Cycle-Based Variant Generation of Business Process Models   Variability management of process models is a major challenge for
Process-Aware Information Systems. Process model variants can be attributed to
any of the following reasons: new technologies, governmental rules,
organizational context or adoption of new standards. Current approaches to
manage variants of process models address issues such as reducing the huge
effort of modeling from scratch, preventing redundancy, and controlling
inconsistency in process models. Although the effort to manage process model
variants has been exerted, there are still limitations. Furthermore, existing
approaches do not focus on variants that come from change in organizational
perspective of process models. Organizational-driven variant management is an
important area that still needs more study that we focus on in this paper.
Object Life Cycle (OLC) is an important aspect that may change from an
organization to another. This paper introduces an approach inspired by real
life scenario to generate consistent process model variants that come from
adaptations in the OLC.
",Computer Science,Computer Science
"Toric Codes, Multiplicative Structure and Decoding   Long linear codes constructed from toric varieties over finite fields, their
multiplicative structure and decoding. The main theme is the inherent
multiplicative structure on toric codes. The multiplicative structure allows
for \emph{decoding}, resembling the decoding of Reed-Solomon codes and aligns
with decoding by error correcting pairs. We have used the multiplicative
structure on toric codes to construct linear secret sharing schemes with
\emph{strong multiplication} via Massey's construction generalizing the Shamir
Linear secret sharing shemes constructed from Reed-Solomon codes. We have
constructed quantum error correcting codes from toric surfaces by the
Calderbank-Shor-Steane method.
",Computer Science; Mathematics,Computer Science
"Computing Simple Multiple Zeros of Polynomial Systems   Given a polynomial system f associated with a simple multiple zero x of
multiplicity {\mu}, we give a computable lower bound on the minimal distance
between the simple multiple zero x and other zeros of f. If x is only given
with limited accuracy, we propose a numerical criterion that f is certified to
have {\mu} zeros (counting multiplicities) in a small ball around x.
Furthermore, for simple double zeros and simple triple zeros whose Jacobian is
of normalized form, we define modified Newton iterations and prove the
quantified quadratic convergence when the starting point is close to the exact
simple multiple zero. For simple multiple zeros of arbitrary multiplicity whose
Jacobian matrix may not have a normalized form, we perform unitary
transformations and modified Newton iterations, and prove its non-quantified
quadratic convergence and its quantified convergence for simple triple zeros.
",Mathematics,Computer Science
"Sparse-Group Bayesian Feature Selection Using Expectation Propagation for Signal Recovery and Network Reconstruction   We present a Bayesian method for feature selection in the presence of
grouping information with sparsity on the between- and within group level.
Instead of using a stochastic algorithm for parameter inference, we employ
expectation propagation, which is a deterministic and fast algorithm. Available
methods for feature selection in the presence of grouping information have a
number of short-comings: on one hand, lasso methods, while being fast,
underestimate the regression coefficients and do not make good use of the
grouping information, and on the other hand, Bayesian approaches, while
accurate in parameter estimation, often rely on the stochastic and slow Gibbs
sampling procedure to recover the parameters, rendering them infeasible e.g.
for gene network reconstruction. Our approach of a Bayesian sparse-group
framework with expectation propagation enables us to not only recover accurate
parameter estimates in signal recovery problems, but also makes it possible to
apply this Bayesian framework to large-scale network reconstruction problems.
The presented method is generic but in terms of application we focus on gene
regulatory networks. We show on simulated and experimental data that the method
constitutes a good choice for network reconstruction regarding the number of
correctly selected features, prediction on new data and reasonable computing
time.
",Statistics,Computer Science; Statistics
"Scalable Realistic Recommendation Datasets through Fractal Expansions   Recommender System research suffers currently from a disconnect between the
size of academic data sets and the scale of industrial production systems. In
order to bridge that gap we propose to generate more massive user/item
interaction data sets by expanding pre-existing public data sets. User/item
incidence matrices record interactions between users and items on a given
platform as a large sparse matrix whose rows correspond to users and whose
columns correspond to items. Our technique expands such matrices to larger
numbers of rows (users), columns (items) and non zero values (interactions)
while preserving key higher order statistical properties. We adapt the
Kronecker Graph Theory to user/item incidence matrices and show that the
corresponding fractal expansions preserve the fat-tailed distributions of user
engagements, item popularity and singular value spectra of user/item
interaction matrices. Preserving such properties is key to building large
realistic synthetic data sets which in turn can be employed reliably to
benchmark Recommender Systems and the systems employed to train them. We
provide algorithms to produce such expansions and apply them to the MovieLens
20 million data set comprising 20 million ratings of 27K movies by 138K users.
The resulting expanded data set has 10 billion ratings, 2 million items and
864K users in its smaller version and can be scaled up or down. A larger
version features 655 billion ratings, 7 million items and 17 million users.
",Computer Science; Statistics,Computer Science
"Networks of planar Hamiltonian systems   We introduce diffusively coupled networks where the dynamical system at each
vertex is planar Hamiltonian. The problems we address are synchronisation and
an analogue of diffusion-driven Turing instability for time-dependent
homogeneous states. As a consequence of the underlying Hamiltonian structure
there exist unusual behaviours compared with networks of coupled limit cycle
oscillators or activator-inhibitor systems.
",Physics; Mathematics,Physics
"Normal forms of dispersive scalar Poisson brackets with two independent variables   We classify the dispersive Poisson brackets with one dependent variable and
two independent variables, with leading order of hydrodynamic type, up to Miura
transformations. We show that, in contrast to the case of a single independent
variable for which a well known triviality result exists, the Miura equivalence
classes are parametrised by an infinite number of constants, which we call
numerical invariants of the brackets. We obtain explicit formulas for the first
few numerical invariants.
",Physics; Mathematics,Mathematics
"Optimal client recommendation for market makers in illiquid financial products   The process of liquidity provision in financial markets can result in
prolonged exposure to illiquid instruments for market makers. In this case,
where a proprietary position is not desired, pro-actively targeting the right
client who is likely to be interested can be an effective means to offset this
position, rather than relying on commensurate interest arising through natural
demand. In this paper, we consider the inference of a client profile for the
purpose of corporate bond recommendation, based on typical recorded information
available to the market maker. Given a historical record of corporate bond
transactions and bond meta-data, we use a topic-modelling analogy to develop a
probabilistic technique for compiling a curated list of client recommendations
for a particular bond that needs to be traded, ranked by probability of
interest. We show that a model based on Latent Dirichlet Allocation offers
promising performance to deliver relevant recommendations for sales traders.
",Statistics,Quantitative Finance
"Efficient Use of Limited-Memory Accelerators for Linear Learning on Heterogeneous Systems   We propose a generic algorithmic building block to accelerate training of
machine learning models on heterogeneous compute systems. Our scheme allows to
efficiently employ compute accelerators such as GPUs and FPGAs for the training
of large-scale machine learning models, when the training data exceeds their
memory capacity. Also, it provides adaptivity to any system's memory hierarchy
in terms of size and processing speed. Our technique is built upon novel
theoretical insights regarding primal-dual coordinate methods, and uses duality
gap information to dynamically decide which part of the data should be made
available for fast processing. To illustrate the power of our approach we
demonstrate its performance for training of generalized linear models on a
large-scale dataset exceeding the memory size of a modern GPU, showing an
order-of-magnitude speedup over existing approaches.
",Computer Science; Statistics,Computer Science; Statistics
"Linearity of stability conditions   We study different concepts of stability for modules over a finite
dimensional algebra: linear stability, given by a ""central charge"", and
nonlinear stability given by the wall-crossing sequence of a ""green path"". Two
other concepts, finite Harder-Narasimhan stratification of the module category
and maximal forward hom-orthogonal sequences of Schurian modules, which are
always equivalent to each other, are shown to be equivalent to nonlinear
stability and to a maximal green sequence, defined using Fomin-Zelevinsky
quiver mutation, in the case the algebra is hereditary.
This is the first of a series of three papers whose purpose is to determine
all maximal green sequences of maximal length for quivers of affine type
$\tilde A$ and determine which are linear. The complete answer will be given in
the final paper [1].
",Mathematics,Mathematics
"Continuous CM-regularity of semihomogeneous vector bundles   We show that if $X$ is an abelian variety of dimension $g \geq 1$ and
${\mathcal E}$ is an M-regular coherent sheaf on $X$, the Castelnuovo-Mumford
regularity of ${\mathcal E}$ with respect to an ample and globally generated
line bundle ${\mathcal O}(1)$ on $X$ is at most $g$, and that equality is
obtained when ${\mathcal E}^{\vee}(1)$ is continuously globally generated. As
an application, we give a numerical characterization of ample semihomogeneous
vector bundles for which this bound is attained.
",Mathematics,Mathematics
"Motion Planning for a UAV with a Straight or Kinked Tether   This paper develops and compares two motion planning algorithms for a
tethered UAV with and without the possibility of the tether contacting the
confined and cluttered environment. Tethered aerial vehicles have been studied
due to their advantages such as power duration, stability, and safety. However,
the disadvantages brought in by the extra tether have not been well
investigated by the robotic locomotion community, especially when the tethered
agent is locomoting in a non-free space occupied with obstacles. In this work,
we propose two motion planning frameworks that (1) reduce the reachable
configuration space by taking into account the tether and (2) deliberately plan
(and relax) the contact point(s) of the tether with the environment and enable
an equivalent reachable configuration space as the non-tethered counterpart
would have. Both methods are tested on a physical robot, Fotokite Pro. With our
approaches, tethered aerial vehicles could find their applications in confined
and cluttered environments with obstacles as opposed to ideal free space, while
still maintaining the advantages from the usage of a tether. The motion
planning strategies are particularly suitable for marsupial heterogeneous
robotic teams, such as visual servoing/assisting for another mobile,
tele-operated primary robot.
",Computer Science,Computer Science
"The Weighted Kendall and High-order Kernels for Permutations   We propose new positive definite kernels for permutations. First we introduce
a weighted version of the Kendall kernel, which allows to weight unequally the
contributions of different item pairs in the permutations depending on their
ranks. Like the Kendall kernel, we show that the weighted version is invariant
to relabeling of items and can be computed efficiently in $O(n \ln(n))$
operations, where $n$ is the number of items in the permutation. Second, we
propose a supervised approach to learn the weights by jointly optimizing them
with the function estimated by a kernel machine. Third, while the Kendall
kernel considers pairwise comparison between items, we extend it by considering
higher-order comparisons among tuples of items and show that the supervised
approach of learning the weights can be systematically generalized to
higher-order permutation kernels.
",Statistics,Computer Science; Statistics
"Optic Disc and Cup Segmentation Methods for Glaucoma Detection with Modification of U-Net Convolutional Neural Network   Glaucoma is the second leading cause of blindness all over the world, with
approximately 60 million cases reported worldwide in 2010. If undiagnosed in
time, glaucoma causes irreversible damage to the optic nerve leading to
blindness. The optic nerve head examination, which involves measurement of
cup-to-disc ratio, is considered one of the most valuable methods of structural
diagnosis of the disease. Estimation of cup-to-disc ratio requires segmentation
of optic disc and optic cup on eye fundus images and can be performed by modern
computer vision algorithms. This work presents universal approach for automatic
optic disc and cup segmentation, which is based on deep learning, namely,
modification of U-Net convolutional neural network. Our experiments include
comparison with the best known methods on publicly available databases
DRIONS-DB, RIM-ONE v.3, DRISHTI-GS. For both optic disc and cup segmentation,
our method achieves quality comparable to current state-of-the-art methods,
outperforming them in terms of the prediction time.
",Computer Science; Statistics,Computer Science; Statistics
"Learning Probabilistic Programs Using Backpropagation   Probabilistic modeling enables combining domain knowledge with learning from
data, thereby supporting learning from fewer training instances than purely
data-driven methods. However, learning probabilistic models is difficult and
has not achieved the level of performance of methods such as deep neural
networks on many tasks. In this paper, we attempt to address this issue by
presenting a method for learning the parameters of a probabilistic program
using backpropagation. Our approach opens the possibility to building deep
probabilistic programming models that are trained in a similar way to neural
networks.
",Computer Science; Statistics,Computer Science; Statistics
"A Fourier analytic approach to inhomogeneous Diophantine approximation   In this paper, we study inhomogeneous Diophantine approximation with rational
numbers of reduced form. The central object to study is the set $W(f,\theta)$
as follows, \begin{eqnarray*} \left\{x\in [0,1]:\left
|x-\frac{m+\theta(n)}{n}\right|<\frac{f(n)}{n}\text{ for infinitely many
coprime pairs of numbers } m,n\right\}, \end{eqnarray*} where
$\{f(n)\}_{n\in\mathbb{N}}$ and $\{\theta(n)\}_{n\in\mathbb{N}}$ are sequences
of real numbers in $[0,1/2]$. We will completely determine the Hausdorff
dimension of $W(f,\theta)$ in terms of $f$ and $\theta$. As a by-product, we
also obtain a new sufficient condition for $W(f,\theta)$ to have full Lebesgue
measure and this result is closely related to the study of \ds with extra
conditions.
",Mathematics,Mathematics
"Environmental feedback drives cooperation in spatial social dilemmas   Exploiting others is beneficial individually but it could also be detrimental
globally. The reverse is also true: a higher cooperation level may change the
environment in a way that is beneficial for all competitors. To explore the
possible consequence of this feedback we consider a coevolutionary model where
the local cooperation level determines the payoff values of the applied
prisoner's dilemma game. We observe that the coevolutionary rule provides a
significantly higher cooperation level comparing to the traditional setup
independently of the topology of the applied interaction graph. Interestingly,
this cooperation supporting mechanism offers lonely defectors a high surviving
chance for a long period hence the relaxation to the final cooperating state
happens logarithmically slow. As a consequence, the extension of the
traditional evolutionary game by considering interactions with the environment
provides a good opportunity for cooperators, but their reward may arrive with
some delay.
",Quantitative Biology,Computer Science; Physics
"Tensorizing Generative Adversarial Nets   Generative Adversarial Network (GAN) and its variants exhibit
state-of-the-art performance in the class of generative models. To capture
higher-dimensional distributions, the common learning procedure requires high
computational complexity and a large number of parameters. The problem of
employing such massive framework arises when deploying it on a platform with
limited computational power such as mobile phones. In this paper, we present a
new generative adversarial framework by representing each layer as a tensor
structure connected by multilinear operations, aiming to reduce the number of
model parameters by a large factor while preserving the generative performance
and sample quality. To learn the model, we employ an efficient algorithm which
alternatively optimizes both discriminator and generator. Experimental outcomes
demonstrate that our model can achieve high compression rate for model
parameters up to $35$ times when compared to the original GAN for MNIST
dataset.
",Computer Science; Statistics,Computer Science; Statistics
"An Analytical Design Optimization Method for Electric Propulsion Systems of Multicopter UAVs with Desired Hovering Endurance   Multicopters are becoming increasingly important in both civil and military
fields. Currently, most multicopter propulsion systems are designed by
experience and trial-and-error experiments, which are costly and ineffective.
This paper proposes a simple and practical method to help designers find the
optimal propulsion system according to the given design requirements. First,
the modeling methods for four basic components of the propulsion system
including propellers, motors, electric speed controls, and batteries are
studied respectively. Secondly, the whole optimization design problem is
simplified and decoupled into several sub-problems. By solving these
sub-problems, the optimal parameters of each component can be obtained
respectively. Finally, based on the obtained optimal component parameters, the
optimal product of each component can be quickly located and determined from
the corresponding database. Experiments and statistical analyses demonstrate
the effectiveness of the proposed method.
",Computer Science,Computer Science
"Holomorphic Hermite polynomials in two variables   Generalizations of the Hermite polynomials to many variables and/or to the
complex domain have been located in mathematical and physical literature for
some decades. Polynomials traditionally called complex Hermite ones are mostly
understood as polynomials in $z$ and $\bar{z}$ which in fact makes them
polynomials in two real variables with complex coefficients. The present paper
proposes to investigate for the first time holomorphic Hermite polynomials in
two variables. Their algebraic and analytic properties are developed here.
While the algebraic properties do not differ too much for those considered so
far, their analytic features are based on a kind of non-rotational
orthogonality invented by van Eijndhoven and Meyers. Inspired by their
invention we merely follow the idea of Bargmann's seminal paper (1961) giving
explicit construction of reproducing kernel Hilbert spaces based on those
polynomials. ""Homotopic"" behavior of our new formation culminates in comparing
it to the very classical Bargmann space of two variables on one edge and the
aforementioned Hermite polynomials in $z$ and $\bar{z}$ on the other. Unlike in
the case of Bargmann's basis our Hermite polynomials are not product ones but
factorize to it when bonded together with the first case of limit properties
leading both to the Bargmann basis and suitable form of the reproducing kernel.
Also in the second limit we recover standard results obeyed by Hermite
polynomials in $z$ and $\bar{z}$.
",Mathematics,Mathematics
"Conditions for the invertibility of dual energy data   The Alvarez-Macovski method [Alvarez, R. E and Macovski, A.,
""Energy-selective reconstructions in X-ray computerized tomography"", Phys. Med.
Biol. (1976), 733--44] requires the inversion of the transformation from the
line integrals of the basis set coefficients to measurements with multiple
x-ray spectra. Analytical formulas for invertibility of the transformation from
two measurements to two line integrals are derived. It is found that
non-invertible systems have near zero Jacobian determinants on a nearly
straight line in the line integrals plane. Formulas are derived for the points
where the line crosses the axes, thus determining the line. Additional formulas
are derived for the values of the terms of the Jacobian determinant at the
endpoints of the line of non-invertibility. The formulas are applied to a set
of spectra including one suggested by Levine that is not invertible as well as
similar spectra that are invertible and voltage switched x-ray tube spectra
that are also invertible. An iterative inverse transformation algorithm
exhibits large errors with non-invertible spectra.
",Physics,Physics
"Controlling Multimode Optomechanical Interactions via Interference   We demonstrate optomechanical interference in a multimode system, in which an
optical mode couples to two mechanical modes. A phase-dependent
excitation-coupling approach is developed, which enables the observation of
constructive and destructive optomechanical interferences. The destructive
interference prevents the coupling of the mechanical system to the optical
mode, suppressing optically-induced mechanical damping. These studies establish
optomechanical interference as an essential tool for controlling the
interactions between light and mechanical oscillators.
",Physics,Physics
"Rayleigh-Brillouin light scattering spectroscopy of nitrous oxide (N$_2$O)   High signal-to-noise and high-resolution light scattering spectra are
measured for nitrous oxide (N$_2$O) gas at an incident wavelength of 403.00 nm,
at 90$^\circ$ scattering, at room temperature and at gas pressures in the range
$0.5-4$ bar. The resulting Rayleigh-Brillouin light scattering spectra are
compared to a number of models describing in an approximate manner the
collisional dynamics and energy transfer in this gaseous medium of this
polyatomic molecular species. The Tenti-S6 model, based on macroscopic gas
transport coefficients, reproduces the scattering profiles in the entire
pressure range at less than 2\% deviation at a similar level as does the
alternative kinetic Grad's 6-moment model, which is based on the internal
collisional relaxation as a decisive parameter. A hydrodynamic model fails to
reproduce experimental spectra for the low pressures of 0.5-1 bar, but yields
very good agreement ($< 1$\%) in the pressure range $2-4$ bar. While these
three models have a different physical basis the internal molecular relaxation
derived can for all three be described in terms of a bulk viscosity of $\eta_b
\sim (6 \pm 2) \times 10^{-5}$ Pa$\cdot$s. A 'rough-sphere' model, previously
shown to be effective to describe light scattering in SF$_6$ gas, is not found
to be suitable, likely in view of the non-sphericity and asymmetry of the N-N-O
structured linear polyatomic molecule.
",Physics,Physics
"A Two-Layer Component-Based Allocation for Embedded Systems with GPUs   Component-based development is a software engineering paradigm that can
facilitate the construction of embedded systems and tackle its complexities.
The modern embedded systems have more and more demanding requirements. One way
to cope with such versatile and growing set of requirements is to employ
heterogeneous processing power, i.e., CPU-GPU architectures. The new CPU-GPU
embedded boards deliver an increased performance but also introduce additional
complexity and challenges. In this work, we address the component-to-hardware
allocation for CPU-GPU embedded systems. The allocation for such systems is
much complex due to the increased amount of GPU-related information. For
example, while in traditional embedded systems the allocation mechanism may
consider only the CPU memory usage of components to find an appropriate
allocation scheme, in heterogeneous systems, the GPU memory usage needs also to
be taken into account in the allocation process. This paper aims at decreasing
the component-to-hardware allocation complexity by introducing a 2-layer
component-based architecture for heterogeneous embedded systems. The detailed
CPU-GPU information of the system is abstracted at a high-layer by compacting
connected components into single units that behave as regular components. The
allocator, based on the compacted information received from the high-level
layer, computes, with a decreased complexity, feasible allocation schemes. In
the last part of the paper, the 2-layer allocation method is evaluated using an
existing embedded system demonstrator; namely, an underwater robot.
",Computer Science,Computer Science
"Relative Entropy in CFT   By using Araki's relative entropy, Lieb's convexity and the theory of
singular integrals, we compute the mutual information associated with free
fermions, and we deduce many results about entropies for chiral CFT's which are
embedded into free fermions, and their extensions. Such relative entropies in
CFT are here computed explicitly for the first time in a mathematical rigorous
way. Our results agree with previous computations by physicists based on
heuristic arguments; in addition we uncover a surprising connection with the
theory of subfactors, in particular by showing that a certain duality, which is
argued to be true on physical grounds, is in fact violated if the global
dimension of the conformal net is greater than $1.$
",Mathematics,Physics
"DoShiCo Challenge: Domain Shift in Control Prediction   Training deep neural network policies end-to-end for real-world applications
so far requires big demonstration datasets in the real world or big sets
consisting of a large variety of realistic and closely related 3D CAD models.
These real or virtual data should, moreover, have very similar characteristics
to the conditions expected at test time. These stringent requirements and the
time consuming data collection processes that they entail, are currently the
most important impediment that keeps deep reinforcement learning from being
deployed in real-world applications. Therefore, in this work we advocate an
alternative approach, where instead of avoiding any domain shift by carefully
selecting the training data, the goal is to learn a policy that can cope with
it. To this end, we propose the DoShiCo challenge: to train a model in very
basic synthetic environments, far from realistic, in a way that it can be
applied in more realistic environments as well as take the control decisions on
real-world data. In particular, we focus on the task of collision avoidance for
drones. We created a set of simulated environments that can be used as
benchmark and implemented a baseline method, exploiting depth prediction as an
auxiliary task to help overcome the domain shift. Even though the policy is
trained in very basic environments, it can learn to fly without collisions in a
very different realistic simulated environment. Of course several benchmarks
for reinforcement learning already exist - but they never include a large
domain shift. On the other hand, several benchmarks in computer vision focus on
the domain shift, but they take the form of a static datasets instead of
simulated environments. In this work we claim that it is crucial to take the
two challenges together in one benchmark.
",Computer Science,Computer Science
"Spectral Projector-Based Graph Fourier Transforms   The paper presents the graph Fourier transform (GFT) of a signal in terms of
its spectral decomposition over the Jordan subspaces of the graph adjacency
matrix $A$. This representation is unique and coordinate free, and it leads to
unambiguous definition of the spectral components (""harmonics"") of a graph
signal. This is particularly meaningful when $A$ has repeated eigenvalues, and
it is very useful when $A$ is defective or not diagonalizable (as it may be the
case with directed graphs). Many real world large sparse graphs have defective
adjacency matrices. We present properties of the GFT and show it to satisfy a
generalized Parseval inequality and to admit a total variation ordering of the
spectral components. We express the GFT in terms of spectral projectors and
present an illustrative example for a real world large urban traffic dataset.
",Computer Science,Computer Science
"From modelling of systems with constraints to generalized geometry and back to numerics   In this note we describe how some objects from generalized geometry appear in
the qualitative analysis and numerical simulation of mechanical systems. In
particular we discuss double vector bundles and Dirac structures. It turns out
that those objects can be naturally associated to systems with constraints --
we recall the mathematical construction in the context of so called implicit
Lagrangian systems. We explain how they can be used to produce new numerical
methods, that we call Dirac integrators.
On a test example of a simple pendulum in a gravity field we compare the
Dirac integrators with classical explicit and implicit methods, we pay special
attention to conservation of constrains. Then, on a more advanced example of
the Ziegler column we show that the choice of numerical methods can indeed
affect the conclusions of qualitative analysis of the dynamics of mechanical
systems. We also tell why we think that Dirac integrators are appropriate for
this kind of systems by explaining the relation with the notions of geometric
degree of non-conservativity and kinematic structural stability.
",Computer Science,Physics
"Field dependence of non-reciprocal magnons in chiral MnSi   Spin waves in chiral magnetic materials are strongly influenced by the
Dzyaloshinskii-Moriya interaction resulting in intriguing phenomena like
non-reciprocal magnon propagation and magnetochiral dichroism. Here, we study
the non-reciprocal magnon spectrum of the archetypical chiral magnet MnSi and
its evolution as a function of magnetic field covering the field-polarized and
conical helix phase. Using inelastic neutron scattering, the magnon energies
and their spectral weights are determined quantitatively after deconvolution
with the instrumental resolution. In the field-polarized phase the imaginary
part of the dynamical susceptibility $\chi''(\varepsilon, {\bf q})$ is shown to
be asymmetric with respect to wavevectors ${\bf q}$ longitudinal to the applied
magnetic field ${\bf H}$, which is a hallmark of chiral magnetism. In the
helimagnetic phase, $\chi''(\varepsilon, {\bf q})$ becomes increasingly
symmetric with decreasing ${\bf H}$ due to the formation of helimagnon bands
and the activation of additional spinflip and non-spinflip scattering channels.
The neutron spectra are in excellent quantitative agreement with the low-energy
theory of cubic chiral magnets with a single fitting parameter being the
damping rate of spin waves.
",Physics,Physics
"The Quest for Scalability and Accuracy in the Simulation of the Internet of Things: an Approach based on Multi-Level Simulation   This paper presents a methodology for simulating the Internet of Things (IoT)
using multi-level simulation models. With respect to conventional simulators,
this approach allows us to tune the level of detail of different parts of the
model without compromising the scalability of the simulation. As a use case, we
have developed a two-level simulator to study the deployment of smart services
over rural territories. The higher level is base on a coarse grained,
agent-based adaptive parallel and distributed simulator. When needed, this
simulator spawns OMNeT++ model instances to evaluate in more detail the issues
concerned with wireless communications in restricted areas of the simulated
world. The performance evaluation confirms the viability of multi-level
simulations for IoT environments.
",Computer Science,Computer Science
"Action-conditional Sequence Modeling for Recommendation   In many online applications interactions between a user and a web-service are
organized in a sequential way, e.g., user browsing an e-commerce website. In
this setting, recommendation system acts throughout user navigation by showing
items. Previous works have addressed this recommendation setup through the task
of predicting the next item user will interact with. In particular, Recurrent
Neural Networks (RNNs) has been shown to achieve substantial improvements over
collaborative filtering baselines. In this paper, we consider interactions
triggered by the recommendations of deployed recommender system in addition to
browsing behavior. Indeed, it is reported that in online services interactions
with recommendations represent up to 30\% of total interactions. Moreover, in
practice, recommender system can greatly influence user behavior by promoting
specific items. In this paper, we extend the RNN modeling framework by taking
into account user interaction with recommended items. We propose and evaluate
RNN architectures that consist of the recommendation action module and the
state-action fusion module. Using real-world large-scale datasets we
demonstrate improved performance on the next item prediction task compared to
the baselines.
",Statistics,Computer Science
"Sleep Stage Classification Based on Multi-level Feature Learning and Recurrent Neural Networks via Wearable Device   This paper proposes a practical approach for automatic sleep stage
classification based on a multi-level feature learning framework and Recurrent
Neural Network (RNN) classifier using heart rate and wrist actigraphy derived
from a wearable device. The feature learning framework is designed to extract
low- and mid-level features. Low-level features capture temporal and frequency
domain properties and mid-level features learn compositions and structural
information of signals. Since sleep staging is a sequential problem with
long-term dependencies, we take advantage of RNNs with Bidirectional Long
Short-Term Memory (BLSTM) architectures for sequence data learning. To simulate
the actual situation of daily sleep, experiments are conducted with a resting
group in which sleep is recorded in resting state, and a comprehensive group in
which both resting sleep and non-resting sleep are included.We evaluate the
algorithm based on an eight-fold cross validation to classify five sleep stages
(W, N1, N2, N3, and REM). The proposed algorithm achieves weighted precision,
recall and F1 score of 58.0%, 60.3%, and 58.2% in the resting group and 58.5%,
61.1%, and 58.5% in the comprehensive group, respectively. Various comparison
experiments demonstrate the effectiveness of feature learning and BLSTM. We
further explore the influence of depth and width of RNNs on performance. Our
method is specially proposed for wearable devices and is expected to be
applicable for long-term sleep monitoring at home. Without using too much prior
domain knowledge, our method has the potential to generalize sleep disorder
detection.
",Computer Science; Statistics,Computer Science
"A Novel Approach to Forecasting Financial Volatility with Gaussian Process Envelopes   In this paper we use Gaussian Process (GP) regression to propose a novel
approach for predicting volatility of financial returns by forecasting the
envelopes of the time series. We provide a direct comparison of their
performance to traditional approaches such as GARCH. We compare the forecasting
power of three approaches: GP regression on the absolute and squared returns;
regression on the envelope of the returns and the absolute returns; and
regression on the envelope of the negative and positive returns separately. We
use a maximum a posteriori estimate with a Gaussian prior to determine our
hyperparameters. We also test the effect of hyperparameter updating at each
forecasting step. We use our approaches to forecast out-of-sample volatility of
four currency pairs over a 2 year period, at half-hourly intervals. From three
kernels, we select the kernel giving the best performance for our data. We use
two published accuracy measures and four statistical loss functions to evaluate
the forecasting ability of GARCH vs GPs. In mean squared error the GP's perform
20% better than a random walk model, and 50% better than GARCH for the same
data.
",Computer Science; Statistics,Statistics
"Better than counting: Density profiles from force sampling   Calculating one-body density profiles in equilibrium via particle-based
simulation methods involves counting of events of particle occurrences at
(histogram-resolved) space points. Here we investigate an alternative method
based on a histogram of the local force density. Via an exact sum rule the
density profile is obtained with a simple spatial integration. The method
circumvents the inherent ideal gas fluctuations. We have tested the method in
Monte Carlo, Brownian Dynamics and Molecular Dynamics simulations. The results
carry a statistical uncertainty smaller than that of the standard, counting,
method, reducing therefore the computation time.
",Physics,Physics
"Generic Dynamical Phase Transition in One-Dimensional Bulk-Driven Lattice Gases with Exclusion   Dynamical phase transitions are crucial features of the fluctuations of
statistical systems, corresponding to boundaries between qualitatively
different mechanisms of maintaining unlikely values of dynamical observables
over long periods of time. They manifest themselves in the form of
non-analyticities in the large deviation function of those observables. In this
paper, we look at bulk-driven exclusion processes with open boundaries. It is
known that the standard asymmetric simple exclusion process exhibits a
dynamical phase transition in the large deviations of the current of particles
flowing through it. That phase transition has been described thanks to specific
calculation methods relying on the model being exactly solvable, but more
general methods have also been used to describe the extreme large deviations of
that current, far from the phase transition. We extend those methods to a large
class of models based on the ASEP, where we add arbitrary spatial
inhomogeneities in the rates and short-range potentials between the particles.
We show that, as for the regular ASEP, the large deviation function of the
current scales differently with the size of the system if one considers very
high or very low currents, pointing to the existence of a dynamical phase
transition between those two regimes: high current large deviations are
extensive in the system size, and the typical states associated to them are
Coulomb gases, which are correlated ; low current large deviations do not
depend on the system size, and the typical states associated to them are
anti-shocks, consistently with a hydrodynamic behaviour. Finally, we illustrate
our results numerically on a simple example, and we interpret the transition in
terms of the current pushing beyond its maximal hydrodynamic value, as well as
relate it to the appearance of Tracy-Widom distributions in the relaxation
statistics of such models.
",Physics,Physics
"Explicit construction of RIP matrices is Ramsey-hard   Matrices $\Phi\in\R^{n\times p}$ satisfying the Restricted Isometry Property
(RIP) are an important ingredient of the compressive sensing methods. While it
is known that random matrices satisfy the RIP with high probability even for
$n=\log^{O(1)}p$, the explicit construction of such matrices defied the
repeated efforts, and the most known approaches hit the so-called $\sqrt{n}$
sparsity bottleneck. The notable exception is the work by Bourgain et al
\cite{bourgain2011explicit} constructing an $n\times p$ RIP matrix with
sparsity $s=\Theta(n^{{1\over 2}+\epsilon})$, but in the regime
$n=\Omega(p^{1-\delta})$.
In this short note we resolve this open question in a sense by showing that
an explicit construction of a matrix satisfying the RIP in the regime
$n=O(\log^2 p)$ and $s=\Theta(n^{1\over 2})$ implies an explicit construction
of a three-colored Ramsey graph on $p$ nodes with clique sizes bounded by
$O(\log^2 p)$ -- a question in the extremal combinatorics which has been open
for decades.
",Statistics,Mathematics
"Liveness Verification and Synthesis: New Algorithms for Recursive Programs   We consider the problems of liveness verification and liveness synthesis for
recursive programs. The liveness verification problem (LVP) is to decide
whether a given omega-context-free language is contained in a given
omega-regular language. The liveness synthesis problem (LSP) is to compute a
strategy so that a given omega-context-free game, when played along the
strategy, is guaranteed to derive a word in a given omega-regular language. The
problems are known to be EXPTIME-complete and EXPTIME-complete, respectively.
Our contributions are new algorithms with optimal time complexity. For LVP, we
generalize recent lasso-finding algorithms (also known as Ramsey-based
algorithms) from finite to recursive programs. For LSP, we generalize a recent
summary-based algorithm from finite to infinite words. Lasso finding and
summaries have proven to be efficient in a number of implementations for the
finite state and finite word setting.
",Computer Science,Computer Science
"A revision of the subtract-with-borrow random number generators   The most popular and widely used subtract-with-borrow generator, also known
as RANLUX, is reimplemented as a linear congruential generator using large
integer arithmetic with the modulus size of 576 bits. Modern computers, as well
as the specific structure of the modulus inferred from RANLUX, allow for the
development of a fast modular multiplication -- the core of the procedure. This
was previously believed to be slow and have too high cost in terms of computing
resources. Our tests show a significant gain in generation speed which is
comparable with other fast, high quality random number generators. An
additional feature is the fast skipping of generator states leading to a
seeding scheme which guarantees the uniqueness of random number sequences.
",Computer Science; Physics,Computer Science
"Wasserstein Learning of Deep Generative Point Process Models   Point processes are becoming very popular in modeling asynchronous sequential
data due to their sound mathematical foundation and strength in modeling a
variety of real-world phenomena. Currently, they are often characterized via
intensity function which limits model's expressiveness due to unrealistic
assumptions on its parametric form used in practice. Furthermore, they are
learned via maximum likelihood approach which is prone to failure in
multi-modal distributions of sequences. In this paper, we propose an
intensity-free approach for point processes modeling that transforms nuisance
processes to a target one. Furthermore, we train the model using a
likelihood-free leveraging Wasserstein distance between point processes.
Experiments on various synthetic and real-world data substantiate the
superiority of the proposed point process model over conventional ones.
",Computer Science; Statistics,Computer Science; Statistics
"Preference Modeling by Exploiting Latent Components of Ratings   Understanding user preference is essential to the optimization of recommender
systems. As a feedback of user's taste, rating scores can directly reflect the
preference of a given user to a given product. Uncovering the latent components
of user ratings is thus of significant importance for learning user interests.
In this paper, a new recommendation approach, called LCR, was proposed by
investigating the latent components of user ratings. The basic idea is to
decompose an existing rating into several components via a cost-sensitive
learning strategy. Specifically, each rating is assigned to several latent
factor models and each model is updated according to its predictive errors.
Afterwards, these accumulated predictive errors of models are utilized to
decompose a rating into several components, each of which is treated as an
independent part to retrain the latent factor models. Finally, all latent
factor models are combined linearly to estimate predictive ratings for users.
In contrast to existing methods, LCR provides an intuitive preference modeling
strategy via multiple component analysis at an individual perspective.
Meanwhile, it is verified by the experimental results on several benchmark
datasets that the proposed method is superior to the state-of-art methods in
terms of recommendation accuracy.
",Computer Science,Computer Science; Statistics
"Nonvanishing of central $L$-values of Maass forms   With the method of moments and the mollification method, we study the central
$L$-values of GL(2) Maass forms of weight $0$ and level $1$ and establish a
positive-proportional nonvanishing result of such values in the aspect of large
spectral parameter in short intervals, which is qualitatively optimal in view
of Weyl's law. As an application of this result and a formula of Katok--Sarnak,
we give a nonvanishing result on the first Fourier coefficients of Maass forms
of weight $\frac{1}{2}$ and level $4$ in the Kohnen plus space.
",Mathematics,Mathematics
"A family of transformed copulas with singular component   In this paper, we present a family of bivariate copulas by transforming a
given copula function with two increasing functions, named as transformed
copula. One distinctive characteristic of the transformed copula is its
singular component along the main diagonal. Conditions guaranteeing the
transformed function to be a copula function are provided, and several classes
of the transformed copulas are given. The singular component along the main
diagonal of the transformed copula is verified, and the tail dependence
coefficients of the transformed copulas are obtained. Finally, some properties
of the transformed copula are discussed, such as the totally positive of order
2 and the concordance order.
",Mathematics; Statistics,Mathematics
"Is Life Most Likely Around Sun-like Stars?   We consider the habitability of Earth-analogs around stars of different
masses, which is regulated by the stellar lifetime, stellar wind-induced
atmospheric erosion, and biologically active ultraviolet (UV) irradiance. By
estimating the timescales associated with each of these processes, we show that
they collectively impose limits on the habitability of Earth-analogs. We
conclude that planets orbiting most M-dwarfs are not likely to host life, and
that the highest probability of complex biospheres is for planets around K- and
G-type stars. Our analysis suggests that the current existence of life near the
Sun is slightly unusual, but not significantly anomalous.
",Physics,Physics
"Rare-earth/transition-metal magnetic interactions in pristine and (Ni,Fe)-doped YCo5 and GdCo5   We present an investigation into the intrinsic magnetic properties of the
compounds YCo5 and GdCo5, members of the RETM5 class of permanent magnets (RE =
rare earth, TM = transition metal). Focusing on Y and Gd provides direct
insight into both the TM magnetization and RE-TM interactions without the
complication of strong crystal field effects. We synthesize single crystals of
YCo5 and GdCo5 using the optical floating zone technique and measure the
magnetization from liquid helium temperatures up to 800 K. These measurements
are interpreted through calculations based on a Green's function formulation of
density-functional theory, treating the thermal disorder of the local magnetic
moments within the coherent potential approximation. The rise in the
magnetization of GdCo5 with temperature is shown to arise from a faster
disordering of the Gd magnetic moments compared to the antiferromagnetically
aligned Co sublattice. We use the calculations to analyze the different Curie
temperatures of the compounds and also compare the molecular (Weiss) fields at
the RE site with previously published neutron scattering experiments. To gain
further insight into the RE-TM interactions, we perform substitutional doping
on the TM site, studying the compounds RECo4.5Ni0.5, RECo4Ni, and RECo4.5Fe0.5.
Both our calculations and experiments on powdered samples find an
increased/decreased magnetization with Fe/Ni doping, respectively. The
calculations further reveal a pronounced dependence on the location of the
dopant atoms of both the Curie temperatures and the Weiss field at the RE site.
",Physics,Physics
"Diversity of preferences can increase collective welfare in sequential exploration problems   In search engines, online marketplaces and other human-computer interfaces
large collectives of individuals sequentially interact with numerous
alternatives of varying quality. In these contexts, trial and error
(exploration) is crucial for uncovering novel high-quality items or solutions,
but entails a high cost for individual users. Self-interested decision makers,
are often better off imitating the choices of individuals who have already
incurred the costs of exploration. Although imitation makes sense at the
individual level, it deprives the group of additional information that could
have been gleaned by individual explorers. In this paper we show that in such
problems, preference diversity can function as a welfare enhancing mechanism.
It leads to a consistent increase in the quality of the consumed alternatives
that outweighs the increased cost of search for the users.
",Computer Science,Computer Science
"Differential quadrature method for space-fractional diffusion equations on 2D irregular domains   In mathematical physics, the space-fractional diffusion equations are of
particular interest in the studies of physical phenomena modelled by Lévy
processes, which are sometimes called super-diffusion equations. In this
article, we develop the differential quadrature (DQ) methods for solving the 2D
space-fractional diffusion equations on irregular domains. The methods in
presence reduce the original equation into a set of ordinary differential
equations (ODEs) by introducing valid DQ formulations to fractional directional
derivatives based on the functional values at scattered nodal points on problem
domain. The required weighted coefficients are calculated by using radial basis
functions (RBFs) as trial functions, and the resultant ODEs are discretized by
the Crank-Nicolson scheme. The main advantages of our methods lie in their
flexibility and applicability to arbitrary domains. A series of illustrated
examples are finally provided to support these points.
",Mathematics,Physics
"An Introduction to Classic DEVS   DEVS is a popular formalism for modelling complex dynamic systems using a
discrete-event abstraction. At this abstraction level, a timed sequence
ofpertinent ""events"" input to a system (or internal, in the case of timeouts)
cause instantaneous changes to the state of the system. Between events, the
state does not change, resulting in a a piecewise constant state trajectory.
Main advantages of DEVS are its rigorous formal definition, and its support for
modular composition.
This chapter introduces the Classic DEVS formalism in a bottom-up fashion,
using a simple traffic light example. The syntax and operational semantics of
Atomic (i.e., non-hierarchical) models are intruced first. The semantics of
Coupled (hierarchical) models is then given by translation into Atomic DEVS
models. As this formal ""flattening"" is not efficient, a modular abstract
simulator which operates directly on the coupled model is also presented. This
is the common basis for subsequent efficient implementations. We continue to
actual applications of DEVS modelling and simulation, as seen in performance
analysis for queueing systems. Finally, we present some of the shortcomings in
the Classic DEVS formalism, and show solutions to them in the form of variants
of the original formalism.
",Computer Science,Computer Science
"A Fast Algorithm for Solving Henderson's Mixed Model Equation   This article investigates a fast and stable method to solve Henderson's mixed
model equation. The proposed algorithm is stable in that it avoids inverting a
matrix of a large dimension and hence is free from the curse of dimensionality.
This tactic is enabled through row operations performed on the design matrix.
",Statistics,Computer Science
"Learning what matters - Sampling interesting patterns   In the field of exploratory data mining, local structure in data can be
described by patterns and discovered by mining algorithms. Although many
solutions have been proposed to address the redundancy problems in pattern
mining, most of them either provide succinct pattern sets or take the interests
of the user into account-but not both. Consequently, the analyst has to invest
substantial effort in identifying those patterns that are relevant to her
specific interests and goals. To address this problem, we propose a novel
approach that combines pattern sampling with interactive data mining. In
particular, we introduce the LetSIP algorithm, which builds upon recent
advances in 1) weighted sampling in SAT and 2) learning to rank in interactive
pattern mining. Specifically, it exploits user feedback to directly learn the
parameters of the sampling distribution that represents the user's interests.
We compare the performance of the proposed algorithm to the state-of-the-art in
interactive pattern mining by emulating the interests of a user. The resulting
system allows efficient and interleaved learning and sampling, thus
user-specific anytime data exploration. Finally, LetSIP demonstrates favourable
trade-offs concerning both quality-diversity and exploitation-exploration when
compared to existing methods.
",Computer Science; Statistics,Computer Science; Statistics
"Arrow Categories of Monoidal Model Categories   We prove that the arrow category of a monoidal model category, equipped with
the pushout product monoidal structure and the projective model structure, is a
monoidal model category. This answers a question posed by Mark Hovey, and has
the important consequence that it allows for the consideration of a monoidal
product in cubical homotopy theory. As illustrations we include numerous
examples of non-cofibrantly generated monoidal model categories, including
chain complexes, small categories, topological spaces, and pro-categories.
",Mathematics,Mathematics
"On the number of integer polynomials with multiplicatively dependent roots   In this paper, we give some counting results on integer polynomials of fixed
degree and bounded height whose distinct non-zero roots are multiplicatively
dependent. These include sharp lower bounds, upper bounds and asymptotic
formulas for various cases, although in general there is a logarithmic gap
between lower and upper bounds.
",Mathematics,Mathematics
"Tailoring Architecture Centric Design Method with Rapid Prototyping   Many engineering processes exist in the industry, text books and
international standards. However, in practice rarely any of the processes are
followed consistently and literally. It is observed across industries the
processes are altered based on the requirements of the projects. Two features
commonly lacking from many engineering processes are, 1) the formal capacity to
rapidly develop prototypes in the rudimentary stage of the project, 2)
transitioning of requirements into architectural designs, when and how to
evaluate designs and how to use the throw away prototypes throughout the system
lifecycle. Prototypes are useful for eliciting requirements, generating
customer feedback and identifying, examining or mitigating risks in a project
where the product concept is at a cutting edge or not fully perceived. Apart
from the work that the product is intended to do, systemic properties like
availability, performance and modifiability matter as much as functionality.
Architects must even these concerns with the method they select to promote
these systemic properties and at the same time equip the stakeholders with the
desired functionality. Architectural design and prototyping is one of the key
ways to build the right product embedded with the desired systemic properties.
Once the product is built it can be almost impossible to retrofit the system
with the desired attributes. This paper customizes the architecture centric
development method with rapid prototyping to achieve the above-mentioned goals
and reducing the number of iterations across the stages of ACDM.
",Computer Science,Computer Science
"Integrated Fabry-Perot cavities as a mechanism for enhancing micro-ring resonator performance   We propose and experimentally demonstrate the enhancement in the filtering
quality (Q) factor of an integrated micro-ring resonator (MRR) by embedding it
in an integrated Fabry-Perot (FP) cavity formed by cascaded Sagnac loop
reflectors (SLRs). By utilizing coherent interference within the FP cavity to
reshape the transmission spectrum of the MRR, both the Q factor and the
extinction ratio (ER) can be significantly improved. The device is
theoretically analyzed, and practically fabricated on a silicon-on-insulator
(SOI) wafer. Experimental results show that up to 11-times improvement in Q
factor, together with an 8-dB increase in ER, can be achieved via our proposed
method. The impact of varying structural parameters on the device performance
is also investigated and verified by the measured spectra of the fabricated
devices with different structural parameters.
",Physics,Physics
"On architectural choices in deep learning: From network structure to gradient convergence and parameter estimation   We study mechanisms to characterize how the asymptotic convergence of
backpropagation in deep architectures, in general, is related to the network
structure, and how it may be influenced by other design choices including
activation type, denoising and dropout rate. We seek to analyze whether network
architecture and input data statistics may guide the choices of learning
parameters and vice versa. Given the broad applicability of deep architectures,
this issue is interesting both from theoretical and a practical standpoint.
Using properties of general nonconvex objectives (with first-order
information), we first build the association between structural, distributional
and learnability aspects of the network vis-à-vis their interaction with
parameter convergence rates. We identify a nice relationship between feature
denoising and dropout, and construct families of networks that achieve the same
level of convergence. We then derive a workflow that provides systematic
guidance regarding the choice of network sizes and learning parameters often
mediated4 by input statistics. Our technical results are corroborated by an
extensive set of evaluations, presented in this paper as well as independent
empirical observations reported by other groups. We also perform experiments
showing the practical implications of our framework for choosing the best
fully-connected design for a given problem.
",Computer Science; Mathematics; Statistics,Computer Science; Statistics
"Parameter Estimation in Finite Mixture Models by Regularized Optimal Transport: A Unified Framework for Hard and Soft Clustering   In this short paper, we formulate parameter estimation for finite mixture
models in the context of discrete optimal transportation with convex
regularization. The proposed framework unifies hard and soft clustering methods
for general mixture models. It also generalizes the celebrated
$k$\nobreakdash-means and expectation-maximization algorithms in relation to
associated Bregman divergences when applied to exponential family mixture
models.
",Computer Science; Statistics,Computer Science; Statistics
"Using Inertial Sensors for Position and Orientation Estimation   In recent years, MEMS inertial sensors (3D accelerometers and 3D gyroscopes)
have become widely available due to their small size and low cost. Inertial
sensor measurements are obtained at high sampling rates and can be integrated
to obtain position and orientation information. These estimates are accurate on
a short time scale, but suffer from integration drift over longer time scales.
To overcome this issue, inertial sensors are typically combined with additional
sensors and models. In this tutorial we focus on the signal processing aspects
of position and orientation estimation using inertial sensors. We discuss
different modeling choices and a selected number of important algorithms. The
algorithms include optimization-based smoothing and filtering as well as
computationally cheaper extended Kalman filter and complementary filter
implementations. The quality of their estimates is illustrated using both
experimental and simulated data.
",Computer Science,Computer Science
"Low-Latency Millimeter-Wave Communications: Traffic Dispersion or Network Densification?   This paper investigates two strategies to reduce the communication delay in
future wireless networks: traffic dispersion and network densification. A
hybrid scheme that combines these two strategies is also considered. The
probabilistic delay and effective capacity are used to evaluate performance.
For probabilistic delay, the violation probability of delay, i.e., the
probability that the delay exceeds a given tolerance level, is characterized in
terms of upper bounds, which are derived by applying stochastic network
calculus theory. In addition, to characterize the maximum affordable arrival
traffic for mmWave systems, the effective capacity, i.e., the service
capability with a given quality-of-service (QoS) requirement, is studied. The
derived bounds on the probabilistic delay and effective capacity are validated
through simulations. These numerical results show that, for a given average
system gain, traffic dispersion, network densification, and the hybrid scheme
exhibit different potentials to reduce the end-to-end communication delay. For
instance, traffic dispersion outperforms network densification, given high
average system gain and arrival rate, while it could be the worst option,
otherwise. Furthermore, it is revealed that, increasing the number of
independent paths and/or relay density is always beneficial, while the
performance gain is related to the arrival rate and average system gain,
jointly. Therefore, a proper transmission scheme should be selected to optimize
the delay performance, according to the given conditions on arrival traffic and
system service capability.
",Computer Science,Computer Science
"Bohm's approach to quantum mechanics: Alternative theory or practical picture?   Since its inception Bohmian mechanics has been generally regarded as a
hidden-variable theory aimed at providing an objective description of quantum
phenomena. To date, this rather narrow conception of Bohm's proposal has caused
it more rejection than acceptance. Now, after 65 years of Bohmian mechanics,
should still be such an interpretational aspect the prevailing appraisal? Why
not favoring a more pragmatic view, as a legitimate picture of quantum
mechanics, on equal footing in all respects with any other more conventional
quantum picture? These questions are used here to introduce a discussion on an
alternative way to deal with Bohmian mechanics at present, enhancing its aspect
as an efficient and useful picture or formulation to tackle, explore, describe
and explain quantum phenomena where phase and correlation (entanglement) are
key elements. This discussion is presented through two complementary blocks.
The first block is aimed at briefly revisiting the historical context that gave
rise to the appearance of Bohmian mechanics, and how this approach or analogous
ones have been used in different physical contexts. This discussion is used to
emphasize a more pragmatic view to the detriment of the more conventional
hidden-variable (ontological) approach that has been a leitmotif within the
quantum foundations. The second block focuses on some particular formal aspects
of Bohmian mechanics supporting the view presented here, with special emphasis
on the physical meaning of the local phase field and the associated velocity
field encoded within the wave function. As an illustration, a simple model of
Young's two-slit experiment is considered. The simplicity of this model allows
to understand in an easy manner how the information conveyed by the Bohmian
formulation relates to other more conventional concepts in quantum mechanics.
This sort of pedagogical application is also aimed at ...
",Physics,Physics
"Statistical Timing Analysis for Latch-Controlled Circuits with Reduced Iterations and Graph Transformations   Level-sensitive latches are widely used in high- performance designs. For
such circuits efficient statistical timing analysis algorithms are needed to
take increasing process vari- ations into account. But existing methods solving
this problem are still computationally expensive and can only provide the yield
at a given clock period. In this paper we propose a method combining reduced
iterations and graph transformations. The reduced iterations extract setup time
constraints and identify a subgraph for the following graph transformations
handling the constraints from nonpositive loops. The combined algorithms are
very efficient, more than 10 times faster than other existing methods, and
result in a parametric minimum clock period, which together with the hold time
constraints can be used to compute the yield at any given clock period very
easily.
",Computer Science,Computer Science
"Expressions of Sentiments During Code Reviews: Male vs. Female   Background: As most of the software development organizations are
male-dominated, female developers encountering various negative workplace
experiences reported feeling like they ""do not belong"". Exposures to
discriminatory expletives or negative critiques from their male colleagues may
further exacerbate those feelings. Aims: The primary goal of this study is to
identify the differences in expressions of sentiments between male and female
developers during various software engineering tasks. Method: On this goal, we
mined the code review repositories of six popular open source projects. We used
a semi-automated approach leveraging the name as well as multiple social
networks to identify the gender of a developer. Using SentiSE, a customized and
state-of-the-art sentiment analysis tool for the software engineering domain,
we classify each communication as negative, positive, or neutral. We also
compute the frequencies of sentiment words, emoticons, and expletives used by
each developer. Results: Our results suggest that the likelihood of using
sentiment words, emoticons, and expletives during code reviews varies based on
the gender of a developer, as females are significantly less likely to express
sentiments than males. Although female developers were more neutral to their
male colleagues than to another female, male developers from three out of the
six projects were not only writing more frequent negative comments but also
withholding positive encouragements from their female counterparts. Conclusion:
Our results provide empirical evidence of another factor behind the negative
work place experiences encountered by the female developers that may be
contributing to the diminishing number of females in the SE industry.
",Computer Science,Computer Science
"Spin diffusion from an inhomogeneous quench in an integrable system   Generalised hydrodynamics predicts universal ballistic transport in
integrable lattice systems when prepared in generic inhomogeneous initial
states. However, the ballistic contribution to transport can vanish in systems
with additional discrete symmetries. Here we perform large scale numerical
simulations of spin dynamics in the anisotropic Heisenberg $XXZ$ spin $1/2$
chain starting from an inhomogeneous mixed initial state which is symmetric
with respect to a combination of spin-reversal and spatial reflection. In the
isotropic and easy-axis regimes we find non-ballistic spin transport which we
analyse in detail in terms of scaling exponents of the transported
magnetisation and scaling profiles of the spin density. While in the easy-axis
regime we find accurate evidence of normal diffusion, the spin transport in the
isotropic case is clearly super-diffusive, with the scaling exponent very close
to $2/3$, but with universal scaling dynamics which obeys the diffusion
equation in nonlinearly scaled time.
",Physics,Physics
"Optimal Stopping for Interval Estimation in Bernoulli Trials   We propose an optimal sequential methodology for obtaining confidence
intervals for a binomial proportion $\theta$. Assuming that an i.i.d. random
sequence of Benoulli($\theta$) trials is observed sequentially, we are
interested in designing a)~a stopping time $T$ that will decide when is the
best time to stop sampling the process, and b)~an optimum estimator
$\hat{\theta}_{T}$ that will provide the optimum center of the interval
estimate of $\theta$. We follow a semi-Bayesian approach, where we assume that
there exists a prior distribution for $\theta$, and our goal is to minimize the
average number of samples while we guarantee a minimal coverage probability
level. The solution is obtained by applying standard optimal stopping theory
and computing the optimum pair $(T,\hat{\theta}_{T})$ numerically. Regarding
the optimum stopping time component $T$, we demonstrate that it enjoys certain
very uncommon characteristics not encountered in solutions of other classical
optimal stopping problems. Finally, we compare our method with the optimum
fixed-sample-size procedure but also with existing alternative sequential
schemes.
",Statistics,Mathematics; Statistics
"Smoothing for the fractional Schrodinger equation on the torus and the real line   In this paper we study the cubic fractional nonlinear Schrodinger equation
(NLS) on the torus and on the real line. Combining the normal form and the
restricted norm methods we prove that the nonlinear part of the solution is
smoother than the initial data. Our method applies to both focusing and
defocusing nonlinearities. In the case of full dispersion (NLS) and on the
torus, the gain is a full derivative, while on the real line we get a
derivative smoothing with an $\epsilon$ loss. Our result lowers the regularity
requirement of a recent theorem of Kappeler et al. on the periodic defocusing
cubic NLS, and extends it to the focusing case and to the real line. We also
obtain estimates on the higher order Sobolev norms of the global smooth
solutions in the defocusing case.
",Mathematics,Mathematics
"Stability and instability of the sub-extremal Reissner-Nordström black hole interior for the Einstein-Maxwell-Klein-Gordon equations in spherical symmetry   We show non-linear stability and instability results in spherical symmetry
for the interior of a charged black hole -approaching a sub-extremal
Reissner-Nordström background fast enough at infinity- in presence of a
massive and charged scalar field, motivated by the strong cosmic censorship
conjecture in that setting :
1. Stability : We prove that spherically symmetric characteristic initial
data to the Einstein-Maxwell- Klein-Gordon equations approaching a
Reissner-Nordström background with a sufficiently decaying polynomial decay
rate on the event horizon gives rise to a space-time possessing a Cauchy
horizon in a neighbourhood of time-like infinity. Moreover if the decay is even
stronger, we prove that the spacetime metric admits a continuous extension to
the Cauchy horizon. This generalizes the celebrated stability result of
Dafermos for Einstein-Maxwell-real-scalar-field in spherical symmetry.
2. Instability : We prove that for the class of space-times considered in the
stability part, whose scalar field in addition obeys a polynomial averaged-L^2
(consistent) lower bound on the event horizon, the scalar field obeys an
integrated lower bound transversally to the Cauchy horizon. As a consequence we
prove that the non-degenerate energy is infinite on any null surface crossing
the Cauchy horizon and the curvature of a geodesic vector field blows up at the
Cauchy horizon near time-like infinity. This generalizes an instability result
due to Luk and Oh for Einstein-Maxwell-real-scalar-field in spherical symmetry.
This instability of the black hole interior can also be viewed as a step
towards the resolution of the C^2 strong cosmic censorship conjecture for
one-ended asymptotically initial data.
",Mathematics,Physics
"On the apparent permeability of porous media in rarefied gas flows   The apparent gas permeability of the porous medium is an important parameter
in the prediction of unconventional gas production, which was first
investigated systematically by Klinkenberg in 1941 and found to increase with
the reciprocal mean gas pressure (or equivalently, the Knudsen number).
Although the underlying rarefaction effects are well-known, the reason that the
correction factor in Klinkenberg's famous equation decreases when the Knudsen
number increases has not been fully understood. Most of the studies idealize
the porous medium as a bundle of straight cylindrical tubes, however, according
to the gas kinetic theory, this only results in an increase of the correction
factor with the Knudsen number, which clearly contradicts Klinkenberg's
experimental observations. Here, by solving the Bhatnagar-Gross-Krook equation
in simplified (but not simple) porous media, we identify, for the first time,
two key factors that can explain Klinkenberg's experimental results: the
tortuous flow path and the non-unitary tangential momentum accommodation
coefficient for the gas-surface interaction. Moreover, we find that
Klinkenberg's results can only be observed when the ratio between the apparent
and intrinsic permeabilities is $\lesssim30$; at large ratios (or Knudsen
numbers) the correction factor increases with the Knudsen number. Our numerical
results could also serve as benchmarking cases to assess the accuracy of
macroscopic models and/or numerical schemes for the modeling/simulation of
rarefied gas flows in complex geometries over a wide range of gas rarefaction.
",Physics,Physics
"One level density of low-lying zeros of quadratic and quartic Hecke $L$-functions   In this paper, we prove some one level density results for the low-lying
zeros of famliies of quadratic and quartic Hecke $L$-functions of the Gaussian
field. As corollaries, we deduce that, respectively, at least $94.27 \%$ and
$5\%$ of the members of the quadratic family and the quartic family do not
vanish at the central point.
",Mathematics,Mathematics
"Mapping the aberrations of a wide-field spectrograph using a photonic comb   We demonstrate a new approach to calibrating the spectral-spatial response of
a wide-field spectrograph using a fibre etalon comb. Conventional wide-field
instruments employed on front-line telescopes are mapped with a grid of
diffraction-limited holes cut into a focal plane mask. The aberrated grid
pattern in the image plane typically reveals n-symmetric (e.g. pincushion)
distortion patterns over the field arising from the optical train. This
approach is impractical in the presence of a dispersing element because the
diffraction-limited spots in the focal plane are imaged as an array of
overlapping spectra. Instead we propose a compact solution that builds on
recent developments in fibre-based Fabry-Perot etalons. We introduce a novel
approach to near-field illumination that exploits a 25cm commercial telescope
and the propagation of skew rays in a multimode fibre. The mapping of the
optical transfer function across the full field is represented accurately
(<0.5% rms residual) by an orthonormal set of Chebyshev moments. Thus we are
able to reconstruct the full 4Kx4K CCD image of the dispersed output from the
optical fibres using this mapping, as we demonstrate. Our method removes one of
the largest sources of systematic error in multi-object spectroscopy.
",Physics,Physics
"Conditional Model Selection in Mixed-Effects Models with cAIC4   Model selection in mixed models based on the conditional distribution is
appropriate for many practical applications and has been a focus of recent
statistical research. In this paper we introduce the R-package cAIC4 that
allows for the computation of the conditional Akaike Information Criterion
(cAIC). Computation of the conditional AIC needs to take into account the
uncertainty of the random effects variance and is therefore not
straightforward. We introduce a fast and stable implementation for the
calculation of the cAIC for linear mixed models estimated with lme4 and
additive mixed models estimated with gamm4 . Furthermore, cAIC4 offers a
stepwise function that allows for a fully automated stepwise selection scheme
for mixed models based on the conditional AIC. Examples of many possible
applications are presented to illustrate the practical impact and easy handling
of the package.
",Statistics,Statistics
"Throughput Optimal Beam Alignment in Millimeter Wave Networks   Millimeter wave communications rely on narrow-beam transmissions to cope with
the strong signal attenuation at these frequencies, thus demanding precise beam
alignment between transmitter and receiver. The communication overhead incurred
to achieve beam alignment may become a severe impairment in mobile networks.
This paper addresses the problem of optimizing beam alignment acquisition, with
the goal of maximizing throughput. Specifically, the algorithm jointly
determines the portion of time devoted to beam alignment acquisition, as well
as, within this portion of time, the optimal beam search parameters, using the
framework of Markov decision processes. It is proved that a bisection search
algorithm is optimal, and that it outperforms exhaustive and iterative search
algorithms proposed in the literature. The duration of the beam alignment phase
is optimized so as to maximize the overall throughput. The numerical results
show that the throughput, optimized with respect to the duration of the beam
alignment phase, achievable under the exhaustive algorithm is 88.3% lower than
that achievable under the bisection algorithm. Similarly, the throughput
achievable by the iterative search algorithm for a division factor of 4 and 8
is, respectively, 12.8% and 36.4% lower than that achievable by the bisection
algorithm.
",Computer Science,Computer Science
"Stochastic Functional Gradient Path Planning in Occupancy Maps   Planning safe paths is a major building block in robot autonomy. It has been
an active field of research for several decades, with a plethora of planning
methods. Planners can be generally categorised as either trajectory optimisers
or sampling-based planners. The latter is the predominant planning paradigm for
occupancy maps. Trajectory optimisation entails major algorithmic changes to
tackle contextual information gaps caused by incomplete sensor coverage of the
map. However, the benefits are substantial, as trajectory optimisers can reason
on the trade-off between path safety and efficiency.
In this work, we improve our previous work on stochastic functional gradient
planners. We introduce a novel expressive path representation based on kernel
approximation, that allows cost effective model updates based on stochastic
samples. The main drawback of the previous stochastic functional gradient
planner was the cubic cost, stemming from its non-parametric path
representation. Our novel approximate kernel based model, on the other hand,
has a fixed linear cost that depends solely on the number of features used to
represent the path. We show that the stochasticity of the samples is crucial
for the planner and present comparisons to other state-of-the-art planning
methods in both simulation and with real occupancy data. The experiments
demonstrate the advantages of the stochastic approximate kernel method for path
planning in occupancy maps.
",Computer Science,Computer Science
"A Time Hierarchy Theorem for the LOCAL Model   The celebrated Time Hierarchy Theorem for Turing machines states, informally,
that more problems can be solved given more time. The extent to which a time
hierarchy-type theorem holds in the distributed LOCAL model has been open for
many years. It is consistent with previous results that all natural problems in
the LOCAL model can be classified according to a small constant number of
complexities, such as $O(1),O(\log^* n), O(\log n), 2^{O(\sqrt{\log n})}$, etc.
In this paper we establish the first time hierarchy theorem for the LOCAL
model and prove that several gaps exist in the LOCAL time hierarchy.
1. We define an infinite set of simple coloring problems called Hierarchical
$2\frac{1}{2}$-Coloring}. A correctly colored graph can be confirmed by simply
checking the neighborhood of each vertex, so this problem fits into the class
of locally checkable labeling (LCL) problems. However, the complexity of the
$k$-level Hierarchical $2\frac{1}{2}$-Coloring problem is $\Theta(n^{1/k})$,
for $k\in\mathbb{Z}^+$. The upper and lower bounds hold for both general graphs
and trees, and for both randomized and deterministic algorithms.
2. Consider any LCL problem on bounded degree trees. We prove an
automatic-speedup theorem that states that any randomized $n^{o(1)}$-time
algorithm solving the LCL can be transformed into a deterministic $O(\log
n)$-time algorithm. Together with a previous result, this establishes that on
trees, there are no natural deterministic complexities in the ranges
$\omega(\log^* n)$---$o(\log n)$ or $\omega(\log n)$---$n^{o(1)}$.
3. We expose a gap in the randomized time hierarchy on general graphs. Any
randomized algorithm that solves an LCL problem in sublogarithmic time can be
sped up to run in $O(T_{LLL})$ time, which is the complexity of the distributed
Lovasz local lemma problem, currently known to be $\Omega(\log\log n)$ and
$O(\log n)$.
",Computer Science,Computer Science
"Functoriality and uniformity in Hrushovski's groupoid-cover correspondence   The correspondence between definable connected groupoids in a theory $T$ and
internal generalised imaginary sorts of $T$, established by Hrushovski in
[""Groupoids, imaginaries and internal covers,"" Turkish Journal of Mathematics,
2012], is here extended in two ways: First, it is shown that the correspondence
is in fact an equivalence of categories, with respect to appropriate notions of
morphism. Secondly, the equivalence of categories is shown to vary uniformly in
definable families, with respect to an appropriate relativisation of these
categories. Some elaboration on Hrushovki's original constructions are also
included.
",Mathematics,Mathematics
"Camera Calibration by Global Constraints on the Motion of Silhouettes   We address the problem of epipolar geometry using the motion of silhouettes.
Such methods match epipolar lines or frontier points across views, which are
then used as the set of putative correspondences. We introduce an approach that
improves by two orders of magnitude the performance over state-of-the-art
methods, by significantly reducing the number of outliers in the putative
matching. We model the frontier points' correspondence problem as constrained
flow optimization, requiring small differences between their coordinates over
consecutive frames. Our approach is formulated as a Linear Integer Program and
we show that due to the nature of our problem, it can be solved efficiently in
an iterative manner. Our method was validated on four standard datasets
providing accurate calibrations across very different viewpoints.
",Computer Science,Computer Science
"Essential Dimension of Generic Symbols in Characteristic p   In this article the $p$-essential dimension of generic symbols over fields of
characteristic $p$ is studied. In particular, the $p$-essential dimension of
the length $\ell$ generic $p$-symbol of degree $n+1$ is bounded below by
$n+\ell$ when the base field is algebraically closed of characteristic $p$. The
proof uses new techniques for working with residues in Milne-Kato
$p$-cohomology and builds on work of Babic and Chernousov in the Witt group in
characteristic 2. Two corollaries on $p$-symbol algebras (i.e, degree 2
symbols) result from this work. The generic $p$-symbol algebra of length $\ell$
is shown to have $p$-essential dimension equal to $\ell+1$ as a $p$-torsion
Brauer class. The second is a lower bound of $\ell+1$ on the $p$-essential
dimension of the functor $\mathrm{Alg}_{p^\ell,p}$. Roughly speaking this says
that you will need at least $\ell+1$ independent parameters to be able to
specify any given algebra of degree $p^{\ell}$ and exponent $p$ over a field of
characteristic $p$ and improves on the previously established lower bound of 3.
",Mathematics,Mathematics
"A Graph Model with Indirect Co-location Links   Graph models are widely used to analyse diffusion processes embedded in
social contacts and to develop applications. A range of graph models are
available to replicate the underlying social structures and dynamics
realistically. However, most of the current graph models can only consider
concurrent interactions among individuals in the co-located interaction
networks. However, they do not account for indirect interactions that can
transmit spreading items to individuals who visit the same locations at
different times but within a certain time limit. The diffusion phenomena
occurring through direct and indirect interactions is called same place
different time (SPDT) diffusion. This paper introduces a model to synthesize
co-located interaction graphs capturing both direct interactions, where
individuals meet at a location, and indirect interactions, where individuals
visit the same location at different times within a set timeframe. We analyze
60 million location updates made by 2 million users from a social networking
application to characterize the graph properties, including the space-time
correlations and its time evolving characteristics, such as bursty or ongoing
behaviors. The generated synthetic graph reproduces diffusion dynamics of a
realistic contact graph, and reduces the prediction error by up to 82% when
compare to other contact graph models demonstrating its potential for
forecasting epidemic spread.
",Computer Science,Computer Science
"On conditional parity as a notion of non-discrimination in machine learning   We identify conditional parity as a general notion of non-discrimination in
machine learning. In fact, several recently proposed notions of
non-discrimination, including a few counterfactual notions, are instances of
conditional parity. We show that conditional parity is amenable to statistical
analysis by studying randomization as a general mechanism for achieving
conditional parity and a kernel-based test of conditional parity.
",Computer Science; Statistics,Computer Science; Statistics
"Introduction to compact and discrete quantum groups   These are notes from introductory lectures at the graduate school
""Topological Quantum Groups"" in Będlewo (June 28--July 11, 2015). The notes
present the passage from Hopf algebras to compact quantum groups and sketch the
notion of discrete quantum groups viewed as duals of compact quantum groups.
",Mathematics,Mathematics
"Cross-Correlation Redshift Calibration Without Spectroscopic Calibration Samples in DES Science Verification Data   Galaxy cross-correlations with high-fidelity redshift samples hold the
potential to precisely calibrate systematic photometric redshift uncertainties
arising from the unavailability of complete and representative training and
validation samples of galaxies. However, application of this technique in the
Dark Energy Survey (DES) is hampered by the relatively low number density,
small area, and modest redshift overlap between photometric and spectroscopic
samples. We propose instead using photometric catalogs with reliable
photometric redshifts for photo-z calibration via cross-correlations. We verify
the viability of our proposal using redMaPPer clusters from the Sloan Digital
Sky Survey (SDSS) to successfully recover the redshift distribution of SDSS
spectroscopic galaxies. We demonstrate how to combine photo-z with
cross-correlation data to calibrate photometric redshift biases while
marginalizing over possible clustering bias evolution in either the calibration
or unknown photometric samples. We apply our method to DES Science Verification
(DES SV) data in order to constrain the photometric redshift distribution of a
galaxy sample selected for weak lensing studies, constraining the mean of the
tomographic redshift distributions to a statistical uncertainty of $\Delta z
\sim \pm 0.01$. We forecast that our proposal can in principle control
photometric redshift uncertainties in DES weak lensing experiments at a level
near the intrinsic statistical noise of the experiment over the range of
redshifts where redMaPPer clusters are available. Our results provide strong
motivation to launch a program to fully characterize the systematic errors from
bias evolution and photo-z shapes in our calibration procedure.
",Physics,Physics
"Weighted estimates for positive operators and Doob maximal operators on filtered measure spaces   We characterize strong type and weak type inequalities with two weights for
positive operators on filtered measure spaces. These estimates are
probabilistic analogues of two-weight inequalities for positive operators
associated to the dyadic cubes in $\mathbb R^n$ due to Lacey, Sawyer and
Uriarte-Tuero \cite{LaSaUr}. Several mixed bounds for the Doob maximal operator
on filtered measure spaces are also obtained. In fact, Hytönen-Pérez
type and Lerner-Moen type norm estimates for Doob maximal operator are
established. Our approaches are mainly based on the construction of principal
sets.
",Mathematics,Mathematics
"Stacking and stability   Stacking is a general approach for combining multiple models toward greater
predictive accuracy. It has found various application across different domains,
ensuing from its meta-learning nature. Our understanding, nevertheless, on how
and why stacking works remains intuitive and lacking in theoretical insight. In
this paper, we use the stability of learning algorithms as an elemental
analysis framework suitable for addressing the issue. To this end, we analyze
the hypothesis stability of stacking, bag-stacking, and dag-stacking and
establish a connection between bag-stacking and weighted bagging. We show that
the hypothesis stability of stacking is a product of the hypothesis stability
of each of the base models and the combiner. Moreover, in bag-stacking and
dag-stacking, the hypothesis stability depends on the sampling strategy used to
generate the training set replicates. Our findings suggest that 1) subsampling
and bootstrap sampling improve the stability of stacking, and 2) stacking
improves the stability of both subbagging and bagging.
",Computer Science; Statistics,Computer Science; Statistics
"A Clinical and Finite Elements Study of Stress Urinary Incontinence in Women Using Fluid-Structure Interactions   Stress Urinary Incontinence (SUI) or urine leakage from urethra occurs due to
an increase in abdominal pressure resulting from stress like a cough or jumping
height. SUI is more frequent among post-menopausal women. In the absence of
bladder contraction, vesical pressure exceeds from urethral pressure leading to
urine leakage. Despite a large number of patients diagnosed with this problem,
few studies have investigated its function and mechanics. The main goal of this
study is to model bladder and urethra computationally under an external
pressure like sneezing. Finite Element Method and Fluid-Structure Interactions
are utilized for simulation. Linear mechanical properties assigned to the
bladder and urethra and pressure boundary conditions are indispensable in this
model. The results show good accordance between the clinical data and predicted
values of the computational models, such as the pressure at the center of the
bladder. This indicates that numerical methods and simplified physics of
biological systems like inferior urinary tract are helpful to achieve the
results similar to clinical results, in order to investigate pathological
conditions.
",Computer Science,Physics
"Field-induced coexistence of $s_{++}$ and $s_{\pm}$ superconducting states in dirty multiband superconductors   In multiband systems, such as iron-based superconductors, the superconducting
states with locking and anti-locking of the interband phase differences, are
usually considered as mutually exclusive. For example, a dirty two-band system
with interband impurity scattering undergoes a sharp crossover between the
$s_{\pm}$ state (which favors phase anti locking) and the $s_{++}$ state (which
favors phase locking). We discuss here that the situation can be much more
complex in the presence of an external field or superconducting currents. In an
external applied magnetic field, dirty two-band superconductors do not feature
a sharp $s_{\pm}\to s_{++}$ crossover but rather a washed-out crossover to a
finite region in the parameter space where both $s_{\pm}$ and $s_{++}$ states
can coexist for example as a lattice or a microemulsion of inclusions of
different states. The current-carrying regions such as the regions near vortex
cores can exhibit an $s_\pm$ state while it is the $s_{++}$ state that is
favored in the bulk. This coexistence of both states can even be realized in
the Meissner state at the domain's boundaries featuring Meissner currents. We
demonstrate that there is a magnetic-field-driven crossover between the pure
$s_{\pm}$ and the $s_{++}$ states.
",Physics,Physics
"Iteratively Linearized Reweighted Alternating Direction Method of Multipliers for a Class of Nonconvex Problems   In this paper, we consider solving a class of nonconvex and nonsmooth
problems frequently appearing in signal processing and machine learning
research. The traditional alternating direction method of multipliers
encounters troubles in both mathematics and computations in solving the
nonconvex and nonsmooth subproblem. In view of this, we propose a reweighted
alternating direction method of multipliers. In this algorithm, all subproblems
are convex and easy to solve. We also provide several guarantees for the
convergence and prove that the algorithm globally converges to a critical point
of an auxiliary function with the help of the Kurdyka-{\L}ojasiewicz property.
Several numerical results are presented to demonstrate the efficiency of the
proposed algorithm.
",Computer Science; Statistics,Computer Science; Mathematics
"Stability and Transparency Analysis of a Bilateral Teleoperation in Presence of Data Loss   This paper presents a novel approach for stability and transparency analysis
for bilateral teleoperation in the presence of data loss in communication
media. A new model for data loss is proposed based on a set of periodic
continuous pulses and its finite series representation. The passivity of the
overall system is shown using wave variable approach including the newly
defined model for data loss. Simulation results are presented to show the
effectiveness of the proposed approach.
",Computer Science,Computer Science
"From Pragmatic to Systematic Software Process Improvement: An Evaluated Approach   Software processes improvement (SPI) is a challenging task, as many different
stakeholders, project settings, and contexts and goals need to be considered.
SPI projects are often operated in a complex and volatile environment and,
thus, require a sound management that is resource-intensive requiring many
stakeholders to contribute to the process assessment, analysis, design,
realisation, and deployment. Although there exist many valuable SPI approaches,
none address the needs of both process engineers and project managers. This
article presents an Artefact-based Software Process Improvement & Management
approach (ArSPI) that closes this gap. ArSPI was developed and tested across
several SPI projects in large organisations in Germany and Eastern Europe. The
approach further encompasses a template for initiating, performing, and
managing SPI projects by defining a set of 5 key artefacts and 24 support
artefacts. We present ArSPI and discus results of its validation indicating
ArSPI to be a helpful instrument to set up and steer SPI projects.
",Computer Science,Computer Science
"Asymmetry-Induced Synchronization in Oscillator Networks   A scenario has recently been reported in which in order to stabilize complete
synchronization of an oscillator network---a symmetric state---the symmetry of
the system itself has to be broken by making the oscillators nonidentical. But
how often does such behavior---which we term asymmetry-induced synchronization
(AISync)---occur in oscillator networks? Here we present the first general
scheme for constructing AISync systems and demonstrate that this behavior is
the norm rather than the exception in a wide class of physical systems that can
be seen as multilayer networks. Since a symmetric network in complete synchrony
is the basic building block of cluster synchronization in more general
networks, AISync should be common also in facilitating cluster synchronization
by breaking the symmetry of the cluster subnetworks.
",Physics,Computer Science
"On spectral properties of high-dimensional spatial-sign covariance matrices in elliptical distributions with applications   Spatial-sign covariance matrix (SSCM) is an important substitute of sample
covariance matrix (SCM) in robust statistics. This paper investigates the SSCM
on its asymptotic spectral behaviors under high-dimensional elliptical
populations, where both the dimension $p$ of observations and the sample size
$n$ tend to infinity with their ratio $p/n\to c\in (0, \infty)$. The empirical
spectral distribution of this nonparametric scatter matrix is shown to converge
in distribution to a generalized Marčenko-Pastur law. Beyond this, a new
central limit theorem (CLT) for general linear spectral statistics of the SSCM
is also established. For polynomial spectral statistics, explicit formulae of
the limiting mean and covarance functions in the CLT are provided. The derived
results are then applied to an estimation procedure and a test procedure for
the spectrum of the shape component of population covariance matrices.
",Mathematics; Statistics,Mathematics; Statistics
"Fully Optical Spacecraft Communications: Implementing an Omnidirectional PV-Cell Receiver and 8Mb/s LED Visible Light Downlink with Deep Learning Error Correction   Free space optical communication techniques have been the subject of numerous
investigations in recent years, with multiple missions expected to fly in the
near future. Existing methods require high pointing accuracies, drastically
driving up overall system cost. Recent developments in LED-based visible light
communication (VLC) and past in-orbit experiments have convinced us that the
technology has reached a critical level of maturity. On these premises, we
propose a new optical communication system utilizing a VLC downlink and a high
throughput, omnidirectional photovoltaic cell receiver system. By performing
error-correction via deep learning methods and by utilizing phase-delay
interference, the system is able to deliver data rates that match those of
traditional laser-based solutions. A prototype of the proposed system has been
constructed, demonstrating the scheme to be a feasible alternative to
laser-based methods. This creates an opportunity for the full scale development
of optical communication techniques on small spacecraft as a backup telemetry
beacon or as a high throughput link.
",Computer Science,Computer Science
"Simply Exponential Approximation of the Permanent of Positive Semidefinite Matrices   We design a deterministic polynomial time $c^n$ approximation algorithm for
the permanent of positive semidefinite matrices where $c=e^{\gamma+1}\simeq
4.84$. We write a natural convex relaxation and show that its optimum solution
gives a $c^n$ approximation of the permanent. We further show that this factor
is asymptotically tight by constructing a family of positive semidefinite
matrices.
",Computer Science,Mathematics
"End-to-end Lung Nodule Detection in Computed Tomography   Computer aided diagnostic (CAD) system is crucial for modern med-ical
imaging. But almost all CAD systems operate on reconstructed images, which were
optimized for radiologists. Computer vision can capture features that is subtle
to human observers, so it is desirable to design a CAD system op-erating on the
raw data. In this paper, we proposed a deep-neural-network-based detection
system for lung nodule detection in computed tomography (CT). A
primal-dual-type deep reconstruction network was applied first to convert the
raw data to the image space, followed by a 3-dimensional convolutional neural
network (3D-CNN) for the nodule detection. For efficient network training, the
deep reconstruction network and the CNN detector was trained sequentially
first, then followed by one epoch of end-to-end fine tuning. The method was
evaluated on the Lung Image Database Consortium image collection (LIDC-IDRI)
with simulated forward projections. With 144 multi-slice fanbeam pro-jections,
the proposed end-to-end detector could achieve comparable sensitivity with the
reference detector, which was trained and applied on the fully-sampled image
data. It also demonstrated superior detection performance compared to detectors
trained on the reconstructed images. The proposed method is general and could
be expanded to most detection tasks in medical imaging.
",Computer Science; Statistics,Computer Science
"On slowly rotating axisymmetric solutions of the Einstein-Euler equations   In recent works we have constructed axisymmetric solutions to the
Euler-Poisson equations which give mathematical models of slowly uniformly
rotating gaseous stars. We try to extend this result to the study of solutions
of the Einstein-Euler equations in the framework of the general theory of
relativity. Although many interesting studies have been done about axisymmetric
metric in the general theory of relativity, they are restricted to the region
of the vacuum. Mathematically rigorous existence theorem of the axisymmetric
interior solutions of the stationary metric corresponding to the
energy-momentum tensor of the perfect fluid with non-zero pressure may be not
yet established until now except only one found in the pioneering work by U.
Heilig done in 1993. In this article, along a different approach to that of
Heilig's work, axisymmetric stationary solutions of the Einstein-Euler
equations are constructed near those of the Euler-Poisson equations when the
speed of light is sufficiently large in the considered system of units, or,
when the gravitational field is sufficiently weak.
",Mathematics,Physics
"Alternate Estimation of a Classifier and the Class-Prior from Positive and Unlabeled Data   We consider a problem of learning a binary classifier only from positive data
and unlabeled data (PU learning) and estimating the class-prior in unlabeled
data under the case-control scenario. Most of the recent methods of PU learning
require an estimate of the class-prior probability in unlabeled data, and it is
estimated in advance with another method. However, such a two-step approach
which first estimates the class prior and then trains a classifier may not be
the optimal approach since the estimation error of the class-prior is not taken
into account when a classifier is trained. In this paper, we propose a novel
unified approach to estimating the class-prior and training a classifier
alternately. Our proposed method is simple to implement and computationally
efficient. Through experiments, we demonstrate the practical usefulness of the
proposed method.
",Statistics,Computer Science; Statistics
"Sensitivity Analysis for matched pair analysis of binary data: From worst case to average case analysis   In matched observational studies where treatment assignment is not
randomized, sensitivity analysis helps investigators determine how sensitive
their estimated treatment effect is to some unmeasured con- founder. The
standard approach calibrates the sensitivity analysis according to the worst
case bias in a pair. This approach will result in a conservative sensitivity
analysis if the worst case bias does not hold in every pair. In this paper, we
show that for binary data, the standard approach can be calibrated in terms of
the average bias in a pair rather than worst case bias. When the worst case
bias and average bias differ, the average bias interpretation results in a less
conservative sensitivity analysis and more power. In many studies, the average
case calibration may also carry a more natural interpretation than the worst
case calibration and may also allow researchers to incorporate additional data
to establish an empirical basis with which to calibrate a sensitivity analysis.
We illustrate this with a study of the effects of cellphone use on the
incidence of automobile accidents. Finally, we extend the average case
calibration to the sensitivity analysis of confidence intervals for
attributable effects.
",Statistics,Computer Science; Statistics
"Landau-Ginzburg theory of cortex dynamics: Scale-free avalanches emerge at the edge of synchronization   Understanding the origin, nature, and functional significance of complex
patterns of neural activity, as recorded by diverse electrophysiological and
neuroimaging techniques, is a central challenge in neuroscience. Such patterns
include collective oscillations emerging out of neural synchronization as well
as highly heterogeneous outbursts of activity interspersed by periods of
quiescence, called ""neuronal avalanches."" Much debate has been generated about
the possible scale invariance or criticality of such avalanches and its
relevance for brain function. Aimed at shedding light onto this, here we
analyze the large-scale collective properties of the cortex by using a
mesoscopic approach following the principle of parsimony of Landau-Ginzburg.
Our model is similar to that of Wilson-Cowan for neural dynamics but crucially,
includes stochasticity and space; synaptic plasticity and inhibition are
considered as possible regulatory mechanisms. Detailed analyses uncover a phase
diagram including down-state, synchronous, asynchronous, and up-state phases
and reveal that empirical findings for neuronal avalanches are consistently
reproduced by tuning our model to the edge of synchronization. This reveals
that the putative criticality of cortical dynamics does not correspond to a
quiescent-to-active phase transition as usually assumed in theoretical
approaches but to a synchronization phase transition, at which incipient
oscillations and scale-free avalanches coexist. Furthermore, our model also
accounts for up and down states as they occur (e.g., during deep sleep). This
approach constitutes a framework to rationalize the possible collective phases
and phase transitions of cortical networks in simple terms, thus helping to
shed light on basic aspects of brain functioning from a very broad perspective.
",Quantitative Biology,Quantitative Biology
"A Polynomial Time Match Test for Large Classes of Extended Regular Expressions   In the present paper, we study the match test for extended regular
expressions. We approach this NP-complete problem by introducing a novel
variant of two-way multihead automata, which reveals that the complexity of the
match test is determined by a hidden combinatorial property of extended regular
expressions, and it shows that a restriction of the corresponding parameter
leads to rich classes with a polynomial time match test. For presentational
reasons, we use the concept of pattern languages in order to specify extended
regular expressions. While this decision, formally, slightly narrows the scope
of our results, an extension of our concepts and results to more general
notions of extended regular expressions is straightforward.
",Computer Science,Computer Science
"Off The Beaten Lane: AI Challenges In MOBAs Beyond Player Control   MOBAs represent a huge segment of online gaming and are growing as both an
eSport and a casual genre. The natural starting point for AI researchers
interested in MOBAs is to develop an AI to play the game better than a human -
but MOBAs have many more challenges besides adversarial AI. In this paper we
introduce the reader to the wider context of MOBA culture, propose a range of
challenges faced by the community today, and posit concrete AI projects that
can be undertaken to begin solving them.
",Computer Science,Computer Science
"Quantum Teleportation and Super-dense Coding in Operator Algebras   Let $\mathcal{B}_d$ be the unital $C^*$-algebra generated by the elements
$u_{jk}, \, 0 \le i, j \le d-1$, satisfying the relations that $[u_{j,k}]$ is a
unitary operator, and let $C^*(\mathbb{F}_{d^2})$ be the full group
$C^*$-algebra of free group of $d^2$ generators. Based on the idea of
teleportation and super-dense coding in quantum information theory, we exhibit
the two $*$-isomorphisms $M_d(C^*(\mathbb{F}_{d^2}))\cong \mathcal{B}_d\rtimes
\mathbb{Z}_d\rtimes \mathbb{Z}_d$ and $M_d(\mathcal{B}_d)\cong
C^*(\mathbb{F}_{d^2})\rtimes \mathbb{Z}_d\rtimes \mathbb{Z}_d$, for certain
actions of $\mathbb{Z}_d$. As an application, we show that for any $d,m\ge 2$
with $(d,m)\neq (2,2)$, the matrix-valued generalization of the (tensor
product) quantum correlation set of $d$ inputs and $m$ outputs is not closed.
",Mathematics,Computer Science
"Correlating the nanostructure of Al-oxide with deposition conditions and dielectric contributions of two-level systems in perspective of superconducting quantum circuits   This work is concerned with Al/Al-oxide(AlO$_{x}$)/Al-layer systems which are
important for Josephson-junction-based superconducting devices such as quantum
bits. The device performance is limited by noise, which has been to a large
degree assigned to the presence and properties of two-level tunneling systems
in the amorphous AlO$_{x}$ tunnel barrier. The study is focused on the
correlation of the fabrication conditions, nanostructural and nanochemical
properties and the occurrence of two-level tunneling systems with particular
emphasis on the AlO$_{x}$-layer. Electron-beam evaporation with two different
processes and sputter deposition were used for structure fabrication, and the
effect of illumination by ultraviolet light during Al-oxide formation is
elucidated. Characterization was performed by analytical transmission electron
microscopy and low-temperature dielectric measurements. We show that the
fabrication conditions have a strong impact on the nanostructural and
nanochemical properties of the layer systems and the properties of two-level
tunneling systems. Based on the understanding of the observed structural
characteristics, routes are derived towards the fabrication of
Al/AlO$_{x}$/Al-layers systems with improved properties.
",Physics,Physics
"Steady Galactic Dynamos and Observational Consequences I: Halo Magnetic Fields   We study the global consequences in the halos of spiral galaxies of the
steady, axially symmetric, mean field dynamo. We use the classical theory but
add the possibility of using the velocity field components as parameters in
addition to the helicity and diffusivity. The analysis is based on the simplest
version of the theory and uses scale-invariant solutions. The velocity field
(subject to restrictions) is a scale invariant field in a `pattern' frame, in
place of a full dynamical theory. The `pattern frame' of reference may either
be the systemic frame or some rigidly rotating spiral pattern frame. One type
of solution for the magnetic field yields off-axis, spirally wound, magnetic
field lines. These predict sign changes in the Faraday screen rotation measure
in every quadrant of the halo of an edge-on galaxy. Such rotation measure
oscillations have been observed in the CHANG-ES survey.
",Physics,Physics
"Efficient Propagation of Uncertainties in Manufacturing Supply Chains: Time Buckets, L-leap and Multilevel Monte Carlo   Uncertainty propagation of large scale discrete supply chains can be
prohibitive when a large number of events occur during the simulated period and
discrete event simulations (DES) are costly. We present a time bucket method to
approximate and accelerate the DES of supply chains. Its stochastic version,
which we call the L(logistic)-leap method, can be viewed as an extension of the
leap methods, e.g., tau-leap, D-leap, developed in the chemical engineering
community for the acceleration of stochastic DES of chemical reactions. The
L-leap method instantaneously updates the system state vector at discrete time
points and the production rates and policies of a supply chain are assumed to
be stationary during each time bucket. We propose to use Multilevel Monte Carlo
(MLMC) to efficiently propagate the uncertainties in a supply chain network,
where the levels are naturally defined by the sizes of the time buckets of the
simulations. We demonstrate the efficiency and accuracy of our methods using
four numerical examples derived from a real world manufacturing material flow.
In these examples, our multilevel L-leap approach can be faster than the
standard Monte Carlo (MC) method by one or two orders of magnitudes without
compromising the accuracy.
",Statistics,Computer Science
"Spatially Adaptive Colocalization Analysis in Dual-Color Fluorescence Microscopy   Colocalization analysis aims to study complex spatial associations between
bio-molecules via optical imaging techniques. However, existing colocalization
analysis workflows only assess an average degree of colocalization within a
certain region of interest and ignore the unique and valuable spatial
information offered by microscopy. In the current work, we introduce a new
framework for colocalization analysis that allows us to quantify colocalization
levels at each individual location and automatically identify pixels or regions
where colocalization occurs. The framework, referred to as spatially adaptive
colocalization analysis (SACA), integrates a pixel-wise local kernel model for
colocalization quantification and a multi-scale adaptive propagation-separation
strategy for utilizing spatial information to detect colocalization in a
spatially adaptive fashion. Applications to simulated and real biological
datasets demonstrate the practical merits of SACA in what we hope to be an
easily applicable and robust colocalization analysis method. In addition,
theoretical properties of SACA are investigated to provide rigorous statistical
justification.
",Statistics,Computer Science; Statistics
"A note on a new paradox in superluminal signalling   The Tolman paradox is well known as a base for demonstrating the causality
violation by faster-than-light signals within special relativity. It is
constructed using a two-way exchange of faster-than-light signals between two
inertial observers who are in a relative motion receding one from another.
Recently a one-way superluminal signalling arrangement was suggested as a
possible construction of a causal paradox. In this note we show that this
suggestion is not correct, and no causality principle violation can occur in
any one-way signalling by the use of faster-than-light particles and signals.
",Physics,Physics
"Embedded tori with prescribed mean curvature   We construct a sequence of compact, oriented, embedded, two-dimensional
surfaces of genus one into Euclidean 3-space with prescribed, almost constant,
mean curvature of the form $H(X)=1+{A}{|X|^{-\gamma}}$ for $|X|$ large, when
$A<0$ and $\gamma\in(0,2)$. Such surfaces are close to sections of unduloids
with small necksize, folded along circumferences centered at the origin and
with larger and larger radii. The construction involves a deep study of the
corresponding Jacobi operators, an application of the Lyapunov-Schmidt
reduction method and some variational argument.
",Mathematics,Mathematics
"Data-Augmented Contact Model for Rigid Body Simulation   Accurately modeling contact behaviors for real-world, near-rigid materials
remains a grand challenge for existing rigid-body physics simulators. This
paper introduces a data-augmented contact model that incorporates analytical
solutions with observed data to predict the 3D contact impulse which could
result in rigid bodies bouncing, sliding or spinning in all directions. Our
method enhances the expressiveness of the standard Coulomb contact model by
learning the contact behaviors from the observed data, while preserving the
fundamental contact constraints whenever possible. For example, a classifier is
trained to approximate the transitions between static and dynamic frictions,
while non-penetration constraint during collision is enforced analytically. Our
method computes the aggregated effect of contact for the entire rigid body,
instead of predicting the contact force for each contact point individually,
removing the exponential decline in accuracy as the number of contact points
increases.
",Computer Science,Physics
"Numerical non-LTE 3D radiative transfer using a multigrid method   3D non-LTE radiative transfer problems are computationally demanding, and
this sets limits on the size of the problems that can be solved. So far
Multilevel Accelerated Lambda Iteration (MALI) has been to the method of choice
to perform high-resolution computations in multidimensional problems. The
disadvantage of MALI is that its computing time scales as $\mathcal{O}(n^2)$,
with $n$ the number of grid points. When the grid gets finer, the computational
cost increases quadratically. We aim to develop a 3D non-LTE radiative transfer
code that is more efficient than MALI. We implement a non-linear multigrid,
fast approximation storage scheme, into the existing Multi3D radiative transfer
code. We verify our multigrid implementation by comparing with MALI
computations. We show that multigrid can be employed in realistic problems with
snapshots from 3D radiative-MHD simulations as input atmospheres. With
multigrid, we obtain a factor 3.3-4.5 speedup compared to MALI. With
full-multigrid the speed-up increases to a factor 6. The speedup is expected to
increase for input atmospheres with more grid points and finer grid spacing.
Solving 3D non-LTE radiative transfer problems using non-linear multigrid
methods can be applied to realistic atmospheres with a substantial speed-up.
",Physics,Computer Science
"Hypergraph Convolution and Hypergraph Attention   Recently, graph neural networks have attracted great attention and achieved
prominent performance in various research fields. Most of those algorithms have
assumed pairwise relationships of objects of interest. However, in many real
applications, the relationships between objects are in higher-order, beyond a
pairwise formulation. To efficiently learn deep embeddings on the high-order
graph-structured data, we introduce two end-to-end trainable operators to the
family of graph neural networks, i.e., hypergraph convolution and hypergraph
attention. Whilst hypergraph convolution defines the basic formulation of
performing convolution on a hypergraph, hypergraph attention further enhances
the capacity of representation learning by leveraging an attention module. With
the two operators, a graph neural network is readily extended to a more
flexible model and applied to diverse applications where non-pairwise
relationships are observed. Extensive experimental results with semi-supervised
node classification demonstrate the effectiveness of hypergraph convolution and
hypergraph attention.
",Computer Science; Statistics,Computer Science; Statistics
"Simultaneous dynamic characterization of charge and structural motion during ferroelectric switching   Monitoring structural changes in ferroelectric thin films during electric
field-induced polarization switching is important for a full microscopic
understanding of the coupled motion of charges, atoms and domain walls. We
combine standard ferroelectric test-cycles with time-resolved x-ray diffraction
to investigate the response of a nanoscale ferroelectric oxide capacitor upon
charging, discharging and switching. Piezoelectric strain develops during the
electronic RC time constant and additionally during structural domain-wall
creep. The complex atomic motion during ferroelectric polarization reversal
starts with a negative piezoelectric response to the charge flow triggered by
voltage pulses. Incomplete screening limits the compressive strain. The
piezoelectric modulation of the unit cell tweaks the energy barrier between the
two polarization states. Domain wall motion is evidenced by a broadening of the
in-plane components of Bragg reflections. Such simultaneous measurements on a
working device elucidate and visualize the complex interplay of charge flow and
structural motion and challenges theoretical modelling.
",Physics,Physics
"Reconstruction formulas for Photoacoustic Imaging in Attenuating Media   In this paper we study the problem of photoacoustic inversion in a weakly
attenuating medium. We present explicit reconstruction formulas in such media
and show that the inversion based on such formulas is moderately ill--posed.
Moreover, we present a numerical algorithm for imaging and demonstrate in
numerical experiments the feasibility of this approach.
",Mathematics,Computer Science
"On reductions of the discrete Kadomtsev--Petviashvili-type equations   The reduction by restricting the spectral parameters $k$ and $k'$ on a
generic algebraic curve of degree $\mathcal{N}$ is performed for the discrete
AKP, BKP and CKP equations, respectively. A variety of two-dimensional discrete
integrable systems possessing a more general solution structure arise from the
reduction, and in each case a unified formula for generic positive integer
$\mathcal{N}\geq 2$ is given to express the corresponding reduced integrable
lattice equations. The obtained extended two-dimensional lattice models give
rise to many important integrable partial difference equations as special
degenerations. Some new integrable lattice models such as the discrete
Sawada--Kotera, Kaup--Kupershmidt and Hirota--Satsuma equations in extended
form are given as examples within the framework.
",Physics,Mathematics
"Ensemble Clustering for Graphs   We propose an ensemble clustering algorithm for graphs (ECG), which is based
on the Louvain algorithm and the concept of consensus clustering. We validate
our approach by replicating a recently published study comparing graph
clustering algorithms over artificial networks, showing that ECG outperforms
the leading algorithms from that study. We also illustrate how the ensemble
obtained with ECG can be used to quantify the presence of community structure
in the graph.
",Computer Science; Statistics,Computer Science; Statistics
"Long-term Blood Pressure Prediction with Deep Recurrent Neural Networks   Existing methods for arterial blood pressure (BP) estimation directly map the
input physiological signals to output BP values without explicitly modeling the
underlying temporal dependencies in BP dynamics. As a result, these models
suffer from accuracy decay over a long time and thus require frequent
calibration. In this work, we address this issue by formulating BP estimation
as a sequence prediction problem in which both the input and target are
temporal sequences. We propose a novel deep recurrent neural network (RNN)
consisting of multilayered Long Short-Term Memory (LSTM) networks, which are
incorporated with (1) a bidirectional structure to access larger-scale context
information of input sequence, and (2) residual connections to allow gradients
in deep RNN to propagate more effectively. The proposed deep RNN model was
tested on a static BP dataset, and it achieved root mean square error (RMSE) of
3.90 and 2.66 mmHg for systolic BP (SBP) and diastolic BP (DBP) prediction
respectively, surpassing the accuracy of traditional BP prediction models. On a
multi-day BP dataset, the deep RNN achieved RMSE of 3.84, 5.25, 5.80 and 5.81
mmHg for the 1st day, 2nd day, 4th day and 6th month after the 1st day SBP
prediction, and 1.80, 4.78, 5.0, 5.21 mmHg for corresponding DBP prediction,
respectively, which outperforms all previous models with notable improvement.
The experimental results suggest that modeling the temporal dependencies in BP
dynamics significantly improves the long-term BP prediction accuracy.
",Computer Science; Statistics,Computer Science; Statistics
"Stochastic Multi-armed Bandits in Constant Space   We consider the stochastic bandit problem in the sublinear space setting,
where one cannot record the win-loss record for all $K$ arms. We give an
algorithm using $O(1)$ words of space with regret \[
\sum_{i=1}^{K}\frac{1}{\Delta_i}\log \frac{\Delta_i}{\Delta}\log T \] where
$\Delta_i$ is the gap between the best arm and arm $i$ and $\Delta$ is the gap
between the best and the second-best arms. If the rewards are bounded away from
$0$ and $1$, this is within an $O(\log 1/\Delta)$ factor of the optimum regret
possible without space constraints.
",Computer Science; Statistics,Computer Science
"Bayesian hierarchical weighting adjustment and survey inference   We combine Bayesian prediction and weighted inference as a unified approach
to survey inference. The general principles of Bayesian analysis imply that
models for survey outcomes should be conditional on all variables that affect
the probability of inclusion. We incorporate the weighting variables under the
framework of multilevel regression and poststratification, as a byproduct
generating model-based weights after smoothing. We investigate deep
interactions and introduce structured prior distributions for smoothing and
stability of estimates. The computation is done via Stan and implemented in the
open source R package ""rstanarm"" ready for public use. Simulation studies
illustrate that model-based prediction and weighting inference outperform
classical weighting. We apply the proposal to the New York Longitudinal Study
of Wellbeing. The new approach generates robust weights and increases
efficiency for finite population inference, especially for subsets of the
population.
",Statistics,Statistics
"Minimax Game-Theoretic Approach to Multiscale H-infinity Optimal Filtering   Sensing in complex systems requires large-scale information exchange and
on-the-go communications over heterogeneous networks and integrated processing
platforms. Many networked cyber-physical systems exhibit hierarchical
infrastructures of information flows, which naturally leads to a multi-level
tree-like information structure in which each level corresponds to a particular
scale of representation. This work focuses on the multiscale fusion of data
collected at multiple levels of the system. We propose a multiscale state-space
model to represent multi-resolution data over the hierarchical information
system and formulate a multi-stage dynamic zero-sum game to design a
multi-scale $H_{\infty}$ robust filter. We present numerical experiments for
one and two-dimensional signals and provide a comparative analysis of the
minimax filter with the standard Kalman filter to show the improvement in
signal-to-noise ratio (SNR).
",Computer Science,Computer Science
"Opinion evolution in time-varying social influence networks with prejudiced agents   Investigation of social influence dynamics requires mathematical models that
are ""simple"" enough to admit rigorous analysis, and yet sufficiently ""rich"" to
capture salient features of social groups. Thus, the mechanism of iterative
opinion pooling from (DeGroot, 1974), which can explain the generation of
consensus, was elaborated in (Friedkin and Johnsen, 1999) to take into account
individuals' ongoing attachments to their initial opinions, or prejudices. The
""anchorage"" of individuals to their prejudices may disable reaching consensus
and cause disagreement in a social influence network. Further elaboration of
this model may be achieved by relaxing its restrictive assumption of a
time-invariant influence network. During opinion dynamics on an issue, arcs of
interpersonal influence may be added or subtracted from the network, and the
influence weights assigned by an individual to his/her neighbors may alter. In
this paper, we establish new important properties of the (Friedkin and Johnsen,
1999) opinion formation model, and also examine its extension to time-varying
social influence networks.
",Computer Science; Physics; Mathematics,Computer Science; Physics
"The exit time finite state projection scheme: bounding exit distributions and occupation measures of continuous-time Markov chains   We introduce the exit time finite state projection (ETFSP) scheme, a
truncation-based method that yields approximations to the exit distribution and
occupation measure associated with the time of exit from a domain (i.e., the
time of first passage to the complement of the domain) of time-homogeneous
continuous-time Markov chains. We prove that: (i) the computed approximations
bound the measures from below; (ii) the total variation distances between the
approximations and the measures decrease monotonically as states are added to
the truncation; and (iii) the scheme converges, in the sense that, as the
truncation tends to the entire state space, the total variation distances tend
to zero. Furthermore, we give a computable bound on the total variation
distance between the exit distribution and its approximation, and we delineate
the cases in which the bound is sharp. We also revisit the related finite state
projection scheme and give a comprehensive account of its theoretical
properties. We demonstrate the use of the ETFSP scheme by applying it to two
biological examples: the computation of the first passage time associated with
the expression of a gene, and the fixation times of competing species subject
to demographic noise.
",Quantitative Biology,Quantitative Biology
"Precise Recovery of Latent Vectors from Generative Adversarial Networks   Generative adversarial networks (GANs) transform latent vectors into visually
plausible images. It is generally thought that the original GAN formulation
gives no out-of-the-box method to reverse the mapping, projecting images back
into latent space. We introduce a simple, gradient-based technique called
stochastic clipping. In experiments, for images generated by the GAN, we
precisely recover their latent vector pre-images 100% of the time. Additional
experiments demonstrate that this method is robust to noise. Finally, we show
that even for unseen images, our method appears to recover unique encodings.
",Computer Science; Statistics,Computer Science
"A Stochastic Control Approach to Managed Futures Portfolios   We study a stochastic control approach to managed futures portfolios.
Building on the Schwartz 97 stochastic convenience yield model for commodity
prices, we formulate a utility maximization problem for dynamically trading a
single-maturity futures or multiple futures contracts over a finite horizon. By
analyzing the associated Hamilton-Jacobi-Bellman (HJB) equation, we solve the
investor's utility maximization problem explicitly and derive the optimal
dynamic trading strategies in closed form. We provide numerical examples and
illustrate the optimal trading strategies using WTI crude oil futures data.
",Quantitative Finance,Quantitative Finance
"Efficient cold outflows driven by cosmic rays in high redshift galaxies and their global effects on the IGM   We present semi-analytical models of galactic outflows in high redshift
galaxies driven by both hot thermal gas and non-thermal cosmic rays. Thermal
pressure alone may not sustain a large scale outflow in low mass galaxies (i.e
$M\sim 10^8$~M$_\odot$), in the presence of supernovae (SNe) feedback with
large mass loading. We show that inclusion of cosmic ray pressure allows
outflow solutions even in these galaxies. In massive galaxies for the same
energy efficiency, cosmic ray driven winds can propagate to larger distances
compared to pure thermally driven winds. On an average gas in the cosmic ray
driven winds has a lower temperature which could aid detecting it through
absorption lines in the spectra of background sources. Using our constrained
semi-analytical models of galaxy formation (that explains the observed UV
luminosity functions of galaxies) we study the influence of cosmic ray driven
winds on the properties of the intergalactic medium (IGM) at different
redshifts. In particular, we study the volume filling factor, average
metallicity, cosmic ray and magnetic field energy densities for models invoking
atomic cooled and molecular cooled halos. We show that the cosmic rays in the
IGM could have enough energy that can be transferred to the thermal gas in
presence of magnetic fields to influence the thermal history of the
intergalactic medium. The significant volume filling and resulting strength of
IGM magnetic fields can also account for recent $\gamma$-ray observations of
blazars.
",Physics,Physics
"A Secular Resonant Origin for the Loneliness of Hot Jupiters   Despite decades of inquiry, the origin of giant planets residing within a few
tenths of an astronomical unit from their host stars remains unclear.
Traditionally, these objects are thought to have formed further out before
subsequently migrating inwards. However, the necessity of migration has been
recently called into question with the emergence of in-situ formation models of
close-in giant planets. Observational characterization of the transiting
sub-sample of close-in giants has revealed that ""warm"" Jupiters, possessing
orbital periods longer than roughly 10 days more often possess close-in,
co-transiting planetary companions than shorter period ""hot"" Jupiters, that are
usually lonely. This finding has previously been interpreted as evidence that
smooth, early migration or in situ formation gave rise to warm Jupiter-hosting
systems, whereas more violent, post-disk migration pathways sculpted hot
Jupiter-hosting systems. In this work, we demonstrate that both classes of
planet may arise via early migration or in-situ conglomeration, but that the
enhanced loneliness of hot Jupiters arises due to a secular resonant
interaction with the stellar quadrupole moment. Such an interaction tilts the
orbits of exterior, lower mass planets, removing them from transit surveys
where the hot Jupiter is detected. Warm Jupiter-hosting systems, in contrast,
retain their coplanarity due to the weaker influence of the host star's
quadrupolar potential relative to planet-disk interactions. In this way, hot
Jupiters and warm Jupiters are placed within a unified theoretical framework
that may be readily validated or falsified using data from upcoming missions
such as TESS.
",Physics,Physics
"Towards Robust Interpretability with Self-Explaining Neural Networks   Most recent work on interpretability of complex machine learning models has
focused on estimating $\textit{a posteriori}$ explanations for previously
trained models around specific predictions. $\textit{Self-explaining}$ models
where interpretability plays a key role already during learning have received
much less attention. We propose three desiderata for explanations in general --
explicitness, faithfulness, and stability -- and show that existing methods do
not satisfy them. In response, we design self-explaining models in stages,
progressively generalizing linear classifiers to complex yet architecturally
explicit models. Faithfulness and stability are enforced via regularization
specifically tailored to such models. Experimental results across various
benchmark datasets show that our framework offers a promising direction for
reconciling model complexity and interpretability.
",Statistics,Computer Science; Statistics
"On constraining projections of future climate using observations and simulations from multiple climate models   A new Bayesian framework is presented that can constrain projections of
future climate using historical observations by exploiting robust estimates of
emergent relationships between multiple climate models. We argue that emergent
relationships can be interpreted as constraints on model inadequacy, but that
projections may be biased if we do not account for internal variability in
climate model projections. We extend the previously proposed coexchangeable
framework to account for natural variability in the Earth system and internal
variability simulated by climate models. A detailed theoretical comparison with
previous multi-model projection frameworks is provided.
The proposed framework is applied to projecting surface temperature in the
Arctic at the end of the 21st century. A subset of available climate models are
selected in order to satisfy the assumptions of the framework. All available
initial condition runs from each model are utilized in order maximize the
utility of the data. Projected temperatures in some regions are more than 2C
lower when constrained by historical observations. The uncertainty about the
climate response is reduced by up to 30% where strong constraints exist.
",Statistics,Physics
"Enhancing Blood Glucose Prediction with Meal Absorption and Physical Exercise Information   Objective: Numerous glucose prediction algorithm have been proposed to
empower type 1 diabetes (T1D) management. Most of these algorithms only account
for input such as glucose, insulin and carbohydrate, which limits their
performance. Here, we present a novel glucose prediction algorithm which, in
addition to standard inputs, accounts for meal absorption and physical exercise
information to enhance prediction accuracy. Methods: a compartmental model of
glucose-insulin dynamics combined with a deconvolution technique for state
estimation is employed for glucose prediction. In silico data corresponding
from the 10 adult subjects of UVa-Padova simulator, and clinical data from 10
adults with T1D were used. Finally, a comparison against a validated glucose
prediction algorithm based on a latent variable with exogenous input (LVX)
model is provided. Results: For a prediction horizon of 60 minutes, accounting
for meal absorption and physical exercise improved glucose forecasting
accuracy. In particular, root mean square error (mg/dL) went from 26.68 to
23.89, p<0.001 (in silico data); and from 37.02 to 35.96, p<0.001 (clinical
data - only meal information). Such improvement in accuracy was translated into
significant improvements on hypoglycaemia and hyperglycaemia prediction.
Finally, the performance of the proposed algorithm is statistically superior to
that of the LVX algorithm (26.68 vs. 32.80, p<0.001 (in silico data); 37.02 vs.
49.17, p<0.01 (clinical data). Conclusion: Taking into account meal absorption
and physical exercise information improves glucose prediction accuracy.
",Computer Science; Quantitative Biology,Statistics
"Graph Product Multilayer Networks: Spectral Properties and Applications   This paper aims to establish theoretical foundations of graph product
multilayer networks (GPMNs), a family of multilayer networks that can be
obtained as a graph product of two or more factor networks. Cartesian, direct
(tensor), and strong product operators are considered, and then generalized. We
first describe mathematical relationships between GPMNs and their factor
networks regarding their degree/strength, adjacency, and Laplacian spectra, and
then show that those relationships can still hold for nonsimple and generalized
GPMNs. Applications of GPMNs are discussed in three areas: predicting epidemic
thresholds, modeling propagation in nontrivial space and time, and analyzing
higher-order properties of self-similar networks. Directions of future research
are also discussed.
",Computer Science; Physics,Computer Science; Statistics
"MOEMS deformable mirror testing in cryo for future optical instrumentation   MOEMS Deformable Mirrors (DM) are key components for next generation
instruments with innovative adaptive optics systems, in existing telescopes and
in the future ELTs. These DMs must perform at room temperature as well as in
cryogenic and vacuum environment. Ideally, the MOEMS-DMs must be designed to
operate in such environment. We present some major rules for designing /
operating DMs in cryo and vacuum. We chose to use interferometry for the full
characterization of these devices, including surface quality measurement in
static and dynamical modes, at ambient and in vacuum/cryo. Thanks to our
previous set-up developments, we placed a compact cryo-vacuum chamber designed
for reaching 10-6 mbar and 160K, in front of our custom Michelson
interferometer, able to measure performances of the DM at actuator/segment
level as well as whole mirror level, with a lateral resolution of 2{\mu}m and a
sub-nanometric z-resolution. Using this interferometric bench, we tested the
Iris AO PTT111 DM: this unique and robust design uses an array of single
crystalline silicon hexagonal mirrors with a pitch of 606{\mu}m, able to move
in tip, tilt and piston with strokes from 5 to 7{\mu}m, and tilt angle in the
range of +/-5mrad. They exhibit typically an open-loop flat surface figure as
good as <20nm rms. A specific mount including electronic and opto-mechanical
interfaces has been designed for fitting in the test chamber. Segment
deformation, mirror shaping, open-loop operation are tested at room and cryo
temperature and results are compared. The device could be operated successfully
at 160K. An additional, mainly focus-like, 500 nm deformation is measured at
160K; we were able to recover the best flat in cryo by correcting the focus and
local tip-tilts on some segments. Tests on DM with different mirror thicknesses
(25{\mu}m and 50{\mu}m) and different coatings (silver and gold) are currently
under way.
",Physics,Physics
"Variational Bayesian dropout: pitfalls and fixes   Dropout, a stochastic regularisation technique for training of neural
networks, has recently been reinterpreted as a specific type of approximate
inference algorithm for Bayesian neural networks. The main contribution of the
reinterpretation is in providing a theoretical framework useful for analysing
and extending the algorithm. We show that the proposed framework suffers from
several issues; from undefined or pathological behaviour of the true posterior
related to use of improper priors, to an ill-defined variational objective due
to singularity of the approximating distribution relative to the true
posterior. Our analysis of the improper log uniform prior used in variational
Gaussian dropout suggests the pathologies are generally irredeemable, and that
the algorithm still works only because the variational formulation annuls some
of the pathologies. To address the singularity issue, we proffer Quasi-KL (QKL)
divergence, a new approximate inference objective for approximation of
high-dimensional distributions. We show that motivations for variational
Bernoulli dropout based on discretisation and noise have QKL as a limit.
Properties of QKL are studied both theoretically and on a simple practical
example which shows that the QKL-optimal approximation of a full rank Gaussian
with a degenerate one naturally leads to the Principal Component Analysis
solution.
",Statistics,Statistics
"Virtually free finite-normal-subgroup-free groups are strongly verbally closed   Any virtually free group $H$ containing no non-trivial finite normal subgroup
(e.g., the infinite dihedral group) is a retract of any finitely generated
group containing $H$ as a verbally closed subgroup.
",Mathematics,Mathematics
"Multi-SpaM: a Maximum-Likelihood approach to Phylogeny reconstruction based on Multiple Spaced-Word Matches   Motivation: Word-based or `alignment-free' methods for phylogeny
reconstruction are much faster than traditional approaches, but they are
generally less accurate. Most of these methods calculate pairwise distances for
a set of input sequences, for example from word frequencies, from so-called
spaced-word matches or from the average length of common substrings.
Results: In this paper, we propose the first word-based approach to tree
reconstruction that is based on multiple sequence comparison and Maximum
Likelihood. Our algorithm first samples small, gap-free alignments involving
four taxa each. For each of these alignments, it then calculates a quartet tree
and, finally, the program Quartet MaxCut is used to infer a super tree topology
for the full set of input taxa from the calculated quartet trees. Experimental
results show that trees calculated with our approach are of high quality.
Availability: The source code of the program is available at
this https URL
Contact: thomas.dencker@stud.uni-goettingen.de
",Quantitative Biology,Computer Science
"Design of Capacity Approaching Ensembles of LDPC Codes for Correlated Sources using EXIT Charts   This paper is concerned with the design of capacity approaching ensembles of
Low-Densiy Parity-Check (LDPC) codes for correlated sources. We consider
correlated binary sources where the data is encoded independently at each
source through a systematic LDPC encoder and sent over two independent
channels. At the receiver, a iterative joint decoder consisting of two
component LDPC decoders is considered where the encoded bits at the output of
each component decoder are used at the other decoder as the a priori
information. We first provide asymptotic performance analysis using the concept
of extrinsic information transfer (EXIT) charts. Compared to the conventional
EXIT charts devised to analyze LDPC codes for point to point communication, the
proposed EXIT charts have been completely modified to able to accommodate the
systematic nature of the codes as well as the iterative behavior between the
two component decoders. Then the developed modified EXIT charts are deployed to
design ensembles for different levels of correlation. Our results show that as
the average degree of the designed ensembles grow, the thresholds corresponding
to the designed ensembles approach the capacity. In particular, for ensembles
with average degree of around 9, the gap to capacity is reduced to about 0.2dB.
Finite block length performance evaluation is also provided for the designed
ensembles to verify the asymptotic results.
",Computer Science,Computer Science
"ArchiveWeb: collaboratively extending and exploring web archive collections - How would you like to work with your collections?   Curated web archive collections contain focused digital content which is
collected by archiving organizations, groups, and individuals to provide a
representative sample covering specific topics and events to preserve them for
future exploration and analysis. In this paper, we discuss how to best support
collaborative construction and exploration of these collections through the
ArchiveWeb system. ArchiveWeb has been developed using an iterative
evaluation-driven design-based research approach, with considerable user
feedback at all stages. The first part of this paper describes the important
insights we gained from our initial requirements engineering phase during the
first year of the project and the main functionalities of the current
ArchiveWeb system for searching, constructing, exploring, and discussing web
archive collections. The second part summarizes the feedback we received on
this version from archiving organizations and libraries, as well as our
corresponding plans for improving and extending the system for the next
release.
",Computer Science,Computer Science
"Infinitary generalizations of Deligne's completeness theorem   Given a regular cardinal $\kappa$ such that $\kappa^{<\kappa}=\kappa$, we
study a class of toposes with enough points, the $\kappa$-separable toposes.
These are equivalent to sheaf toposes over a site with $\kappa$-small limits
that has at most $\kappa$ many objects and morphisms, the (basis for the)
topology being generated by at most $\kappa$ many covering families, and that
satisfy a further exactness property $T$. We prove that these toposes have
enough $\kappa$-points, that is, points whose inverse image preserve all
$\kappa$-small limits. This generalizes the separable toposes of Makkai and
Reyes, that are a particular case when $\kappa=\omega$, when property $T$ is
trivially satisfied. This result is essentially a completeness theorem for a
certain infinitary logic that we call $\kappa$-geometric, where conjunctions of
less than $\kappa$ formulas and existential quantification on less than
$\kappa$ many variables is allowed. We prove that $\kappa$-geometric theories
have a $\kappa$-classifying topos having property $T$, the universal property
being that models of the theory in a Grothendieck topos with property $T$
correspond to $\kappa$-geometric morphisms (geometric morphisms the inverse
image of which preserves all $\kappa$-small limits) into that topos. Moreover,
we prove that $\kappa$-separable toposes occur as the $\kappa$-classifying
toposes of $\kappa$-geometric theories of at most $\kappa$ many axioms in
canonical form, and that every such $\kappa$-classifying topos is
$\kappa$-separable. Finally, we consider the case when $\kappa$ is weakly
compact and study the $\kappa$-classifying topos of a $\kappa$-coherent theory
(with at most $\kappa$ many axioms), that is, a theory where only disjunction
of less than $\kappa$ formulas are allowed, obtaining a version of Deligne's
theorem for $\kappa$-coherent toposes.
",Mathematics,Mathematics
"Endomorphism Algebras of Abelian varieties with special reference to Superelliptic Jacobians   This is (mostly) a survey article. We use an information about Galois
properties of points of small order on an abelian variety in order to describe
its endomorphism algebra over an algebraic closure of the ground field. We
discuss in detail applications to jacobians of cyclic covers of the projective
line.
",Mathematics,Mathematics
"Finite-Time Distributed Linear Equation Solver for Minimum $l_1$ Norm Solutions   This paper proposes distributed algorithms for multi-agent networks to
achieve a solution in finite time to a linear equation $Ax=b$ where $A$ has
full row rank, and with the minimum $l_1$-norm in the underdetermined case
(where $A$ has more columns than rows). The underlying network is assumed to be
undirected and fixed, and an analytical proof is provided for the proposed
algorithm to drive all agents' individual states to converge to a common value,
viz a solution of $Ax=b$, which is the minimum $l_1$-norm solution in the
underdetermined case. Numerical simulations are also provided as validation of
the proposed algorithms.
",Computer Science,Computer Science
"On the vanishing of self extensions over Cohen-Macaulay local rings   The celebrated Auslander-Reiten Conjecture, on the vanishing of self
extensions of a module, is one of the long-standing conjectures in ring theory.
Although it is still open, there are several results in the literature that
establish the conjecture over Gorenstein rings under certain conditions. The
purpose of this article is to obtain extensions of such results over
Cohen-Macaulay local rings that admit canonical modules. In particular, our
main result recovers theorems of Araya, and Ono and Yoshino simultaneously.
",Mathematics,Mathematics
"Particle-hole Asymmetry in the Cuprate Pseudogap Measured with Time-Resolved Spectroscopy   One of the most puzzling features of high-temperature cuprate superconductors
is the pseudogap state, which appears above the temperature at which
superconductivity is destroyed. There remain fundamental questions regarding
its nature and its relation to superconductivity. But to address these
questions, we must first determine whether the pseudogap and superconducting
states share a common property: particle-hole symmetry. We introduce a new
technique to test particle-hole symmetry by using laser pulses to manipulate
and measure the chemical potential on picosecond time scales. The results
strongly suggest that the asymmetry in the density of states is inverted in the
pseudogap state, implying a particle-hole asymmetric gap. Independent of
interpretation, these results can test theoretical predictions of the density
of states in cuprates.
",Physics,Physics
"High Contrast Observations of Bright Stars with a Starshade   Starshades are a leading technology to enable the direct detection and
spectroscopic characterization of Earth-like exoplanets. In an effort to
advance starshade technology through system level demonstrations, the
McMath-Pierce Solar Telescope was adapted to enable the suppression of
astronomical sources with a starshade. The long baselines achievable with the
heliostat provide measurements of starshade performance at a flight-like
Fresnel number and resolution, aspects critical to the validation of optical
models. The heliostat has provided the opportunity to perform the first
astronomical observations with a starshade and has made science accessible in a
unique parameter space, high contrast at moderate inner working angles. On-sky
images are valuable for developing the experience and tools needed to extract
science results from future starshade observations. We report on high contrast
observations of nearby stars provided by a starshade. We achieve 5.6e-7
contrast at 30 arcseconds inner working angle on the star Vega and provide new
photometric constraints on background stars near Vega.
",Physics,Physics
"A Topologist's View of Kinematic Maps and Manipulation Complexity   In this paper we combine a survey of the most important topological
properties of kinematic maps that appear in robotics, with the exposition of
some basic results regarding the topological complexity of a map. In
particular, we discuss mechanical devices that consist of rigid parts connected
by joints and show how the geometry of the joints determines the forward
kinematic map that relates the configuration of joints with the pose of the
end-effector of the device. We explain how to compute the dimension of the
joint space and describe topological obstructions for a kinematic map to be a
fibration or to admit a continuous section. In the second part of the paper we
define the complexity of a continuous map and show how the concept can be
viewed as a measure of the difficulty to find a robust manipulation plan for a
given mechanical device. We also derive some basic estimates for the complexity
and relate it to the degree of instability of a manipulation plan.
",Computer Science; Mathematics,Computer Science
"Optimal Jittered Sampling for two Points in the Unit Square   Jittered Sampling is a refinement of the classical Monte Carlo sampling
method. Instead of picking $n$ points randomly from $[0,1]^2$, one partitions
the unit square into $n$ regions of equal measure and then chooses a point
randomly from each partition. Currently, no good rules for how to partition the
space are available. In this paper, we present a solution for the special case
of subdividing the unit square by a decreasing function into two regions so as
to minimize the expected squared $\mathcal{L}_2-$discrepancy. The optimal
partitions are given by a \textit{highly} nonlinear integral equation for which
we determine an approximate solution. In particular, there is a break of
symmetry and the optimal partition is not into two sets of equal measure. We
hope this stimulates further interest in the construction of good partitions.
",Mathematics,Mathematics
"Isotope Shifts in the 7s$\rightarrow$8s Transition of Francium: Measurements and Comparison to \textit{ab initio} Theory   We observe the electric-dipole forbidden $7s\rightarrow8s$ transition in the
francium isotopes $^{208-211}$Fr and $^{213}$Fr using a two-photon excitation
scheme. We collect the atoms online from an accelerator and confine them in a
magneto optical trap for the measurements. In combination with previous
measurements of the $7s\rightarrow7p_{1/2}$ transition we perform a King Plot
analysis. We compare the thus determined ratio of the field shift constants
(1.230 $\pm$ 0.019) to results obtained from new ab initio calculations (1.234
$\pm$ 0.010) and find excellent agreement.
",Physics,Physics
"Motions about a fixed point by hypergeometric functions: new non-complex analytical solutions and integration of the herpolhode   We study four problems in the dynamics of a body moving about a fixed point,
providing a non-complex, analytical solution for all of them. For the first
two, we will work on the motion first integrals. For the symmetrical heavy
body, that is the Lagrange-Poisson case, we compute the second and third Euler
angles in explicit and real forms by means of multiple hypergeometric functions
(Lauricella, functions). Releasing the weight load but adding the complication
of the asymmetry, by means of elliptic integrals of third kind, we provide the
precession angle completing some previous treatments of the Euler-Poinsot case.
Integrating then the relevant differential equation, we reach the finite polar
equation of a special trajectory named the {\it herpolhode}. In the last
problem we keep the symmetry of the first problem, but without the weight, and
take into account a viscous dissipation. The approach of first integrals is no
longer practicable in this situation and the Euler equations are faced directly
leading to dumped goniometric functions obtained as particular occurrences of
Bessel functions of order $-1/2$.
",Mathematics,Mathematics
"SUBIC: A Supervised Bi-Clustering Approach for Precision Medicine   Traditional medicine typically applies one-size-fits-all treatment for the
entire patient population whereas precision medicine develops tailored
treatment schemes for different patient subgroups. The fact that some factors
may be more significant for a specific patient subgroup motivates clinicians
and medical researchers to develop new approaches to subgroup detection and
analysis, which is an effective strategy to personalize treatment. In this
study, we propose a novel patient subgroup detection method, called Supervised
Biclustring (SUBIC) using convex optimization and apply our approach to detect
patient subgroups and prioritize risk factors for hypertension (HTN) in a
vulnerable demographic subgroup (African-American). Our approach not only finds
patient subgroups with guidance of a clinically relevant target variable but
also identifies and prioritizes risk factors by pursuing sparsity of the input
variables and encouraging similarity among the input variables and between the
input and target variables
",Computer Science; Statistics,Statistics
"Non-singular spacetimes with a negative cosmological constant: IV. Stationary black hole solutions with matter fields   We use an elliptic system of equations with complex coefficients for a set of
complex-valued tensor fields as a tool to construct infinite-dimensional
families of non-singular stationary black holes, real-valued Lorentzian
solutions of the Einstein-Maxwell-dilaton-scalar
fields-Yang-Mills-Higgs-Chern-Simons-$f(R)$ equations with a negative
cosmological constant. The families include an infinite-dimensional family of
solutions with the usual AdS conformal structure at conformal infinity.
",Mathematics,Mathematics
"On Bayesian Exponentially Embedded Family for Model Order Selection   In this paper, we derive a Bayesian model order selection rule by using the
exponentially embedded family method, termed Bayesian EEF. Unlike many other
Bayesian model selection methods, the Bayesian EEF can use vague proper priors
and improper noninformative priors to be objective in the elicitation of
parameter priors. Moreover, the penalty term of the rule is shown to be the sum
of half of the parameter dimension and the estimated mutual information between
parameter and observed data. This helps to reveal the EEF mechanism in
selecting model orders and may provide new insights into the open problems of
choosing an optimal penalty term for model order selection and choosing a good
prior from information theoretic viewpoints. The important example of linear
model order selection is given to illustrate the algorithms and arguments.
Lastly, the Bayesian EEF that uses Jeffreys prior coincides with the EEF rule
derived by frequentist strategies. This shows another interesting relationship
between the frequentist and Bayesian philosophies for model selection.
",Statistics,Computer Science; Statistics
"Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs   Generative Adversarial Networks (GANs) have shown remarkable success as a
framework for training models to produce realistic-looking data. In this work,
we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to
produce realistic real-valued multi-dimensional time series, with an emphasis
on their application to medical data. RGANs make use of recurrent neural
networks in the generator and the discriminator. In the case of RCGANs, both of
these RNNs are conditioned on auxiliary information. We demonstrate our models
in a set of toy datasets, where we show visually and quantitatively (using
sample likelihood and maximum mean discrepancy) that they can successfully
generate realistic time-series. We also describe novel evaluation methods for
GANs, where we generate a synthetic labelled training dataset, and evaluate on
a real test set the performance of a model trained on the synthetic data, and
vice-versa. We illustrate with these metrics that RCGANs can generate
time-series data useful for supervised training, with only minor degradation in
performance on real test data. This is demonstrated on digit classification
from 'serialised' MNIST and by training an early warning system on a medical
dataset of 17,000 patients from an intensive care unit. We further discuss and
analyse the privacy concerns that may arise when using RCGANs to generate
realistic synthetic medical time series data.
",Computer Science; Statistics,Computer Science; Statistics
"Darboux and Binary Darboux Transformations for Discrete Integrable Systems. II. Discrete Potential mKdV Equation   The paper presents two results. First it is shown how the discrete potential
modified KdV equation and its Lax pairs in matrix form arise from the
Hirota-Miwa equation by a 2-periodic reduction. Then Darboux transformations
and binary Darboux transformations are derived for the discrete potential
modified KdV equation and it is shown how these may be used to construct exact
solutions.
",Physics,Mathematics
"Bi-Lagrangian structures and Teichmüller theory   This paper has two purposes: the first is to study several structures on
manifolds in the general setting of real and complex differential geometry; the
second is to apply this study to Teichmüller theory. We primarily focus on
bi-Lagrangian structures, which are the data of a symplectic structure and a
pair of transverse Lagrangian foliations, and are equivalent to para-Kähler
structures. First we carefully study real and complex bi-Lagrangian structures
and discuss other closely related structures and their interrelationships. Next
we prove the existence of a canonical complex bi-Lagrangian structure in the
complexification of any real-analytic Kähler manifold and showcase its
properties. We later use this bi-Lagrangian structure to construct a natural
almost hyper-Hermitian structure. We then specialize our study to moduli spaces
of geometric structures on closed surfaces, which tend to have a rich
symplectic structure. We show that some of the recognized geometric features of
these moduli spaces are formal consequences of the general theory, while
revealing other new geometric features. We also gain clarity on several
well-known results of Teichmüller theory by deriving them from pure
differential geometric machinery.
",Mathematics,Mathematics
"Development of a 32-channel ASIC for an X-ray APD Detector onboard the ISS   We report on the design and performance of a mixed-signal application
specific integrated circuit (ASIC) dedicated to avalanche photodiodes (APDs) in
order to detect hard X-ray emissions in a wide energy band onboard the
International Space Station. To realize wide-band detection from 20 keV to 1
MeV, we use Ce:GAGG scintillators, each coupled to an APD, with low-noise
front-end electronics capable of achieving a minimum energy detection threshold
of 20 keV. The developed ASIC has the ability to read out 32-channel APD
signals using 0.35 $\mu$m CMOS technology, and an analog amplifier at the input
stage is designed to suppress the capacitive noise primarily arising from the
large detector capacitance of the APDs. The ASIC achieves a performance of 2099
e$^{-}$ + 1.5 e$^{-}$/pF at root mean square (RMS) with a wide 300 fC dynamic
range. Coupling a reverse-type APD with a Ce:GAGG scintillator, we obtain an
energy resolution of 6.7% (FWHM) at 662 keV and a minimum detectable energy of
20 keV at room temperature (20 $^{\circ}$C). Furthermore, we examine the
radiation tolerance for space applications by using a 90 MeV proton beam,
confirming that the ASIC is free of single-event effects and can operate
properly without serious degradation in analog and digital processing.
",Physics,Physics
"ClusterNet: Detecting Small Objects in Large Scenes by Exploiting Spatio-Temporal Information   Object detection in wide area motion imagery (WAMI) has drawn the attention
of the computer vision research community for a number of years. WAMI proposes
a number of unique challenges including extremely small object sizes, both
sparse and densely-packed objects, and extremely large search spaces (large
video frames). Nearly all state-of-the-art methods in WAMI object detection
report that appearance-based classifiers fail in this challenging data and
instead rely almost entirely on motion information in the form of background
subtraction or frame-differencing. In this work, we experimentally verify the
failure of appearance-based classifiers in WAMI, such as Faster R-CNN and a
heatmap-based fully convolutional neural network (CNN), and propose a novel
two-stage spatio-temporal CNN which effectively and efficiently combines both
appearance and motion information to significantly surpass the state-of-the-art
in WAMI object detection. To reduce the large search space, the first stage
(ClusterNet) takes in a set of extremely large video frames, combines the
motion and appearance information within the convolutional architecture, and
proposes regions of objects of interest (ROOBI). These ROOBI can contain from
one to clusters of several hundred objects due to the large video frame size
and varying object density in WAMI. The second stage (FoveaNet) then estimates
the centroid location of all objects in that given ROOBI simultaneously via
heatmap estimation. The proposed method exceeds state-of-the-art results on the
WPAFB 2009 dataset by 5-16% for moving objects and nearly 50% for stopped
objects, as well as being the first proposed method in wide area motion imagery
to detect completely stationary objects.
",Computer Science,Computer Science
"The discrete logarithm problem over prime fields: the safe prime case. The Smart attack, non-canonical lifts and logarithmic derivatives   In this brief note we connect the discrete logarithm problem over prime
fields in the safe prime case to the logarithmic derivative.
",Computer Science; Mathematics,Mathematics
"An Asynchronous Parallel Approach to Sparse Recovery   Asynchronous parallel computing and sparse recovery are two areas that have
received recent interest. Asynchronous algorithms are often studied to solve
optimization problems where the cost function takes the form $\sum_{i=1}^M
f_i(x)$, with a common assumption that each $f_i$ is sparse; that is, each
$f_i$ acts only on a small number of components of $x\in\mathbb{R}^n$. Sparse
recovery problems, such as compressed sensing, can be formulated as
optimization problems, however, the cost functions $f_i$ are dense with respect
to the components of $x$, and instead the signal $x$ is assumed to be sparse,
meaning that it has only $s$ non-zeros where $s\ll n$. Here we address how one
may use an asynchronous parallel architecture when the cost functions $f_i$ are
not sparse in $x$, but rather the signal $x$ is sparse. We propose an
asynchronous parallel approach to sparse recovery via a stochastic greedy
algorithm, where multiple processors asynchronously update a vector in shared
memory containing information on the estimated signal support. We include
numerical simulations that illustrate the potential benefits of our proposed
asynchronous method.
",Computer Science,Computer Science
"Generating Query Suggestions to Support Task-Based Search   We address the problem of generating query suggestions to support users in
completing their underlying tasks (which motivated them to search in the first
place). Given an initial query, these query suggestions should provide a
coverage of possible subtasks the user might be looking for. We propose a
probabilistic modeling framework that obtains keyphrases from multiple sources
and generates query suggestions from these keyphrases. Using the test suites of
the TREC Tasks track, we evaluate and analyze each component of our model.
",Computer Science,Computer Science
"Hardy Spaces over Half-strip Domains   We define Hardy spaces $H^p(\Omega_\pm)$ on half-strip domain~$\Omega_+$ and
$\Omega_-= \mathbb{C}\setminus\overline{\Omega_+}$, where $0<p<\infty$, and
prove that functions in $H^p(\Omega_\pm)$ has non-tangential boundary limit
a.e. on $\Gamma$, the common boundary of $\Omega_\pm$. We then prove that
Cauchy integral of functions in $L^p(\Gamma)$ are in $H^p(\Omega_\pm)$, where
$1<p<\infty$, that is, Cauchy transform is bounded. Besides, if $1\leqslant
p<\infty$, then $H^p(\Omega_\pm)$ functions are the Cauchy integral of their
non-tangential boundary limits. We also establish an isomorphism between
$H^p(\Omega_\pm)$ and $H^p(\mathbb{C}_\pm)$, the classical Hardy spaces over
upper and lower half complex planes.
",Mathematics,Mathematics
"Simple And Efficient Architecture Search for Convolutional Neural Networks   Neural networks have recently had a lot of success for many tasks. However,
neural network architectures that perform well are still typically designed
manually by experts in a cumbersome trial-and-error process. We propose a new
method to automatically search for well-performing CNN architectures based on a
simple hill climbing procedure whose operators apply network morphisms,
followed by short optimization runs by cosine annealing. Surprisingly, this
simple method yields competitive results, despite only requiring resources in
the same order of magnitude as training a single network. E.g., on CIFAR-10,
our method designs and trains networks with an error rate below 6% in only 12
hours on a single GPU; training for one day reduces this error further, to
almost 5%.
",Computer Science; Statistics,Computer Science
"Status maximization as a source of fairness in a networked dictator game   Human behavioural patterns exhibit selfish or competitive, as well as
selfless or altruistic tendencies, both of which have demonstrable effects on
human social and economic activity. In behavioural economics, such effects have
traditionally been illustrated experimentally via simple games like the
dictator and ultimatum games. Experiments with these games suggest that, beyond
rational economic thinking, human decision-making processes are influenced by
social preferences, such as an inclination to fairness. In this study we
suggest that the apparent gap between competitive and altruistic human
tendencies can be bridged by assuming that people are primarily maximising
their status, i.e., a utility function different from simple profit
maximisation. To this end we analyse a simple agent-based model, where
individuals play the repeated dictator game in a social network they can
modify. As model parameters we consider the living costs and the rate at which
agents forget infractions by others. We find that individual strategies used in
the game vary greatly, from selfish to selfless, and that both of the above
parameters determine when individuals form complex and cohesive social
networks.
",Computer Science; Quantitative Finance,Computer Science; Physics
"Regret Bounds for Reinforcement Learning via Markov Chain Concentration   We give a simple optimistic algorithm for which it is easy to derive regret
bounds of $\tilde{O}(\sqrt{t_{\rm mix} SAT})$ after $T$ steps in uniformly
ergodic Markov decision processes with $S$ states, $A$ actions, and mixing time
parameter $t_{\rm mix}$. These bounds are the first regret bounds in the
general, non-episodic setting with an optimal dependence on all given
parameters. They could only be improved by using an alternative mixing time
parameter.
",Statistics,Computer Science; Statistics
"Model Order Selection Rules For Covariance Structure Classification   The adaptive classification of the interference covariance matrix structure
for radar signal processing applications is addressed in this paper. This
represents a key issue because many detection architectures are synthesized
assuming a specific covariance structure which may not necessarily coincide
with the actual one due to the joint action of the system and environment
uncertainties. The considered classification problem is cast in terms of a
multiple hypotheses test with some nested alternatives and the theory of Model
Order Selection (MOS) is exploited to devise suitable decision rules. Several
MOS techniques, such as the Akaike, Takeuchi, and Bayesian information criteria
are adopted and the corresponding merits and drawbacks are discussed. At the
analysis stage, illustrating examples for the probability of correct model
selection are presented showing the effectiveness of the proposed rules.
",Mathematics; Statistics,Computer Science; Statistics
"Kondo destruction in a quantum paramagnet with magnetic frustration   We report results of isothermal magnetotransport and susceptibility
measurements at elevated magnetic fields B down to very low temperatures T on
high-quality single crystals of the frustrated Kondo-lattice system CePdAl.
They reveal a B*(T) line within the paramagnetic part of the phase diagram.
This line denotes a thermally broadened 'small'-to-'large' Fermi surface
crossover which substantially narrows upon cooling. At B_0* = B*(T=0) = (4.6
+/- 0.1) T, this B*(T) line merges with two other crossover lines, viz. Tp(B)
below and T_FL(B) above B_0*. Tp characterizes a frustration-dominated
spin-liquid state, while T_FL is the Fermi-liquid temperature associated with
the lattice Kondo effect. Non-Fermi-liquid phenomena which are commonly
observed near a 'Kondo destruction' quantum critical point cannot be resolved
in CePdAl. Our observations reveal a rare case where Kondo coupling,
frustration and quantum criticality are closely intertwined.
",Physics,Physics
"Bounds on poloidal kinetic energy in plane layer convection   A numerical method is presented which conveniently computes upper bounds on
heat transport and poloidal energy in plane layer convection for infinite and
finite Prandtl numbers. The bounds obtained for the heat transport coincide
with earlier results. These bounds imply upper bounds for the poloidal energy
which follow directly from the definitions of dissipation and energy. The same
constraints used for computing upper bounds on the heat transport lead to
improved bounds for the poloidal energy.
",Physics,Physics
"A time change strategy to model reporting delay dynamics in claims reserving   This paper considers the problem of predicting the number of claims that have
already incurred in past exposure years, but which have not yet been reported
to the insurer. This is an important building block in the risk management
strategy of an insurer since the company should be able to fulfill its
liabilities with respect to such claims. Our approach puts emphasis on modeling
the time between the occurrence and reporting of claims, the so-called
reporting delay. Using data at a daily level we propose a micro-level model for
the heterogeneity in reporting delay caused by calendar day effects in the
reporting process, such as the weekday pattern and holidays. A simulation study
identifies the strengths and weaknesses of our approach in several scenarios
compared to traditional methods to predict the number of incurred but not
reported claims from aggregated data (i.e. the chain ladder method). We also
illustrate our model on a European general liability insurance data set and
conclude that the granular approach compared to the chain ladder method is more
robust with respect to volatility in the occurrence process. Our framework can
be extended to other predictive problems where interest goes to events that
incurred in the past but which are subject to an observation delay (e.g. the
number of infections during an epidemic).
",Quantitative Finance,Statistics
"The efficiency of community detection by most similar node pairs   Community analysis is an important way to ascertain whether or not a complex
system consists of sub-structures with different properties. In this paper, we
give a two level community structure analysis for the SSCI journal system by
most similar co-citation pattern. Five different strategies for the selection
of most similar node (journal) pairs are introduced. The efficiency is checked
by the normalized mutual information technique. Statistical properties and
comparisons of the community results show that both of the two level detection
could give instructional information for the community structure of complex
systems. Further comparisons of the five strategies indicates that, the most
efficient strategy is to assign nodes with maximum similarity into the same
community whether the similarity information is complete or not, while random
selection generates small world local community with no inside order. These
results give valuable indication for efficient community detection by most
similar node pairs.
",Computer Science,Computer Science; Statistics
"Modular representations in type A with a two-row nilpotent central character   We study the category of representations of $\mathfrak{sl}_{m+2n}$ in
positive characteristic, whose p-character is a nilpotent whose Jordan type is
the two-row partition (m+n,n). In a previous paper with Anno, we used
Bezrukavnikov-Mirkovic-Rumynin's theory of positive characteristic localization
and exotic t-structures to give a geometric parametrization of the simples
using annular crossingless matchings. Building on this, here we give
combinatorial dimension formulae for the simple objects, and compute the
Jordan-Holder multiplicities of the simples inside the baby Vermas (in special
case where n=1, i.e. that a subregular nilpotent, these were known from work of
Jantzen). We use Cautis-Kamnitzer's geometric categorification of the tangle
calculus to study the images of the simple objects under the [BMR] equivalence.
The dimension formulae may be viewed as a positive characteristic analogue of
the combinatorial character formulae for simple objects in parabolic category O
for $\mathfrak{sl}_{m+2n}$, due to Lascoux and Schutzenberger.
",Mathematics,Mathematics
"Sparse and Smooth Prior for Bayesian Linear Regression with Application to ETEX Data   Sparsity of the solution of a linear regression model is a common
requirement, and many prior distributions have been designed for this purpose.
A combination of the sparsity requirement with smoothness of the solution is
also common in application, however, with considerably fewer existing prior
models. In this paper, we compare two prior structures, the Bayesian fused
lasso (BFL) and least-squares with adaptive prior covariance matrix (LS-APC).
Since only variational solution was published for the latter, we derive a Gibbs
sampling algorithm for its inference and Bayesian model selection. The method
is designed for high dimensional problems, therefore, we discuss numerical
issues associated with evaluation of the posterior. In simulation, we show that
the LS-APC prior achieves results comparable to that of the Bayesian Fused
Lasso for piecewise constant parameter and outperforms the BFL for parameters
of more general shapes. Another advantage of the LS-APC priors is revealed in
real application to estimation of the release profile of the European Tracer
Experiment (ETEX). Specifically, the LS-APC model provides more conservative
uncertainty bounds when the regressor matrix is not informative.
",Statistics,Statistics
"Preliminary Experiments using Subjective Logic for the Polyrepresentation of Information Needs   According to the principle of polyrepresentation, retrieval accuracy may
improve through the combination of multiple and diverse information object
representations about e.g. the context of the user, the information sought, or
the retrieval system. Recently, the principle of polyrepresentation was
mathematically expressed using subjective logic, where the potential
suitability of each representation for improving retrieval performance was
formalised through degrees of belief and uncertainty. No experimental evidence
or practical application has so far validated this model. We extend the work of
Lioma et al. (2010), by providing a practical application and analysis of the
model. We show how to map the abstract notions of belief and uncertainty to
real-life evidence drawn from a retrieval dataset. We also show how to estimate
two different types of polyrepresentation assuming either (a) independence or
(b) dependence between the information objects that are combined. We focus on
the polyrepresentation of different types of context relating to user
information needs (i.e. work task, user background knowledge, ideal answer) and
show that the subjective logic model can predict their optimal combination
prior and independently to the retrieval process.
",Computer Science,Computer Science
"The Malgrange Form and Fredholm Determinants   We consider the factorization problem of matrix symbols relative to a closed
contour, i.e., a Riemann-Hilbert problem, where the symbol depends analytically
on parameters. We show how to define a function $\tau$ which is locally
analytic on the space of deformations and that is expressed as a Fredholm
determinant of an operator of ""integrable"" type in the sense of
Its-Izergin-Korepin-Slavnov. The construction is not unique and the
non-uniqueness highlights the fact that the tau function is really the section
of a line bundle.
",Physics,Mathematics
"Numerical studies of Thompson's group F and related groups   We have developed polynomial-time algorithms to generate terms of the
cogrowth series for groups $\mathbb{Z}\wr \mathbb{Z},$ the lamplighter group,
$(\mathbb{Z}\wr \mathbb{Z})\wr \mathbb{Z}$ and the Navas-Brin group $B.$ We
have also given an improved algorithm for the coefficients of Thompson's group
$F,$ giving 32 terms of the cogrowth series. We develop numerical techniques to
extract the asymptotics of these various cogrowth series. We present improved
rigorous lower bounds on the growth-rate of the cogrowth series for Thompson's
group $F$ using the method from \cite{HHR15} applied to our extended series. We
also generalise their method by showing that it applies to loops on any locally
finite graph. Unfortunately, lower bounds less than 16 do not help in
determining amenability.
Again for Thompson's group $F$ we prove that, if the group is amenable, there
cannot be a sub-dominant stretched exponential term in the
asymptotics\footnote{ }. Yet the numerical data provides compelling evidence
for the presence of such a term. This observation suggests a potential path to
a proof of non-amenability: If the universality class of the cogrowth sequence
can be determined rigorously, it will likely prove non-amenability.
We estimate the asymptotics of the cogrowth coefficients of $F$ to be $$ c_n
\sim c \cdot \mu^n \cdot \kappa^{n^\sigma \log^\delta{n}} \cdot n^g,$$ where
$\mu \approx 15,$ $\kappa \approx 1/e,$ $\sigma \approx 1/2,$ $\delta \approx
1/2,$ and $g \approx -1.$ The growth constant $\mu$ must be 16 for amenability.
These two approaches, plus a third based on extrapolating lower bounds, support
the conjecture \cite{ERvR15, HHR15} that the group is not amenable.
",Mathematics,Mathematics
"A three-dimensional symmetry result for a phase transition equation in the genuinely nonlocal regime   We consider bounded solutions of the nonlocal Allen-Cahn equation $$
(-\Delta)^s u=u-u^3\qquad{\mbox{ in }}{\mathbb{R}}^3,$$ under the monotonicity
condition $\partial_{x_3}u>0$ and in the genuinely nonlocal regime in
which~$s\in\left(0,\frac12\right)$. Under the limit assumptions $$
\lim_{x_n\to-\infty} u(x',x_n)=-1\quad{\mbox{ and }}\quad \lim_{x_n\to+\infty}
u(x',x_n)=1,$$ it has been recently shown that~$u$ is necessarily $1$D, i.e. it
depends only on one Euclidean variable. The goal of this paper is to obtain a
similar result without assuming such limit conditions. This type of results can
be seen as nonlocal counterparts of the celebrated conjecture formulated by
Ennio De Giorgi.
",Mathematics,Mathematics
"Attentive Convolutional Neural Network based Speech Emotion Recognition: A Study on the Impact of Input Features, Signal Length, and Acted Speech   Speech emotion recognition is an important and challenging task in the realm
of human-computer interaction. Prior work proposed a variety of models and
feature sets for training a system. In this work, we conduct extensive
experiments using an attentive convolutional neural network with multi-view
learning objective function. We compare system performance using different
lengths of the input signal, different types of acoustic features and different
types of emotion speech (improvised/scripted). Our experimental results on the
Interactive Emotional Motion Capture (IEMOCAP) database reveal that the
recognition performance strongly depends on the type of speech data independent
of the choice of input features. Furthermore, we achieved state-of-the-art
results on the improvised speech data of IEMOCAP.
",Computer Science,Computer Science
"The role of local-geometrical-orders on the growth of dynamic-length-scales in glass-forming liquids   The precise nature of complex structural relaxation as well as an explanation
for the precipitous growth of relaxation time in cooling glass-forming liquids
are essential to the understanding of vitrification of liquids. The dramatic
increase of relaxation time is believed to be caused by the growth of one or
more correlation lengths, which has received much attention recently. Here, we
report a direct link between the growth of a specific local-geometrical-order
and an increase of dynamic-length-scale as the atomic dynamics in metallic
glass-forming liquids slow down. Although several types of local
geometrical-orders are present in these metallic liquids, the growth of
icosahedral ordering is found to be directly related to the increase of the
dynamic-length-scale. This finding suggests an intriguing scenario that the
transient icosahedral ordering could be the origin of the dynamic-length-scale
in metallic glass-forming liquids.
",Physics,Physics
"Penalized Maximum Tangent Likelihood Estimation and Robust Variable Selection   We introduce a new class of mean regression estimators -- penalized maximum
tangent likelihood estimation -- for high-dimensional regression estimation and
variable selection. We first explain the motivations for the key ingredient,
maximum tangent likelihood estimation (MTE), and establish its asymptotic
properties. We further propose a penalized MTE for variable selection and show
that it is $\sqrt{n}$-consistent, enjoys the oracle property. The proposed
class of estimators consists penalized $\ell_2$ distance, penalized exponential
squared loss, penalized least trimmed square and penalized least square as
special cases and can be regarded as a mixture of minimum Kullback-Leibler
distance estimation and minimum $\ell_2$ distance estimation. Furthermore, we
consider the proposed class of estimators under the high-dimensional setting
when the number of variables $d$ can grow exponentially with the sample size
$n$, and show that the entire class of estimators (including the aforementioned
special cases) can achieve the optimal rate of convergence in the order of
$\sqrt{\ln(d)/n}$. Finally, simulation studies and real data analysis
demonstrate the advantages of the penalized MTE.
",Statistics,Mathematics; Statistics
"Utilizing artificial neural networks to predict demand for weather-sensitive products at retail stores   One key requirement for effective supply chain management is the quality of
its inventory management. Various inventory management methods are typically
employed for different types of products based on their demand patterns,
product attributes, and supply network. In this paper, our goal is to develop
robust demand prediction methods for weather sensitive products at retail
stores. We employ historical datasets from Walmart, whose customers and markets
are often exposed to extreme weather events which can have a huge impact on
sales regarding the affected stores and products. We want to accurately predict
the sales of 111 potentially weather-sensitive products around the time of
major weather events at 45 of Walmart retails locations in the U.S.
Intuitively, we may expect an uptick in the sales of umbrellas before a big
thunderstorm, but it is difficult for replenishment managers to predict the
level of inventory needed to avoid being out-of-stock or overstock during and
after that storm. While they rely on a variety of vendor tools to predict sales
around extreme weather events, they mostly employ a time-consuming process that
lacks a systematic measure of effectiveness. We employ all the methods critical
to any analytics project and start with data exploration. Critical features are
extracted from the raw historical dataset for demand forecasting accuracy and
robustness. In particular, we employ Artificial Neural Network for forecasting
demand for each product sold around the time of major weather events. Finally,
we evaluate our model to evaluate their accuracy and robustness.
",Computer Science; Statistics,Computer Science
"A Combinatorial Approach to the Opposite Bi-Free Partial $S$-Transform   In this paper, we present a combinatorial approach to the opposite 2-variable
bi-free partial $S$-transforms where the opposite multiplication is used on the
right. In addition, extensions of this partial $S$-transforms to the
conditional bi-free and operator-valued bi-free settings are discussed.
",Mathematics,Mathematics
"Knowledge Engineering for Hybrid Deductive Databases   Modern knowledge base systems frequently need to combine a collection of
databases in different formats: e.g., relational databases, XML databases, rule
bases, ontologies, etc. In the deductive database system DDBASE, we can manage
these different formats of knowledge and reason about them. Even the file
systems on different computers can be part of the knowledge base. Often, it is
necessary to handle different versions of a knowledge base. E.g., we might want
to find out common parts or differences of two versions of a relational
database.
We will examine the use of abstractions of rule bases by predicate dependency
and rule predicate graphs. Also the proof trees of derived atoms can help to
compare different versions of a rule base. Moreover, it might be possible to
have derivations joining rules with other formalisms of knowledge
representation.
Ontologies have shown their benefits in many applications of intelligent
systems, and there have been many proposals for rule languages compatible with
the semantic web stack, e.g., SWRL, the semantic web rule language. Recently,
ontologies are used in hybrid systems for specifying the provenance of the
different components.
",Computer Science,Computer Science
"Interval-based Prediction Uncertainty Bound Computation in Learning with Missing Values   The problem of machine learning with missing values is common in many areas.
A simple approach is to first construct a dataset without missing values simply
by discarding instances with missing entries or by imputing a fixed value for
each missing entry, and then train a prediction model with the new dataset. A
drawback of this naive approach is that the uncertainty in the missing entries
is not properly incorporated in the prediction. In order to evaluate prediction
uncertainty, the multiple imputation (MI) approach has been studied, but the
performance of MI is sensitive to the choice of the probabilistic model of the
true values in the missing entries, and the computational cost of MI is high
because multiple models must be trained. In this paper, we propose an
alternative approach called the Interval-based Prediction Uncertainty Bounding
(IPUB) method. The IPUB method represents the uncertainties due to missing
entries as intervals, and efficiently computes the lower and upper bounds of
the prediction results when all possible training sets constructed by imputing
arbitrary values in the intervals are considered. The IPUB method can be
applied to a wide class of convex learning algorithms including penalized
least-squares regression, support vector machine (SVM), and logistic
regression. We demonstrate the advantages of the IPUB method by comparing it
with an existing method in numerical experiment with benchmark datasets.
",Statistics,Computer Science; Statistics
"Learning with Bounded Instance- and Label-dependent Label Noise   Instance- and label-dependent label noise (ILN) is widely existed in
real-world datasets but has been rarely studied. In this paper, we focus on a
particular case of ILN where the label noise rates, representing the
probabilities that the true labels of examples flip into the corrupted labels,
have upper bounds. We propose to handle this bounded instance- and
label-dependent label noise under two different conditions. First,
theoretically, we prove that when the marginal distributions $P(X|Y=+1)$ and
$P(X|Y=-1)$ have non-overlapping supports, we can recover every noisy example's
true label and perform supervised learning directly on the cleansed examples.
Second, for the overlapping situation, we propose a novel approach to learn a
well-performing classifier which needs only a few noisy examples to be labeled
manually. Experimental results demonstrate that our method works well on both
synthetic and real-world datasets.
",Statistics,Computer Science; Statistics
"Efficient Estimation of Linear Functionals of Principal Components   We study principal component analysis (PCA) for mean zero i.i.d. Gaussian
observations $X_1,\dots, X_n$ in a separable Hilbert space $\mathbb{H}$ with
unknown covariance operator $\Sigma.$ The complexity of the problem is
characterized by its effective rank ${\bf r}(\Sigma):= \frac{{\rm
tr}(\Sigma)}{\|\Sigma\|},$ where ${\rm tr}(\Sigma)$ denotes the trace of
$\Sigma$ and $\|\Sigma\|$ denotes its operator norm. We develop a method of
bias reduction in the problem of estimation of linear functionals of
eigenvectors of $\Sigma.$ Under the assumption that ${\bf r}(\Sigma)=o(n),$ we
establish the asymptotic normality and asymptotic properties of the risk of the
resulting estimators and prove matching minimax lower bounds, showing their
semi-parametric optimality.
",Mathematics; Statistics,Mathematics; Statistics
"Time-lagged autoencoders: Deep learning of slow collective variables for molecular kinetics   Inspired by the success of deep learning techniques in the physical and
chemical sciences, we apply a modification of an autoencoder type deep neural
network to the task of dimension reduction of molecular dynamics data. We can
show that our time-lagged autoencoder reliably finds low-dimensional embeddings
for high-dimensional feature spaces which capture the slow dynamics of the
underlying stochastic processes - beyond the capabilities of linear dimension
reduction techniques.
",Computer Science; Physics; Statistics,Statistics
"Canonical sine and cosine Transforms For Integrable Boehmians   In this paper we define canonical sine and cosine transform, convolution
operations, prove convolution theorems in space of integrable functions on real
space. Further, obtain some results require to construct the spaces of
integrable Boehmians then extend this canonical sine and canonical cosine
transforms to space of integrable Boehmians and obtain their properties.
",Mathematics,Mathematics
"Efficiently Manifesting Asynchronous Programming Errors in Android Apps   Android, the #1 mobile app framework, enforces the single-GUI-thread model,
in which a single UI thread manages GUI rendering and event dispatching. Due to
this model, it is vital to avoid blocking the UI thread for responsiveness. One
common practice is to offload long-running tasks into async threads. To achieve
this, Android provides various async programming constructs, and leaves
developers themselves to obey the rules implied by the model. However, as our
study reveals, more than 25% apps violate these rules and introduce
hard-to-detect, fail-stop errors, which we term as aysnc programming errors
(APEs). To this end, this paper introduces APEChecker, a technique to
automatically and efficiently manifest APEs. The key idea is to characterize
APEs as specific fault patterns, and synergistically combine static analysis
and dynamic UI exploration to detect and verify such errors. Among the 40
real-world Android apps, APEChecker unveils and processes 61 APEs, of which 51
are confirmed (83.6% hit rate). Specifically, APEChecker detects 3X more APEs
than the state-of-art testing tools (Monkey, Sapienz and Stoat), and reduces
testing time from half an hour to a few minutes. On a specific type of APEs,
APEChecker confirms 5X more errors than the data race detection tool,
EventRacer, with very few false alarms.
",Computer Science,Computer Science
"Variational methods for degenerate Kirchhoff equations   For a degenerate autonomous Kirchhoff equation which is set on $\mathbb{R}^N$
and involves the Berestycki-Lions type nonlinearity, we cope with the cases
$N=2,3$ and $N\geq5$ by using mountain pass and symmetric mountain pass
approaches and by using Clark theorem respectively.
",Mathematics,Mathematics
"Reinforcement Learning with a Corrupted Reward Channel   No real-world reward function is perfect. Sensory errors and software bugs
may result in RL agents observing higher (or lower) rewards than they should.
For example, a reinforcement learning agent may prefer states where a sensory
error gives it the maximum reward, but where the true reward is actually small.
We formalise this problem as a generalised Markov Decision Problem called
Corrupt Reward MDP. Traditional RL methods fare poorly in CRMDPs, even under
strong simplifying assumptions and when trying to compensate for the possibly
corrupt rewards. Two ways around the problem are investigated. First, by giving
the agent richer data, such as in inverse reinforcement learning and
semi-supervised reinforcement learning, reward corruption stemming from
systematic sensory errors may sometimes be completely managed. Second, by using
randomisation to blunt the agent's optimisation, reward corruption can be
partially managed under some assumptions.
",Computer Science; Statistics,Computer Science
"Crystal structure, site selectivity, and electronic structure of layered chalcogenide LaOBiPbS3   We have investigated the crystal structure of LaOBiPbS3 using neutron
diffraction and synchrotron X-ray diffraction. From structural refinements, we
found that the two metal sites, occupied by Bi and Pb, were differently
surrounded by the sulfur atoms. Calculated bond valence sum suggested that one
metal site was nearly trivalent and the other was nearly divalent. Neutron
diffraction also revealed site selectivity of Bi and Pb in the LaOBiPbS3
structure. These results suggested that the crystal structure of LaOBiPbS3 can
be regarded as alternate stacks of the rock-salt-type Pb-rich sulfide layers
and the LaOBiS2-type Bi-rich layers. From band calculations for an ideal
(LaOBiS2)(PbS) system, we found that the S bands of the PbS layer were
hybridized with the Bi bands of the BiS plane at around the Fermi energy, which
resulted in the electronic characteristics different from that of LaOBiS2.
Stacking the rock-salt type sulfide (chalcogenide) layers and the BiS2-based
layered structure could be a new strategy to exploration of new BiS2-based
layered compounds, exotic two-dimensional electronic states, or novel
functionality.
",Physics,Physics
"Second-order constrained variational problems on Lie algebroids: applications to optimal control   The aim of this work is to study, from an intrinsic and geometric point of
view, second-order constrained variational problems on Lie algebroids, that is,
optimization problems defined by a cost functional which depends on
higher-order derivatives of admissible curves on a Lie algebroid. Extending the
classical Skinner and Rusk formalism for the mechanics in the context of Lie
algebroids, for second-order constrained mechanical systems, we derive the
corresponding dynamical equations. We find a symplectic Lie subalgebroid where,
under some mild regularity conditions, the second-order constrained variational
problem, seen as a presymplectic Hamiltonian system, has a unique solution. We
study the relationship of this formalism with the second-order constrained
Euler-Poincaré and Lagrange-Poincaré equations, among others. Our study is
applied to the optimal control of mechanical systems.
",Mathematics,Mathematics
"Subset Synchronization in Monotonic Automata   We study extremal and algorithmic questions of subset and careful
synchronization in monotonic automata. We show that several synchronization
problems that are hard in general automata can be solved in polynomial time in
monotonic automata, even without knowing a linear order of the states preserved
by the transitions. We provide asymptotically tight bounds on the maximum
length of a shortest word synchronizing a subset of states in a monotonic
automaton and a shortest word carefully synchronizing a partial monotonic
automaton. We provide a complexity framework for dealing with problems for
monotonic weakly acyclic automata over a three-letter alphabet, and use it to
prove NP-completeness and inapproximability of problems such as {\sc Finite
Automata Intersection} and the problem of computing the rank of a subset of
states in this class. We also show that checking whether a monotonic partial
automaton over a four-letter alphabet is carefully synchronizing is NP-hard.
Finally, we give a simple necessary and sufficient condition when a strongly
connected digraph with a selected subset of vertices can be transformed into a
deterministic automaton where the corresponding subset of states is
synchronizing.
",Computer Science,Computer Science
"Closed almost-Kähler 4-manifolds of constant non-negative Hermitian holomorphic sectional curvature are Kähler   We show that a closed almost Kähler 4-manifold of globally constant
holomorphic sectional curvature $k\geq 0$ with respect to the canonical
Hermitian connection is automatically Kähler. The same result holds for $k<0$
if we require in addition that the Ricci curvature is J-invariant. The proofs
are based on the observation that such manifolds are self-dual, so that
Chern-Weil theory implies useful integral formulas, which are then combined
with results from Seiberg--Witten theory.
",Mathematics,Mathematics
"The Trace Criterion for Kernel Bandwidth Selection for Support Vector Data Description   Support vector data description (SVDD) is a popular anomaly detection
technique. The SVDD classifier partitions the whole data space into an
$\textit{inlier}$ region, which consists of the region $\textit{near}$ the
training data, and an $\textit{outlier}$ region, which consists of points
$\textit{away}$ from the training data. The computation of the SVDD classifier
requires a kernel function, for which the Gaussian kernel is a common choice.
The Gaussian kernel has a bandwidth parameter, and it is important to set the
value of this parameter correctly for good results. A small bandwidth leads to
overfitting such that the resulting SVDD classifier overestimates the number of
anomalies, whereas a large bandwidth leads to underfitting and an inability to
detect many anomalies. In this paper, we present a new unsupervised method for
selecting the Gaussian kernel bandwidth. Our method, which exploits the
low-rank representation of the kernel matrix to suggest a kernel bandwidth
value, is competitive with existing bandwidth selection methods.
",Computer Science,Computer Science; Statistics
"The Dynamic Geometry of Interaction Machine: A Call-by-need Graph Rewriter   Girard's Geometry of Interaction (GoI), a semantics designed for linear logic
proofs, has been also successfully applied to programming language semantics.
One way is to use abstract machines that pass a token on a fixed graph along a
path indicated by the GoI. These token-passing abstract machines are space
efficient, because they handle duplicated computation by repeating the same
moves of a token on the fixed graph. Although they can be adapted to obtain
sound models with regard to the equational theories of various evaluation
strategies for the lambda calculus, it can be at the expense of significant
time costs. In this paper we show a token-passing abstract machine that can
implement evaluation strategies for the lambda calculus, with certified time
efficiency. Our abstract machine, called the Dynamic GoI Machine (DGoIM),
rewrites the graph to avoid replicating computation, using the token to find
the redexes. The flexibility of interleaving token transitions and graph
rewriting allows the DGoIM to balance the trade-off of space and time costs.
This paper shows that the DGoIM can implement call-by-need evaluation for the
lambda calculus by using a strategy of interleaving token passing with as much
graph rewriting as possible. Our quantitative analysis confirms that the DGoIM
with this strategy of interleaving the two kinds of possible operations on
graphs can be classified as ""efficient"" following Accattoli's taxonomy of
abstract machines.
",Computer Science,Computer Science
"simode: R Package for statistical inference of ordinary differential equations using separable integral-matching   In this paper we describe simode: Separable Integral Matching for Ordinary
Differential Equations. The statistical methodologies applied in the package
focus on several minimization procedures of an integral-matching criterion
function, taking advantage of the mathematical structure of the differential
equations like separability of parameters from equations. Application of
integral based methods to parameter estimation of ordinary differential
equations was shown to yield more accurate and stable results comparing to
derivative based ones. Linear features such as separability were shown to ease
optimization and inference. We demonstrate the functionalities of the package
using various systems of ordinary differential equations.
",Statistics,Mathematics; Statistics
"Electrode Reactions in Slowly Relaxing Media   Standard models of reaction kinetics in condensed materials rely on the
Boltzmann-Gibbs distribution for the population of reactants at the top of the
free energy barrier separating them from the products. While energy dissipation
and quantum effects at the barrier top can potentially affect the transmission
coefficient entering the rate preexponential factor, much stronger dynamical
effects on the reaction barrier are caused by the breakdown of ergodicity for
populating the reaction barrier (violation of the Boltzmann-Gibbs statistics).
When the spectrum of medium modes coupled to the reaction coordinate includes
fluctuations slower than the reaction rate, such nuclear motions dynamically
freeze on the reaction time-scale and do not contribute to the activation
barrier. Here we consider the consequences of this scenario for electrode
reactions in slowly relaxing media. Changing electrode overpotential speeds
electrode electron transfer up, potentially cutting through the spectrum of
nuclear modes coupled to the reaction coordinate. The reorganization energy of
electrochemical electron transfer becomes a function of the electrode
overpotential, switching between the thermodynamic value at low rates to the
nonergodic limit at higher rates. The sharpness of this transition depends of
the relaxation spectrum of the medium. The reorganization energy experiences a
sudden drop with increasing overpotential for a medium with a Debye relaxation,
but becomes a much shallower function of the overpotential for media with
stretched exponential dynamics. The latter scenario characterizes electron
transfer in ionic liquids. The analysis of electrode reactions in
room-temperature ionic liquids shows that the magnitude of the free energy of
nuclear solvation is significantly below its thermodynamic limit.
",Physics,Physics
"Bäcklund Transformations for the Boussinesq Equation and Merging Solitons   The Bäcklund transformation (BT) for the ""good"" Boussinesq equation and its
superposition principles are presented and applied. Unlike many other standard
integrable equations, the Boussinesq equation does not have a strictly
algebraic superposition principle for 2 BTs, but it does for 3. We present
associated lattice systems. Applying the BT to the trivial solution generates
standard solitons but also what we call ""merging solitons"" --- solutions in
which two solitary waves (with related speeds) merge into a single one. We use
the superposition principles to generate a variety of interesting solutions,
including superpositions of a merging soliton with $1$ or $2$ regular solitons,
and solutions that develop a singularity in finite time which then disappears
at some later finite time. We prove a Wronskian formula for the solutions
obtained by applying a general sequence of BTs on the trivial solution.
Finally, we show how to obtain the standard conserved quantities of the
Boussinesq equation from the BT, and how the hierarchy of local symmetries
follows in a simple manner from the superposition principle for 3 BTs.
",Physics; Mathematics,Mathematics
"Backward Monte-Carlo applied to muon transport   We discuss a backward Monte-Carlo technique for muon transport problem, with
emphasis on its application in muography. Backward Monte-Carlo allows exclusive
sampling of a final state by reversing the simulation flow. In practice it can
be made analogous to an adjoint Monte-Carlo, though it is more versatile for
muon transport. A backward Monte-Carlo was implemented as a dedicated muon
transport library: PUMAS. It is shown for case studies relevant for muography
imaging that the implementations of forward and backward Monte-Carlo schemes
agree to better than 1%.
",Physics,Physics
"SHINE: Signed Heterogeneous Information Network Embedding for Sentiment Link Prediction   In online social networks people often express attitudes towards others,
which forms massive sentiment links among users. Predicting the sign of
sentiment links is a fundamental task in many areas such as personal
advertising and public opinion analysis. Previous works mainly focus on textual
sentiment classification, however, text information can only disclose the ""tip
of the iceberg"" about users' true opinions, of which the most are unobserved
but implied by other sources of information such as social relation and users'
profile. To address this problem, in this paper we investigate how to predict
possibly existing sentiment links in the presence of heterogeneous information.
First, due to the lack of explicit sentiment links in mainstream social
networks, we establish a labeled heterogeneous sentiment dataset which consists
of users' sentiment relation, social relation and profile knowledge by
entity-level sentiment extraction method. Then we propose a novel and flexible
end-to-end Signed Heterogeneous Information Network Embedding (SHINE) framework
to extract users' latent representations from heterogeneous networks and
predict the sign of unobserved sentiment links. SHINE utilizes multiple deep
autoencoders to map each user into a low-dimension feature space while
preserving the network structure. We demonstrate the superiority of SHINE over
state-of-the-art baselines on link prediction and node recommendation in two
real-world datasets. The experimental results also prove the efficacy of SHINE
in cold start scenario.
",Computer Science; Statistics,Computer Science
"Learning a Robust Society of Tracking Parts   Object tracking is an essential task in computer vision that has been studied
since the early days of the field. Being able to follow objects that undergo
different transformations in the video sequence, including changes in scale,
illumination, shape and occlusions, makes the problem extremely difficult. One
of the real challenges is to keep track of the changes in objects appearance
and not drift towards the background clutter. Different from previous
approaches, we obtain robustness against background with a tracker model that
is composed of many different parts. They are classifiers that respond at
different scales and locations. The tracker system functions as a society of
parts, each having its own role and level of credibility. Reliable classifiers
decide the tracker's next move, while newcomers are first monitored before
gaining the necessary level of reliability to participate in the decision
process. Some parts that loose their consistency are rejected, while others
that show consistency for a sufficiently long time are promoted to permanent
roles. The tracker system, as a whole, could also go through different phases,
from the usual, normal functioning to states of weak agreement and even crisis.
The tracker system has different governing rules in each state. What truly
distinguishes our work from others is not necessarily the strength of
individual tracking parts, but the way in which they work together and build a
strong and robust organization. We also propose an efficient way to learn
simultaneously many tracking parts, with a single closed-form formulation. We
obtain a fast and robust tracker with state of the art performance on the
challenging OTB50 dataset.
",Computer Science,Computer Science
"Generalized Biplots for Multidimensional Scaled Projections   Dimension reduction and visualization is a staple of data analytics. Methods
such as Principal Component Analysis (PCA) and Multidimensional Scaling (MDS)
provide low dimensional (LD) projections of high dimensional (HD) data while
preserving an HD relationship between observations. Traditional biplots assign
meaning to the LD space of a PCA projection by displaying LD axes for the
attributes. These axes, however, are specific to the linear projection used in
PCA. MDS projections, which allow for arbitrary stress and dissimilarity
functions, require special care when labeling the LD space. We propose an
iterative scheme to plot an LD axis for each attribute based on the
user-specified stress and dissimilarity metrics. We discuss the details of our
general biplot methodology, its relationship with PCA-derived biplots, and
provide examples using real data.
",Statistics,Computer Science; Statistics
"Channel surfaces in Lie sphere geometry   We discuss channel surfaces in the context of Lie sphere geometry and
characterise them as certain $\Omega_{0}$-surfaces. Since $\Omega_{0}$-surfaces
possess a rich transformation theory, we study the behaviour of channel
surfaces under these transformations. Furthermore, by using certain Dupin
cyclide congruences, we characterise Ribaucour pairs of channel surfaces.
",Mathematics,Mathematics
"Modeling Game Avatar Synergy and Opposition through Embedding in Multiplayer Online Battle Arena Games   Multiplayer Online Battle Arena (MOBA) games have received increasing
worldwide popularity recently. In such games, players compete in teams against
each other by controlling selected game avatars, each of which is designed with
different strengths and weaknesses. Intuitively, putting together game avatars
that complement each other (synergy) and suppress those of opponents
(opposition) would result in a stronger team. In-depth understanding of synergy
and opposition relationships among game avatars benefits player in making
decisions in game avatar drafting and gaining better prediction of match
events. However, due to intricate design and complex interactions between game
avatars, thorough understanding of their relationships is not a trivial task.
In this paper, we propose a latent variable model, namely Game Avatar
Embedding (GAE), to learn avatars' numerical representations which encode
synergy and opposition relationships between pairs of avatars. The merits of
our model are twofold: (1) the captured synergy and opposition relationships
are sensible to experienced human players' perception; (2) the learned
numerical representations of game avatars allow many important downstream
tasks, such as similar avatar search, match outcome prediction, and avatar pick
recommender. To our best knowledge, no previous model is able to simultaneously
support both features. Our quantitative and qualitative evaluations on real
match data from three commercial MOBA games illustrate the benefits of our
model.
",Computer Science,Computer Science
"Orientability of the moduli space of Spin(7)-instantons   Let $(M,\Omega)$ be a closed $8$-dimensional manifold equipped with a
generically non-integrable $\mathrm{Spin}(7)$-structure $\Omega$. We prove that
if $\mathrm{Hom}(H^{3}(M,\mathbb{Z}), \mathbb{Z}_{2}) = 0$ then the moduli
space of irreducible $\mathrm{Spin}(7)$-instantons on $(M,\Omega)$ with gauge
group $\mathrm{SU}(r)$, $r\geq 2$, is orientable.
",Mathematics,Mathematics
"Secure and Reconfigurable Network Design for Critical Information Dissemination in the Internet of Battlefield Things (IoBT)   The Internet of things (IoT) is revolutionizing the management and control of
automated systems leading to a paradigm shift in areas such as smart homes,
smart cities, health care, transportation, etc. The IoT technology is also
envisioned to play an important role in improving the effectiveness of military
operations in battlefields. The interconnection of combat equipment and other
battlefield resources for coordinated automated decisions is referred to as the
Internet of battlefield things (IoBT). IoBT networks are significantly
different from traditional IoT networks due to the battlefield specific
challenges such as the absence of communication infrastructure, and the
susceptibility of devices to cyber and physical attacks. The combat efficiency
and coordinated decision-making in war scenarios depends highly on real-time
data collection, which in turn relies on the connectivity of the network and
the information dissemination in the presence of adversaries. This work aims to
build the theoretical foundations of designing secure and reconfigurable IoBT
networks. Leveraging the theories of stochastic geometry and mathematical
epidemiology, we develop an integrated framework to study the communication of
mission-critical data among different types of network devices and consequently
design the network in a cost effective manner.
",Computer Science,Computer Science
"On certain type of difference polynomials of meromorphic functions   In this paper, we investigate zeros of difference polynomials of the form
$f(z)^nH(z, f)-s(z)$, where $f(z)$ is a meromorphic function, $H(z, f)$ is a
difference polynomial of $f(z)$ and $s(z)$ is a small function. We first obtain
some inequalities for the relationship of the zero counting function of
$f(z)^nH(z, f)-s(z)$ and the characteristic function and pole counting function
of $f(z)$. Based on these inequalities, we establish some difference analogues
of a classical result of Hayman for meromorphic functions. Some special cases
are also investigated. These results improve previous findings.
",Mathematics,Mathematics
"Abundances in photoionized nebulae of the Local Group and nucleosynthesis of intermediate mass stars   Photoionized nebulae, comprising HII regions and planetary nebulae, are
excellent laboratories to investigate the nucleosynthesis and chemical
evolution of several elements in the Galaxy and other galaxies of the Local
Group. Our purpose in this investigation is threefold: (i) compare the
abundances of HII regions and planetary nebulae in each system in order to
investigate the differences derived from the age and origin of these objects,
(ii) compare the chemical evolution in different systems, such as the Milky
Way, the Magellanic Clouds, and other galaxies of the Local Group, and (iii)
investigate to what extent the nucleosynthesis contributions from the
progenitor stars affect the observed abundances in planetary nebulae, which
constrains the nucleosynthesis of intermediate mass stars. We show that all
objects in the samples present similar trends concerning distance-independent
correlations, and some constraints can be defined on the production of He and N
by the PN progenitor stars.
",Physics,Physics
"On unique continuation for solutions of the Schr{ö}dinger equation on trees   We prove that if a solution of the time-dependent Schr{ö}dinger equation on
an homogeneous tree with bounded potential decays fast at two distinct times
then the solution is trivial. For the free Schr{ö}dinger operator, we use the
spectral theory of the Laplacian and complex analysis and obtain a
characterization of the initial conditions that lead to a sharp decay at any
time. We then use the recent spectral decomposition of the Schr{ö}dinger
operator with compactly supported potential due to Colin de Verdi{è}rre and
Turc to extend our results in the presence of such potentials. Finally, we use
real variable methods first introduced by Escauriaza, Kenig, Ponce and Vega to
establish a general sharp result in the case of bounded potentials.
",Mathematics,Mathematics
"Econometric Modeling of Regional Electricity Spot Prices in the Australian Market   Wholesale electricity markets are increasingly integrated via high voltage
interconnectors, and inter-regional trade in electricity is growing. To model
this, we consider a spatial equilibrium model of price formation, where
constraints on inter-regional flows result in three distinct equilibria in
prices. We use this to motivate an econometric model for the distribution of
observed electricity spot prices that captures many of their unique empirical
characteristics. The econometric model features supply and inter-regional trade
cost functions, which are estimated using Bayesian monotonic regression
smoothing methodology. A copula multivariate time series model is employed to
capture additional dependence -- both cross-sectional and serial-- in regional
prices. The marginal distributions are nonparametric, with means given by the
regression means. The model has the advantage of preserving the heavy
right-hand tail in the predictive densities of price. We fit the model to
half-hourly spot price data in the five interconnected regions of the
Australian national electricity market. The fitted model is then used to
measure how both supply and price shocks in one region are transmitted to the
distribution of prices in all regions in subsequent periods. Finally, to
validate our econometric model, we show that prices forecast using the proposed
model compare favorably with those from some benchmark alternatives.
",Statistics,Statistics
"Central limit theorems for entropy-regularized optimal transport on finite spaces and statistical applications   The notion of entropy-regularized optimal transport, also known as Sinkhorn
divergence, has recently gained popularity in machine learning and statistics,
as it makes feasible the use of smoothed optimal transportation distances for
data analysis. The Sinkhorn divergence allows the fast computation of an
entropically regularized Wasserstein distance between two probability
distributions supported on a finite metric space of (possibly) high-dimension.
For data sampled from one or two unknown probability distributions, we derive
the distributional limits of the empirical Sinkhorn divergence and its centered
version (Sinkhorn loss). We also propose a bootstrap procedure which allows to
obtain new test statistics for measuring the discrepancies between multivariate
probability distributions. Our work is inspired by the results of Sommerfeld
and Munk (2016) on the asymptotic distribution of empirical Wasserstein
distance on finite space using unregularized transportation costs. Incidentally
we also analyze the asymptotic distribution of entropy-regularized Wasserstein
distances when the regularization parameter tends to zero. Simulated and real
datasets are used to illustrate our approach.
",Mathematics; Statistics,Mathematics; Statistics
"The geometry of some generalized affine Springer fibers   We study basic geometric properties of some group analogue of affine Springer
fibers and compare with the classical Lie algebra affine Springer fibers. The
main purpose is to formulate a conjecture that relates the number of
irreducible components of such varieties for a reductive group $G$ to certain
weight multiplicities defined by the Langlands dual group $\hat{G}$. We prove
our conjecture in the case of unramified conjugacy class.
",Mathematics,Mathematics
"Dynamic Rank Maximal Matchings   We consider the problem of matching applicants to posts where applicants have
preferences over posts. Thus the input to our problem is a bipartite graph G =
(A U P,E), where A denotes a set of applicants, P is a set of posts, and there
are ranks on edges which denote the preferences of applicants over posts. A
matching M in G is called rank-maximal if it matches the maximum number of
applicants to their rank 1 posts, subject to this the maximum number of
applicants to their rank 2 posts, and so on.
We consider this problem in a dynamic setting, where vertices and edges can
be added and deleted at any point. Let n and m be the number of vertices and
edges in an instance G, and r be the maximum rank used by any rank-maximal
matching in G. We give a simple O(r(m+n))-time algorithm to update an existing
rank-maximal matching under each of these changes. When r = o(n), this is
faster than recomputing a rank-maximal matching completely using a known
algorithm like that of Irving et al., which takes time O(min((r + n,
r*sqrt(n))m).
",Computer Science,Computer Science
"Stable components in the parameter plane of meromorphic functions of finite type   We study the parameter planes of certain one-dimensional, dynamically-defined
slices of holomorphic families of meromorphic transcendental maps of finite
type for which infinity is not an asymptotic value. Our planes are defined by
constraining the orbits of all but one of the asymptotic values. We study the
structure of the regions in the parameter plane where the dynamics have a free
asymptotic value that is attracted to an attracting periodic orbit. The tangent
family is an example that has been studied in detail, \cite{KK}, and our goal
is to understand in the general context of meromorphic functions to what extent
the structure one sees in parameter plane of the tangent family is generic.
For the analogous dynamically defined slices of parameter spaces for rational
maps, there are Mandelbrot-like components that have a unique {\em center} that
can be used to give a combinatorial descripton of the components. No such
center exists for our slices, no matter how carefully they are chosen. We can
show, however, that there is an analogous concept, a ""virtual"" center, that
plays essentially the same role as a center would have. It lies on the boundary
of the component and corresponds to a map for which there is a ""virtual cycle"",
namely, an iterate of the free asymptotic value that lands at a pole. These
virtual centers are dense in the bifurcation locus of our one-dimensional
families.
",Mathematics,Mathematics
"Modeling and Simulation of the Dynamics of the Quick Return Mechanism: A Bond Graph Approach   This paper applies the multibond graph approach for rigid multibody systems
to model the dynamics of general spatial mechanisms. The commonly used quick
return mechanism which comprises of revolute as well as prismatic joints has
been chosen as a representative example to demonstrate the application of this
technique and its resulting advantages. In this work, the links of the quick
return mechanism are modeled as rigid bodies. The rigid links are then coupled
at the joints based on the nature of constraint. This alternative method of
formulation of system dynamics, using Bond Graphs, offers a rich set of
features that include pictorial representation of the dynamics of translation
and rotation for each link of the mechanism in the inertial frame,
representation and handling of constraints at the joints, depiction of
causality, obtaining dynamic reaction forces and moments at various locations
in the mechanism and so on. Yet another advantage of this approach is that the
coding for simulation can be carried out directly from the Bond Graph in an
algorithmic manner, without deriving system equations. In this work, the
program code for simulation is written in MATLAB. The vector and tensor
operations are conveniently represented in MATLAB, resulting in a compact and
optimized code. The simulation results are plotted and discussed in detail.
",Computer Science,Computer Science
"Comparison of forcing functions in magnetohydrodynamic turbulence   Results are presented of direct numerical simulations of incompressible,
homogeneous magnetohydrodynamic turbulence without a mean magnetic field,
subject to different mechanical forcing functions commonly used in the
literature. Specifically, the forces are negative damping (which uses the
large-scale velocity field as a forcing function), a nonhelical random force,
and a nonhelical static sinusoidal force (analogous to helical ABC forcing).
The time evolution of the three ideal invariants (energy, magnetic helicity and
cross helicity), the time-averaged energy spectra, the energy ratios and the
dissipation ratios are examined. All three forcing functions produce
qualitatively similar steady states with regards to the time evolution of the
energy and magnetic helicity. However, differences in the cross helicity
evolution are observed, particularly in the case of the static sinusoidal
method of energy injection. Indeed, an ensemble of sinusoidally-forced
simulations with identical parameters shows significant variations in the cross
helicity over long time periods, casting some doubt on the validity of the
principle of ergodicity in systems in which the injection of helicity cannot be
controlled. Cross helicity can unexpectedly enter the system through the
forcing function and must be carefully monitored.
",Physics,Physics
"Warm dark matter and the ionization history of the Universe   In warm dark matter scenarios structure formation is suppressed on small
scales with respect to the cold dark matter case, reducing the number of
low-mass halos and the fraction of ionized gas at high redshifts and thus,
delaying reionization. This has an impact on the ionization history of the
Universe and measurements of the optical depth to reionization, of the
evolution of the global fraction of ionized gas and of the thermal history of
the intergalactic medium, can be used to set constraints on the mass of the
dark matter particle. However, the suppression of the fraction of ionized
medium in these scenarios can be partly compensated by varying other
parameters, as the ionization efficiency or the minimum mass for which halos
can host star-forming galaxies. Here we use different data sets regarding the
ionization and thermal histories of the Universe and, taking into account the
degeneracies from several astrophysical parameters, we obtain a lower bound on
the mass of thermal warm dark matter candidates of $m_X > 1.3$ keV, or $m_s >
5.5$ keV for the case of sterile neutrinos non-resonantly produced in the early
Universe, both at 90\% confidence level.
",Physics,Physics
"Role of Skin Friction Drag during Flow-Induced Reconfiguration of a Flexible Thin Plate   We investigate drag reduction due to the flow-induced reconfiguration of a
flexible thin plate in presence of skin friction drag at low Reynolds Number.
The plate is subjected to a uniform free stream and is tethered at one end. We
extend existing models in the literature to account for the skin friction drag.
The total drag on the plate with respect to a rigid upright plate decreases due
to flow-induced reconfiguration and further reconfiguration increases the total
drag due to increase in skin friction drag. A critical value of Cauchy number
($Ca$) exists at which the total drag on the plate with respect to a rigid
upright plate is minimum at a given Reynolds number. The reconfigured shape of
the plate for this condition is unique, beyond which the total drag increases
on the plate even with reconfiguration. The ratio of the form drag coefficient
for an upright rigid plate and skin drag coefficient for a horizontal rigid
plate ($\lambda$) determines the critical Cauchy number ($Ca_{cr}$). We propose
modification in the drag scaling with free stream velocity ($F_{x}$ ${\propto}$
$U^{n}$) in presence of the skin friction drag. The following expressions of
$n$ are found for $0.01 \leq Re \leq 1$, $n = 4/5 + {\lambda}/5$ for 1 $\leq$
$Ca$ $<$ $Ca_{cr}$ and $n = 1 + {\lambda}/5$ for $Ca_{cr} \leq Ca \leq 300$,
where $Re$ is Reynolds number. We briefly discuss the combined effect of the
skin friction drag and buoyancy on the drag reduction. An assessment of the
feasibility of experiments is presented in order to translate the present model
to physical systems.
",Physics,Physics
"Variational Continual Learning   This paper develops variational continual learning (VCL), a simple but
general framework for continual learning that fuses online variational
inference (VI) and recent advances in Monte Carlo VI for neural networks. The
framework can successfully train both deep discriminative models and deep
generative models in complex continual learning settings where existing tasks
evolve over time and entirely new tasks emerge. Experimental results show that
VCL outperforms state-of-the-art continual learning methods on a variety of
tasks, avoiding catastrophic forgetting in a fully automatic way.
",Computer Science; Statistics,Statistics
"Optimizing expected word error rate via sampling for speech recognition   State-level minimum Bayes risk (sMBR) training has become the de facto
standard for sequence-level training of speech recognition acoustic models. It
has an elegant formulation using the expectation semiring, and gives large
improvements in word error rate (WER) over models trained solely using
cross-entropy (CE) or connectionist temporal classification (CTC). sMBR
training optimizes the expected number of frames at which the reference and
hypothesized acoustic states differ. It may be preferable to optimize the
expected WER, but WER does not interact well with the expectation semiring, and
previous approaches based on computing expected WER exactly involve expanding
the lattices used during training. In this paper we show how to perform
optimization of the expected WER by sampling paths from the lattices used
during conventional sMBR training. The gradient of the expected WER is itself
an expectation, and so may be approximated using Monte Carlo sampling. We show
experimentally that optimizing WER during acoustic model training gives 5%
relative improvement in WER over a well-tuned sMBR baseline on a 2-channel
query recognition task (Google Home).
",Computer Science; Statistics,Computer Science; Statistics
"How production networks amplify economic growth   Technological improvement is the most important cause of long-term economic
growth, but the factors that drive it are still not fully understood. In
standard growth models technology is treated in the aggregate, and a main goal
has been to understand how growth depends on factors such as knowledge
production. But an economy can also be viewed as a network, in which producers
purchase goods, convert them to new goods, and sell them to households or other
producers. Here we develop a simple theory that shows how the network
properties of an economy can amplify the effects of technological improvements
as they propagate along chains of production. A key property of an industry is
its output multiplier, which can be understood as the average number of
production steps required to make a good. The model predicts that the output
multiplier of an industry predicts future changes in prices, and that the
average output multiplier of a country predicts future economic growth. We test
these predictions using data from the World Input Output Database and find
results in good agreement with the model. The results show how purely
structural properties of an economy, that have nothing to do with innovation or
human creativity, can exert an important influence on long-term growth.
",Quantitative Finance,Computer Science; Physics
"Bridging Static and Dynamic Program Analysis using Fuzzy Logic   Static program analysis is used to summarize properties over all dynamic
executions. In a unifying approach based on 3-valued logic properties are
either assigned a definite value or unknown. But in summarizing a set of
executions, a property is more accurately represented as being biased towards
true, or towards false. Compilers use program analysis to determine benefit of
an optimization. Since benefit (e.g., performance) is justified based on the
common case understanding bias is essential in guiding the compiler.
Furthermore, successful optimization also relies on understanding the quality
of the information, i.e. the plausibility of the bias. If the quality of the
static information is too low to form a decision we would like a mechanism that
improves dynamically.
We consider the problem of building such a reasoning framework and present
the fuzzy data-flow analysis. Our approach generalize previous work that use
3-valued logic. We derive fuzzy extensions of data-flow analyses used by the
lazy code motion optimization and unveil opportunities previous work would not
detect due to limited expressiveness. Furthermore we show how the results of
our analysis can be used in an adaptive classifier that improve as the
application executes.
",Computer Science,Computer Science
"Effect of Meltdown and Spectre Patches on the Performance of HPC Applications   In this work we examine how the updates addressing Meltdown and Spectre
vulnerabilities impact the performance of HPC applications. To study this we
use the application kernel module of XDMoD to test the performance before and
after the application of the vulnerability patches. We tested the performance
difference for multiple application and benchmarks including: NWChem, NAMD,
HPCC, IOR, MDTest and IMB. The results show that although some specific
functions can have performance decreased by as much as 74%, the majority of
individual metrics indicates little to no decrease in performance. The
real-world applications show a 2-3% decrease in performance for single node
jobs and a 5-11% decrease for parallel multi node jobs.
",Computer Science,Computer Science
"Simulating Dirac models with ultracold atoms in optical lattices   We present a general model allowing ""quantum simulation"" of one-dimensional
Dirac models with 2- and 4-component spinors using ultracold atoms in driven 1D
tilted optical latices. The resulting Dirac physics is illustrated by one of
its well-known manifestations, Zitterbewegung. This general model can be
extended and applied with great flexibility to more complex situations.
",Physics,Physics
"Hausdorff Measure: Lost in Translation   In the present article we describe how one can define Hausdorff measure
allowing empty elements in coverings, and using infinite countable coverings
only. In addition, we discuss how the use of different nonequivalent
interpretations of the notion ""countable set"", that is typical for classical
and modern mathematics, may lead to contradictions.
",Mathematics,Mathematics
"Strong convergence rates of modified truncated EM method for stochastic differential equations   Motivated by truncated EM method introduced by Mao (2015), a new explicit
numerical method named modified truncated Euler-Maruyama method is developed in
this paper. Strong convergence rates of the given numerical scheme to the exact
solutions to stochastic differential equations are investigated under given
conditions in this paper. Compared with truncated EM method, the given
numerical simulation strongly converges to the exact solution at fixed time $T$
and over a time interval $[0,T]$ under weaker sufficient conditions. Meanwhile,
the convergence rates are also obtained for both cases. Two examples are
provided to support our conclusions.
",Mathematics,Mathematics
"An Analytic Formula for Numbers of Restricted Partitions from Conformal Field Theory   We study the correlators of irregular vertex operators in two-dimensional
conformal field theory (CFT) in order to propose an exact analytic formula for
calculating numbers of partitions, that is:
1) for given $N,k$, finding the total number $\lambda(N|k)$ of length $k$
partitions of $N$: $N=n_1+...+n_k;0<n_1\leq{n_2}...\leq{n_k}$.
2) finding the total number $\lambda(N)=\sum_{k=1}^N\lambda(N|k)$ of
partitions of a natural number $N$
We propose an exact analytic expression for $\lambda(N|k)$ by relating
two-point short-distance correlation functions of irregular vertex operators in
$c=1$ conformal field theory ( the form of the operators is established in this
paper): with the first correlator counting the partitions in the upper
half-plane and the second one obtained from the first correlator by conformal
transformations of the form $f(z)=h(z)e^{-{i\over{z}}}$ where $h(z)$ is regular
and non-vanishing at $z=0$. The final formula for $\lambda(N|k)$ is given in
terms of regularized ($\epsilon$-ordered) finite series in the generalized
higher-derivative Schwarzians and incomplete Bell polynomials of the above
conformal transformation at $z=i\epsilon$ ($\epsilon\rightarrow{0}$)
",Mathematics,Mathematics
"Continuous Measurement of an Atomic Current   We are interested in dynamics of quantum many-body systems under continuous
observation, and its physical realizations involving cold atoms in lattices. In
the present work we focus on continuous measurement of atomic currents in
lattice models, including the Hubbard model. We describe a Cavity QED setup,
where measurement of a homodyne current provides a faithful representation of
the atomic current as a function of time. We employ the quantum optical
description in terms of a diffusive stochastic Schrödinger equation to follow
the time evolution of the atomic system conditional to observing a given
homodyne current trajectory, thus accounting for the competition between the
Hamiltonian evolution and measurement back-action. As an illustration, we
discuss minimal models of atomic dynamics and continuous current measurement on
rings with synthetic gauge fields, involving both real space and synthetic
dimension lattices (represented by internal atomic states). Finally, by `not
reading' the current measurements the time evolution of the atomic system is
governed by a master equation, where - depending on the microscopic details of
our CQED setups - we effectively engineer a current coupling of our system to a
quantum reservoir. This provides novel scenarios of dissipative dynamics
generating `dark' pure quantum many-body states.
",Physics,Physics
"Behavior Revealed in Mobile Phone Usage Predicts Loan Repayment   Many households in developing countries lack formal financial histories,
making it difficult for banks to extend loans, and for potential borrowers to
receive them. However, many of these households have mobile phones, which
generate rich data about behavior. This paper shows that behavioral signatures
in mobile phone data predict loan default, using call records matched to loan
outcomes. In a middle income South American country, individuals in the highest
quintile of risk by our measure are 2.8 times more likely to default than those
in the lowest quintile. On our sample of individuals with (thin) financial
histories, our method outperforms models using credit bureau information, both
within time and when tested on a different time period. The method forms the
basis for new forms of lending that reach the unbanked.
",Computer Science,Computer Science
"Categorification of sign-skew-symmetric cluster algebras and some conjectures on g-vectors   Using the unfolding method given in \cite{HL}, we prove the conjectures on
sign-coherence and a recurrence formula respectively of ${\bf g}$-vectors for
acyclic sign-skew-symmetric cluster algebras. As a following consequence, the
conjecture is affirmed in the same case which states that the ${\bf g}$-vectors
of any cluster form a basis of $\mathbb Z^n$. Also, the additive
categorification of an acyclic sign-skew-symmetric cluster algebra $\mathcal
A(\Sigma)$ is given, which is realized as $(\mathcal C^{\widetilde Q},\Gamma)$
for a Frobenius $2$-Calabi-Yau category $\mathcal C^{\widetilde Q}$ constructed
from an unfolding $(Q,\Gamma)$ of the acyclic exchange matrix $B$ of $\mathcal
A(\Sigma)$.
",Mathematics,Mathematics
"Topological thermal Hall effect due to Weyl magnons   We present the first theoretical evidence of zero magnetic field topological
(anomalous) thermal Hall effect due to Weyl magnons. Here, we consider Weyl
magnons in stacked noncoplanar frustrated kagomé antiferromagnets recently
proposed by Owerre, [arXiv:1708.04240]. The Weyl magnons in this system result
from macroscopically broken time-reversal symmetry by the scalar spin chirality
of noncoplanar chiral spin textures. Most importantly, they come from the
lowest excitation, therefore they can be easily observed experimentally at low
temperatures due to the population effect. Similar to electronic Weyl nodes
close to the Fermi energy, Weyl magnon nodes in the lowest excitation are the
most important. Indeed, we show that the topological (anomalous) thermal Hall
effect in this system arises from nonvanishing Berry curvature due to Weyl
magnon nodes in the lowest excitation, and it depends on their distribution
(distance) in momentum space. The present result paves the way to directly
probe low excitation Weyl magnons and macroscopically broken time-reversal
symmetry in three-dimensional frustrated magnets with the anomalous thermal
Hall effect.
",Physics,Physics
"Focusing on a Probability Element: Parameter Selection of Message Importance Measure in Big Data   Message importance measure (MIM) is applicable to characterize the importance
of information in the scenario of big data, similar to entropy in information
theory. In fact, MIM with a variable parameter can make an effect on the
characterization of distribution. Furthermore, by choosing an appropriate
parameter of MIM, it is possible to emphasize the message importance of a
certain probability element in a distribution. Therefore, parametric MIM can
play a vital role in anomaly detection of big data by focusing on probability
of an anomalous event. In this paper, we propose a parameter selection method
of MIM focusing on a probability element and then present its major properties.
In addition, we discuss the parameter selection with prior probability, and
investigate the availability in a statistical processing model of big data for
anomaly detection problem.
",Computer Science; Mathematics,Computer Science; Statistics
"SOI RF Switch for Wireless Sensor Network   The objective of this research was to design a 0-5 GHz RF SOI switch, with
0.18um power Jazz SOI technology by using Cadence software, for health care
applications. This paper introduces the design of a RF switch implemented in
shunt-series topology. An insertion loss of 0.906 dB and an isolation of 30.95
dB were obtained at 5 GHz. The switch also achieved a third order distortion of
53.05 dBm and 1 dB compression point reached 50.06dBm. The RF switch
performance meets the desired specification requirements.
",Computer Science,Computer Science
"Poisson--Gamma Dynamical Systems   We introduce a new dynamical system for sequentially observed multivariate
count data. This model is based on the gamma--Poisson construction---a natural
choice for count data---and relies on a novel Bayesian nonparametric prior that
ties and shrinks the model parameters, thus avoiding overfitting. We present an
efficient MCMC inference algorithm that advances recent work on augmentation
schemes for inference in negative binomial models. Finally, we demonstrate the
model's inductive bias using a variety of real-world data sets, showing that it
exhibits superior predictive performance over other models and infers highly
interpretable latent structure.
",Computer Science; Statistics,Statistics
"Brownian forgery of statistical dependences   The balance held by Brownian motion between temporal regularity and
randomness is embodied in a remarkable way by Levy's forgery of continuous
functions. Here we describe how this property can be extended to forge
arbitrary dependences between two statistical systems, and then establish a new
Brownian independence test based on fluctuating random paths. We also argue
that this result allows revisiting the theory of Brownian covariance from a
physical perspective and opens the possibility of engineering nonlinear
correlation measures from more general functional integrals.
",Mathematics; Statistics,Mathematics; Statistics
"A more symmetric picture for Kasparov's KK-bifunctor   For C*-algebras $A$ and $B$, we generalize the notion of a quasihomomorphism
from $A$ to $B$, due to Cuntz, by considering quasihomomorphisms from some
C*-algebra $C$ to $B$ such that $C$ surjects onto $A$, and the two maps forming
a quasihomomorphism agree on the kernel of this surjection. Under an additional
assumption, the group of homotopy classes of such generalized
quasihomomorphisms coincides with $KK(A,B)$. This makes the definition of
Kasparov's bifunctor slightly more symmetric and gives more flexibility for
constructing elements of $KK$-groups. These generalized quasihomomorphisms can
be viewed as pairs of maps directly from $A$ (instead of various $C$'s), but
these maps need not be $*$-homomorphisms.
",Mathematics,Mathematics
"Semi-Supervised Generation with Cluster-aware Generative Models   Deep generative models trained with large amounts of unlabelled data have
proven to be powerful within the domain of unsupervised learning. Many real
life data sets contain a small amount of labelled data points, that are
typically disregarded when training generative models. We propose the
Cluster-aware Generative Model, that uses unlabelled information to infer a
latent representation that models the natural clustering of the data, and
additional labelled data points to refine this clustering. The generative
performances of the model significantly improve when labelled information is
exploited, obtaining a log-likelihood of -79.38 nats on permutation invariant
MNIST, while also achieving competitive semi-supervised classification
accuracies. The model can also be trained fully unsupervised, and still improve
the log-likelihood performance with respect to related methods.
",Computer Science; Statistics,Statistics
"A trans-disciplinary review of deep learning research for water resources scientists   Deep learning (DL), a new-generation of artificial neural network research,
has transformed industries, daily lives and various scientific disciplines in
recent years. DL represents significant progress in the ability of neural
networks to automatically engineer problem-relevant features and capture highly
complex data distributions. I argue that DL can help address several major new
and old challenges facing research in water sciences such as
inter-disciplinarity, data discoverability, hydrologic scaling, equifinality,
and needs for parameter regionalization. This review paper is intended to
provide water resources scientists and hydrologists in particular with a simple
technical overview, trans-disciplinary progress update, and a source of
inspiration about the relevance of DL to water. The review reveals that various
physical and geoscientific disciplines have utilized DL to address data
challenges, improve efficiency, and gain scientific insights. DL is especially
suited for information extraction from image-like data and sequential data.
Techniques and experiences presented in other disciplines are of high relevance
to water research. Meanwhile, less noticed is that DL may also serve as a
scientific exploratory tool. A new area termed 'AI neuroscience,' where
scientists interpret the decision process of deep networks and derive insights,
has been born. This budding sub-discipline has demonstrated methods including
correlation-based analysis, inversion of network-extracted features,
reduced-order approximations by interpretable models, and attribution of
network decisions to inputs. Moreover, DL can also use data to condition
neurons that mimic problem-specific fundamental organizing units, thus
revealing emergent behaviors of these units. Vast opportunities exist for DL to
propel advances in water sciences.
",Computer Science; Statistics,Computer Science; Statistics
"Spin-Frustrated Pyrochlore Chains in the Volcanic Mineral Kamchatkite (KCu3OCl(SO4)2)   Search of new frustrated magnetic systems is of a significant importance for
physics studying the condensed matter. The platform for geometric frustration
of magnetic systems can be provided by copper oxocentric tetrahedra (OCu4)
forming the base of crystalline structures of copper minerals from Tolbachik
volcanos in Kamchatka. The present work was devoted to a new frustrated
antiferromagnetic - kamchatkite (KCu3OCl(SO4)2). The calculation of the sign
and strength of magnetic couplings in KCu3OCl(SO4)2 has been performed on the
basis of structural data by the phenomenological crystal chemistry method with
taking into account corrections on the Jahn-Teller orbital degeneracy of Cu2.
It has been established that kamchatkite (KCu3OCl(SO4)2) contains AFM
spin-frustrated chains of the pyrochlore type composed of cone-sharing Cu4
tetrahedra. Strong AFM intrachain and interchain couplings compete with each
other. Frustration of magnetic couplings in tetrahedral chains is combined with
the presence of electric polarization.
",Physics,Physics
"A Koszul sign map   We define a Koszul sign map encoding the Koszul sign convention. A
cohomological interpretation is given.
",Mathematics,Mathematics
"Align and Copy: UZH at SIGMORPHON 2017 Shared Task for Morphological Reinflection   This paper presents the submissions by the University of Zurich to the
SIGMORPHON 2017 shared task on morphological reinflection. The task is to
predict the inflected form given a lemma and a set of morpho-syntactic
features. We focus on neural network approaches that can tackle the task in a
limited-resource setting. As the transduction of the lemma into the inflected
form is dominated by copying over lemma characters, we propose two recurrent
neural network architectures with hard monotonic attention that are strong at
copying and, yet, substantially different in how they achieve this. The first
approach is an encoder-decoder model with a copy mechanism. The second approach
is a neural state-transition system over a set of explicit edit actions,
including a designated COPY action. We experiment with character alignment and
find that naive, greedy alignment consistently produces strong results for some
languages. Our best system combination is the overall winner of the SIGMORPHON
2017 Shared Task 1 without external resources. At a setting with 100 training
samples, both our approaches, as ensembles of models, outperform the next best
competitor.
",Computer Science,Computer Science
"Strain broadening of the 1042-nm zero-phonon line of the NV- center in diamond: a promising spectroscopic tool for defect tomography   The negatively charged nitrogen-vacancy (NV-) center in diamond is a
promising candidate for many quantum applications. Here, we examine the
splitting and broadening of the center's infrared (IR) zero-phonon line (ZPL).
We develop a model for these effects that accounts for the strain induced by
photo-dependent microscopic distributions of defects. We apply this model to
interpret observed variations of the IR ZPL shape with temperature and
photoexcitation conditions. We identify an anomalous temperature dependent
broadening mechanism and that defects other than the substitutional nitrogen
center significantly contribute to strain broadening. The former conclusion
suggests the presence of a strong Jahn-Teller effect in the center's singlet
levels and the latter indicates that major sources of broadening are yet to be
identified. These conclusions have important implications for the understanding
of the center and the engineering of diamond quantum devices. Finally, we
propose that the IR ZPL can be used as a sensitive spectroscopic tool for
probing microscopic strain fields and performing defect tomography.
",Physics,Physics
"Generative Temporal Models with Memory   We consider the general problem of modeling temporal data with long-range
dependencies, wherein new observations are fully or partially predictable based
on temporally-distant, past observations. A sufficiently powerful temporal
model should separate predictable elements of the sequence from unpredictable
elements, express uncertainty about those unpredictable elements, and rapidly
identify novel elements that may help to predict the future. To create such
models, we introduce Generative Temporal Models augmented with external memory
systems. They are developed within the variational inference framework, which
provides both a practical training methodology and methods to gain insight into
the models' operation. We show, on a range of problems with sparse, long-term
temporal dependencies, that these models store information from early in a
sequence, and reuse this stored information efficiently. This allows them to
perform substantially better than existing models based on well-known recurrent
neural networks, like LSTMs.
",Computer Science; Statistics,Computer Science; Statistics
"An Optimal Algorithm for Online Unconstrained Submodular Maximization   We consider a basic problem at the interface of two fundamental fields:
submodular optimization and online learning. In the online unconstrained
submodular maximization (online USM) problem, there is a universe
$[n]=\{1,2,...,n\}$ and a sequence of $T$ nonnegative (not necessarily
monotone) submodular functions arrive over time. The goal is to design a
computationally efficient online algorithm, which chooses a subset of $[n]$ at
each time step as a function only of the past, such that the accumulated value
of the chosen subsets is as close as possible to the maximum total value of a
fixed subset in hindsight. Our main result is a polynomial-time no-$1/2$-regret
algorithm for this problem, meaning that for every sequence of nonnegative
submodular functions, the algorithm's expected total value is at least $1/2$
times that of the best subset in hindsight, up to an error term sublinear in
$T$. The factor of $1/2$ cannot be improved upon by any polynomial-time online
algorithm when the submodular functions are presented as value oracles.
Previous work on the offline problem implies that picking a subset uniformly at
random in each time step achieves zero $1/4$-regret.
A byproduct of our techniques is an explicit subroutine for the two-experts
problem that has an unusually strong regret guarantee: the total value of its
choices is comparable to twice the total value of either expert on rounds it
did not pick that expert. This subroutine may be of independent interest.
",Statistics,Computer Science; Statistics
"$b$-symbol distance distribution of repeated-root cyclic codes   Symbol-pair codes, introduced by Cassuto and Blaum [1], have been raised for
symbol-pair read channels. This new idea is motivated by the limitation of the
reading process in high-density data storage technologies. Yaakobi et al. [8]
introduced codes for $b$-symbol read channels, where the read operation is
performed as a consecutive sequence of $b>2$ symbols. In this paper, we come up
with a method to compute the $b$-symbol-pair distance of two $n$-tuples, where
$n$ is a positive integer. Also, we deal with the $b$-symbol-pair distances of
some kind of cyclic codes of length $p^e$ over $\mathbb{F}_{p^m}$.
",Computer Science,Computer Science
"Homogeneous Kobayashi-hyperbolic manifolds with automorphism group of subcritical dimension   We determine all connected homogeneous Kobayashi-hyperbolic manifolds of
dimension $n\ge 2$ whose holomorphic automorphism group has dimension $n^2-3$.
This result complements existing classifications for automorphism group
dimension $n^2-2$ (which is in some sense critical) and greater.
",Mathematics,Mathematics
"Transforming Coroutining Logic Programs into Equivalent CHR Programs   We extend a technique called Compiling Control. The technique transforms
coroutining logic programs into logic programs that, when executed under the
standard left-to-right selection rule (and not using any delay features) have
the same computational behavior as the coroutining program. In recent work, we
revised Compiling Control and reformulated it as an instance of Abstract
Conjunctive Partial Deduction. This work was mostly focused on the program
analysis performed in Compiling Control. In the current paper, we focus on the
synthesis of the transformed program. Instead of synthesizing a new logic
program, we synthesize a CHR(Prolog) program which mimics the coroutining
program. The synthesis to CHR yields programs containing only simplification
rules, which are particularly amenable to certain static analysis techniques.
The programs are also more concise and readable and can be ported to CHR
implementations embedded in other languages than Prolog.
",Computer Science,Computer Science
"MAT: A Multimodal Attentive Translator for Image Captioning   In this work we formulate the problem of image captioning as a multimodal
translation task. Analogous to machine translation, we present a
sequence-to-sequence recurrent neural networks (RNN) model for image caption
generation. Different from most existing work where the whole image is
represented by convolutional neural network (CNN) feature, we propose to
represent the input image as a sequence of detected objects which feeds as the
source sequence of the RNN model. In this way, the sequential representation of
an image can be naturally translated to a sequence of words, as the target
sequence of the RNN model. To represent the image in a sequential way, we
extract the objects features in the image and arrange them in a order using
convolutional neural networks. To further leverage the visual information from
the encoded objects, a sequential attention layer is introduced to selectively
attend to the objects that are related to generate corresponding words in the
sentences. Extensive experiments are conducted to validate the proposed
approach on popular benchmark dataset, i.e., MS COCO, and the proposed model
surpasses the state-of-the-art methods in all metrics following the dataset
splits of previous work. The proposed approach is also evaluated by the
evaluation server of MS COCO captioning challenge, and achieves very
competitive results, e.g., a CIDEr of 1.029 (c5) and 1.064 (c40).
",Computer Science,Computer Science
"Discriminant analysis in small and large dimensions   We study the distributional properties of the linear discriminant function
under the assumption of normality by comparing two groups with the same
covariance matrix but different mean vectors. A stochastic representation for
the discriminant function coefficients is derived which is then used to obtain
their asymptotic distribution under the high-dimensional asymptotic regime. We
investigate the performance of the classification analysis based on the
discriminant function in both small and large dimensions. A stochastic
representation is established which allows to compute the error rate in an
efficient way. We further compare the calculated error rate with the optimal
one obtained under the assumption that the covariance matrix and the two mean
vectors are known. Finally, we present an analytical expression of the error
rate calculated in the high-dimensional asymptotic regime. The finite-sample
properties of the derived theoretical results are assessed via an extensive
Monte Carlo study.
",Mathematics; Statistics,Mathematics; Statistics
"Weak type operator Lipschitz and commutator estimates for commuting tuples   Let $f: \mathbb{R}^d \to\mathbb{R}$ be a Lipschitz function. If $B$ is a
bounded self-adjoint operator and if $\{A_k\}_{k=1}^d$ are commuting bounded
self-adjoint operators such that $[A_k,B]\in L_1(H),$ then
$$\|[f(A_1,\cdots,A_d),B]\|_{1,\infty}\leq
c(d)\|\nabla(f)\|_{\infty}\max_{1\leq k\leq d}\|[A_k,B]\|_1,$$ where $c(d)$ is
a constant independent of $f$, $\mathcal{M}$ and $A,B$ and
$\|\cdot\|_{1,\infty}$ denotes the weak $L_1$-norm. If $\{X_k\}_{k=1}^d$
(respectively, $\{Y_k\}_{k=1}^d$) are commuting bounded self-adjoint operators
such that $X_k-Y_k\in L_1(H),$ then
$$\|f(X_1,\cdots,X_d)-f(Y_1,\cdots,Y_d)\|_{1,\infty}\leq
c(d)\|\nabla(f)\|_{\infty}\max_{1\leq k\leq d}\|X_k-Y_k\|_1.$$
",Mathematics,Mathematics
"Mechanisms for bacterial gliding motility on soft substrates   The motility mechanism of certain rod-shaped bacteria has long been a
mystery, since no external appendages are involved in their motion which is
known as gliding. However, the physical principles behind gliding motility
still remain poorly understood. Using myxobacteria as a canonical example of
such organisms, we identify here the physical principles behind gliding
motility, and develop a theoretical model that predicts a two-regime behavior
of the gliding speed as a function of the substrate stiffness. Our theory
describes the elastic, viscous, and capillary interactions between the
bacterial membrane carrying a traveling wave, the secreted slime acting as a
lubricating film, and the substrate which we model as a soft solid. Defining
the myxobacterial gliding as the horizontal motion on the substrate under zero
net force, we find the two-regime behavior is due to two different mechanisms
of motility thrust. On stiff substrates, the thrust arises from the bacterial
shape deformations creating a flow of slime that exerts a pressure along the
bacterial length. This pressure in conjunction with the bacterial shape
provides the necessary thrust for propulsion. However, we show that such a
mechanism cannot lead to gliding on very soft substrates. Instead, we show that
capillary effects lead to the formation of a ridge at the slime-substrate-air
interface, which creates a thrust in the form of a localized pressure gradient
at the tip of the bacteria. To test our theory, we perform experiments with
isolated cells on agar substrates of varying stiffness and find the measured
gliding speeds to be in good agreement with the predictions from our
elasto-capillary-hydrodynamic model. The physical mechanisms reported here
serve as an important step towards an accurate theory of friction and
substrate-mediated interaction between bacteria in a swarm of cells
proliferating in soft media.
",Quantitative Biology,Physics
"Nonlinear Modal Decoupling Based Power System Transient Stability Analysis   Nonlinear modal decoupling (NMD) was recently proposed to nonlinearly
transform a multi-oscillator system into a number of decoupled oscillators
which together behave the same as the original system in an extended
neighborhood of the equilibrium. Each oscillator has just one degree of freedom
and hence can easily be analyzed to infer the stability of the original system
associated with one electromechanical mode. As the first attempt of applying
the NMD methodology to realistic power system models, this paper proposes an
NMD-based transient stability analysis approach. For a multi-machine power
system, the approach first derives decoupled nonlinear oscillators by a
coordinates transformation, and then applies Lyapunov stability analysis to
oscillators to assess the stability of the original system. Nonlinear modal
interaction is also considered. The approach can be efficiently applied to a
large-scale power grid by conducting NMD regarding only selected modes. Case
studies on a 3-machine 9-bus system and an NPCC 48-machine 140-bus system show
the potentials of the approach in transient stability analysis for
multi-machine systems.
",Computer Science,Computer Science
"Self-Calibration of Mobile Manipulator Kinematic and Sensor Extrinsic Parameters Through Contact-Based Interaction   We present a novel approach for mobile manipulator self-calibration using
contact information. Our method, based on point cloud registration, is applied
to estimate the extrinsic transform between a fixed vision sensor mounted on a
mobile base and an end effector. Beyond sensor calibration, we demonstrate that
the method can be extended to include manipulator kinematic model parameters,
which involves a non-rigid registration process. Our procedure uses on-board
sensing exclusively and does not rely on any external measurement devices,
fiducial markers, or calibration rigs. Further, it is fully automatic in the
general case. We experimentally validate the proposed method on a custom mobile
manipulator platform, and demonstrate centimetre-level post-calibration
accuracy in positioning of the end effector using visual guidance only. We also
discuss the stability properties of the registration algorithm, in order to
determine the conditions under which calibration is possible.
",Computer Science,Computer Science
"Alpha-Divergences in Variational Dropout   We investigate the use of alternative divergences to Kullback-Leibler (KL) in
variational inference(VI), based on the Variational Dropout \cite{kingma2015}.
Stochastic gradient variational Bayes (SGVB) \cite{aevb} is a general framework
for estimating the evidence lower bound (ELBO) in Variational Bayes. In this
work, we extend the SGVB estimator with using Alpha-Divergences, which are
alternative to divergences to VI' KL objective. The Gaussian dropout can be
seen as a local reparametrization trick of the SGVB objective. We extend the
Variational Dropout to use alpha divergences for variational inference. Our
results compare $\alpha$-divergence variational dropout with standard
variational dropout with correlated and uncorrelated weight noise. We show that
the $\alpha$-divergence with $\alpha \rightarrow 1$ (or KL divergence) is still
a good measure for use in variational inference, in spite of the efficient use
of Alpha-divergences for Dropout VI \cite{Li17}. $\alpha \rightarrow 1$ can
yield the lowest training error, and optimizes a good lower bound for the
evidence lower bound (ELBO) among all values of the parameter $\alpha \in
[0,\infty)$.
",Computer Science; Statistics,Mathematics; Statistics
"Identifying Clickbait Posts on Social Media with an Ensemble of Linear Models   The purpose of a clickbait is to make a link so appealing that people click
on it. However, the content of such articles is often not related to the title,
shows poor quality, and at the end leaves the reader unsatisfied.
To help the readers, the organizers of the clickbait challenge
(this http URL) asked the participants to build a machine
learning model for scoring articles with respect to their ""clickbaitness"".
In this paper we propose to solve the clickbait problem with an ensemble of
Linear SVM models, and our approach was tested successfully in the challenge:
it showed great performance of 0.036 MSE and ranked 3rd among all the solutions
to the contest.
",Computer Science,Computer Science
"Entanglement in topological systems   These lecture notes on entanglement in topological systems are part of the
48th IFF Spring School 2017 on Topological Matter: Topological Insulators,
Skyrmions and Majoranas at the Forschungszentrum Juelich, Germany. They cover a
short discussion on topologically ordered phases and review the two main tools
available for detecting topological order - the entanglement entropy and the
entanglement spectrum.
",Physics,Physics
"Operator algebraic approach to inverse and stability theorems for amenable groups   We prove an inverse theorem for the Gowers $U^2$-norm for maps $G\to\mathcal
M$ from an countable, discrete, amenable group $G$ into a von Neumann algebra
$\mathcal M$ equipped with an ultraweakly lower semi-continuous, unitarily
invariant (semi-)norm $\Vert\cdot\Vert$. We use this result to prove a
stability result for unitary-valued $\varepsilon$-representations $G\to\mathcal
U(\mathcal M)$ with respect to $\Vert\cdot \Vert$.
",Mathematics,Mathematics
"Critical Vertices and Edges in $H$-free Graphs   A vertex or edge in a graph is critical if its deletion reduces the chromatic
number of the graph by 1. We consider the problems of deciding whether a graph
has a critical vertex or edge, respectively. We give a complexity dichotomy for
both problems restricted to $H$-free graphs, that is, graphs with no induced
subgraph isomorphic to $H$. Moreover, we show that an edge is critical if and
only if its contraction reduces the chromatic number by 1. Hence, we also
obtain a complexity dichotomy for the problem of deciding if a graph has an
edge whose contraction reduces the chromatic number by 1.
",Computer Science,Mathematics
"Depression and Self-Harm Risk Assessment in Online Forums   Users suffering from mental health conditions often turn to online resources
for support, including specialized online support communities or general
communities such as Twitter and Reddit. In this work, we present a neural
framework for supporting and studying users in both types of communities. We
propose methods for identifying posts in support communities that may indicate
a risk of self-harm, and demonstrate that our approach outperforms strong
previously proposed methods for identifying such posts. Self-harm is closely
related to depression, which makes identifying depressed users on general
forums a crucial related task. We introduce a large-scale general forum dataset
(""RSDD"") consisting of users with self-reported depression diagnoses matched
with control users. We show how our method can be applied to effectively
identify depressed users from their use of language alone. We demonstrate that
our method outperforms strong baselines on this general forum dataset.
",Computer Science,Computer Science
"Interleaved Group Convolutions for Deep Neural Networks   In this paper, we present a simple and modularized neural network
architecture, named interleaved group convolutional neural networks (IGCNets).
The main point lies in a novel building block, a pair of two successive
interleaved group convolutions: primary group convolution and secondary group
convolution. The two group convolutions are complementary: (i) the convolution
on each partition in primary group convolution is a spatial convolution, while
on each partition in secondary group convolution, the convolution is a
point-wise convolution; (ii) the channels in the same secondary partition come
from different primary partitions. We discuss one representative advantage:
Wider than a regular convolution with the number of parameters and the
computation complexity preserved. We also show that regular convolutions, group
convolution with summation fusion, and the Xception block are special cases of
interleaved group convolutions. Empirical results over standard benchmarks,
CIFAR-$10$, CIFAR-$100$, SVHN and ImageNet demonstrate that our networks are
more efficient in using parameters and computation complexity with similar or
higher accuracy.
",Computer Science,Computer Science
"Group-Server Queues   By analyzing energy-efficient management of data centers, this paper proposes
and develops a class of interesting {\it Group-Server Queues}, and establishes
two representative group-server queues through loss networks and impatient
customers, respectively. Furthermore, such two group-server queues are given
model descriptions and necessary interpretation. Also, simple mathematical
discussion is provided, and simulations are made to study the expected queue
lengths, the expected sojourn times and the expected virtual service times. In
addition, this paper also shows that this class of group-server queues are
often encountered in many other practical areas including communication
networks, manufacturing systems, transportation networks, financial networks
and healthcare systems. Note that the group-server queues are always used to
design effectively dynamic control mechanisms through regrouping and
recombining such many servers in a large-scale service system by means of, for
example, bilateral threshold control, and customers transfer to the buffer or
server groups. This leads to the large-scale service system that is divided
into several adaptive and self-organizing subsystems through scheduling of
batch customers and regrouping of service resources, which make the middle
layer of this service system more effectively managed and strengthened under a
dynamic, real-time and even reward optimal framework. Based on this,
performance of such a large-scale service system may be improved greatly in
terms of introducing and analyzing such group-server queues. Therefore, not
only analysis of group-server queues is regarded as a new interesting research
direction, but there also exists many theoretical challenges, basic
difficulties and open problems in the area of queueing networks.
",Computer Science,Computer Science
"Entire holomorphic curves into projective spaces intersecting a generic hypersurface of high degree   In this note, we establish the following Second Main Theorem type estimate
for every entire non-algebraically degenerate holomorphic curve
$f\colon\mathbb{C}\rightarrow\mathbb{P}^n(\mathbb{C})$, in present of a {\sl
generic} hypersuface $D\subset\mathbb{P}^n(\mathbb{C})$ of sufficiently high
degree $d\geq 15(5n+1)n^n$: \[ T_f(r) \leq \,N_f^{[1]}(r,D) + O\big(\log T_f(r)
+ \log r \big)\parallel, \] where $T_f(r)$ and $N_f^{[1]}(r,D)$ stand for the
order function and the $1$-truncated counting function in Nevanlinna theory.
This inequality quantifies recent results on the logarithmic Green--Griffiths
conjecture.
",Mathematics,Mathematics
"Distributed Edge Caching Scheme Considering the Tradeoff Between the Diversity and Redundancy of Cached Content   Caching popular contents at the edge of cellular networks has been proposed
to reduce the load, and hence the cost of backhaul links. It is significant to
decide which files should be cached and where to cache them. In this paper, we
propose a distributed caching scheme considering the tradeoff between the
diversity and redundancy of base stations' cached contents. Whether it is
better to cache the same or different contents in different base stations? To
find out this, we formulate an optimal redundancy caching problem. Our goal is
to minimize the total transmission cost of the network, including cost within
the radio access network (RAN) and cost incurred by transmission to the core
network via backhaul links. The optimal redundancy ratio under given system
configuration is obtained with adapted particle swarm optimization (PSO)
algorithm. We analyze the impact of important system parameters through
Monte-Carlo simulation. Results show that the optimal redundancy ratio is
mainly influenced by two parameters, which are the backhaul to RAN unit cost
ratio and the steepness of file popularity distribution. The total cost can be
reduced by up to 54% at given unit cost ratio of backhaul to RAN when the
optimal redundancy ratio is selected. Under typical file request pattern, the
reduction amount can be up to 57%.
",Computer Science,Computer Science
"Symplectic resolutions for Higgs moduli spaces   In this paper, we study the algebraic symplectic geometry of the singular
moduli spaces of Higgs bundles of degree $0$ and rank $n$ on a compact Riemann
surface $X$ of genus $g$. In particular, we prove that such moduli spaces are
symplectic singularities, in the sense of Beauville [Bea00], and admit a
projective symplectic resolution if and only if $g=1$ or $(g, n)=(2,2)$. These
results are an application of a recent paper by Bellamy and Schedler [BS16] via
the so-called Isosingularity Theorem.
",Mathematics,Mathematics
"Sobolev GAN   We propose a new Integral Probability Metric (IPM) between distributions: the
Sobolev IPM. The Sobolev IPM compares the mean discrepancy of two distributions
for functions (critic) restricted to a Sobolev ball defined with respect to a
dominant measure $\mu$. We show that the Sobolev IPM compares two distributions
in high dimensions based on weighted conditional Cumulative Distribution
Functions (CDF) of each coordinate on a leave one out basis. The Dominant
measure $\mu$ plays a crucial role as it defines the support on which
conditional CDFs are compared. Sobolev IPM can be seen as an extension of the
one dimensional Von-Mises Cramér statistics to high dimensional
distributions. We show how Sobolev IPM can be used to train Generative
Adversarial Networks (GANs). We then exploit the intrinsic conditioning implied
by Sobolev IPM in text generation. Finally we show that a variant of Sobolev
GAN achieves competitive results in semi-supervised learning on CIFAR-10,
thanks to the smoothness enforced on the critic by Sobolev GAN which relates to
Laplacian regularization.
",Computer Science; Statistics,Computer Science; Statistics
"Collaborative similarity analysis of multilayer developer-project bipartite network   To understand the multiple relations between developers and projects on
GitHub as a whole, we model them as a multilayer bipartite network and analyze
the degree distributions, the nearest neighbors' degree distributions and their
correlations with degree, and the collaborative similarity distributions and
their correlations with degree. Our results show that all degree distributions
have a power-law form, especially, the degree distribution of projects in
watching layer has double power-law form. Negative correlations between nearest
neighbors' degree and degree for both developers and projects are observed in
both layers, exhibiting a disassortative mixing pattern. The collaborative
similarity of both developers and projects negatively correlates with degree in
watching layer, while a positive correlations is observed for developers in
forking layer and no obvious correlation is observed for projects in forking
layer.
",Computer Science; Physics,Computer Science
"Ultraslow fluctuations in the pseudogap states of HgBa$_{2}$CaCu$_{2}$O$_{6+d}$   We report the transverse relaxation rates 1/$T_2$'s of the $^{63}$Cu nuclear
spin-echo envelope for double-layer high-$T_c$ cuprate superconductors
HgBa$_{2}$CaCu$_{2}$O$_{6+d}$ from underdoped to overdoped. The relaxation rate
1/$T_{2L}$ of the exponential function (Lorentzian component) shows a peak at
220$-$240 K in the underdoped ($T_c$ = 103 K) and the optimally doped ($T_c$ =
127 K) samples but no peak in the overdoped ($T_c$ = 93 K) sample. The
enhancement in 1/$T_{2L}$ suggests development of the zero frequency components
of local field fluctuations. Ultraslow fluctuations are hidden in the pseudogap
states.
",Physics,Physics
"Distributed Deep Transfer Learning by Basic Probability Assignment   Transfer learning is a popular practice in deep neural networks, but
fine-tuning of large number of parameters is a hard task due to the complex
wiring of neurons between splitting layers and imbalance distributions of data
in pretrained and transferred domains. The reconstruction of the original
wiring for the target domain is a heavy burden due to the size of
interconnections across neurons. We propose a distributed scheme that tunes the
convolutional filters individually while backpropagates them jointly by means
of basic probability assignment. Some of the most recent advances in evidence
theory show that in a vast variety of the imbalanced regimes, optimizing of
some proper objective functions derived from contingency matrices prevents
biases towards high-prior class distributions. Therefore, the original filters
get gradually transferred based on individual contributions to overall
performance of the target domain. This largely reduces the expected complexity
of transfer learning whilst highly improves precision. Our experiments on
standard benchmarks and scenarios confirm the consistent improvement of our
distributed deep transfer learning strategy.
",Computer Science; Statistics,Computer Science; Statistics
"Twin-beam real-time position estimation of micro-objects in 3D   Various optical methods for measuring positions of micro-objects in 3D have
been reported in the literature. Nevertheless, majority of them are not
suitable for real-time operation, which is needed, for example, for feedback
position control. In this paper, we present a method for real-time estimation
of the position of micro-objects in 3D; the method is based on twin-beam
illumination and it requires only a very simple hardware setup whose essential
part is a standard image sensor without any lens. Performance of the proposed
method is tested during a micro-manipulation task in which the estimated
position served as a feedback for the controller. The experiments show that the
estimate is accurate to within ~3 um in the lateral position and ~7 um in the
axial distance with the refresh rate of 10 Hz. Although the experiments are
done using spherical objects, the presented method could be modified to handle
non-spherical objects as well.
",Computer Science; Physics,Physics
"Learning with Confident Examples: Rank Pruning for Robust Classification with Noisy Labels   Noisy PN learning is the problem of binary classification when training
examples may be mislabeled (flipped) uniformly with noise rate rho1 for
positive examples and rho0 for negative examples. We propose Rank Pruning (RP)
to solve noisy PN learning and the open problem of estimating the noise rates,
i.e. the fraction of wrong positive and negative labels. Unlike prior
solutions, RP is time-efficient and general, requiring O(T) for any
unrestricted choice of probabilistic classifier with T fitting time. We prove
RP has consistent noise estimation and equivalent expected risk as learning
with uncorrupted labels in ideal conditions, and derive closed-form solutions
when conditions are non-ideal. RP achieves state-of-the-art noise estimation
and F1, error, and AUC-PR for both MNIST and CIFAR datasets, regardless of the
amount of noise and performs similarly impressively when a large portion of
training examples are noise drawn from a third distribution. To highlight, RP
with a CNN classifier can predict if an MNIST digit is a ""one""or ""not"" with
only 0.25% error, and 0.46 error across all digits, even when 50% of positive
examples are mislabeled and 50% of observed positive labels are mislabeled
negative examples.
",Computer Science; Statistics,Computer Science; Statistics
"A fast algorithm for maximal propensity score matching   We present a new algorithm which detects the maximal possible number of
matched disjoint pairs satisfying a given caliper when a bipartite matching is
done with respect to a scalar index (e.g., propensity score), and constructs a
corresponding matching. Variable width calipers are compatible with the
technique, provided that the width of the caliper is a Lipschitz function of
the index. If the observations are ordered with respect to the index then the
matching needs $O(N)$ operations, where $N$ is the total number of subjects to
be matched. The case of 1-to-$n$ matching is also considered.
We offer also a new fast algorithm for optimal complete one-to-one matching
on a scalar index when the treatment and control groups are of the same size.
This allows us to improve greedy nearest neighbor matching on a scalar index.
Keywords: propensity score matching, nearest neighbor matching, matching with
caliper, variable width caliper.
",Computer Science; Statistics,Computer Science; Mathematics
"Graded components of Local cohomology modules II   Let $A$ be a commutative Noetherian ring containing a field $K$ of
characteristic zero and let $R= A[X_1, \ldots, X_m]$. Consider $R$ as standard
graded with $°A=0$ and $°X_i=1$ for all $i$. We present a few results
about the behavior of the graded components of local cohomology modules
$H_I^i(R)$ where $I$ is an arbitrary homogeneous ideal in $R$. We mostly
restrict our attention to the Vanishing, Tameness and Rigidity problems.
",Mathematics,Mathematics
"Network analyses of 4D genome datasets automate detection of community-scale gene structure and plasticity   Chromosome conformation capture and Hi-C technologies provide gene-gene
proximity datasets of stationary cells, revealing chromosome territories,
topologically associating domains, and chromosome topology. Imaging of tagged
DNA sequences in live cells through the lac operator reporter system provides
dynamic datasets of chromosomal loci. Chromosome modeling explores the
mechanisms underlying 3D genome structure and dynamics. Here, we automate 4D
genome dataset analysis with network-based tools as an alternative to gene-gene
proximity statistics and visual structure determination. Temporal network
models and community detection algorithms are applied to 4D modeling of G1 in
budding yeast with transient crosslinking of $5 kb$ domains in the nucleolus,
analyzing datasets from four decades of transient binding timescales. Network
tools detect and track transient gene communities (clusters) within the
nucleolus, their size, number, persistence time, and frequency of gene
exchanges. An optimal, weak binding affinity is revealed that maximizes
community-scale plasticity whereby large communities persist, frequently
exchanging genes.
",Quantitative Biology,Quantitative Biology
"Kitting in the Wild through Online Domain Adaptation   Technological developments call for increasing perception and action
capabilities of robots. Among other skills, vision systems that can adapt to
any possible change in the working conditions are needed. Since these
conditions are unpredictable, we need benchmarks which allow to assess the
generalization and robustness capabilities of our visual recognition
algorithms. In this work we focus on robotic kitting in unconstrained
scenarios. As a first contribution, we present a new visual dataset for the
kitting task. Differently from standard object recognition datasets, we provide
images of the same objects acquired under various conditions where camera,
illumination and background are changed. This novel dataset allows for testing
the robustness of robot visual recognition algorithms to a series of different
domain shifts both in isolation and unified. Our second contribution is a novel
online adaptation algorithm for deep models, based on batch-normalization
layers, which allows to continuously adapt a model to the current working
conditions. Differently from standard domain adaptation algorithms, it does not
require any image from the target domain at training time. We benchmark the
performance of the algorithm on the proposed dataset, showing its capability to
fill the gap between the performances of a standard architecture and its
counterpart adapted offline to the given target domain.
",Computer Science,Computer Science
"Unifying the micro and macro properties of AGN feeding and feedback   We unify the feeding and feedback of supermassive black holes with the global
properties of galaxies, groups, and clusters, by linking for the first time the
physical mechanical efficiency at the horizon and Mpc scale. The macro hot halo
is tightly constrained by the absence of overheating and overcooling as probed
by X-ray data and hydrodynamic simulations ($\varepsilon_{\rm BH} \simeq$
10$^{-3}\, T_{\rm x,7.4}$). The micro flow is shaped by general relativistic
effects tracked by state-of-the-art GR-RMHD simulations ($\varepsilon_\bullet
\simeq$ 0.03). The SMBH properties are tied to the X-ray halo temperature
$T_{\rm x}$, or related cosmic scaling relation (as $L_{\rm x}$). The model is
minimally based on first principles, as conservation of energy and mass
recycling. The inflow occurs via chaotic cold accretion (CCA), the rain of cold
clouds condensing out of the quenched cooling flow and recurrently funneled via
inelastic collisions. Within 100s gravitational radii, the accretion energy is
transformed into ultrafast 10$^4$ km s$^{-1}$ outflows (UFOs) ejecting most of
the inflowing mass. At larger radii the energy-driven outflow entrains
progressively more mass: at roughly kpc scale, the velocities of the
hot/warm/cold outflows are a few 10$^3$, 1000, 500 km s$^{-1}$, with median
mass rates ~10, 100, several 100 M$_\odot$ yr$^{-1}$, respectively. The unified
CCA model is consistent with the observations of nuclear UFOs, and ionized,
neutral, and molecular macro outflows. We provide step-by-step implementation
for subgrid simulations, (semi)analytic works, or observational interpretations
which require self-regulated AGN feedback at coarse scales, avoiding the
a-posteriori fine-tuning of efficiencies.
",Physics,Physics
"Does the Testing Level affect the Prevalence of Coincidental Correctness?   Researchers have previously shown that Coincidental Correctness (CC) is
prevalent; however, the benchmarks they used are considered inadequate
nowadays. They have also recognized the negative impact of CC on the
effectiveness of fault localization and testing. The aim of this paper is to
study Coincidental Correctness, using more realistic code, mainly from the
perspective of unit testing. This stems from the fact that the practice of unit
testing has grown tremendously in recent years due to the wide adoption of
software development processes, such as Test-Driven Development. We quantified
the presence of CC in unit testing using the Defects4J benchmark. This entailed
manually injecting two code checkers for each of the 395 defects in Defects4J:
1) a weak checker that detects weak CC tests by monitoring whether the defect
was reached; and 2) a strong checker that detects strong CC tests by monitoring
whether the defect was reached and the program has transitioned into an
infectious state. We also conducted preliminary experiments (using Defects4J,
NanoXML and JTidy) to assess the pervasiveness of CC at the unit testing level
in comparison to that at the integration and system levels. Our study showed
that unit testing is not immune to CC, as it exhibited 7.2x more strong CC
tests than failing tests and 8.3x more weak CC tests than failing tests.
However, our preliminary results suggested that it might be less prone to CC
than integration testing and system testing.
",Computer Science,Computer Science
"O$^2$TD: (Near)-Optimal Off-Policy TD Learning   Temporal difference learning and Residual Gradient methods are the most
widely used temporal difference based learning algorithms; however, it has been
shown that none of their objective functions is optimal w.r.t approximating the
true value function $V$. Two novel algorithms are proposed to approximate the
true value function $V$. This paper makes the following contributions: (1) A
batch algorithm that can help find the approximate optimal off-policy
prediction of the true value function $V$. (2) A linear computational cost (per
step) near-optimal algorithm that can learn from a collection of off-policy
samples. (3) A new perspective of the emphatic temporal difference learning
which bridges the gap between off-policy optimality and off-policy stability.
",Computer Science; Statistics,Computer Science; Statistics
"A new magnetic phase in the nickelate perovskite TlNiO$_3$   The RNiO$_3$ perovskites are known to order antiferromagnetically below a
material-dependent Néel temperature $T_\text{N}$. We report experimental
evidence indicating the existence of a second magnetically-ordered phase in
TlNiO$_3$ above $T_\text{N} = 104$ K, obtained using nuclear magnetic resonance
and muon spin rotation spectroscopy. The new phase, which persists up to a
temperature $T_\text{N}^* = 202$ K, is suppressed by the application of an
external magnetic field of approximately 1 T. It is not yet known if such a
phase also exists in other perovskite nickelates.
",Physics,Physics
"A wide field-of-view crossed Dragone optical system using the anamorphic aspherical surfaces   A side-fed crossed Dragone telescope provides a wide field-of-view. This type
of a telescope is commonly employed in the measurement of cosmic microwave
background (CMB) polarization, which requires an image-space telecentric
telescope with a large focal plane over broadband coverage. We report the
design of the wide field-of-view crossed Dragone optical system using the
anamorphic aspherical surfaces with correction terms up to the 10th order. We
achieved the Strehl ratio larger than 0.95 over 32 by 18 square degrees at 150
GHz. This design is an image-space telecentric and fully diffraction-limited
system below 400 GHz. We discuss the optical performance in the uniformity of
the axially symmetric point spread function and telecentricity over the
field-of-view. We also address the analysis to evaluate the polarization
properties, including the instrumental polarization, extinction rate, and
polarization angle rotation. This work is a part of programs to design a
compact multi-color wide field-of-view telescope for LiteBIRD, which is a next
generation CMB polarization satellite.
",Physics,Physics
"DiVM: Model Checking with LLVM and Graph Memory   In this paper, we introduce the concept of a virtual machine with
graph-organised memory as a versatile backend for both explicit-state and
abstraction-driven verification of software. Our virtual machine uses the LLVM
IR as its instruction set, enriched with a small set of hypercalls. We show
that the provided hypercalls are sufficient to implement a small operating
system, which can then be linked with applications to provide a
POSIX-compatible verification environment. Finally, we demonstrate the
viability of the approach through a comparison with a more
traditionally-designed LLVM model checker.
",Computer Science,Computer Science
"Stellar population synthesis based modelling of the Milky Way using asteroseismology of dwarfs and subgiants from Kepler   Early attempts to apply asteroseismology to study the Galaxy have already
shown unexpected discrepancies for the mass distribution of stars between the
Galactic models and the data; a result that is still unexplained. Here, we
revisit the analysis of the asteroseismic sample of dwarf and subgiant stars
observed by Kepler and investigate in detail the possible causes for the
reported discrepancy. We investigate two models of the Milky Way based on
stellar population synthesis, Galaxia and TRILEGAL. In agreement with previous
results, we find that TRILEGAL predicts more massive stars compared to Galaxia,
and that TRILEGAL predicts too many blue stars compared to 2MASS observations.
Both models fail to match the distribution of the stellar sample in $(\log
g,T_{\rm eff})$ space, pointing to inaccuracies in the models and/or the
assumed selection function. When corrected for this mismatch in $(\log g,T_{\rm
eff})$ space, the mass distribution calculated by Galaxia is broader and the
mean is shifted toward lower masses compared to that of the observed stars.
This behaviour is similar to what has been reported for the Kepler red giant
sample. The shift between the mass distributions is equivalent to a change of
2\% in $\nu_{\rm max}$, which is within the current uncertainty in the
$\nu_{\rm max}$ scaling relation. Applying corrections to the $\Delta \nu$
scaling relation predicted by the stellar models makes the observed mass
distribution significantly narrower, but there is no change to the mean.
",Physics,Physics
"The fundamental factor of optical interference   It has been widely accepted that electric field alone is the fundamental
factor for optical interference, since Wiener's experiments in 1890 proved that
the electric field plays such a dominant role. A group of experiments were
demonstrated against Wiener's experiments under the condition that the
interference fringes made by optical standing waves could have been
distinguished from the fringes of equal thickness between the inner surface of
emulsion and the plane mirror used to build the optical standing waves. It was
found that the Bragg diffraction from the interference fringes formed by the
standing waves did not exist. This means optical standing waves did not blacken
the photographic emulsion, or the electric field did not play such a dominant
role. Therefore, instead of the electric-field energy density solely in
proportion to the electric-field square, Energy Flux in Interference was
proposed to represent the intensity of optical interference-field and approved
in the derivation of equations for the interference. The derived equations
indicate that both the electric-field vector and the magnetic-field vector are
in phase and have equal amount of energy densities at the interference maxima
of two light beams. Thus, the magnetic-field vector acts the same role as the
electric-field vector on light interacting with substance. The fundamental
factor of optical interference is electromagnetic energy flux densities rather
than electric-field alone, or the intensity of optical interference fringes
should be the energy flux density, not electric-field energy density.
",Physics,Physics
"Matrix divisors on Riemann surfaces and Lax operator algebras   Matrix divisors are introduced in the work by A.Weil (1938) which is
considered as a starting point of the theory of holomorphic vector bundles on
Riemann surfaces. In this theory matrix divisors play the role similar to the
role of usual divisors in the theory of line bundles. Moreover, they provide
explicit coordinates (Tyurin parameters) in an open subset of the moduli space
of stable vector bundles. These coordinates turned out to be helpful in
integration of soliton equations.
We would like to gain attention to one more relationship between matrix
divisors of vector G-bundles (where G is a complex semi-simple Lie group) and
the theory of integrable systems, namely to the relationship with Lax operator
algebras. The result we obtain can be briefly formulated as follows: the moduli
space of matrix divisors with certain discrete invariants and fixed support is
a homogeneous space. Its tangent space at the unit is naturally isomorphic to
the quotient space of M-operators by L-operators, both spaces essentially
defined by the same invariants (the result goes back to Krichever, 2001). We
give one more description of the same space in terms of root systems.
",Mathematics,Mathematics
"Spitzer Secondary Eclipses of Qatar-1b   Previous secondary eclipse observations of the hot Jupiter Qatar-1b in the Ks
band suggest that it may have an unusually high day side temperature,
indicative of minimal heat redistribution. There have also been indications
that the orbit may be slightly eccentric, possibly forced by another planet in
the system. We investigate the day side temperature and orbital eccentricity
using secondary eclipse observations with Spitzer. We observed the secondary
eclipse with Spitzer/IRAC in subarray mode, in both 3.6 and 4.5 micron
wavelengths. We used pixel-level decorrelation to correct for Spitzer's
intra-pixel sensitivity variations and thereby obtain accurate eclipse depths
and central phases. Our 3.6 micron eclipse depth is 0.149 +/- 0.051% and the
4.5 micron depth is 0.273 +/- 0.049%. Fitting a blackbody planet to our data
and two recent Ks band eclipse depths indicates a brightness temperature of
1506 +/- 71K. Comparison to model atmospheres for the planet indicates that its
degree of longitudinal heat redistribution is intermediate between fully
uniform and day side only. The day side temperature of the planet is unlikely
to be as high (1885K) as indicated by the ground-based eclipses in the Ks band,
unless the planet's emergent spectrum deviates strongly from model atmosphere
predictions. The average central phase for our Spitzer eclipses is 0.4984 +/-
0.0017, yielding e cos(omega) = -0.0028 +/- 0.0027. Our results are consistent
with a circular orbit, and we constrain e cos(omega) much more strongly than
has been possible with previous observations.
",Physics,Physics
"Characterization and control of linear coupling using turn-by-turn beam position monitor data in storage rings   We introduce a new application of measuring symplectic generators to
characterize and control the linear betatron coupling in storage rings. From
synchronized and consecutive BPM (Beam Position Monitor) turn-by-turn (TbT)
readings, symplectic Lie generators describing the coupled linear dynamics are
extracted. Four plane-crossing terms in the generators directly characterize
the coupling between the horizontal and the vertical planes. Coupling control
can be accomplished by utilizing the dependency of these plane-crossing terms
on skew quadrupoles. The method has been successfully demonstrated to reduce
the vertical effective emittance down to the diffraction limit in the newly
constructed National Synchrotron Light Source II (NSLS-II) storage ring. This
method can be automatized to realize linear coupling feedback control with
negligible disturbance on machine operation.
",Physics,Physics
"A Data-Driven Framework for Assessing Cold Load Pick-up Demand in Service Restoration   Cold load pick-up (CLPU) has been a critical concern to utilities.
Researchers and industry practitioners have underlined the impact of CLPU on
distribution system design and service restoration. The recent large-scale
deployment of smart meters has provided the industry with a huge amount of data
that is highly granular, both temporally and spatially. In this paper, a
data-driven framework is proposed for assessing CLPU demand of residential
customers using smart meter data. The proposed framework consists of two
interconnected layers: 1) At the feeder level, a nonlinear auto-regression
model is applied to estimate the diversified demand during the system
restoration and calculate the CLPU demand ratio. 2) At the customer level,
Gaussian Mixture Models (GMM) and probabilistic reasoning are used to quantify
the CLPU demand increase. The proposed methodology has been verified using real
smart meter data and outage cases.
",Computer Science,Computer Science
"Thinking Fast and Slow with Deep Learning and Tree Search   Sequential decision making problems, such as structured prediction, robotic
control, and game playing, require a combination of planning policies and
generalisation of those plans. In this paper, we present Expert Iteration
(ExIt), a novel reinforcement learning algorithm which decomposes the problem
into separate planning and generalisation tasks. Planning new policies is
performed by tree search, while a deep neural network generalises those plans.
Subsequently, tree search is improved by using the neural network policy to
guide search, increasing the strength of new plans. In contrast, standard deep
Reinforcement Learning algorithms rely on a neural network not only to
generalise plans, but to discover them too. We show that ExIt outperforms
REINFORCE for training a neural network to play the board game Hex, and our
final tree search agent, trained tabula rasa, defeats MoHex 1.0, the most
recent Olympiad Champion player to be publicly released.
",Computer Science,Computer Science
"A General Algorithm to Calculate the Inverse Principal $p$-th Root of Symmetric Positive Definite Matrices   We address the general mathematical problem of computing the inverse $p$-th
root of a given matrix in an efficient way. A new method to construct iteration
functions that allow calculating arbitrary $p$-th roots and their inverses of
symmetric positive definite matrices is presented. We show that the order of
convergence is at least quadratic and that adaptively adjusting a parameter $q$
always leads to an even faster convergence. In this way, a better performance
than with previously known iteration schemes is achieved. The efficiency of the
iterative functions is demonstrated for various matrices with different
densities, condition numbers and spectral radii.
",Mathematics,Mathematics
"Stability of a Volterra Integral Equation on Time Scales   In this paper, we study Hyers-Ulam stability for integral equation of
Volterra type in time scale setting. Moreover we study the stability of the
considered equation in Hyers-Ulam-Rassias sense. Our technique depends on
successive approximation method, and we use time scale variant of induction
principle to show that equation (1.1) is stable on unbounded domains in
Hyers-Ulam-Rassias sense.
",Mathematics,Mathematics
"Performance Analysis of MEC Approach for Haplotype Assembly   The Minimum Error Correction (MEC) approach is used as a metric for
reconstruction of haplotypes from NGS reads. In this paper, we show that the
MEC may encounter with imprecise reconstructed haplotypes for some NGS devices.
Specifically, using mathematical derivations, we evaluate this approach for the
SOLiD, Illumina, 454, Ion, Pacific BioSciences, Oxford Nanopore, and 10X
Genomics devices. Our results reveal that the MEC yields inexact haplotypes for
the Illumina MiniSeq, 454 GS Junior+, Ion PGM 314, and Oxford Nanopore MK 1
MinION.
",Quantitative Biology,Physics
"Chaotic dynamics around cometary nuclei   We apply a generalized Kepler map theory to describe the qualitative chaotic
dynamics around cometary nuclei, based on accessible observational data for
five comets whose nuclei are well-documented to resemble dumb-bells. The sizes
of chaotic zones around the nuclei and the Lyapunov times of the motion inside
these zones are estimated. In the case of Comet 1P/Halley, the circumnuclear
chaotic zone seems to engulf an essential part of the Hill sphere, at least for
orbits of moderate to high eccentricity.
",Physics,Physics
"Diversity of Abundance Patterns of Light Neutron-capture Elements in Very-metal-poor Stars   We determine the abundances of neutron-capture elements from Sr to Eu for
five very-metal-poor stars (-3<[Fe/H]<-2) in the Milky Way halo to reveal the
origin of light neutron-capture elements. Previous spectroscopic studies have
shown evidence of at least two components in the r-process; one referred to as
the ""main r-process"" and the other as the ""weak r-process,"" which is mainly
responsible for producing heavy and light neutron-capture elements,
respectively. Observational studies of metal-poor stars suggest that there is a
universal pattern in the main r-process, similar to the abundance pattern of
the r-process component of solar-system material. Still, it is uncertain
whether the abundance pattern of the weak r-process shows universality or
diversity, due to the sparseness of measured light neutron-capture elements. We
have detected the key elements, Mo, Ru, and Pd, in five target stars to give an
answer to this question. The abundance patterns of light neutron-capture
elements from Sr to Pd suggest a diversity in the weak r-process. In
particular, scatter in the abundance ratio between Ru and Pd is significant
when the abundance patterns are normalized at Zr. Our results are compared with
the elemental abundances predicted by nucleosynthesis models of supernovae with
parameters such as electron fraction or proto-neutron-star mass, to investigate
sources of such diversity in the abundance patterns of light neutron-capture
elements. This paper presents that the variation in the abundances of observed
stars can be explained with a small range of parameters, which can serve as
constraints on future modeling of supernova models.
",Physics,Physics
"Multi-pass configuration for Improved Squeezed Vacuum Generation in Hot Rb Vapor   We study a squeezed vacuum field generated in hot Rb vapor via the
polarization self-rotation effect. Our previous experiments showed that the
amount of observed squeezing may be limited by the contamination of the
squeezed vacuum output with higher-order spatial modes, also generated inside
the cell. Here, we demonstrate that the squeezing can be improved by making the
light interact several times with a less dense atomic ensemble. With
optimization of some parameters we can achieve up to -2.6 dB of squeezing in
the multi-pass case, which is 0.6 dB improvement compared to the single-pass
experimental configuration. Our results show that other than the optical depth
of the medium, the spatial mode structure and cell configuration also affect
the squeezing level.
",Physics,Physics
"Pattern Search Multidimensional Scaling   We present a novel view of nonlinear manifold learning using derivative-free
optimization techniques. Specifically, we propose an extension of the classical
multi-dimensional scaling (MDS) method, where instead of performing gradient
descent, we sample and evaluate possible ""moves"" in a sphere of fixed radius
for each point in the embedded space. A fixed-point convergence guarantee can
be shown by formulating the proposed algorithm as an instance of General
Pattern Search (GPS) framework. Evaluation on both clean and noisy synthetic
datasets shows that pattern search MDS can accurately infer the intrinsic
geometry of manifolds embedded in high-dimensional spaces. Additionally,
experiments on real data, even under noisy conditions, demonstrate that the
proposed pattern search MDS yields state-of-the-art results.
",Statistics,Computer Science; Statistics
"On a method for constructing the Lax pairs for integrable models via quadratic ansatz   A method for constructing the Lax pairs for nonlinear integrable models is
suggested. First we look for a nonlinear invariant manifold to the
linearization of the given equation. Examples show that such invariant manifold
does exist and can effectively be found. Actually it is defined by a quadratic
form. As a result we get a nonlinear Lax pair consisting of the linearized
equation and the invariant manifold. Our second step consists of finding an
appropriate change of the variables to linearize the found nonlinear Lax pair.
The desired change of the variables is again defined by a quadratic form. The
method is illustrated by the well-known KdV equation and the modified Volterra
chain. New Lax pairs are found. The formal asymptotic expansions for their
eigenfunctions are constructed around the singular values of the spectral
parameter. By applying the method of the formal diagonalization to these Lax
pairs the infinite series of the local conservation laws are obtained for the
corresponding nonlinear models.
",Physics,Mathematics
"Mechanisms of near-surface structural evolution in nanocrystalline materials during sliding contact   The wear-driven structural evolution of nanocrystalline Cu was simulated with
molecular dynamics under constant normal loads, followed by a quantitative
analysis. While the microstructure far away from the sliding contact remains
unchanged, grain growth accompanied by partial dislocations and twin formation
was observed near the contact surface, with more rapid coarsening promoted by
higher applied normal loads. The structural evolution continues with increasing
number of sliding cycles and eventually saturates to a stable distinct layer of
coarsened grains, separated from the finer matrix by a steep gradient in grain
size. The coarsening process is balanced by the rate of material removal when
the normal load is high enough. The observed structural evolution leads to an
increase in hardness and decrease in friction coefficient, which also saturate
after a number of sliding cycles. This work provides important mechanistic
understanding of nanocrystalline wear, while also introducing a methodology for
atomistic simulations of cyclic wear damage under constant applied normal
loads.
",Physics,Physics
"Stochastic Gradient Descent: Going As Fast As Possible But Not Faster   When applied to training deep neural networks, stochastic gradient descent
(SGD) often incurs steady progression phases, interrupted by catastrophic
episodes in which loss and gradient norm explode. A possible mitigation of such
events is to slow down the learning process. This paper presents a novel
approach to control the SGD learning rate, that uses two statistical tests. The
first one, aimed at fast learning, compares the momentum of the normalized
gradient vectors to that of random unit vectors and accordingly gracefully
increases or decreases the learning rate. The second one is a change point
detection test, aimed at the detection of catastrophic learning episodes; upon
its triggering the learning rate is instantly halved. Both abilities of
speeding up and slowing down the learning rate allows the proposed approach,
called SALeRA, to learn as fast as possible but not faster. Experiments on
standard benchmarks show that SALeRA performs well in practice, and compares
favorably to the state of the art.
",Statistics,Statistics
"On the Azuma inequality in spaces of subgaussian of rank $p$ random variables   For $p > 1$ let a function $\varphi_p(x) = x^2/2$ if $|x|\le 1$ and
$\varphi_p(x) = 1/p|x|^p -1/p + 1/2$ if $|x| > 1$. For a random variable $\xi$
let $\tau_{\varphi_p}(\xi)$ denote $\inf\{c\ge 0 :\;
\forall_{\lambda\in\mathbb{R}}\;
\ln\mathbb{E}\exp(\lambda\xi)\le\varphi_p(c\lambda)\}$; $\tau_{\varphi_p}$ is a
norm in a space $Sub_{\varphi_p}(\Omega) =\{\xi:
\; \tau_{\varphi_p}(\xi) <\infty\}$ of $\varphi_p$-subgaussian random
variables which we call {\it subgaussian of rank $p$ random variables}. For $p
= 2$ we have the classic subgaussian random variables. The Azuma inequality
gives an estimate on the probability of the deviations of a zero-mean
martingale $(\xi_n)_{n\ge 0}$ with bounded increments from zero. In its classic
form is assumed that $\xi_0 = 0$. In this paper it is shown a version of the
Azuma inequality under assumption that $\xi_0$ is any subgaussian of rank $p$
random variable.
",Mathematics,Mathematics
"Can Neural Machine Translation be Improved with User Feedback?   We present the first real-world application of methods for improving neural
machine translation (NMT) with human reinforcement, based on explicit and
implicit user feedback collected on the eBay e-commerce platform. Previous work
has been confined to simulation experiments, whereas in this paper we work with
real logged feedback for offline bandit learning of NMT parameters. We conduct
a thorough analysis of the available explicit user judgments---five-star
ratings of translation quality---and show that they are not reliable enough to
yield significant improvements in bandit learning. In contrast, we successfully
utilize implicit task-based feedback collected in a cross-lingual search task
to improve task-specific and machine translation quality metrics.
",Statistics,Computer Science; Statistics
"Mutual Alignment Transfer Learning   Training robots for operation in the real world is a complex, time consuming
and potentially expensive task. Despite significant success of reinforcement
learning in games and simulations, research in real robot applications has not
been able to match similar progress. While sample complexity can be reduced by
training policies in simulation, such policies can perform sub-optimally on the
real platform given imperfect calibration of model dynamics. We present an
approach -- supplemental to fine tuning on the real robot -- to further benefit
from parallel access to a simulator during training and reduce sample
requirements on the real robot. The developed approach harnesses auxiliary
rewards to guide the exploration for the real world agent based on the
proficiency of the agent in simulation and vice versa. In this context, we
demonstrate empirically that the reciprocal alignment for both agents provides
further benefit as the agent in simulation can adjust to optimize its behaviour
for states commonly visited by the real-world agent.
",Computer Science,Computer Science
"Ultraproducts of crossed product von Neumann algebras   We study a relationship between the ultraproduct of a crossed product von
Neumann algebra and the crossed product of an ultraproduct von Neumann algebra.
As an application, the continuous core of an ultraproduct von Neumann algebra
is described.
",Mathematics,Mathematics
"Bianchi type-II universe with wet dark fluid in General Theory of Relativity   In this paper, dark energy models of the universe filled with wet dark fluid
are constructed in the framework of LRS Bianchi type-II space-time in General
Theory of Relativity. A new equation of state modeled on the equation of state
$p$=$\gamma(\rho - \rho_*)$, which can describe a liquid including water, is
used. The exact solutions of Einstein's field equations are obtained in
quadrature form and the models corresponding to the cases $\gamma = 0$ and
$\gamma = 1$ are discussed in detail.
",Physics,Physics
"The Wavefunction of the Collapsing Bose-Einstein Condensate   Bose-Einstein condensates with tunable interatomic interactions have been
studied intensely in recent experiments. The investigation of the collapse of a
condensate following a sudden change in the nature of the interaction from
repulsive to attractive has led to the observation of a remnant condensate that
did not undergo further collapse. We suggest that this high-density remnant is
in fact the absolute minimum of the energy, if the attractive atomic
interactions are nonlocal, and is therefore inherently stable. We show that a
variational trial function consisting of a superposition of two distinct
gaussians is an accurate representation of the wavefunction of the ground state
of the conventional local Gross-Pitaevskii field equation for an attractive
condensate and gives correctly the points of emergence of instability. We then
use such a superposition of two gaussians as a variational trial function in
order to calculate the minima of the energy when it includes a nonlocal
interaction term. We use experimental data in order to study the long range of
the nonlocal interaction, showing that they agree very well with a
dimensionally derived expression for this range.
",Physics,Physics
"No iterated identities satisfied by all finite groups   We show that there is no iterated identity satisfied by all finite groups.
For $w$ being a non-trivial word of length $l$, we show that there exists a
finite group $G$ of cardinality at most $\exp(l^C)$ which does not satisfy the
iterated identity $w$. The proof uses the approach of Borisov and Sapir, who
used dynamics of polynomial mappings for the proof of non residual finiteness
of some groups.
",Mathematics,Mathematics
"Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows   We present Sequential Neural Likelihood (SNL), a new method for Bayesian
inference in simulator models, where the likelihood is intractable but
simulating data from the model is possible. SNL trains an autoregressive flow
on simulated data in order to learn a model of the likelihood in the region of
high posterior density. A sequential training procedure guides simulations and
reduces simulation cost by orders of magnitude. We show that SNL is more
robust, more accurate and requires less tuning than related neural-based
methods, and we discuss diagnostics for assessing calibration, convergence and
goodness-of-fit.
",Statistics,Statistics
"Randomized Composable Coresets for Matching and Vertex Cover   A common approach for designing scalable algorithms for massive data sets is
to distribute the computation across, say $k$, machines and process the data
using limited communication between them. A particularly appealing framework
here is the simultaneous communication model whereby each machine constructs a
small representative summary of its own data and one obtains an
approximate/exact solution from the union of the representative summaries. If
the representative summaries needed for a problem are small, then this results
in a communication-efficient and round-optimal protocol. While many fundamental
graph problems admit efficient solutions in this model, two prominent problems
are notably absent from the list of successes, namely, the maximum matching
problem and the minimum vertex cover problem. Indeed, it was shown recently
that for both these problems, even achieving a polylog$(n)$ approximation
requires essentially sending the entire input graph from each machine.
The main insight of our work is that the intractability of matching and
vertex cover in the simultaneous communication model is inherently connected to
an adversarial partitioning of the underlying graph across machines. We show
that when the underlying graph is randomly partitioned across machines, both
these problems admit randomized composable coresets of size $\widetilde{O}(n)$
that yield an $\widetilde{O}(1)$-approximate solution. This results in an
$\widetilde{O}(1)$-approximation simultaneous protocol for these problems with
$\widetilde{O}(nk)$ total communication when the input is randomly partitioned
across $k$ machines. We further prove the optimality of our results. Finally,
by a standard application of composable coresets, our results also imply
MapReduce algorithms with the same approximation guarantee in one or two rounds
of communication
",Computer Science,Computer Science
"Structure of a Parabolic Partial Differential Equation on Graphs and Digital spaces. Solution of PDE on Digital Spaces: a Klein Bottle, a Projective Plane, a 4D Sphere and a Moebius Band   This paper studies the structure of a parabolic partial differential equation
on graphs and digital n-dimensional manifolds, which are digital models of
continuous n-manifolds. Conditions for the existence of solutions of equations
are determined and investigated. Numerical solutions of the equation on a Klein
bottle, a projective plane, a 4D sphere and a Moebius strip are presented.
",Computer Science; Mathematics,Mathematics
"Measuring the unmeasurable - a project of domestic violence risk prediction and management   The prevention of domestic violence (DV) have aroused serious concerns in
Taiwan because of the disparity between the increasing amount of reported DV
cases that doubled over the past decade and the scarcity of social workers.
Additionally, a large amount of data was collected when social workers use the
predominant case management approach to document case reports information.
However, these data were not properly stored or organized.
To improve the efficiency of DV prevention and risk management, we worked
with Taipei City Government and utilized the 2015 data from its DV database to
perform a spatial pattern analysis of the reports of DV cases to build a DV
risk map. However, during our map building process, the issue of confounding
bias arose because we were not able to verify if reported cases truly reflected
real violence occurrence or were simply false reports from potential victim's
neighbors. Therefore, we used the random forest method to build a repeat
victimization risk prediction model. The accuracy and F1-measure of our model
were 96.3% and 62.8%. This model helped social workers differentiate the risk
level of new cases, which further reduced their major workload significantly.
To our knowledge, this is the first project that utilized machine learning in
DV prevention. The research approach and results of this project not only can
improve DV prevention process, but also be applied to other social work or
criminal prevention areas.
",Computer Science,Computer Science; Statistics
"Shape and Energy Consistent Pseudopotentials for Correlated Electron systems   A method is developed for generating pseudopotentials for use in
correlated-electron calculations. The paradigms of shape and energy consistency
are combined and defined in terms of correlated-electron wave-functions. The
resulting energy consistent correlated electron pseudopotentials (eCEPPs) are
constructed for H, Li--F, Sc--Fe, and Cu. Their accuracy is quantified by
comparing the relaxed molecular geometries and dissociation energies they
provide with all electron results, with all quantities evaluated using coupled
cluster singles doubles and triples calculations. Errors inherent in the
pseudopotentials are also compared with those arising from a number of
approximations commonly used with pseudopotentials. The eCEPPs provide a
significant improvement in optimised geometries and dissociation energies for
small molecules, with errors for the latter being an order-of-magnitude smaller
than for Hartree-Fock-based pseudopotentials available in the literature.
Gaussian basis sets are optimised for use with these pseudopotentials.
",Physics,Physics
"Estimation of lactate threshold with machine learning techniques in recreational runners   Lactate threshold is considered an essential parameter when assessing
performance of elite and recreational runners and prescribing training
intensities in endurance sports. However, the measurement of blood lactate
concentration requires expensive equipment and the extraction of blood samples,
which are inconvenient for frequent monitoring. Furthermore, most recreational
runners do not have access to routine assessment of their physical fitness by
the aforementioned equipment so they are not able to calculate the lactate
threshold without resorting to an expensive and specialized centre. Therefore,
the main objective of this study is to create an intelligent system capable of
estimating the lactate threshold of recreational athletes participating in
endurance running sports. The solution here proposed is based on a machine
learning system which models the lactate evolution using recurrent neural
networks and includes the proposal of standardization of the temporal axis as
well as a modification of the stratified sampling method. The results show that
the proposed system accurately estimates the lactate threshold of 89.52% of the
athletes and its correlation with the experimentally measured lactate threshold
is very high (R=0,89). Moreover, its behaviour with the test dataset is as good
as with the training set, meaning that the generalization power of the model is
high. Therefore, in this study a machine learning based system is proposed as
alternative to the traditional invasive lactate threshold measurement tests for
recreational runners.
",Statistics,Statistics
"Throughput-Improving Control of Highways Facing Stochastic Perturbations   In this article, we study the problem of controlling a highway segment facing
stochastic perturbations, such as recurrent incidents and moving bottlenecks.
To model traffic flow under perturbations, we use the cell-transmission model
with Markovian capacities. The control inputs are: (i) the inflows that are
sent to various on-ramps to the highway (for managing traffic demand), and (ii)
the priority levels assigned to the on-ramp traffic relative to the mainline
traffic (for allocating highway capacity). The objective is to maximize the
throughput while ensuring that on-ramp queues remain bounded in the long-run.
We develop a computational approach to solving this stability-constrained,
throughput-maximization problem. Firstly, we use the classical drift condition
in stability analysis of Markov processes to derive a sufficient condition for
boundedness of on-ramp queues. Secondly, we show that our control design
problem can be formulated as a mixed integer program with linear or bilinear
constraints, depending on the complexity of Lyapunov function involved in the
stability condition. Finally, for specific types of capacity perturbations, we
derive intuitive criteria for managing demand and/or selecting priority levels.
These criteria suggest that inflows and priority levels should be determined
simultaneously such that traffic queues are placed at locations that discharge
queues fast. We illustrate the performance benefits of these criteria through a
computational study of a segment on Interstate 210 in California, USA.
",Computer Science,Computer Science
"Adaptive Interference Removal for Un-coordinated Radar/Communication Co-existence   Most existing approaches to co-existing communication/radar systems assume
that the radar and communication systems are coordinated, i.e., they share
information, such as relative position, transmitted waveforms and channel
state. In this paper, we consider an un-coordinated scenario where a
communication receiver is to operate in the presence of a number of radars, of
which only a sub-set may be active, which poses the problem of estimating the
active waveforms and the relevant parameters thereof, so as to cancel them
prior to demodulation. Two algorithms are proposed for such a joint waveform
estimation/data demodulation problem, both exploiting sparsity of a proper
representation of the interference and of the vector containing the errors of
the data block, so as to implement an iterative joint interference removal/data
demodulation process. The former algorithm is based on classical on-grid
compressed sensing (CS), while the latter forces an atomic norm (AN)
constraint: in both cases the radar parameters and the communication
demodulation errors can be estimated by solving a convex problem. We also
propose a way to improve the efficiency of the AN-based algorithm. The
performance of these algorithms are demonstrated through extensive simulations,
taking into account a variety of conditions concerning both the interferers and
the respective channel states.
",Computer Science; Statistics,Computer Science
"Tunable high-harmonic generation by chromatic focusing of few-cycle laser pulses   In this work we study the impact of chromatic focusing of few-cycle laser
pulses on high-order harmonic generation (HHG) through analysis of the emitted
extreme ultraviolet (XUV) radiation. Chromatic focusing is usually avoided in
the few-cycle regime, as the pulse spatio-temporal structure may be highly
distorted by the spatiotemporal aberrations. Here, however, we demonstrate it
as an additional control parameter to modify the generated XUV radiation. We
present experiments where few-cycle pulses are focused by a singlet lens in a
Kr gas jet. The chromatic distribution of focal lengths allows us to tune HHG
spectra by changing the relative singlet-target distance. Interestingly, we
also show that the degree of chromatic aberration needed to this control does
not degrade substantially the harmonic conversion efficiency, still allowing
for the generation of supercontinua with the chirped-pulse scheme, demonstrated
previously for achromatic focussing. We back up our experiments with
theoretical simulations reproducing the experimental HHG results depending on
diverse parameters (input pulse spectral phase, pulse duration, focus position)
and proving that, under the considered parameters, the attosecond pulse train
remains very similar to the achromatic case, even showing cases of isolated
attosecond pulse generation for near single-cycle driving pulses.
",Physics,Physics
"Measuring the effects of Loop Quantum Cosmology in the CMB data   In this Essay we investigate the observational signatures of Loop Quantum
Cosmology (LQC) in the CMB data. First, we concentrate on the dynamics of LQC
and we provide the basic cosmological functions. We then obtain the power
spectrum of scalar and tensor perturbations in order to study the performance
of LQC against the latest CMB data. We find that LQC provides a robust
prediction for the main slow-roll parameters, like the scalar spectral index
and the tensor-to-scalar fluctuation ratio, which are in excellent agreement
within $1\sigma$ with the values recently measured by the Planck collaboration.
This result indicates that LQC can be seen as an alternative scenario with
respect to that of standard inflation.
",Physics,Physics
"Model Averaging for Generalized Linear Model with Covariates that are Missing completely at Random   In this paper, we consider the estimation of generalized linear models with
covariates that are missing completely at random. We propose a model averaging
estimation method and prove that the corresponding model averaging estimator is
asymptotically optimal under certain assumptions. Simulaiton results illustrate
that this method has better performance than other alternatives under most
situations.
",Mathematics; Statistics,Mathematics; Statistics
"Adaptive Exact Learning of Decision Trees from Membership Queries   In this paper we study the adaptive learnability of decision trees of depth
at most $d$ from membership queries. This has many applications in automated
scientific discovery such as drugs development and software update problem.
Feldman solves the problem in a randomized polynomial time algorithm that asks
$\tilde O(2^{2d})\log n$ queries and Kushilevitz-Mansour in a deterministic
polynomial time algorithm that asks $ 2^{18d+o(d)}\log n$ queries. We improve
the query complexity of both algorithms. We give a randomized polynomial time
algorithm that asks $\tilde O(2^{2d}) + 2^{d}\log n$ queries and a
deterministic polynomial time algorithm that asks $2^{5.83d}+2^{2d+o(d)}\log n$
queries.
",Computer Science; Statistics,Computer Science
"Entity Linking for Queries by Searching Wikipedia Sentences   We present a simple yet effective approach for linking entities in queries.
The key idea is to search sentences similar to a query from Wikipedia articles
and directly use the human-annotated entities in the similar sentences as
candidate entities for the query. Then, we employ a rich set of features, such
as link-probability, context-matching, word embeddings, and relatedness among
candidate entities as well as their related entities, to rank the candidates
under a regression based framework. The advantages of our approach lie in two
aspects, which contribute to the ranking process and final linking result.
First, it can greatly reduce the number of candidate entities by filtering out
irrelevant entities with the words in the query. Second, we can obtain the
query sensitive prior probability in addition to the static link-probability
derived from all Wikipedia articles. We conduct experiments on two benchmark
datasets on entity linking for queries, namely the ERD14 dataset and the GERDAQ
dataset. Experimental results show that our method outperforms state-of-the-art
systems and yields 75.0% in F1 on the ERD14 dataset and 56.9% on the GERDAQ
dataset.
",Computer Science,Computer Science
"Covering Groups of Nonconnected Topological Groups and 2-Groups   We investigate the universal cover of a topological group that is not
necessarily connected. Its existence as a topological group is governed by a
Taylor cocycle, an obstruction in 3-cohomology. Alternatively, it always exists
as a topological 2-group. The splitness of this 2-group is also governed by an
obstruction in 3-cohomology, a Sinh cocycle. We give explicit formulas for both
obstructions and show that they are inverse of each other.
",Mathematics,Mathematics
"Dirac operators with $W^{1,\infty}$-potential under codimension one collapse   We study the behavior of the spectrum of the Dirac operator together with a
symmetric $W^{1, \infty}$-potential on spin manifolds under a collapse of
codimension one with bounded sectional curvature and diameter. If there is an
induced spin structure on the limit space $N$ then there are convergent
eigenvalues which converge to the spectrum of a first order differential
operator $D$ on $N$ together with a symmetric $W^{1,\infty}$-potential. If $N$
is orientable and the dimension of the limit space is even then $D$ is the
Dirac operator $D^N$ on $N$ and if the dimension of the limit space is odd,
then $D = D^N \oplus -D^N$.
",Mathematics,Mathematics
"Twitter and the Press: an Ego-Centred Analysis   Ego networks have proved to be a valuable tool for understanding the
relationships that individuals establish with their peers, both in offline and
online social networks. Particularly interesting are the cognitive constraints
associated with the interactions between the ego and the members of their ego
network, whereby individuals cannot maintain meaningful interactions with more
than 150 people, on average. In this work, we focus on the ego networks of
journalists on Twitter, and we investigate whether they feature the same
characteristics observed for other relevant classes of Twitter users, like
politicians and generic users. Our findings are that journalists are generally
more active and interact with more people than generic users. Their ego network
structure is very aligned with reference models derived from the social brain
hypothesis and observed in general human ego networks. Remarkably, the
similarity is even higher than the one of politicians and generic users ego
networks. This may imply a greater cognitive involvement with Twitter than with
other social interaction means. Moreover, the ego networks of journalists are
much stabler than those of politicians and generic users, and the ego-alter
ties are often information-driven.
",Computer Science,Computer Science
"The affine approach to homogeneous geodesics in homogeneous Finsler spaces   In a recent paper, it was claimed that any homogeneous Finsler space of odd
dimension admits a homogeneous geodesic through any point. For the proof, the
algebraic method dealing with the reductive decomposition of the Lie algebra of
the isometry group was used. However, the proof contains a serious gap. In the
present paper, homogeneous geodesics in Finsler homogeneous spaces are studied
using the affine method, which was developed in earlier papers by the author.
The mentioned statement is proved correctly and it is further proved that any
homogeneous Berwald space or homogeneous reversible Finsler space admits a
homogeneous geodesic through any point.
",Mathematics,Mathematics
"Indoor Frame Recovery from Refined Line Segments   An important yet challenging problem in understanding indoor scene is
recovering indoor frame structure from a monocular image. It is more difficult
when occlusions and illumination vary, and object boundaries are weak. To
overcome these difficulties, a new approach based on line segment refinement
with two constraints is proposed. First, the line segments are refined by four
consecutive operations, i.e., reclassifying, connecting, fitting, and voting.
Specifically, misclassified line segments are revised by the reclassifying
operation, some short line segments are joined by the connecting operation, the
undetected key line segments are recovered by the fitting operation with the
help of the vanishing points, the line segments converging on the frame are
selected by the voting operation. Second, we construct four frame models
according to four classes of possible shooting angles of the monocular image,
the natures of all frame models are introduced via enforcing the cross ratio
and depth constraints. The indoor frame is then constructed by fitting those
refined line segments with related frame model under the two constraints, which
jointly advance the accuracy of the frame. Experimental results on a collection
of over 300 indoor images indicate that our algorithm has the capability of
recovering the frame from complex indoor scenes.
",Computer Science,Computer Science
"Robust reputation-based ranking on multipartite rating networks   The spread of online reviews, ratings and opinions and its growing influence
on people's behavior and decisions boosted the interest to extract meaningful
information from this data deluge. Hence, crowdsourced ratings of products and
services gained a critical role in business, governments, and others. We
propose a new reputation-based ranking system utilizing multipartite rating
subnetworks, that clusters users by their similarities, using Kolmogorov
complexity. Our system is novel in that it reflects a diversity of
opinions/preferences by assigning possibly distinct rankings, for the same
item, for different groups of users. We prove the convergence and efficiency of
the system and show that it copes better with spamming/spurious users, and it
is more robust to attacks than state-of-the-art approaches.
",Computer Science,Computer Science
"Free constructions and coproducts of d-frames   A general theory of presentations for d-frames does not yet exist. We review
the difficulties and give sufficient conditions for when they can be overcome.
As an application we prove that the category of d-frames is closed under
coproducts.
",Computer Science; Mathematics,Mathematics
"Lions' formula for RKHSs of real harmonic functions on Lipschitz domains   Let $ \Omega$ be a bounded Lipschitz domain of $ \mathbb{R}^{d}.$ The purpose
of this paper is to establish Lions' formula for reproducing kernel Hilbert
spaces $\mathcal H^s(\Omega)$ of real harmonic functions elements of the usual
Sobolev space $H^s(\Omega)$ for $s\geq 0.$ To this end, we provide a functional
characterization of $\mathcal H^s(\Omega)$ via some new families of positive
self-adjoint operators, describe their trace data and discuss the values of $s$
for which they are RKHSs. Also a construction of an orthonormal basis of
$\mathcal H^s(\Omega)$ is established.
",Mathematics,Mathematics; Statistics
"Poisson distribution for gaps between sums of two squares and level spacings for toral point scatterers   We investigate the level spacing distribution for the quantum spectrum of the
square billiard. Extending work of Connors--Keating, and Smilansky, we
formulate an analog of the Hardy--Littlewood prime $k$-tuple conjecture for
sums of two squares, and show that it implies that the spectral gaps, after
removing degeneracies and rescaling, are Poisson distributed. Consequently, by
work of Rudnick and Ueberschär, the level spacings of arithmetic toral point
scatterers, in the weak coupling limit, are also Poisson distributed. We also
give numerical evidence for the conjecture and its implications.
",Physics; Mathematics,Mathematics
"Analytic Expressions for the Inner-Rim Structure of Passively Heated Protoplanetary Disks   We analytically derive the expressions for the structure of the inner region
of protoplanetary disks based on the results from the recent hydrodynamical
simulations. The inner part of a disk can be divided into four regions:
dust-free region with gas temperature in the optically thin limit, optically
thin dust halo, optically thick condensation front and the classical optically
thick region in order from the inside. We derive the dust-to-gas mass ratio
profile in the dust halo using the fact that partial dust condensation
regulates the temperature to the dust evaporation temperature. Beyond the dust
halo, there is an optically thick condensation front where all the available
silicate gas condenses out. The curvature of the condensation surface is
determined by the condition that the surface temperature must be nearly equal
to the characteristic temperature $\sim 1200{\,\rm K}$. We derive the mid-plane
temperature in the outer two regions using the two-layer approximation with the
additional heating by the condensation front for the outermost region. As a
result, the overall temperature profile is step-like with steep gradients at
the borders between the outer three regions. The borders might act as planet
traps where the inward migration of planets due to gravitational interaction
with the gas disk stops. The temperature at the border between the two
outermost regions coincides with the temperature needed to activate
magnetorotational instability, suggesting that the inner edge of the dead zone
must lie at this border. The radius of the dead-zone inner edge predicted from
our solution is $\sim$ 2-3 times larger than that expected from the classical
optically thick temperature.
",Physics,Physics
"Towards Gene Expression Convolutions using Gene Interaction Graphs   We study the challenges of applying deep learning to gene expression data. We
find experimentally that there exists non-linear signal in the data, however is
it not discovered automatically given the noise and low numbers of samples used
in most research. We discuss how gene interaction graphs (same pathway,
protein-protein, co-expression, or research paper text association) can be used
to impose a bias on a deep model similar to the spatial bias imposed by
convolutions on an image. We explore the usage of Graph Convolutional Neural
Networks coupled with dropout and gene embeddings to utilize the graph
information. We find this approach provides an advantage for particular tasks
in a low data regime but is very dependent on the quality of the graph used. We
conclude that more work should be done in this direction. We design experiments
that show why existing methods fail to capture signal that is present in the
data when features are added which clearly isolates the problem that needs to
be addressed.
",Statistics; Quantitative Biology,Statistics
"The ANTARES Collaboration: Contributions to ICRC 2017 Part II: The multi-messenger program   Papers on the ANTARES multi-messenger program, prepared for the 35th
International Cosmic Ray Conference (ICRC 2017, Busan, South Korea) by the
ANTARES Collaboration
",Physics,Physics
"Low Resolution Face Recognition Using a Two-Branch Deep Convolutional Neural Network Architecture   We propose a novel couple mappings method for low resolution face recognition
using deep convolutional neural networks (DCNNs). The proposed architecture
consists of two branches of DCNNs to map the high and low resolution face
images into a common space with nonlinear transformations. The branch
corresponding to transformation of high resolution images consists of 14 layers
and the other branch which maps the low resolution face images to the common
space includes a 5-layer super-resolution network connected to a 14-layer
network. The distance between the features of corresponding high and low
resolution images are backpropagated to train the networks. Our proposed method
is evaluated on FERET data set and compared with state-of-the-art competing
methods. Our extensive experimental results show that the proposed method
significantly improves the recognition performance especially for very low
resolution probe face images (11.4% improvement in recognition accuracy).
Furthermore, it can reconstruct a high resolution image from its corresponding
low resolution probe image which is comparable with state-of-the-art
super-resolution methods in terms of visual quality.
",Computer Science,Computer Science
"Observation of Intrinsic Half-metallic Behavior of CrO$_2$ (100) Epitaxial Films by Bulk-sensitive Spin-resolved PES   We have investigated the electronic states and spin polarization of
half-metallic ferromagnet CrO$_2$ (100) epitaxial films by bulk-sensitive
spin-resolved photoemission spectroscopy with a focus on non-quasiparticle
(NQP) states derived from electron-magnon interactions. We found that the
averaged values of the spin polarization are approximately 100% and 40% at 40 K
and 300 K, respectively. This is consistent with the previously reported result
[H. Fujiwara et al., Appl. Phys. Lett. 106, 202404 (2015).]. At 100 K, peculiar
spin depolarization was observed at the Fermi level ($E_{F}$), which is
supported by theoretical calculations predicting NQP states. This suggests the
possible appearance of NQP states in CrO$_2$. We also compare the temperature
dependence of our spin polarizations with that of the magnetization.
",Physics,Physics
"A multi-layered energy consumption model for smart wireless acoustic sensor networks   Smart sensing is expected to become a pervasive technology in smart cities
and environments of the near future. These services are improving their
capabilities due to integrated devices shrinking in size while maintaining
their computational power, which can run diverse Machine Learning algorithms
and achieve high performance in various data-processing tasks. One attractive
sensor modality to be used for smart sensing are acoustic sensors, which can
convey highly informative data while keeping a moderate energy consumption.
Unfortunately, the energy budget of current wireless sensor networks is usually
not enough to support the requirements of standard microphones. Therefore,
energy efficiency needs to be increased at all layers --- sensing, signal
processing and communication --- in order to bring wireless smart acoustic
sensors into the market. To help to attain this goal, this paper introduces
WASN-EM: an energy consumption model for wireless acoustic sensors networks
(WASN), whose aim is to aid in the development of novel techniques to increase
the energy-efficient of smart wireless acoustic sensors. This model provides a
first step of exploration prior to custom design of a smart wireless acoustic
sensor, and also can be used to compare the energy consumption of different
protocols.
",Computer Science,Computer Science
"Simultaneously Learning Neighborship and Projection Matrix for Supervised Dimensionality Reduction   Explicitly or implicitly, most of dimensionality reduction methods need to
determine which samples are neighbors and the similarity between the neighbors
in the original highdimensional space. The projection matrix is then learned on
the assumption that the neighborhood information (e.g., the similarity) is
known and fixed prior to learning. However, it is difficult to precisely
measure the intrinsic similarity of samples in high-dimensional space because
of the curse of dimensionality. Consequently, the neighbors selected according
to such similarity might and the projection matrix obtained according to such
similarity and neighbors are not optimal in the sense of classification and
generalization. To overcome the drawbacks, in this paper we propose to let the
similarity and neighbors be variables and model them in low-dimensional space.
Both the optimal similarity and projection matrix are obtained by minimizing a
unified objective function. Nonnegative and sum-to-one constraints on the
similarity are adopted. Instead of empirically setting the regularization
parameter, we treat it as a variable to be optimized. It is interesting that
the optimal regularization parameter is adaptive to the neighbors in
low-dimensional space and has intuitive meaning. Experimental results on the
YALE B, COIL-100, and MNIST datasets demonstrate the effectiveness of the
proposed method.
",Statistics,Computer Science; Statistics
"Weak Label Supervision for Monaural Source Separation Using Non-negative Denoising Variational Autoencoders   Deep learning models are very effective in source separation when there are
large amounts of labeled data available. However it is not always possible to
have carefully labeled datasets. In this paper, we propose a weak supervision
method that only uses class information rather than source signals for learning
to separate short utterance mixtures. We associate a variational autoencoder
(VAE) with each class within a non-negative model. We demonstrate that deep
convolutional VAEs provide a prior model to identify complex signals in a sound
mixture without having access to any source signal. We show that the separation
results are on par with source signal supervision.
",Computer Science,Computer Science; Statistics
"Stability of semi-wavefronts for delayed reaction-diffusion equations   This paper deals with the asymptotic behavior of solutions to the delayed
monostable equation: $(*)$ $u_{t}(t,x) = u_{xx}(t,x) - u(t,x) + g(u(t-h,x)),$
$x \in \mathbb{R},\ t >0,$ where $h>0$ and the reaction term $g: \mathbb{R}_+
\to \mathbb{R}_+$ has exactly two fixed points (zero and $\kappa >0$). Under
certain condition on the derivative of $g$ at $\kappa$, the global stability of
fast wavefronts is proved. Also, the stability of the $leading \ edge$ of
semi-wavefronts for $(*)$ with $g$ satisfying $g(u)\leq g'(0)u, u\in\R_+,$ is
established
",Mathematics,Mathematics
"Quantifying telescope phase discontinuities external to AO-systems by use of Phase Diversity and Focal Plane Sharpening   We propose and apply two methods to estimate pupil plane phase
discontinuities for two realistic scenarios on VLT and Keck. The methods use
both Phase Diversity and a form of image sharpening. For the case of VLT, we
simulate the `low wind effect' (LWE) which is responsible for focal plane
errors in the SPHERE system in low wind and good seeing conditions. We
successfully estimate the simulated LWE using both methods, and show that they
are complimentary to one another. We also demonstrate that single image Phase
Diversity (also known as Phase Retrieval with diversity) is also capable of
estimating the simulated LWE when using the natural de-focus on the SPHERE/DTTS
imager. We demonstrate that Phase Diversity can estimate the LWE to within 30
nm RMS WFE, which is within the allowable tolerances to achieve a target SPHERE
contrast of 10$^{-6}$. Finally, we simulate 153 nm RMS of piston errors on the
mirror segments of Keck and produce NIRC2 images subject to these effects. We
show that a single, diverse image with 1.5 waves (PV) of focus can be used to
estimate this error to within 29 nm RMS WFE, and a perfect correction of our
estimation would increase the Strehl ratio of a NIRC2 image by 12\%
",Physics,Physics
"Towards automation of data quality system for CERN CMS experiment   Daily operation of a large-scale experiment is a challenging task,
particularly from perspectives of routine monitoring of quality for data being
taken. We describe an approach that uses Machine Learning for the automated
system to monitor data quality, which is based on partial use of data qualified
manually by detector experts. The system automatically classifies marginal
cases: both of good an bad data, and use human expert decision to classify
remaining ""grey area"" cases.
This study uses collision data collected by the CMS experiment at LHC in
2010. We demonstrate that proposed workflow is able to automatically process at
least 20\% of samples without noticeable degradation of the result.
",Computer Science,Computer Science; Statistics
"Character sums for elliptic curve densities   If $E$ is an elliptic curve over $\mathbb{Q}$, then it follows from work of
Serre and Hooley that, under the assumption of the Generalized Riemann
Hypothesis, the density of primes $p$ such that the group of
$\mathbb{F}_p$-rational points of the reduced curve $\tilde{E}(\mathbb{F}_p)$
is cyclic can be written as an infinite product $\prod \delta_\ell$ of local
factors $\delta_\ell$ reflecting the degree of the $\ell$-torsion fields,
multiplied by a factor that corrects for the entanglements between the various
torsion fields. We show that this correction factor can be interpreted as a
character sum, and the resulting description allows us to easily determine
non-vanishing criteria for it. We apply this method in a variety of other
settings. Among these, we consider the aforementioned problem with the
additional condition that the primes $p$ lie in a given arithmetic progression.
We also study the conjectural constants appearing in Koblitz's conjecture, a
conjecture which relates to the density of primes $p$ for which the cardinality
of the group of $\mathbb{F}_p$-points of $E$ is prime.
",Mathematics,Mathematics
"Pebble accretion at the origin of water in Europa   Despite the fact that the observed gradient in water content among the
Galilean satellites is globally consistent with a formation in a circum-Jovian
disk on both sides of the snowline, the mechanisms that led to a low water mass
fraction in Europa ($\sim$$8\%$) are not yet understood. Here, we present new
modeling results of solids transport in the circum-Jovian disk accounting for
aerodynamic drag, turbulent diffusion, surface temperature evolution and
sublimation of water ice. We find that the water mass fraction of pebbles
(e.g., solids with sizes of 10$^{-2}$ -- 1 m) as they drift inward is globally
consistent with the current water content of the Galilean system. This opens
the possibility that each satellite could have formed through pebble accretion
within a delimited region whose boundaries were defined by the position of the
snowline. This further implies that the migration of the forming satellites was
tied to the evolution of the snowline so that Europa fully accreted from
partially dehydrated material in the region just inside of the snowline.
",Physics,Physics
"Space-time domain solutions of the wave equation by a non-singular boundary integral method and Fourier transform   The general space-time evolution of the scattering of an incident acoustic
plane wave pulse by an arbitrary configuration of targets is treated by
employing a recently developed non-singular boundary integral method to solve
the Helmholtz equation in the frequency domain from which the fast Fourier
transform is used to obtain the full space-time solution of the wave equation.
The non-singular boundary integral solution can enforce the radiation boundary
condition at infinity exactly and can account for multiple scattering effects
at all spacings between scatterers without adverse effects on the numerical
precision. More generally, the absence of singular kernels in the non-singular
integral equation confers high numerical stability and precision for smaller
numbers of degrees of freedom. The use of fast Fourier transform to obtain the
time dependence is not constrained to discrete time steps and is particularly
efficient for studying the response to different incident pulses by the same
configuration of scatterers. The precision that can be attained using a smaller
number of Fourier components is also quantified.
",Physics,Physics
"Linearized Einstein's field equations   From the Einstein field equations, in a weak-field approximation and for
speeds small compared to the speed of light in vacuum, the following system is
obtained \begin{align*}
\nabla \times \overrightarrow{E_g} & =
-\frac{1}{c} \frac{\partial \overrightarrow{B_g}}{\partial t},
\nabla \cdot \overrightarrow{E_g} \;\; & \approx -4\pi G\rho_g,
\nabla \times \overrightarrow{B_g} & \approx
-\frac{4\pi G}{c^{2}}\overrightarrow{J_g}+
\frac{1}{c}\frac{\partial \overrightarrow{E_g}}{\partial t},
\nabla \cdot \overrightarrow{B_g} \;\; & = 0, \end{align*} where
$\overrightarrow{E_g}$ is the gravitoelectric field, $\overrightarrow{B_g}$ is
the gravitomagnetic field, $\overrightarrow{J_g}$ is the space-time-mass
current density and $\rho_g$ is the space-time-mass density. This last
gravitoelectromagnetic field system is similar to the Maxwell equations, thus
showing an analogy between the electromagnetic theory and gravitation.
",Physics,Physics
"Surjective H-Colouring: New Hardness Results   A homomorphism from a graph G to a graph H is a vertex mapping f from the
vertex set of G to the vertex set of H such that there is an edge between
vertices f(u) and f(v) of H whenever there is an edge between vertices u and v
of G. The H-Colouring problem is to decide whether or not a graph G allows a
homomorphism to a fixed graph H. We continue a study on a variant of this
problem, namely the Surjective H-Colouring problem, which imposes the
homomorphism to be vertex-surjective. We build upon previous results and show
that this problem is NP-complete for every connected graph H that has exactly
two vertices with a self-loop as long as these two vertices are not adjacent.
As a result, we can classify the computational complexity of Surjective
H-Colouring for every graph H on at most four vertices.
",Computer Science; Mathematics,Computer Science
"Relevance of backtracking paths in epidemic spreading on networks   The understanding of epidemics on networks has greatly benefited from the
recent application of message-passing approaches, which allow to derive exact
results for irreversible spreading (i.e. diseases with permanent acquired
immunity) in locally-tree like topologies. This success has suggested the
application of the same approach to reversible epidemics, for which an
individual can contract the epidemic and recover repeatedly. The underlying
assumption is that backtracking paths (i.e. an individual is reinfected by a
neighbor he/she previously infected) do not play a relevant role. In this paper
we show that this is not the case for reversible epidemics, since the neglect
of backtracking paths leads to a formula for the epidemic threshold that is
qualitatively incorrect in the large size limit. Moreover we define a modified
reversible dynamics which explicitly forbids direct backtracking events and
show that this modification completely upsets the phenomenology.
",Computer Science,Computer Science; Mathematics
"Free differential Lie Rota-Baxter algebras and Gröbner-Shirshov bases   We establish the Gröbner-Shirshov bases theory for differential Lie
$\Omega$-algebras. As an application, we give a linear basis of a free
differential Lie Rota-Baxter algebra on a set.
",Mathematics,Mathematics
"Studies to Understand and Optimize the Performance of Scintillation Counters for the Mu2e Cosmic Ray Veto System   In order to optimize the performance of the CRV, reflection studies and aging
studies were conducted.
",Physics,Computer Science
"The maximum number of cycles in a graph with fixed number of edges   The main topic considered is maximizing the number of cycles in a graph with
given number of edges. In 2009, Király conjectured that there is constant $c$
such that any graph with $m$ edges has at most $(1.4)^m$ cycles. In this paper,
it is shown that for sufficiently large $m$, a graph with $m$ edges has at most
$(1.443)^m$ cycles. For sufficiently large $m$, examples of a graph with $m$
edges and $(1.37)^m$ cycles are presented. For a graph with given number of
vertices and edges an upper bound on the maximal number of cycles is given.
Also, exponentially tight bounds are proved for the maximum number of cycles in
a multigraph with given number of edges, as well as in a multigraph with given
number of vertices and edges.
",Mathematics,Computer Science
"A variational-geometric approach for the optimal control of nonholonomic systems   Necessary conditions for existence of normal extremals in optimal control of
systems subject to nonholonomic constraints are derived as solutions of a
constrained second order variational problems. In this work, a geometric
interpretation of the derivation is studied from the theory of Lie algebroids.
We employ such a framework to describe the problem into a unifying formalism
for normal extremals in optimal control of nonholonomic systems and including
situations that have not been considered before in the literature from this
perspective. We show that necessary conditions for existence of extremals in
the optimal control problem can be also determined by a Hamiltonian system on
the cotangent bundle of a skew-symmetric algebroid.
",Mathematics,Mathematics
"Machine learning quantum mechanics: solving quantum mechanics problems using radial basis function networks   Inspired by the recent work of Carleo and Troyer[1], we apply machine
learning methods to quantum mechanics in this article. The radial basis
function network in a discrete basis is used as the variational wavefunction
for the ground state of a quantum system. Variational Monte Carlo(VMC)
calculations are carried out for some simple Hamiltonians. The results are in
good agreements with theoretical values. The smallest eigenvalue of a Hermitian
matrix can also be acquired using VMC calculations. Our results demonstrate
that machine learning techniques are capable of solving quantum mechanical
problems.
",Physics,Computer Science; Mathematics
"The existence and global exponential stability of almost periodic solutions for neutral type CNNs on time scales   In this paper, a class of neutral type competitive neural networks with mixed
time-varying delays and leakage delays on time scales is proposed. Based on the
exponential dichotomy of linear dynamic equations on time scales, Banach's
fixed point theorem and the theory of calculus on time scales, some sufficient
conditions that are independent of the backwards graininess function of the
time scale are obtained for the existence and global exponential stability of
almost periodic solutions for this class of neural networks. The obtained
results are completely new and indicate that both the continuous time and the
discrete time cases of the networks share the same dynamical behavior. Finally,
an examples is given to show the effectiveness of the obtained results.
",Mathematics,Computer Science; Mathematics
"Elicitability and its Application in Risk Management   Elicitability is a property of $\mathbb{R}^k$-valued functionals defined on a
set of distribution functions. These functionals represent statistical
properties of a distribution, for instance its mean, variance, or median. They
are called elicitable if there exists a scoring function such that the expected
score under a distribution takes its unique minimum at the functional value of
this distribution. If such a scoring function exists, it is called strictly
consistent for the functional. Motivated by the recent findings of Fissler and
Ziegel concerning higher order elicitability, this thesis reviews the most
important results, examples, and applications which are found in the relevant
literature. Moreover, we also contribute our own examples and findings in order
to give the reader a well-founded overview of the topic as well as of the most
used tools and techniques. We include necessary and sufficient conditions for
strictly consistent scoring functions, several elicitable as well as
non-elicitable functionals and the use of elicitability in forecast comparison,
regression, and estimation. Special emphasis is placed on quantitative risk
management and the result that Value at Risk and Expected Shortfall are jointly
elicitable.
",Mathematics; Statistics,Statistics
"Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation   Recent work has shown that state-of-the-art classifiers are quite brittle, in
the sense that a small adversarial change of an originally with high confidence
correctly classified input leads to a wrong classification again with high
confidence. This raises concerns that such classifiers are vulnerable to
attacks and calls into question their usage in safety-critical systems. We show
in this paper for the first time formal guarantees on the robustness of a
classifier by giving instance-specific lower bounds on the norm of the input
manipulation required to change the classifier decision. Based on this analysis
we propose the Cross-Lipschitz regularization functional. We show that using
this form of regularization in kernel methods resp. neural networks improves
the robustness of the classifier without any loss in prediction performance.
",Computer Science; Statistics,Computer Science; Statistics
"A sparse linear algebra algorithm for fast computation of prediction variances with Gaussian Markov random fields   Gaussian Markov random fields are used in a large number of disciplines in
machine vision and spatial statistics. The models take advantage of sparsity in
matrices introduced through the Markov assumptions, and all operations in
inference and prediction use sparse linear algebra operations that scale well
with dimensionality. Yet, for very high-dimensional models, exact computation
of predictive variances of linear combinations of variables is generally
computationally prohibitive, and approximate methods (generally interpolation
or conditional simulation) are typically used instead. A set of conditions are
established under which the variances of linear combinations of random
variables can be computed exactly using the Takahashi recursions. The ensuing
computational simplification has wide applicability and may be used to enhance
several software packages where model fitting is seated in a maximum-likelihood
framework. The resulting algorithm is ideal for use in a variety of spatial
statistical applications, including \emph{LatticeKrig} modelling, statistical
downscaling, and fixed rank kriging. It can compute hundreds of thousands exact
predictive variances of linear combinations on a standard desktop with ease,
even when large spatial GMRF models are used.
",Statistics,Statistics
"Multivariate Locally Stationary Wavelet Process Analysis with the mvLSW R Package   This paper describes the R package mvLSW. The package contains a suite of
tools for the analysis of multivariate locally stationary wavelet (LSW) time
series. Key elements include: (i) the simulation of multivariate LSW time
series for a given multivariate evolutionary wavelet spectrum (EWS); (ii)
estimation of the time-dependent multivariate EWS for a given time series;
(iii) estimation of the time-dependent coherence and partial coherence between
time series channels; and, (iv) estimation of approximate confidence intervals
for multivariate EWS estimates. A demonstration of the package is presented via
both a simulated example and a case study with EuStockMarkets from the datasets
package. This paper has been accepted by the Journal of Statistical Software.
Presented code extracts demonstrating the mvLSW package is performed under
version 1.2.1.
",Statistics,Statistics
"Courant's Nodal Domain Theorem for Positivity Preserving Forms   We introduce a notion of nodal domains for positivity preserving forms. This
notion generalizes the classical ones for Laplacians on domains and on graphs.
We prove the Courant nodal domain theorem in this generalized setting using
purely analytical methods.
",Mathematics,Mathematics
"Verification of operational solar flare forecast: Case of Regional Warning Center Japan   In this article, we discuss a verification study of an operational solar
flare forecast in the Regional Warning Center (RWC) Japan. The RWC Japan has
been issuing four-categorical deterministic solar flare forecasts for a long
time. In this forecast verification study, we used solar flare forecast data
accumulated over 16 years (from 2000 to 2015). We compiled the forecast data
together with solar flare data obtained with the Geostationary Operational
Environmental Satellites (GOES). Using the compiled data sets, we estimated
some conventional scalar verification measures with 95% confidence intervals.
We also estimated a multi-categorical scalar verification measure. These scalar
verification measures were compared with those obtained by the persistence
method and recurrence method. As solar activity varied during the 16 years, we
also applied verification analyses to four subsets of forecast-observation pair
data with different solar activity levels. We cannot conclude definitely that
there are significant performance difference between the forecasts of RWC Japan
and the persistence method, although a slightly significant difference is found
for some event definitions. We propose to use a scalar verification measure to
assess the judgment skill of the operational solar flare forecast. Finally, we
propose a verification strategy for deterministic operational solar flare
forecasting.
",Physics; Statistics,Statistics
"On Markov Chain Gradient Descent   Stochastic gradient methods are the workhorse (algorithms) of large-scale
optimization problems in machine learning, signal processing, and other
computational sciences and engineering. This paper studies Markov chain
gradient descent, a variant of stochastic gradient descent where the random
samples are taken on the trajectory of a Markov chain. Existing results of this
method assume convex objectives and a reversible Markov chain and thus have
their limitations. We establish new non-ergodic convergence under wider step
sizes, for nonconvex problems, and for non-reversible finite-state Markov
chains. Nonconvexity makes our method applicable to broader problem classes.
Non-reversible finite-state Markov chains, on the other hand, can mix
substatially faster. To obtain these results, we introduce a new technique that
varies the mixing levels of the Markov chains. The reported numerical results
validate our contributions.
",Statistics,Computer Science; Statistics
"Extrapolating Expected Accuracies for Large Multi-Class Problems   The difficulty of multi-class classification generally increases with the
number of classes. Using data from a subset of the classes, can we predict how
well a classifier will scale with an increased number of classes? Under the
assumptions that the classes are sampled identically and independently from a
population, and that the classifier is based on independently learned scoring
functions, we show that the expected accuracy when the classifier is trained on
k classes is the (k-1)st moment of a certain distribution that can be estimated
from data. We present an unbiased estimation method based on the theory, and
demonstrate its application on a facial recognition example.
",Computer Science; Statistics,Computer Science; Statistics
"Quantum Chebyshev's Inequality and Applications   In this paper we provide new quantum algorithms with polynomial speed-up for
a range of problems for which no such results were known, or we improve
previous algorithms. First, we consider the approximation of the frequency
moments $F_k$ of order $k \geq 3$ in the multi-pass streaming model with
updates (turnstile model). We design a $P$-pass quantum streaming algorithm
with memory $M$ satisfying a tradeoff of $P^2 M = \tilde{O}(n^{1-2/k})$,
whereas the best classical algorithm requires $P M = \Theta(n^{1-2/k})$. Then,
we study the problem of estimating the number $m$ of edges and the number $t$
of triangles given query access to an $n$-vertex graph. We describe optimal
quantum algorithms that perform $\tilde{O}(\sqrt{n}/m^{1/4})$ and
$\tilde{O}(\sqrt{n}/t^{1/6} + m^{3/4}/\sqrt{t})$ queries respectively. This is
a quadratic speed-up compared to the classical complexity of these problems.
For this purpose we develop a new quantum paradigm that we call Quantum
Chebyshev's inequality. Namely we demonstrate that, in a certain model of
quantum sampling, one can approximate with relative error the mean of any
random variable with a number of quantum samples that is linear in the ratio of
the square root of the variance to the mean. Classically the dependency is
quadratic. Our algorithm subsumes a previous result of Montanaro [Mon15]. This
new paradigm is based on a refinement of the Amplitude Estimation algorithm of
Brassard et al. [BHMT02] and of previous quantum algorithms for the mean
estimation problem. We show that this speed-up is optimal, and we identify
another common model of quantum sampling where it cannot be obtained. For our
applications, we also adapt the variable-time amplitude amplification technique
of Ambainis [Amb10] into a variable-time amplitude estimation algorithm.
",Statistics,Computer Science; Mathematics; Statistics
"Visual Detection of Structural Changes in Time-Varying Graphs Using Persistent Homology   Topological data analysis is an emerging area in exploratory data analysis
and data mining. Its main tool, persistent homology, has become a popular
technique to study the structure of complex, high-dimensional data. In this
paper, we propose a novel method using persistent homology to quantify
structural changes in time-varying graphs. Specifically, we transform each
instance of the time-varying graph into metric spaces, extract topological
features using persistent homology, and compare those features over time. We
provide a visualization that assists in time-varying graph exploration and
helps to identify patterns of behavior within the data. To validate our
approach, we conduct several case studies on real world data sets and show how
our method can find cyclic patterns, deviations from those patterns, and
one-time events in time-varying graphs. We also examine whether
persistence-based similarity measure as a graph metric satisfies a set of
well-established, desirable properties for graph metrics.
",Computer Science,Computer Science; Statistics
"NDSHA: robust and reliable seismic hazard assessment   The Neo-Deterministic Seismic Hazard Assessment (NDSHA) method reliably and
realistically simulates the suite of earthquake ground motions that may impact
civil populations as well as their heritage buildings. The modeling technique
is developed from comprehensive physical knowledge of the seismic source
process, the propagation of earthquake waves and their combined interactions
with site effects. NDSHA effectively accounts for the tensor nature of
earthquake ground motions formally described as the tensor product of the
earthquake source functions and the Green Functions of the pathway. NDSHA uses
all available information about the space distribution of large magnitude
earthquake, including Maximum Credible Earthquake (MCE) and geological and
geophysical data. It does not rely on scalar empirical ground motion
attenuation models, as these are often both weakly constrained by available
observations and unable to account for the tensor nature of earthquake ground
motion. Standard NDSHA provides robust and safely conservative hazard estimates
for engineering design and mitigation decision strategies without requiring
(often faulty) assumptions about the probabilistic risk analysis model of
earthquake occurrence. If specific applications may benefit from temporal
information the definition of the Gutenberg-Richter (GR) relation is performed
according to the multi-scale seismicity model and occurrence rate is associated
to each modeled source. Observations from recent destructive earthquakes in
Italy and Nepal have confirmed the validity of NDSHA approach and application,
and suggest that more widespread application of NDSHA will enhance earthquake
safety and resilience of civil populations in all earthquake-prone regions,
especially in tectonically active areas where the historic earthquake record is
too short.
",Physics,Physics
"Fingerprints of angulon instabilities in the spectra of matrix-isolated molecules   The formation of vortices is usually considered to be the main mechanism of
angular momentum disposal in superfluids. Recently, it was predicted that a
superfluid can acquire angular momentum via an alternative, microscopic route
-- namely, through interaction with rotating impurities, forming so-called
`angulon quasiparticles' [Phys. Rev. Lett. 114, 203001 (2015)]. The angulon
instabilities correspond to transfer of a small number of angular momentum
quanta from the impurity to the superfluid, as opposed to vortex instabilities,
where angular momentum is quantized in units of $\hbar$ per atom. Furthermore,
since conventional impurities (such as molecules) represent three-dimensional
(3D) rotors, the angular momentum transferred is intrinsically 3D as well, as
opposed to a merely planar rotation which is inherent to vortices. Herein we
show that the angulon theory can explain the anomalous broadening of the
spectroscopic lines observed for CH$_3$ and NH$_3$ molecules in superfluid
helium nanodroplets, thereby providing a fingerprint of the emerging angulon
instabilities in experiment.
",Physics,Physics
"Revisiting Simple Neural Networks for Learning Representations of Knowledge Graphs   We address the problem of learning vector representations for entities and
relations in Knowledge Graphs (KGs) for Knowledge Base Completion (KBC). This
problem has received significant attention in the past few years and multiple
methods have been proposed. Most of the existing methods in the literature use
a predefined characteristic scoring function for evaluating the correctness of
KG triples. These scoring functions distinguish correct triples (high score)
from incorrect ones (low score). However, their performance vary across
different datasets. In this work, we demonstrate that a simple neural network
based score function can consistently achieve near start-of-the-art performance
on multiple datasets. We also quantitatively demonstrate biases in standard
benchmark datasets, and highlight the need to perform evaluation spanning
various datasets.
",Computer Science; Statistics,Computer Science; Statistics
"Uniqueness and stability of Ricci flow through singularities   We verify a conjecture of Perelman, which states that there exists a
canonical Ricci flow through singularities starting from an arbitrary compact
Riemannian 3-manifold. Our main result is a uniqueness theorem for such flows,
which, together with an earlier existence theorem of Lott and the second named
author, implies Perelman's conjecture. We also show that this flow through
singularities depends continuously on its initial condition and that it may be
obtained as a limit of Ricci flows with surgery.
Our results have applications to the study of diffeomorphism groups of three
manifolds --- in particular to the Generalized Smale Conjecture --- which will
appear in a subsequent paper.
",Mathematics,Mathematics
"Effects of Disorder on the Pressure-Induced Mott Transition in $κ$-BEDT-TTF)$_2$Cu[N(CN)$_2$]Cl   We present a study of the influence of disorder on the Mott metal-insulator
transition for the organic charge-transfer salt
$\kappa$-(BEDT-TTF)$_2$Cu[N(CN)$_2$]Cl. To this end, disorder was introduced
into the system in a controlled way by exposing the single crystals to x-ray
irradiation. The crystals were then fine-tuned across the Mott transition by
the application of continuously controllable He-gas pressure at low
temperatures. Measurements of the thermal expansion and resistance show that
the first-order character of the Mott transition prevails for low irradiation
doses achieved by irradiation times up to 100 h. For these crystals with a
moderate degree of disorder, we find a first-order transition line which ends
in a second-order critical endpoint, akin to the pristine crystals. Compared to
the latter, however, we observe a significant reduction of both, the critical
pressure $p_c$ and the critical temperature $T_c$. This result is consistent
with the theoretically-predicted formation of a soft Coulomb gap in the
presence of strong correlations and small disorder. Furthermore, we
demonstrate, similar to the observation for the pristine sample, that the Mott
transition after 50 h of irradiation is accompanied by sizable lattice effects,
the critical behavior of which can be well described by mean-field theory. Our
results demonstrate that the character of the Mott transition remains
essentially unchanged at a low disorder level. However, after an irradiation
time of 150 h, no clear signatures of a discontinuous metal-insulator
transition could be revealed anymore. These results suggest that, above a
certain disorder level, the metal-insulator transition becomes a smeared
first-order transition with some residual hysteresis.
",Physics,Physics
"A New Tracking Algorithm for Multiple Colloidal Particles Close to Contact   In this paper, we propose a new algorithm based on radial symmetry center
method to track colloidal particles close to contact, where the optical images
of the particles start to overlap in digital video microscopy. This overlapping
effect is important to observe the pair interaction potential in colloidal
studies and it appears as additional interaction in the measurement of the
interaction with conventional tracking analysis. The proposed algorithm in this
work is simple, fast and applicable for not only two particles but also three
and more particles without any modification. The algorithm uses gradient
vectors of the particle intensity distribution, which allows us to use a part
of the symmetric intensity distribution in the calculation of the actual
particle position. In this study, simulations are performed to see the
performance of the proposed algorithm for two and three particles, where the
simulation images are generated by using fitted curve to experimental particle
image for different sized particles. As a result, the algorithm yields the
maximum error smaller than 2 nm for 5.53 {\mu}m silica particles in contact
condition.
",Physics,Physics
"Biochemical Coupling Through Emergent Conservation Laws   Bazhin has analyzed ATP coupling in terms of quasiequilibrium states where
fast reactions have reached an approximate steady state while slow reactions
have not yet reached equilibrium. After an expository introduction to the
relevant aspects of reaction network theory, we review his work and explain the
role of emergent conserved quantities in coupling. These are quantities, left
unchanged by fast reactions, whose conservation forces exergonic processes such
as ATP hydrolysis to drive desired endergonic processes.
",Quantitative Biology,Physics
"Visibility of minorities in social networks   Homophily can put minority groups at a disadvantage by restricting their
ability to establish links with people from a majority group. This can limit
the overall visibility of minorities in the network. Building on a
Barabási-Albert model variation with groups and homophily, we show how the
visibility of minority groups in social networks is a function of (i) their
relative group size and (ii) the presence or absence of homophilic behavior. We
provide an analytical solution for this problem and demonstrate the existence
of asymmetric behavior. Finally, we study the visibility of minority groups in
examples of real-world social networks: sexual contacts, scientific
collaboration, and scientific citation. Our work presents a foundation for
assessing the visibility of minority groups in social networks in which
homophilic or heterophilic behaviour is present.
",Computer Science; Physics,Computer Science; Physics
"Feature uncertainty bounding schemes for large robust nonlinear SVM classifiers   We consider the binary classification problem when data are large and subject
to unknown but bounded uncertainties. We address the problem by formulating the
nonlinear support vector machine training problem with robust optimization. To
do so, we analyze and propose two bounding schemes for uncertainties associated
to random approximate features in low dimensional spaces. The proposed
techniques are based on Random Fourier Features and the Nyström methods. The
resulting formulations can be solved with efficient stochastic approximation
techniques such as stochastic (sub)-gradient, stochastic proximal gradient
techniques or their variants.
",Computer Science; Statistics,Computer Science; Mathematics; Statistics
"Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?   Deep reinforcement learning has achieved many recent successes, but our
understanding of its strengths and limitations is hampered by the lack of rich
environments in which we can fully characterize optimal behavior, and
correspondingly diagnose individual actions against such a characterization.
Here we consider a family of combinatorial games, arising from work of Erdos,
Selfridge, and Spencer, and we propose their use as environments for evaluating
and comparing different approaches to reinforcement learning. These games have
a number of appealing features: they are challenging for current learning
approaches, but they form (i) a low-dimensional, simply parametrized
environment where (ii) there is a linear closed form solution for optimal
behavior from any state, and (iii) the difficulty of the game can be tuned by
changing environment parameters in an interpretable way. We use these
Erdos-Selfridge-Spencer games not only to compare different algorithms, but
test for generalization, make comparisons to supervised learning, analyse
multiagent play, and even develop a self play algorithm. Code can be found at:
this https URL
",Computer Science; Statistics,Computer Science; Statistics
"Systematic Quantum Mechanical Region Determination in QM/MM Simulation   Hybrid quantum mechanical-molecular mechanical (QM/MM) simulations are widely
used in enzyme simulation. Over ten convergence studies of QM/MM methods have
revealed over the past several years that key energetic and structural
properties approach asymptotic limits with only very large (ca. 500-1000 atom)
QM regions. This slow convergence has been observed to be due in part to
significant charge transfer between the core active site and surrounding
protein environment, which cannot be addressed by improvement of MM force
fields or the embedding method employed within QM/MM. Given this slow
convergence, it becomes essential to identify strategies for the most
atom-economical determination of optimal QM regions and to gain insight into
the crucial interactions captured only in large QM regions. Here, we extend and
develop two methods for quantitative determination of QM regions. First, in the
charge shift analysis (CSA) method, we probe the reorganization of electron
density when core active site residues are removed completely, as determined by
large-QM region QM/MM calculations. Second, we introduce the
highly-parallelizable Fukui shift analysis (FSA), which identifies how
core/substrate frontier states are altered by the presence of an additional QM
residue on smaller initial QM regions. We demonstrate that the FSA and CSA
approaches are complementary and consistent on three test case enzymes:
catechol O-methyltransferase, cytochrome P450cam, and hen eggwhite lysozyme. We
also introduce validation strategies and test sensitivities of the two methods
to geometric structure, basis set size, and electronic structure methodology.
Both methods represent promising approaches for the systematic, unbiased
determination of quantum mechanical effects in enzymes and large systems that
necessitate multi-scale modeling.
",Physics,Physics
"Assimilated LVEF: A Bayesian technique combining human intuition with machine measurement for sharper estimates of left ventricular ejection fraction and stronger association with outcomes   The cardiologist's main tool for measuring systolic heart failure is left
ventricular ejection fraction (LVEF). Trained cardiologist's report both a
visual and machine-guided measurement of LVEF, but only use this machine-guided
measurement in analysis. We use a Bayesian technique to combine visual and
machine-guided estimates from the PARTNER-IIA Trial, a cohort of patients with
aortic stenosis at moderate risk treated with bioprosthetic aortic valves, and
find our combined estimate reduces measurement errors and improves the
association between LVEF and a 1-year composite endpoint.
",Statistics,Statistics
"Drug response prediction by ensemble learning and drug-induced gene expression signatures   Chemotherapeutic response of cancer cells to a given compound is one of the
most fundamental information one requires to design anti-cancer drugs. Recent
advances in producing large drug screens against cancer cell lines provided an
opportunity to apply machine learning methods for this purpose. In addition to
cytotoxicity databases, considerable amount of drug-induced gene expression
data has also become publicly available. Following this, several methods that
exploit omics data were proposed to predict drug activity on cancer cells.
However, due to the complexity of cancer drug mechanisms, none of the existing
methods are perfect. One possible direction, therefore, is to combine the
strengths of both the methods and the databases for improved performance. We
demonstrate that integrating a large number of predictions by the proposed
method improves the performance for this task. The predictors in the ensemble
differ in several aspects such as the method itself, the number of tasks method
considers (multi-task vs. single-task) and the subset of data considered
(sub-sampling). We show that all these different aspects contribute to the
success of the final ensemble. In addition, we attempt to use the drug screen
data together with two novel signatures produced from the drug-induced gene
expression profiles of cancer cell lines. Finally, we evaluate the method
predictions by in vitro experiments in addition to the tests on data sets.The
predictions of the methods, the signatures and the software are available from
\url{this http URL}.
",Statistics; Quantitative Biology,Statistics
"A Modified Sigma-Pi-Sigma Neural Network with Adaptive Choice of Multinomials   Sigma-Pi-Sigma neural networks (SPSNNs) as a kind of high-order neural
networks can provide more powerful mapping capability than the traditional
feedforward neural networks (Sigma-Sigma neural networks). In the existing
literature, in order to reduce the number of the Pi nodes in the Pi layer, a
special multinomial P_s is used in SPSNNs. Each monomial in P_s is linear with
respect to each particular variable sigma_i when the other variables are taken
as constants. Therefore, the monomials like sigma_i^n or sigma_i^n sigma_j with
n>1 are not included. This choice may be somehow intuitive, but is not
necessarily the best. We propose in this paper a modified Sigma-Pi-Sigma neural
network (MSPSNN) with an adaptive approach to find a better multinomial for a
given problem. To elaborate, we start from a complete multinomial with a given
order. Then we employ a regularization technique in the learning process for
the given problem to reduce the number of monomials used in the multinomial,
and end up with a new SPSNN involving the same number of monomials (= the
number of nodes in the Pi-layer) as in P_s. Numerical experiments on some
benchmark problems show that our MSPSNN behaves better than the traditional
SPSNN with P_s.
",Statistics,Computer Science; Statistics
"Dimensionality reduction methods for molecular simulations   Molecular simulations produce very high-dimensional data-sets with millions
of data points. As analysis methods are often unable to cope with so many
dimensions, it is common to use dimensionality reduction and clustering methods
to reach a reduced representation of the data. Yet these methods often fail to
capture the most important features necessary for the construction of a Markov
model. Here we demonstrate the results of various dimensionality reduction
methods on two simulation data-sets, one of protein folding and another of
protein-ligand binding. The methods tested include a k-means clustering
variant, a non-linear auto encoder, principal component analysis and tICA. The
dimension-reduced data is then used to estimate the implied timescales of the
slowest process by a Markov state model analysis to assess the quality of the
projection. The projected dimensions learned from the data are visualized to
demonstrate which conformations the various methods choose to represent the
molecular process.
",Computer Science; Statistics,Statistics
"System calibration method for Fourier ptychographic microscopy   Fourier ptychographic microscopy (FPM) is a recently proposed quantitative
phase imaging technique with high resolution and wide field-of-view (FOV). In
current FPM imaging platforms, systematic error sources come from the
aberrations, LED intensity fluctuation, parameter imperfections and noise,
which will severely corrupt the reconstruction results with artifacts. Although
these problems have been researched and some special methods have been proposed
respectively, there is no method to solve all of them. However, the systematic
error is a mixture of various sources in the real situation. It is difficult to
distinguish a kind of error source from another due to the similar artifacts.
To this end, we report a system calibration procedure, termed SC-FPM, based on
the simulated annealing (SA) algorithm, LED intensity correction and adaptive
step-size strategy, which involves the evaluation of an error matric at each
iteration step, followed by the re-estimation of accurate parameters. The great
performance has been achieved both in simulation and experiments. The reported
system calibration scheme improves the robustness of FPM and relaxes the
experiment conditions, which makes the FPM more pragmatic.
",Physics,Physics
"GAMER-2: a GPU-accelerated adaptive mesh refinement code -- accuracy, performance, and scalability   We present GAMER-2, a GPU-accelerated adaptive mesh refinement (AMR) code for
astrophysics. It provides a rich set of features, including adaptive
time-stepping, several hydrodynamic schemes, magnetohydrodynamics,
self-gravity, particles, star formation, chemistry and radiative processes with
GRACKLE, data analysis with yt, and memory pool for efficient object
allocation. GAMER-2 is fully bitwise reproducible. For the performance
optimization, it adopts hybrid OpenMP/MPI/GPU parallelization and utilizes
overlapping CPU computation, GPU computation, and CPU-GPU communication. Load
balancing is achieved using a Hilbert space-filling curve on a level-by-level
basis without the need to duplicate the entire AMR hierarchy on each MPI
process. To provide convincing demonstrations of the accuracy and performance
of GAMER-2, we directly compare with Enzo on isolated disk galaxy simulations
and with FLASH on galaxy cluster merger simulations. We show that the physical
results obtained by different codes are in very good agreement, and GAMER-2
outperforms Enzo and FLASH by nearly one and two orders of magnitude,
respectively, on the Blue Waters supercomputers using $1-256$ nodes. More
importantly, GAMER-2 exhibits similar or even better parallel scalability
compared to the other two codes. We also demonstrate good weak and strong
scaling using up to 4096 GPUs and 65,536 CPU cores, and achieve a uniform
resolution as high as $10{,}240^3$ cells. Furthermore, GAMER-2 can be adopted
as an AMR+GPUs framework and has been extensively used for the wave dark matter
($\psi$DM) simulations. GAMER-2 is open source (available at
this https URL) and new contributions are welcome.
",Physics,Physics
"Symmetry-enforced quantum spin Hall insulators in $π$-flux models   We prove a Lieb-Schultz-Mattis theorem for the quantum spin Hall effect
(QSHE) in two-dimensional $\pi$-flux models. In the presence of time reversal,
$U(1)$ charge conservation and magnetic translation (with $\pi$-flux per unit
cell) symmetries, if a generic interacting Hamiltonian has a unique gapped
symmetric ground state at half filling (i.e. an odd number of electrons per
unit cell), it can only be a QSH insulator. In other words, a trivial Mott
insulator is forbidden by symmetries at half filling. We further show that such
a symmetry-enforced QSHE can be realized in cold atoms, by shaking an optical
lattice and applying a time-dependent Zeeman field.
",Physics,Physics
"Matching neural paths: transfer from recognition to correspondence search   Many machine learning tasks require finding per-part correspondences between
objects. In this work we focus on low-level correspondences - a highly
ambiguous matching problem. We propose to use a hierarchical semantic
representation of the objects, coming from a convolutional neural network, to
solve this ambiguity. Training it for low-level correspondence prediction
directly might not be an option in some domains where the ground-truth
correspondences are hard to obtain. We show how transfer from recognition can
be used to avoid such training. Our idea is to mark parts as ""matching"" if
their features are close to each other at all the levels of convolutional
feature hierarchy (neural paths). Although the overall number of such paths is
exponential in the number of layers, we propose a polynomial algorithm for
aggregating all of them in a single backward pass. The empirical validation is
done on the task of stereo correspondence and demonstrates that we achieve
competitive results among the methods which do not use labeled target domain
data.
",Computer Science,Computer Science; Statistics
"The heat trace for the drifting Laplacian and Schrödinger operators on manifolds   We study the heat trace for both the drifting Laplacian as well as
Schrödinger operators on compact Riemannian manifolds. In the case of a
finite regularity potential or weight function, we prove the existence of a
partial (six term) asymptotic expansion of the heat trace for small times as
well as a suitable remainder estimate. We also demonstrate that the more
precise asymptotic behavior of the remainder is determined by and conversely
distinguishes higher (Sobolev) regularity on the potential or weight function.
In the case of a smooth weight function, we determine the full asymptotic
expansion of the heat trace for the drifting Laplacian for small times. We then
use the heat trace to study the asymptotics of the eigenvalue counting
function. In both cases the Weyl law coincides with the Weyl law for the
Riemannian manifold with the standard Laplace-Beltrami operator. We conclude by
demonstrating isospectrality results for the drifting Laplacian on compact
manifolds.
",Mathematics,Mathematics
"Overfitting Mechanism and Avoidance in Deep Neural Networks   Assisted by the availability of data and high performance computing, deep
learning techniques have achieved breakthroughs and surpassed human performance
empirically in difficult tasks, including object recognition, speech
recognition, and natural language processing. As they are being used in
critical applications, understanding underlying mechanisms for their successes
and limitations is imperative. In this paper, we show that overfitting, one of
the fundamental issues in deep neural networks, is due to continuous gradient
updating and scale sensitiveness of cross entropy loss. By separating samples
into correctly and incorrectly classified ones, we show that they behave very
differently, where the loss decreases in the correct ones and increases in the
incorrect ones. Furthermore, by analyzing dynamics during training, we propose
a consensus-based classification algorithm that enables us to avoid overfitting
and significantly improve the classification accuracy especially when the
number of training samples is limited. As each trained neural network depends
on extrinsic factors such as initial values as well as training data, requiring
consensus among multiple models reduces extrinsic factors substantially; for
statistically independent models, the reduction is exponential. Compared to
ensemble algorithms, the proposed algorithm avoids overgeneralization by not
classifying ambiguous inputs. Systematic experimental results demonstrate the
effectiveness of the proposed algorithm. For example, using only 1000 training
samples from MNIST dataset, the proposed algorithm achieves 95% accuracy,
significantly higher than any of the individual models, with 90% of the test
samples classified.
",Computer Science; Statistics,Computer Science; Statistics
"Biomedical Event Trigger Identification Using Bidirectional Recurrent Neural Network Based Models   Biomedical events describe complex interactions between various biomedical
entities. Event trigger is a word or a phrase which typically signifies the
occurrence of an event. Event trigger identification is an important first step
in all event extraction methods. However many of the current approaches either
rely on complex hand-crafted features or consider features only within a
window. In this paper we propose a method that takes the advantage of recurrent
neural network (RNN) to extract higher level features present across the
sentence. Thus hidden state representation of RNN along with word and entity
type embedding as features avoid relying on the complex hand-crafted features
generated using various NLP toolkits. Our experiments have shown to achieve
state-of-art F1-score on Multi Level Event Extraction (MLEE) corpus. We have
also performed category-wise analysis of the result and discussed the
importance of various features in trigger identification task.
",Computer Science,Computer Science
"Mean-Field Controllability and Decentralized Stabilization of Markov Chains, Part I: Global Controllability and Rational Feedbacks   In this paper, we study the controllability and stabilizability properties of
the Kolmogorov forward equation of a continuous time Markov chain (CTMC)
evolving on a finite state space, using the transition rates as the control
parameters. Firstly, we prove small-time local and global controllability from
and to strictly positive equilibrium configurations when the underlying graph
is strongly connected. Secondly, we show that there always exists a locally
exponentially stabilizing decentralized linear (density-)feedback law that
takes zero valu at equilibrium and respects the graph structure, provided that
the transition rates are allowed to be negative and the desired target density
lies in the interior of the set of probability densities. For bidirected
graphs, that is, graphs where a directed edge in one direction implies an edge
in the opposite direction, we show that this linear control law can be realized
using a decentralized rational feedback law of the form k(x) = a(x) +
b(x)f(x)/g(x) that also respects the graph structure and control constraints
(positivity and zero at equilibrium). This enables the possibility of using
Linear Matrix Inequality (LMI) based tools to algorithmically construct
decentralized density feedback controllers for stabilization of a robotic swarm
to a target task distribution with no task-switching at equilibrium, as we
demonstrate with several numerical examples.
",Computer Science; Mathematics,Computer Science; Mathematics
"Monte Carlo Tree Search with Sampled Information Relaxation Dual Bounds   Monte Carlo Tree Search (MCTS), most famously used in game-play artificial
intelligence (e.g., the game of Go), is a well-known strategy for constructing
approximate solutions to sequential decision problems. Its primary innovation
is the use of a heuristic, known as a default policy, to obtain Monte Carlo
estimates of downstream values for states in a decision tree. This information
is used to iteratively expand the tree towards regions of states and actions
that an optimal policy might visit. However, to guarantee convergence to the
optimal action, MCTS requires the entire tree to be expanded asymptotically. In
this paper, we propose a new technique called Primal-Dual MCTS that utilizes
sampled information relaxation upper bounds on potential actions, creating the
possibility of ""ignoring"" parts of the tree that stem from highly suboptimal
choices. This allows us to prove that despite converging to a partial decision
tree in the limit, the recommended action from Primal-Dual MCTS is optimal. The
new approach shows significant promise when used to optimize the behavior of a
single driver navigating a graph while operating on a ride-sharing platform.
Numerical experiments on a real dataset of 7,000 trips in New Jersey suggest
that Primal-Dual MCTS improves upon standard MCTS by producing deeper decision
trees and exhibits a reduced sensitivity to the size of the action space.
",Computer Science; Mathematics,Computer Science
"Game-Theoretic Choice of Curing Rates Against Networked SIS Epidemics by Human Decision-Makers   We study networks of human decision-makers who independently decide how to
protect themselves against Susceptible-Infected-Susceptible (SIS) epidemics.
Motivated by studies in behavioral economics showing that humans perceive
probabilities in a nonlinear fashion, we examine the impacts of such
misperceptions on the equilibrium protection strategies. In our setting, nodes
choose their curing rates to minimize the infection probability under the
degree-based mean-field approximation of the SIS epidemic plus the cost of
their selected curing rate. We establish the existence of a degree based
equilibrium under both true and nonlinear perceptions of infection
probabilities (under suitable assumptions). When the per-unit cost of curing
rate is sufficiently high, we show that true expectation minimizers choose the
curing rate to be zero at the equilibrium, while curing rate is nonzero under
nonlinear probability weighting.
",Computer Science,Computer Science; Mathematics; Statistics
"Observation of Skyrmions at Room Temperature in Co2FeAl Heusler Alloy Ultrathin Films   Magnetic skyrmions are topological spin structures having immense potential
for energy efficient spintronic devices. However, observations of skyrmions at
room temperature are limited to patterned nanostructures. Here, we report the
observation of stable skyrmions in unpatterned Ta/Co2FeAl(CFA)/MgO thin film
heterostructures at room temperature and in zero external magnetic field
employing magnetic force microscopy. The skyrmions are observed in a trilayer
structure comprised of heavy metal (HM)/ferromagnet (FM)/Oxide interfaces which
result in strong interfacial Dzyaloshinskii-Moriya interaction (i-DMI) as
evidenced by Brillouin light scattering measurements, in agreement with the
results of micromagnetic simulations. We also emphasize on room temperature
observation of multiple skyrmions which can be stabilized for suitable choices
of CFA layer thickness, perpendicular magnetic anisotropy, and i-DMI. These
results open up a new paradigm for designing room temperature spintronic
devices based on skyrmions in FM continuous thin films.
",Physics,Physics
"Interpreted Formalisms for Configurations   Imprecise and incomplete specification of system \textit{configurations}
threatens safety, security, functionality, and other critical system properties
and uselessly enlarges the configuration spaces to be searched by configuration
engineers and auto-tuners. To address these problems, this paper introduces
\textit{interpreted formalisms based on real-world types for configurations}.
Configuration values are lifted to values of real-world types, which we
formalize as \textit{subset types} in Coq. Values of these types are dependent
pairs whose components are values of underlying Coq types and proofs of
additional properties about them. Real-world types both extend and further
constrain \textit{machine-level} configurations, enabling richer, proof-based
checking of their consistency with real-world constraints. Tactic-based proof
scripts are written once to automate the construction of proofs, if proofs
exist, for configuration fields and whole configurations. \textit{Failures to
prove} reveal real-world type errors. Evaluation is based on a case study of
combinatorial optimization of Hadoop performance by meta-heuristic search over
Hadoop configurations spaces.
",Computer Science,Computer Science
"Algorithmic Decision Making in the Presence of Unmeasured Confounding   On a variety of complex decision-making tasks, from doctors prescribing
treatment to judges setting bail, machine learning algorithms have been shown
to outperform expert human judgments. One complication, however, is that it is
often difficult to anticipate the effects of algorithmic policies prior to
deployment, making the decision to adopt them risky. In particular, one
generally cannot use historical data to directly observe what would have
happened had the actions recommended by the algorithm been taken. One standard
strategy is to model potential outcomes for alternative decisions assuming that
there are no unmeasured confounders (i.e., to assume ignorability). But if this
ignorability assumption is violated, the predicted and actual effects of an
algorithmic policy can diverge sharply. In this paper we present a flexible,
Bayesian approach to gauge the sensitivity of predicted policy outcomes to
unmeasured confounders. We show that this policy evaluation problem is a
generalization of estimating heterogeneous treatment effects in observational
studies, and so our methods can immediately be applied to that setting.
Finally, we show, both theoretically and empirically, that under certain
conditions it is possible to construct near-optimal algorithmic policies even
when ignorability is violated. We demonstrate the efficacy of our methods on a
large dataset of judicial actions, in which one must decide whether defendants
awaiting trial should be required to pay bail or can be released without
payment.
",Statistics,Computer Science; Statistics
"Friendship Maintenance and Prediction in Multiple Social Networks   Due to the proliferation of online social networks (OSNs), users find
themselves participating in multiple OSNs. These users leave their activity
traces as they maintain friendships and interact with other users in these
OSNs. In this work, we analyze how users maintain friendship in multiple OSNs
by studying users who have accounts in both Twitter and Instagram.
Specifically, we study the similarity of a user's friendship and the evenness
of friendship distribution in multiple OSNs. Our study shows that most users in
Twitter and Instagram prefer to maintain different friendships in the two OSNs,
keeping only a small clique of common friends in across the OSNs. Based upon
our empirical study, we conduct link prediction experiments to predict missing
friendship links in multiple OSNs using the neighborhood features, neighborhood
friendship maintenance features and cross-link features. Our link prediction
experiments shows that un- supervised methods can yield good accuracy in
predicting links in one OSN using another OSN data and the link prediction
accuracy can be further improved using supervised method with friendship
maintenance and others measures as features.
",Computer Science; Physics,Computer Science
"Optimizing wearable assistive devices with neuromuscular models and optimal control   The coupling of human movement dynamics with the function and design of
wearable assistive devices is vital to better understand the interaction
between the two. Advanced neuromuscular models and optimal control formulations
provide the possibility to study and improve this interaction. In addition,
optimal control can also be used to generate predictive simulations that
generate novel movements for the human model under varying optimization
criterion.
",Computer Science,Quantitative Biology
"A Markov Chain Model for the Cure Rate of Non-Performing Loans   A Markov-chain model is developed for the purpose estimation of the cure rate
of non-performing loans. The technique is performed collectively, on portfolios
and it can be applicable in the process of calculation of credit impairment. It
is efficient in terms of data manipulation costs which makes it accessible even
to smaller financial institutions. In addition, several other applications to
portfolio optimization are suggested.
",Statistics; Quantitative Finance,Quantitative Finance
"Uniqueness and radial symmetry of minimizers for a nonlocal variational problem   In this paper we prove the uniqueness and radial symmetry of minimizers for
variational problems that model several phenomena. The uniqueness is a
consequence of the convexity of the functional. The main technique is Fourier
transform of tempered distributions.
",Mathematics,Mathematics
"Distributed Impedance Control of Latency-Prone Robotic Systems with Series Elastic Actuation   Robotic systems are increasingly relying on distributed feedback controllers
to tackle complex and latency-prone sensing and decision problems. These
demands come at the cost of a growing computational burden and, as a result,
larger controller latencies. To maximize robustness to mechanical disturbances
and achieve high control performance, we emphasize the necessity for executing
damping feedback in close proximity to the control plant while allocating
stiffness feedback in a latency-prone centralized control process.
Additionally, series elastic actuators (SEAs) are becoming prevalent in
torque-controlled robots during recent years to achieve compliant interactions
with environments and humans. However, designing optimal impedance controllers
and characterizing impedance performance for SEAs with time delays and
filtering are still under-explored problems. The presented study addresses the
optimal controller design problem by devising a critically-damped gain design
method for a class of SEA cascaded control architectures, which is composed of
outer-impedance and inner-torque feedback loops. Via the proposed controller
design criterion, we adopt frequency-domain methods to thoroughly analyze the
effects of time delays, filtering and load inertia on SEA impedance
performance. These results are further validated through the analysis,
simulation, and experimental testing on high-performance actuators and on an
omnidirectional mobile base.
",Computer Science,Computer Science
"Information and estimation in Fokker-Planck channels   We study the relationship between information- and estimation-theoretic
quantities in time-evolving systems. We focus on the Fokker-Planck channel
defined by a general stochastic differential equation, and show that the time
derivatives of entropy, KL divergence, and mutual information are characterized
by estimation-theoretic quantities involving an appropriate generalization of
the Fisher information. Our results vastly extend De Bruijn's identity and the
classical I-MMSE relation.
",Computer Science; Mathematics; Statistics,Mathematics
"MOBILITY21: Strategic Investments for Transportation Infrastructure & Technology   America's transportation infrastructure is the backbone of our economy. A
strong infrastructure means a strong America - an America that competes
globally, supports local and regional economic development, and creates jobs.
Strategic investments in our transportation infrastructure are vital to our
national security, economic growth, transportation safety and our technology
leadership. This document outlines critical needs for our transportation
infrastructure, identifies new technology drivers and proposes strategic
investments for safe and efficient air, ground, rail and marine mobility of
people and goods.
",Computer Science,Computer Science
"Parylene-C microfibrous thin films as phononic crystals   Phononic bandgaps of Parylene-C microfibrous thin films (muFTFs) were
computationally determined by treating them as phononic crystals comprising
identical microfibers arranged either on a square or a hexagonal lattice. The
microfibers could be columnar,chevronic, or helical in shape, and the host
medium could be either water or air. All bandgaps were observed to lie in the
0.01-to-162.9-MHz regime, for microfibers of realistically chosen dimensions.
The upper limit of the frequency of bandgaps was the highest for the columnar
muFTF and the lowest for the chiral muFTF. More bandgaps exist when the host
medium is water than air. Complete bandgaps were observed for the columnar
muFTF with microfibers arranged on a hexagonal lattice in air, the chevronic
muFTF with microfibers arranged on a square lattice in water, and the chiral
muFTF with microfibers arranged on a hexagonal lattice in either air or water.
The softness of the Parylene-C muFTFs makes them mechanically tunable, and
their bandgaps can be exploited in multiband ultrasonic filters.
",Physics,Physics
"An Improved Training Procedure for Neural Autoregressive Data Completion   Neural autoregressive models are explicit density estimators that achieve
state-of-the-art likelihoods for generative modeling. The D-dimensional data
distribution is factorized into an autoregressive product of one-dimensional
conditional distributions according to the chain rule. Data completion is a
more involved task than data generation: the model must infer missing variables
for any partially observed input vector. Previous work introduced an
order-agnostic training procedure for data completion with autoregressive
models. Missing variables in any partially observed input vector can be imputed
efficiently by choosing an ordering where observed dimensions precede
unobserved ones and by computing the autoregressive product in this order. In
this paper, we provide evidence that the order-agnostic (OA) training procedure
is suboptimal for data completion. We propose an alternative procedure (OA++)
that reaches better performance in fewer computations. It can handle all data
completion queries while training fewer one-dimensional conditional
distributions than the OA procedure. In addition, these one-dimensional
conditional distributions are trained proportionally to their expected usage at
inference time, reducing overfitting. Finally, our OA++ procedure can exploit
prior knowledge about the distribution of inference completion queries, as
opposed to OA. We support these claims with quantitative experiments on
standard datasets used to evaluate autoregressive generative models.
",Computer Science; Statistics,Statistics
"Nonlinear Unknown Input and State Estimation Algorithm in Mobile Robots   This technical report provides the description and the derivation of a novel
nonlinear unknown input and state estimation algorithm (NUISE) for mobile
robots. The algorithm is designed for real-world robots with nonlinear dynamic
models and subject to stochastic noises on sensing and actuation. Leveraging
sensor readings and planned control commands, the algorithm detects and
quantifies anomalies on both sensors and actuators. Later, we elaborate the
dynamic models of two distinctive mobile robots for the purpose of
demonstrating the application of NUISE. This report serves as a supplementary
document for [1].
",Computer Science,Computer Science
"Placing your Coins on a Shelf   We consider the problem of packing a family of disks ""on a shelf"", that is,
such that each disk touches the $x$-axis from above and such that no two disks
overlap. We prove that the problem of minimizing the distance between the
leftmost point and the rightmost point of any disk is NP-hard. On the positive
side, we show how to approximate this problem within a factor of 4/3 in $O(n
\log n)$ time, and provide an $O(n \log n)$-time exact algorithm for a special
case, in particular when the ratio between the largest and smallest radius is
at most four.
",Computer Science; Mathematics,Computer Science
"Active modulation of electromagnetically induced transparency analogue in terahertz hybrid metal-graphene metamaterials   Metamaterial analogues of electromagnetically induced transparency (EIT) have
been intensively studied and widely employed for slow light and enhanced
nonlinear effects. In particular, the active modulation of the EIT analogue and
well-controlled group delay in metamaterials have shown great prospects in
optical communication networks. Previous studies have focused on the optical
control of the EIT analogue by integrating the photoactive materials into the
unit cell, however, the response time is limited by the recovery time of the
excited carriers in these bulk materials. Graphene has recently emerged as an
exceptional optoelectronic material. It shows an ultrafast relaxation time on
the order of picosecond and its conductivity can be tuned via manipulating the
Fermi energy. Here we integrate a monolayer graphene into metal-based terahertz
(THz) metamaterials, and realize a complete modulation in the resonance
strength of the EIT analogue at the accessible Fermi energy. The physical
mechanism lies in the active tuning the damping rate of the dark mode resonator
through the recombination effect of the conductive graphene. Note that the
monolayer morphology in our work is easier to fabricate and manipulate than
isolated fashion. This work presents a novel modulation strategy of the EIT
analogue in the hybrid metamaterials, and pave the way towards designing very
compact slow light devices to meet future demand of ultrafast optical signal
processing.
",Physics,Physics
"What drives transient behaviour in complex systems?   We study transient behaviour in the dynamics of complex systems described by
a set of non-linear ODE's. Destabilizing nature of transient trajectories is
discussed and its connection with the eigenvalue-based linearization procedure.
The complexity is realized as a random matrix drawn from a modified May-Wigner
model. Based on the initial response of the system, we identify a novel
stable-transient regime. We calculate exact abundances of typical and extreme
transient trajectories finding both Gaussian and Tracy-Widom distributions
known in extreme value statistics. We identify degrees of freedom driving
transient behaviour as connected to the eigenvectors and encoded in a
non-orthogonality matrix $T_0$. We accordingly extend the May-Wigner model to
contain a phase with typical transient trajectories present. An exact norm of
the trajectory is obtained in the vanishing $T_0$ limit where it describes a
normal matrix.
",Physics,Mathematics; Statistics
"Black holes in vector-tensor theories   We study static and spherically symmetric black hole (BH) solutions in
second-order generalized Proca theories with nonminimal vector field derivative
couplings to the Ricci scalar, the Einstein tensor, and the double dual Riemann
tensor. We find concrete Lagrangians which give rise to exact BH solutions by
imposing two conditions of the two identical metric components and the constant
norm of the vector field. These exact solutions are described by either
Reissner-Nordström (RN), stealth Schwarzschild, or extremal RN solutions
with a non-trivial longitudinal mode of the vector field. We then numerically
construct BH solutions without imposing these conditions. For cubic and quartic
Lagrangians with power-law couplings which encompass vector Galileons as the
specific cases, we show the existence of BH solutions with the difference
between two non-trivial metric components. The quintic-order power-law
couplings do not give rise to non-trivial BH solutions regular throughout the
horizon exterior. The sixth-order and intrinsic vector-mode couplings can lead
to BH solutions with a secondary hair. For all the solutions, the vector field
is regular at least at the future or past horizon. The deviation from General
Relativity induced by the Proca hair can be potentially tested by future
measurements of gravitational waves in the nonlinear regime of gravity.
",Physics,Physics
"On Consistency of Compressive Spectral Clustering   Spectral clustering is one of the most popular methods for community
detection in graphs. A key step in spectral clustering algorithms is the eigen
decomposition of the $n{\times}n$ graph Laplacian matrix to extract its $k$
leading eigenvectors, where $k$ is the desired number of clusters among $n$
objects. This is prohibitively complex to implement for very large datasets.
However, it has recently been shown that it is possible to bypass the eigen
decomposition by computing an approximate spectral embedding through graph
filtering of random signals. In this paper, we analyze the working of spectral
clustering performed via graph filtering on the stochastic block model.
Specifically, we characterize the effects of sparsity, dimensionality and
filter approximation error on the consistency of the algorithm in recovering
planted clusters.
",Computer Science; Statistics,Computer Science; Statistics
"A stable numerical strategy for Reynolds-Rayleigh-Plesset coupling   The coupling of Reynolds and Rayleigh-Plesset equations has been used in
several works to simulate lubricated devices considering cavitation. The
numerical strategies proposed so far are variants of a staggered strategy where
Reynolds equation is solved considering the bubble dynamics frozen, and then
the Rayleigh-Plesset equation is solved to update the bubble radius with the
pressure frozen. We show that this strategy has severe stability issues and a
stable methodology is proposed. The proposed methodology performance is
assessed on two physical settings. The first one concerns the propagation of a
decompression wave along a fracture considering the presence of cavitation
nuclei. The second one is a typical journal bearing, in which the coupled model
is compared with the Elrod-Adams model.
",Physics,Physics
"Manuscripts in Time and Space: Experiments in Scriptometrics on an Old French Corpus   Witnesses of medieval literary texts, preserved in manuscript, are layered
objects , being almost exclusively copies of copies. This results in multiple
and hard to distinguish linguistic strata -- the author's scripta interacting
with the scriptae of the various scribes -- in a context where literary written
language is already a dialectal hybrid. Moreover, no single linguistic
phenomenon allows to distinguish between different scriptae, and only the
combination of multiple characteristics is likely to be significant [9] -- but
which ones? The most common approach is to search for these features in a set
of previously selected texts, that are supposed to be representative of a given
scripta. This can induce a circularity, in which texts are used to select
features that in turn characterise them as belonging to a linguistic area. To
counter this issue, this paper offers an unsupervised and corpus-based
approach, in which clustering methods are applied to an Old French corpus to
identify main divisions and groups. Ultimately, scriptometric profiles are
built for each of them.
",Statistics,Computer Science
"New neutrino physics and the altered shapes of solar neutrino spectra   Neutrinos coming from the Sun's core are now measured with a high precision,
and fundamental neutrino oscillations parameters are determined with a good
accuracy. In this work, we estimate the impact that a new neutrino physics
model, the so-called generalized Mikheyev-Smirnov-Wolfenstein (MSW) oscillation
mechanism, has on the shape of some of leading solar neutrino spectra, some of
which will be partially tested by the next generation of solar neutrino
experiments. In these calculations, we use a high-precision standard solar
model in good agreement with helioseismology data. We found that the neutrino
spectra of the different solar nuclear reactions of the proton-proton chains
and carbon-nitrogen-oxygen cycle have quite distinct sensitivities to the new
neutrino physics. The $HeP$ and $^8B$ neutrino spectra are the ones for which
their shapes are more affected when neutrinos interact with quarks in addition
to electrons. The shape of the $^{15}O$ and $^{17}F$ neutrino spectra are also
modified, although in these cases the impact is much smaller. Finally, the
impact in the shape of the $PP$ and $^{13}N$ neutrino spectra is practically
negligible.
",Physics,Physics
"Tanaka formula for strictly stable processes   For symmetric Lévy processes, if the local times exist, the Tanaka formula
has already constructed via the techniques in the potential theory by Salminen
and Yor (2007). In this paper, we study the Tanaka formula for arbitrary
strictly stable processes with index $\alpha \in (1,2)$ including spectrally
positive and negative cases in a framework of Itô's stochastic calculus. Our
approach to the existence of local times for such processes is different from
Bertoin (1996).
",Mathematics,Mathematics
"Coupling Story to Visualization: Using Textual Analysis as a Bridge Between Data and Interpretation   Online writers and journalism media are increasingly combining visualization
(and other multimedia content) with narrative text to create narrative
visualizations. Often, however, the two elements are presented independently of
one another. We propose an approach to automatically integrate text and
visualization elements. We begin with a writer's narrative that presumably can
be supported with visual data evidence. We leverage natural language
processing, quantitative narrative analysis, and information visualization to
(1) automatically extract narrative components (who, what, when, where) from
data-rich stories, and (2) integrate the supporting data evidence with the text
to develop a narrative visualization. We also employ bidirectional interaction
from text to visualization and visualization to text to support reader
exploration in both directions. We demonstrate the approach with a case study
in the data-rich field of sports journalism.
",Computer Science,Computer Science
"Bayesian Scale Estimation for Monocular SLAM Based on Generic Object Detection for Correcting Scale Drift   This work proposes a new, online algorithm for estimating the local scale
correction to apply to the output of a monocular SLAM system and obtain an as
faithful as possible metric reconstruction of the 3D map and of the camera
trajectory. Within a Bayesian framework, it integrates observations from a
deep-learning based generic object detector and a prior on the evolution of the
scale drift. For each observation class, a predefined prior on the heights of
the class objects is used. This allows to define the observations likelihood.
Due to the scale drift inherent to monocular SLAM systems, we integrate a rough
model on the dynamics of scale drift. Quantitative evaluations of the system
are presented on the KITTI dataset, and compared with different approaches. The
results show a superior performance of our proposal in terms of relative
translational error when compared to other monocular systems.
",Computer Science,Computer Science; Statistics
"Affine Rough Models   The goal of this survey article is to explain and elucidate the affine
structure of recent models appearing in the rough volatility literature, and
show how it leads to exponential-affine transform formulas.
",Quantitative Finance,Mathematics; Statistics
"Network Structure of Two-Dimensional Decaying Isotropic Turbulence   The present paper reports on our effort to characterize vortical interactions
in complex fluid flows through the use of network analysis. In particular, we
examine the vortex interactions in two-dimensional decaying isotropic
turbulence and find that the vortical interaction network can be characterized
by a weighted scale-free network. It is found that the turbulent flow network
retains its scale-free behavior until the characteristic value of circulation
reaches a critical value. Furthermore, we show that the two-dimensional
turbulence network is resilient against random perturbations but can be greatly
influenced when forcing is focused towards the vortical structures that are
categorized as network hubs. These findings can serve as a network-analytic
foundation to examine complex geophysical and thin-film flows and take
advantage of the rapidly growing field of network theory, which complements
ongoing turbulence research based on vortex dynamics, hydrodynamic stability,
and statistics. While additional work is essential to extend the mathematical
tools from network analysis to extract deeper physical insights of turbulence,
an understanding of turbulence based on the interaction-based network-theoretic
framework presents a promising alternative in turbulence modeling and control
efforts.
",Physics,Physics
"Comparing Aggregators for Relational Probabilistic Models   Relational probabilistic models have the challenge of aggregation, where one
variable depends on a population of other variables. Consider the problem of
predicting gender from movie ratings; this is challenging because the number of
movies per user and users per movie can vary greatly. Surprisingly, aggregation
is not well understood. In this paper, we show that existing relational models
(implicitly or explicitly) either use simple numerical aggregators that lose
great amounts of information, or correspond to naive Bayes, logistic
regression, or noisy-OR that suffer from overconfidence. We propose new simple
aggregators and simple modifications of existing models that empirically
outperform the existing ones. The intuition we provide on different (existing
or new) models and their shortcomings plus our empirical findings promise to
form the foundation for future representations.
",Computer Science; Statistics,Computer Science; Statistics
"Economic Design of Memory-Type Control Charts: The Fallacy of the Formula Proposed by Lorenzen and Vance (1986)   The memory-type control charts, such as EWMA and CUSUM, are powerful tools
for detecting small quality changes in univariate and multivariate processes.
Many papers on economic design of these control charts use the formula proposed
by Lorenzen and Vance (1986) [Lorenzen, T. J., & Vance, L. C. (1986). The
economic design of control charts: A unified approach. Technometrics, 28(1),
3-10, DOI: 10.2307/1269598]. This paper shows that this formula is not correct
for memory-type control charts and its values can significantly deviate from
the original values even if the ARL values used in this formula are accurately
computed. Consequently, the use of this formula can result in charts that are
not economically optimal. The formula is corrected for memory-type control
charts, but unfortunately the modified formula is not a helpful tool from a
computational perspective. We show that simulation-based optimization is a
possible alternative method.
",Computer Science; Statistics,Computer Science
"Methodological variations in lagged regression for detecting physiologic drug effects in EHR data   We studied how lagged linear regression can be used to detect the physiologic
effects of drugs from data in the electronic health record (EHR). We
systematically examined the effect of methodological variations ((i) time
series construction, (ii) temporal parameterization, (iii) intra-subject
normalization, (iv) differencing (lagged rates of change achieved by taking
differences between consecutive measurements), (v) explanatory variables, and
(vi) regression models) on performance of lagged linear methods in this
context. We generated two gold standards (one knowledge-base derived, one
expert-curated) for expected pairwise relationships between 7 drugs and 4 labs,
and evaluated how the 64 unique combinations of methodological perturbations
reproduce gold standards. Our 28 cohorts included patients in Columbia
University Medical Center/NewYork-Presbyterian Hospital clinical database. The
most accurate methods achieved AUROC of 0.794 for knowledge-base derived gold
standard (95%CI [0.741, 0.847]) and 0.705 for expert-curated gold standard (95%
CI [0.629, 0.781]). We observed a 0.633 mean AUROC (95%CI [0.610, 0.657],
expert-curated gold standard) across all methods that re-parameterize time
according to sequence and use either a joint autoregressive model with
differencing or an independent lag model without differencing. The complement
of this set of methods achieved a mean AUROC close to 0.5, indicating the
importance of these choices. We conclude that time- series analysis of EHR data
will likely rely on some of the beneficial pre-processing and modeling
methodologies identified, and will certainly benefit from continued careful
analysis of methodological perturbations. This study found that methodological
variations, such as pre-processing and representations, significantly affect
results, exposing the importance of evaluating these components when comparing
machine-learning methods.
",Statistics; Quantitative Biology,Statistics
"Henri Bénard: Thermal convection and vortex shedding   We present in this article the work of Henri Bénard (1874-1939), French
physicist who began the systematic experimental study of two hydrodynamic
systems: the thermal convection of fluids heated from below (the
Rayleigh-Bénard convection and the Bénard-Marangoni convection) and the
periodical vortex shedding behind a bluff body in a flow (the
Bénard-Kármán vortex street). Across his scientific biography, we review
the interplay between experiments and theory in these two major subjects of
fluid mechanics.
",Physics,Physics
"High-temperature charge density wave correlations in La$_{1.875}$Ba$_{0.125}$CuO$_{4}$ without spin-charge locking   Although all superconducting cuprates display charge-ordering tendencies,
their low-temperature properties are distinct, impeding efforts to understand
the phenomena within a single conceptual framework. While some systems exhibit
stripes of charge and spin, with a locked periodicity, others host charge
density waves (CDWs) without any obviously related spin order. Here we use
resonant inelastic x-ray scattering (RIXS) to follow the evolution of charge
correlations in the canonical stripe ordered cuprate
La$_{1.875}$Ba$_{0.125}$CuO$_{4}$ (LBCO~$1/8$) across its ordering transition.
We find that high-temperature charge correlations are unlocked from the
wavevector of the spin correlations, signaling analogies to CDW phases in
various other cuprates. This indicates that stripe order at low temperatures is
stabilized by the coupling of otherwise independent charge and spin density
waves, with important implications for the relation between charge and spin
correlations in the cuprates.
",Physics,Physics
"Statistical properties of an enstrophy conserving discretisation for the stochastic quasi-geostrophic equation   A framework of variational principles for stochastic fluid dynamics was
presented by Holm (2015), and these stochastic equations were also derived by
Cotter et al. (2017). We present a conforming finite element discretisation for
the stochastic quasi-geostrophic equation that was derived from this framework.
The discretisation preserves the first two moments of potential vorticity, i.e.
the mean potential vorticity and the enstrophy. Following the work of Dubinkina
and Frank (2007), who investigated the statistical mechanics of discretisations
of the deterministic quasi-geostrophic equation, we investigate the statistical
mechanics of our discretisation of the stochastic quasi-geostrophic equation.
We compare the statistical properties of our discretisation with the Gibbs
distribution under assumption of these conserved quantities, finding that there
is agreement between the statistics under a wide range of set-ups.
",Physics,Physics; Mathematics
"Semi-supervised Conditional GANs   We introduce a new model for building conditional generative models in a
semi-supervised setting to conditionally generate data given attributes by
adapting the GAN framework. The proposed semi-supervised GAN (SS-GAN) model
uses a pair of stacked discriminators to learn the marginal distribution of the
data, and the conditional distribution of the attributes given the data
respectively. In the semi-supervised setting, the marginal distribution (which
is often harder to learn) is learned from the labeled + unlabeled data, and the
conditional distribution is learned purely from the labeled data. Our
experimental results demonstrate that this model performs significantly better
compared to existing semi-supervised conditional GAN models.
",Computer Science; Statistics,Statistics
"Local-global principles in circle packings   We generalize work of Bourgain-Kontorovich and Zhang, proving an almost
local-to-global property for the curvatures of certain circle packings, to a
large class of Kleinian groups. Specifically, we associate in a natural way an
infinite family of integral packings of circles to any Kleinian group $\mathcal
A\leq\textrm{PSL}_2(K)$ satisfying certain conditions, where $K$ is an
imaginary quadratic field, and show that the curvatures of the circles in any
such packing satisfy an almost local-to-global principle. A key ingredient in
the proof of this is that $\mathcal A$ possesses a spectral gap property, which
we prove for any infinite-covolume, geometrically finite, Zariski dense
Kleinian group in $\textrm{PSL}_2(\mathcal{O}_K)$ containing a Zariski dense
subgroup of $\textrm{PSL}_2(\mathbb{Z})$.
",Mathematics,Mathematics
"Robust Gaussian Stochastic Process Emulation   We consider estimation of the parameters of a Gaussian Stochastic Process
(GaSP), in the context of emulation (approximation) of computer models for
which the outcomes are real-valued scalars. The main focus is on estimation of
the GaSP parameters through various generalized maximum likelihood methods,
mostly involving finding posterior modes; this is because full Bayesian
analysis in computer model emulation is typically prohibitively expensive. The
posterior modes that are studied arise from objective priors, such as the
reference prior. These priors have been studied in the literature for the
situation of an isotropic covariance function or under the assumption of
separability in the design of inputs for model runs used in the GaSP
construction. In this paper, we consider more general designs (e.g., a Latin
Hypercube Design) with a class of commonly used anisotropic correlation
functions, which can be written as a product of isotropic correlation
functions, each having an unknown range parameter and a fixed roughness
parameter. We discuss properties of the objective priors and marginal
likelihoods for the parameters of the GaSP and establish the posterior
propriety of the GaSP parameters, but our main focus is to demonstrate that
certain parameterizations result in more robust estimation of the GaSP
parameters than others, and that some parameterizations that are in common use
should clearly be avoided. These results are applicable to many frequently used
covariance functions, e.g., power exponential, Mat{é}rn, rational quadratic
and spherical covariance. We also generalize the results to the GaSP model with
a nugget parameter. Both theoretical and numerical evidence is presented
concerning the performance of the studied procedures.
",Mathematics; Statistics,Computer Science; Mathematics; Statistics
"Pohozaev identity for the fractional $p-$Laplacian on $\mathbb{R}^N$   By virtue of a suitable approximation argument, we prove a Pohozaev identity
for nonlinear nonlocal problems on $\mathbb{R}^N$ involving the fractional
$p-$Laplacian operator. Furthermore we provide an application of the identity
to show that some relevant levels of the energy functional associated with the
problem coincide.
",Mathematics,Mathematics
"Crystal field excitations and magnons: their roles in oxyselenides Pr2O2M2OSe2 (M = Mn, Fe)   We present the results of neutron scattering experiments to study the crystal
and magnetic structures of the Mott-insulating transition metal oxyselenides
Pr2O2M2OSe2 (M = Mn, Fe). The structural role of the non-Kramers Pr3+ ion is
investigated and analysis of Pr3+ crystal field excitations performed.
Long-range order of Pr3+ moments in Pr2O2Fe2OSe2 can be induced by an applied
magnetic field.
",Physics,Physics
"Like trainer, like bot? Inheritance of bias in algorithmic content moderation   The internet has become a central medium through which `networked publics'
express their opinions and engage in debate. Offensive comments and personal
attacks can inhibit participation in these spaces. Automated content moderation
aims to overcome this problem using machine learning classifiers trained on
large corpora of texts manually annotated for offence. While such systems could
help encourage more civil debate, they must navigate inherently normatively
contestable boundaries, and are subject to the idiosyncratic norms of the human
raters who provide the training data. An important objective for platforms
implementing such measures might be to ensure that they are not unduly biased
towards or against particular norms of offence. This paper provides some
exploratory methods by which the normative biases of algorithmic content
moderation systems can be measured, by way of a case study using an existing
dataset of comments labelled for offence. We train classifiers on comments
labelled by different demographic subsets (men and women) to understand how
differences in conceptions of offence between these groups might affect the
performance of the resulting models on various test sets. We conclude by
discussing some of the ethical choices facing the implementers of algorithmic
moderation systems, given various desired levels of diversity of viewpoints
amongst discussion participants.
",Computer Science,Computer Science; Statistics
"Driver Action Prediction Using Deep (Bidirectional) Recurrent Neural Network   Advanced driver assistance systems (ADAS) can be significantly improved with
effective driver action prediction (DAP). Predicting driver actions early and
accurately can help mitigate the effects of potentially unsafe driving
behaviors and avoid possible accidents. In this paper, we formulate driver
action prediction as a timeseries anomaly prediction problem. While the anomaly
(driver actions of interest) detection might be trivial in this context,
finding patterns that consistently precede an anomaly requires searching for or
extracting features across multi-modal sensory inputs. We present such a driver
action prediction system, including a real-time data acquisition, processing
and learning framework for predicting future or impending driver action. The
proposed system incorporates camera-based knowledge of the driving environment
and the driver themselves, in addition to traditional vehicle dynamics. It then
uses a deep bidirectional recurrent neural network (DBRNN) to learn the
correlation between sensory inputs and impending driver behavior achieving
accurate and high horizon action prediction. The proposed system performs
better than other existing systems on driver action prediction tasks and can
accurately predict key driver actions including acceleration, braking, lane
change and turning at durations of 5sec before the action is executed by the
driver.
",Computer Science; Statistics,Computer Science
"Modular System for Shelves and Coasts (MOSSCO v1.0) - a flexible and multi-component framework for coupled coastal ocean ecosystem modelling   Shelf and coastal sea processes extend from the atmosphere through the water
column and into the sea bed. These processes are driven by physical, chemical,
and biological interactions at local scales, and they are influenced by
transport and cross strong spatial gradients. The linkages between domains and
many different processes are not adequately described in current model systems.
Their limited integration level in part reflects lacking modularity and
flexibility; this shortcoming hinders the exchange of data and model components
and has historically imposed supremacy of specific physical driver models. We
here present the Modular System for Shelves and Coasts (MOSSCO,
this http URL), a novel domain and process coupling system
tailored---but not limited--- to the coupling challenges of and applications in
the coastal ocean. MOSSCO builds on the existing coupling technology Earth
System Modeling Framework and on the Framework for Aquatic Biogeochemical
Models, thereby creating a unique level of modularity in both domain and
process coupling; the new framework adds rich metadata, flexible scheduling,
configurations that allow several tens of models to be coupled, and tested
setups for coastal coupled applications. That way, MOSSCO addresses the
technology needs of a growing marine coastal Earth System community that
encompasses very different disciplines, numerical tools, and research
questions.
",Physics,Physics
"Phase-diagram and dynamics of Rydberg-dressed fermions in two-dimensions   We investigate the ground-state properties and the collective modes of a
two-dimensional two-component Rydberg-dressed Fermi liquid in the
dipole-blockade regime. We find instability of the homogeneous system toward
phase separated and density ordered phases, using the Hartree-Fock and
random-phase approximations, respectively. The spectral weight of collective
density oscillations in the homogenous phase also signals the emergence of
density-wave instability. We examine the effect of exchange-hole on the
density-wave instability and on the collective mode dispersion using the
Hubbard local-field factor.
",Physics,Physics
"Stable Limit Theorems for Empirical Processes under Conditional Neighborhood Dependence   This paper introduces a new concept of stochastic dependence among many
random variables which we call conditional neighborhood dependence (CND).
Suppose that there are a set of random variables and a set of sigma algebras
where both sets are indexed by the same set endowed with a neighborhood system.
When the set of random variables satisfies CND, any two non-adjacent sets of
random variables are conditionally independent given sigma algebras having
indices in one of the two sets' neighborhood. Random variables with CND include
those with conditional dependency graphs and a class of Markov random fields
with a global Markov property. The CND property is useful for modeling
cross-sectional dependence governed by a complex, large network. This paper
provides two main results. The first result is a stable central limit theorem
for a sum of random variables with CND. The second result is a Donsker-type
result of stable convergence of empirical processes indexed by a class of
functions satisfying a certain bracketing entropy condition when the random
variables satisfy CND.
",Mathematics; Statistics,Mathematics
"Terrestrial effects of moderately nearby supernovae   Recent data indicate one or more moderately nearby supernovae in the early
Pleistocene, with additional events likely in the Miocene. This has motivated
more detailed computations, using new information about the nature of
supernovae and the distances of these events to describe in more detail the
sorts of effects that are indicated at the Earth. This short
communication/review is designed to describe some of these effects so that they
may possibly be related to changes in the biota around these times.
",Physics,Physics
"The perceived assortativity of social networks: Methodological problems and solutions   Networks describe a range of social, biological and technical phenomena. An
important property of a network is its degree correlation or assortativity,
describing how nodes in the network associate based on their number of
connections. Social networks are typically thought to be distinct from other
networks in being assortative (possessing positive degree correlations);
well-connected individuals associate with other well-connected individuals, and
poorly-connected individuals associate with each other. We review the evidence
for this in the literature and find that, while social networks are more
assortative than non-social networks, only when they are built using
group-based methods do they tend to be positively assortative. Non-social
networks tend to be disassortative. We go on to show that connecting
individuals due to shared membership of a group, a commonly used method, biases
towards assortativity unless a large enough number of censuses of the network
are taken. We present a number of solutions to overcoming this bias by drawing
on advances in sociological and biological fields. Adoption of these methods
across all fields can greatly enhance our understanding of social networks and
networks in general.
",Computer Science; Statistics,Computer Science; Physics
"Waveform and Spectrum Management for Unmanned Aerial Systems Beyond 2025   The application domains of civilian unmanned aerial systems (UASs) include
agriculture, exploration, transportation, and entertainment. The expected
growth of the UAS industry brings along new challenges: Unmanned aerial vehicle
(UAV) flight control signaling requires low throughput, but extremely high
reliability, whereas the data rate for payload data can be significant. This
paper develops UAV number projections and concludes that small and micro UAVs
will dominate the US airspace with accelerated growth between 2028 and 2032. We
analyze the orthogonal frequency division multiplexing (OFDM) waveform because
it can provide the much needed flexibility, spectral efficiency, and,
potentially, reliability and derive suitable OFDM waveform parameters as a
function of UAV flight characteristics. OFDM also lends itself to agile
spectrum access. Based on our UAV growth predictions, we conclude that dynamic
spectrum access is needed and discuss the applicability of spectrum sharing
techniques for future UAS communications.
",Computer Science,Computer Science
"Inhomogeneous Heisenberg Spin Chain and Quantum Vortex Filament as Non-Holonomically Deformed NLS Systems   Through the Hasimoto map, various dynamical systems can be mapped to
different integrodifferential generalizations of Nonlinear Schrodinger (NLS)
family of equations some of which are known to be integrable. Two such
continuum limits, corresponding to the inhomogeneous XXX Heisenberg spin chain
[Balakrishnan, J. Phys. C 15, L1305 (1982)] and that of a thin vortex filament
moving in a superfluid with drag [Shivamoggi, Eur. Phys. J. B 86, 275 (2013)
86; Van Gorder, Phys. Rev. E 91, 053201 (2015)], are shown to be particular
non-holonomic deformations (NHDs) of the standard NLS system involving
generalized parameterizations. Crucially, such NHDs of the NLS system are
restricted to specific spectral orders that exactly complements NHDs of the
original physical systems. The specific non-holonomic constraints associated
with these integrodifferential generalizations additionally posses distinct
semi-classical signature.
",Physics,Physics
"Irreducible network backbones: unbiased graph filtering via maximum entropy   Networks provide an informative, yet non-redundant description of complex
systems only if links represent truly dyadic relationships that cannot be
directly traced back to node-specific properties such as size, importance, or
coordinates in some embedding space. In any real-world network, some links may
be reducible, and others irreducible, to such local properties. This dichotomy
persists despite the steady increase in data availability and resolution, which
actually determines an even stronger need for filtering techniques aimed at
discerning essential links from non-essential ones. Here we introduce a
rigorous method that, for any desired level of statistical significance,
outputs the network backbone that is irreducible to the local properties of
nodes, i.e. their degrees and strengths. Unlike previous approaches, our method
employs an exact maximum-entropy formulation guaranteeing that the filtered
network encodes only the links that cannot be inferred from local information.
Extensive empirical analysis confirms that this approach uncovers essential
backbones that are otherwise hidden amidst many redundant relationships and
inaccessible to other methods. For instance, we retrieve the hub-and-spoke
skeleton of the US airport network and many specialised patterns of
international trade. Being irreducible to local transportation and economic
constraints of supply and demand, these backbones single out genuinely
higher-order wiring principles.
",Computer Science; Physics,Computer Science; Physics
"The Taipan Galaxy Survey: Scientific Goals and Observing Strategy   Taipan is a multi-object spectroscopic galaxy survey starting in 2017 that
will cover 2pi steradians over the southern sky, and obtain optical spectra for
about two million galaxies out to z<0.4. Taipan will use the newly-refurbished
1.2m UK Schmidt Telescope at Siding Spring Observatory with the new TAIPAN
instrument, which includes an innovative 'Starbugs' positioning system capable
of rapidly and simultaneously deploying up to 150 spectroscopic fibres (and up
to 300 with a proposed upgrade) over the 6-deg diameter focal plane, and a
purpose-built spectrograph operating from 370 to 870nm with resolving power
R>2000. The main scientific goals of Taipan are: (i) to measure the distance
scale of the Universe (primarily governed by the local expansion rate, H_0) to
1% precision, and the structure growth rate of structure to 5%; (ii) to make
the most extensive map yet constructed of the mass distribution and motions in
the local Universe, using peculiar velocities based on improved Fundamental
Plane distances, which will enable sensitive tests of gravitational physics;
and (iii) to deliver a legacy sample of low-redshift galaxies as a unique
laboratory for studying galaxy evolution as a function of mass and environment.
The final survey, which will be completed within 5 years, will consist of a
complete magnitude-limited sample (i<17) of about 1.2x10^6 galaxies,
supplemented by an extension to higher redshifts and fainter magnitudes
(i<18.1) of a luminous red galaxy sample of about 0.8x10^6 galaxies.
Observations and data processing will be carried out remotely and in a
fully-automated way, using a purpose-built automated 'virtual observer'
software and an automated data reduction pipeline. The Taipan survey is
deliberately designed to maximise its legacy value, by complementing and
enhancing current and planned surveys of the southern sky at wavelengths from
the optical to the radio.
",Physics,Physics
"Network support of talented people   Network support is a key success factor for talented people. As an example,
the Hungarian Talent Support Network involves close to 1500 Talent Points and
more than 200,000 people. This network started the Hungarian Templeton Program
identifying and helping 315 exceptional cognitive talents. This network is a
part of the European Talent Support Network initiated by the European Council
for High Ability involving more than 300 organizations in over 30 countries in
Europe and extending in other continents. These networks are giving good
examples that talented people often occupy a central, but highly dynamic
position in social networks. The involvement of such 'creative nodes' in
network-related decision making processes is vital, especially in novel
environmental challenges. Such adaptive/learning responses characterize a large
variety of complex systems from proteins, through brains to society. It is
crucial for talent support programs to use these networking and learning
processes to increase their efficiency further.
",Computer Science; Physics,Computer Science; Statistics
"Obstructions to a small hyperbolicity in Helly graphs   It is known that for every graph $G$ there exists the smallest Helly graph
$\cal H(G)$ into which $G$ isometrically embeds ($\cal H(G)$ is called the
injective hull of $G$) such that the hyperbolicity of $\cal H(G)$ is equal to
the hyperbolicity of $G$. Motivated by this, we investigate structural
properties of Helly graphs that govern their hyperbolicity and identify three
isometric subgraphs of the King-grid as structural obstructions to a small
hyperbolicity in Helly graphs.
",Computer Science,Mathematics
"Linear Progress with Exponential Decay in Weakly Hyperbolic Groups   A random walk $w_n$ on a separable, geodesic hyperbolic metric space $X$
converges to the boundary $\partial X$ with probability one when the step
distribution supports two independent loxodromics. In particular, the random
walk makes positive linear progress. Progress is known to be linear with
exponential decay when (1) the step distribution has exponential tail and (2)
the action on $X$ is acylindrical. We extend exponential decay to the
non-acylindrical case.
",Mathematics,Mathematics
"Crime Prediction by Data-Driven Green's Function method   We develop an algorithm that forecasts cascading events, by employing a
Green's function scheme on the basis of the self-exciting point process model.
This method is applied to open data of 10 types of crimes happened in Chicago.
It shows a good prediction accuracy superior to or comparable to the standard
methods which are the expectation-maximization method and prospective hotspot
maps method. We find a cascade influence of the crimes that has a long-time,
logarithmic tail; this result is consistent with an earlier study on
burglaries. This long-tail feature cannot be reproduced by the other standard
methods. In addition, a merit of the Green's function method is the low
computational cost in the case of high density of events and/or large amount of
the training data.
",Computer Science; Physics; Statistics,Computer Science; Statistics
"Social versus Moral preferences in the Ultimatum Game: A theoretical model and an experiment   In the Ultimatum Game (UG) one player, named ""proposer"", has to decide how to
allocate a certain amount of money between herself and a ""responder"". If the
offer is greater than or equal to the responder's minimum acceptable offer
(MAO), then the money is split as proposed, otherwise, neither the proposer nor
the responder get anything. The UG has intrigued generations of behavioral
scientists because people in experiments blatantly violate the equilibrium
predictions that self-interested proposers offer the minimum available non-zero
amount, and self-interested responders accept. Why are these predictions
violated? Previous research has mainly focused on the role of social
preferences. Little is known about the role of general moral preferences for
doing the right thing, preferences that have been shown to play a major role in
other social interactions (e.g., Dictator Game and Prisoner's Dilemma). Here I
develop a theoretical model and an experiment designed to pit social
preferences against moral preferences. I find that, although people recognize
that offering half and rejecting low offers are the morally right things to do,
moral preferences have no causal impact on UG behavior. The experimental data
are indeed well fit by a model according to which: (i) high UG offers are
motivated by inequity aversion and, to a lesser extent, self-interest; (ii)
high MAOs are motivated by inequity aversion.
",Quantitative Biology,Computer Science
"Focusing light through dynamical samples using fast closed-loop wavefront optimization   We describe a fast closed-loop optimization wavefront shaping system able to
focus light through dynamic scattering media. A MEMS-based spatial light
modulator (SLM), a fast photodetector and FPGA electronics are combined to
implement a closed-loop optimization of a wavefront with a single mode
optimization rate of 4.1 kHz. The system performances are demonstrated by
focusing light through colloidal solutions of TiO2 particles in glycerol with
tunable temporal stability.
",Physics,Physics
"Cell Tracking via Proposal Generation and Selection   Microscopy imaging plays a vital role in understanding many biological
processes in development and disease. The recent advances in automation of
microscopes and development of methods and markers for live cell imaging has
led to rapid growth in the amount of image data being captured. To efficiently
and reliably extract useful insights from these captured sequences, automated
cell tracking is essential. This is a challenging problem due to large
variation in the appearance and shapes of cells depending on many factors
including imaging methodology, biological characteristics of cells, cell matrix
composition, labeling methodology, etc. Often cell tracking methods require a
sequence-specific segmentation method and manual tuning of many tracking
parameters, which limits their applicability to sequences other than those they
are designed for. In this paper, we propose 1) a deep learning based cell
proposal method, which proposes candidates for cells along with their scores,
and 2) a cell tracking method, which links proposals in adjacent frames in a
graphical model using edges representing different cellular events and poses
joint cell detection and tracking as the selection of a subset of cell and edge
proposals. Our method is completely automated and given enough training data
can be applied to a wide variety of microscopy sequences. We evaluate our
method on multiple fluorescence and phase contrast microscopy sequences
containing cells of various shapes and appearances from ISBI cell tracking
challenge, and show that our method outperforms existing cell tracking methods.
Code is available at: this https URL
",Computer Science,Computer Science
"Maslov, Chern-Weil and Mean Curvature   We provide an integral formula for the Maslov index of a pair $(E,F)$ over a
surface $\Sigma$, where $E\rightarrow\Sigma$ is a complex vector bundle and
$F\subset E_{|\partial\Sigma}$ is a totally real subbundle. As in Chern-Weil
theory, this formula is written in terms of the curvature of $E$ plus a
boundary contribution.
When $(E,F)$ is obtained via an immersion of $(\Sigma,\partial\Sigma)$ into a
pair $(M,L)$ where $M$ is Kähler and $L$ is totally real, the formula allows
us to control the Maslov index in terms of the geometry of $(M,L)$. We exhibit
natural conditions on $(M,L)$ which lead to bounds and monotonicity results.
",Mathematics,Mathematics
"Hybrid Normed Ideal Perturbations of n-tuples of Operators I   In hybrid normed ideal perturbations of $n$-tuples of operators, the normed
ideal is allowed to vary with the component operators. We begin extending to
this setting the machinery we developed for normed ideal perturbations based on
the modulus of quasicentral approximation and an adaptation of our
non-commutative generalization of the Weyl--von~Neumann theorem. For commuting
$n$-tuples of hermitian operators, the modulus of quasicentral approximation
remains essentially the same when $\cC_n^-$ is replaced by a hybrid $n$-tuple
$\cC_{p_1,\dots}^-,\dots,\cC^-_{p_n}$, $p_1^{-1} + \dots + p_n^{-1} = 1$. The
proof involves singular integrals of mixed homogeneity.
",Mathematics,Mathematics
"Thick Subcategories of the stable category of modules over the exterior algebra I   We study thick subcategories defined by modules of complexity one in
$\underline{\md}R$, where $R$ is the exterior algebra in $n+1$ indeterminates.
",Mathematics,Mathematics
"A Constrained Coupled Matrix-Tensor Factorization for Learning Time-evolving and Emerging Topics   Topic discovery has witnessed a significant growth as a field of data mining
at large. In particular, time-evolving topic discovery, where the evolution of
a topic is taken into account has been instrumental in understanding the
historical context of an emerging topic in a dynamic corpus. Traditionally,
time-evolving topic discovery has focused on this notion of time. However,
especially in settings where content is contributed by a community or a crowd,
an orthogonal notion of time is the one that pertains to the level of expertise
of the content creator: the more experienced the creator, the more advanced the
topic. In this paper, we propose a novel time-evolving topic discovery method
which, in addition to the extracted topics, is able to identify the evolution
of that topic over time, as well as the level of difficulty of that topic, as
it is inferred by the level of expertise of its main contributors. Our method
is based on a novel formulation of Constrained Coupled Matrix-Tensor
Factorization, which adopts constraints well-motivated for, and, as we
demonstrate, are essential for high-quality topic discovery. We qualitatively
evaluate our approach using real data from the Physics and also Programming
Stack Exchange forum, and we were able to identify topics of varying levels of
difficulty which can be linked to external events, such as the announcement of
gravitational waves by the LIGO lab in Physics forum. We provide a quantitative
evaluation of our method by conducting a user study where experts were asked to
judge the coherence and quality of the extracted topics. Finally, our proposed
method has implications for automatic curriculum design using the extracted
topics, where the notion of the level of difficulty is necessary for the proper
modeling of prerequisites and advanced concepts.
",Statistics,Computer Science; Physics
"Bulk crystalline optomechanics   Brillouin processes couple light and sound through optomechanical three-wave
interactions. Within bulk solids, this coupling is mediated by the intrinsic
photo-elastic material response yielding coherent emission of high frequency
(GHz) acoustic phonons. This same interaction produces strong optical
nonlinearities that overtake both Raman or Kerr nonlinearities in practically
all solids. In this paper, we show that the strength and character of Brillouin
interactions are radically altered at low temperatures when the phonon
coherence length surpasses the system size. In this limit, the solid becomes a
coherent optomechanical system with macroscopic (cm-scale) phonon modes
possessing large ($60\ \mu \rm{g}$) motional masses. These phonon modes, which
are formed by shaping the surfaces of the crystal into a confocal phononic
resonator, yield appreciable optomechanical coupling rates (${\sim}100$ Hz),
providing access to ultra-high $Q$-factor ($4.2{\times}10^7$) phonon modes at
high ($12$ GHz) carrier frequencies. The single-pass nonlinear optical
susceptibility is enhanced from its room temperature value by more than four
orders of magnitude. Through use of bulk properties, rather than
nano-structural control, this comparatively simple approach is enticing for the
ability to engineer optomechanical coupling at high frequencies and with high
power handling. In contrast to cavity optomechanics, we show that this system
yields a unique form of dispersive symmetry breaking that enables selective
phonon heating or cooling without an optical cavity (i.e., cavity-less
optomechanics). Extending these results, practically any transparent
crystalline material can be shaped into an optomechanical system as the basis
for materials spectroscopy, new regimes of laser physics, precision metrology,
quantum information processing, and for studies of macroscopic quantum
coherence.
",Physics,Physics
"Deterministic and Randomized Diffusion based Iterative Generalized Hard Thresholding (DiFIGHT) for Distributed Sparse Signal Recovery   In this paper, we propose a distributed iterated hard thresholding algorithm
termed DiFIGHT over a network that is built on the diffusion mechanism and also
propose a modification of the proposed algorithm termed MoDiFIGHT, that has low
complexity in terms of communication in the network. We additionally propose
four different strategies termed RP, RNP, RGPr, and RGNPr that are used to
randomly select a subset of nodes that are subsequently activated to take part
in the distributed algorithm, so as to reduce the mean number of communications
during the run of the distributed algorithm. We present theoretical estimates
of the long run communication per unit time for these different strategies,
when used by the two proposed algorithms. Also, we present an analysis of the
two proposed algorithms and provide provable bounds on their recovery
performance with or without using the random node selection strategies.
Finally, we use numerical studies to show that both when the random strategies
are used as well as when they are not used, the proposed algorithms display
performances far superior to distributed IHT algorithm using consensus
mechanism.
",Computer Science,Computer Science
"Relational Algebra for In-Database Process Mining   The execution logs that are used for process mining in practice are often
obtained by querying an operational database and storing the result in a flat
file. Consequently, the data processing power of the database system cannot be
used anymore for this information, leading to constrained flexibility in the
definition of mining patterns and limited execution performance in mining large
logs. Enabling process mining directly on a database - instead of via
intermediate storage in a flat file - therefore provides additional flexibility
and efficiency. To help facilitate this ideal of in-database process mining,
this paper formally defines a database operator that extracts the 'directly
follows' relation from an operational database. This operator can both be used
to do in-database process mining and to flexibly evaluate process mining
related queries, such as: ""which employee most frequently changes the 'amount'
attribute of a case from one task to the next"". We define the operator using
the well-known relational algebra that forms the formal underpinning of
relational databases. We formally prove equivalence properties of the operator
that are useful for query optimization and present time-complexity properties
of the operator. By doing so this paper formally defines the necessary
relational algebraic elements of a 'directly follows' operator, which are
required for implementation of such an operator in a DBMS.
",Computer Science,Computer Science
"On discrimination between two close distribution tails   The goodness-of-fit test for discrimination of two tail distribution using
higher order statistics is proposed. The consistency of proposed test is proved
for two different alternatives. We do not assume belonging the corresponding
distribution function to a maximum domain of attraction.
",Mathematics; Statistics,Mathematics; Statistics
"Sparsity constrained split feasibility for dose-volume constraints in inverse planning of intensity-modulated photon or proton therapy   A split feasibility formulation for the inverse problem of
intensity-modulated radiation therapy (IMRT) treatment planning with
dose-volume constraints (DVCs) included in the planning algorithm is presented.
It involves a new type of sparsity constraint that enables the inclusion of a
percentage-violation constraint in the model problem and its handling by
continuous (as opposed to integer) methods. We propose an iterative algorithmic
framework for solving such a problem by applying the feasibility-seeking
CQ-algorithm of Byrne combined with the automatic relaxation method (ARM) that
uses cyclic projections. Detailed implementation instructions are furnished.
Functionality of the algorithm was demonstrated through the creation of an
intensity-modulated proton therapy plan for a simple 2D C-shaped geometry and
also for a realistic base-of-skull chordoma treatment site. Monte Carlo
simulations of proton pencil beams of varying energy were conducted to obtain
dose distributions for the 2D test case. A research release of the Pinnacle3
proton treatment planning system was used to extract pencil beam doses for a
clinical base-of-skull chordoma case. In both cases the beamlet doses were
calculated to satisfy dose-volume constraints according to our new algorithm.
Examination of the dose-volume histograms following inverse planning with our
algorithm demonstrated that it performed as intended. The application of our
proposed algorithm to dose-volume constraint inverse planning was successfully
demonstrated. Comparison with optimized dose distributions from the research
release of the Pinnacle3 treatment planning system showed the algorithm could
achieve equivalent or superior results.
",Physics; Mathematics,Physics
"Resolving the notorious case of conical intersections for coupled cluster dynamics   The motion of electrons and nuclei in photochemical events often involve
conical intersections, degeneracies between electronic states. They serve as
funnels for nuclear relaxation - on the femtosecond scale - in processes where
the electrons and nuclei couple nonadiabatically. Accurate ab initio quantum
chemical models are essential for interpreting experimental measurements of
such phenomena. In this paper we resolve a long-standing problem in coupled
cluster theory, presenting the first formulation of the theory that correctly
describes conical intersections between excited electronic states of the same
symmetry. This new development demonstrates that the highly accurate coupled
cluster theory can be applied to describe dynamics on excited electronic states
involving conical intersections.
",Physics,Physics
"Closed-loop field development optimization with multipoint geostatistics and statistical assessment   Closed-loop field development (CLFD) optimization is a comprehensive
framework for optimal development of subsurface resources. CLFD involves three
major steps: 1) optimization of full development plan based on current set of
models, 2) drilling new wells and collecting new spatial and temporal
(production) data, 3) model calibration based on all data. This process is
repeated until the optimal number of wells is drilled. This work introduces an
efficient CLFD implementation for complex systems described by multipoint
geostatistics (MPS). Model calibration is accomplished in two steps:
conditioning to spatial data by a geostatistical simulation method, and
conditioning to production data by optimization-based PCA. A statistical
procedure is presented to assess the performance of CLFD. Methodology is
applied to an oil reservoir example for 25 different true-model cases.
Application of a single-step of CLFD, improved the true NPV in 64%--80% of
cases. The full CLFD procedure (with three steps) improved the true NPV in 96%
of cases, with an average improvement of 37%.
",Computer Science; Statistics,Statistics
"Gravitational octree code performance evaluation on Volta GPU   In this study, the gravitational octree code originally optimized for the
Fermi, Kepler, and Maxwell GPU architectures is adapted to the Volta
architecture. The Volta architecture introduces independent thread scheduling
requiring either the insertion of the explicit synchronizations at appropriate
locations or the enforcement of the same implicit synchronizations as do the
Pascal or earlier architectures by specifying \texttt{-gencode
arch=compute\_60,code=sm\_70}. The performance measurements on Tesla V100, the
current flagship GPU by NVIDIA, revealed that the $N$-body simulations of the
Andromeda galaxy model with $2^{23} = 8388608$ particles took $3.8 \times
10^{-2}$~s or $3.3 \times 10^{-2}$~s per step for each case. Tesla V100
achieves a 1.4 to 2.2-fold acceleration in comparison with Tesla P100, the
flagship GPU in the previous generation. The observed speed-up of 2.2 is
greater than 1.5, which is the ratio of the theoretical peak performance of the
two GPUs. The independence of the units for integer operations from those for
floating-point number operations enables the overlapped execution of integer
and floating-point number operations. It hides the execution time of the
integer operations leading to the speed-up rate above the theoretical peak
performance ratio. Tesla V100 can execute $N$-body simulation with up to $25
\times 2^{20} = 26214400$ particles, and it took $2.0 \times 10^{-1}$~s per
step. It corresponds to $3.5$~TFlop/s, which is 22\% of the single-precision
theoretical peak performance.
",Computer Science,Physics
"Action Robust Reinforcement Learning and Applications in Continuous Control   A policy is said to be robust if it maximizes the reward while considering a
bad, or even adversarial, model. In this work we formalize two new criteria of
robustness to action uncertainty. Specifically, we consider two scenarios in
which the agent attempts to perform an action $\mathbf{a}$, and (i) with
probability $\alpha$, an alternative adversarial action $\bar{\mathbf{a}}$ is
taken, or (ii) an adversary adds a perturbation to the selected action in the
case of continuous action space. We show that our criteria are related to
common forms of uncertainty in robotics domains, such as the occurrence of
abrupt forces, and suggest algorithms in the tabular case. Building on the
suggested algorithms, we generalize our approach to deep reinforcement learning
(DRL) and provide extensive experiments in the various MuJoCo domains. Our
experiments show that not only does our approach produce robust policies, but
it also improves the performance in the absence of perturbations. This
generalization indicates that action-robustness can be thought of as implicit
regularization in RL problems.
",Computer Science; Statistics,Computer Science; Statistics
"Does Your Phone Know Your Touch?   This paper explores supervised techniques for continuous anomaly detection
from biometric touch screen data. A capacitive sensor array used to mimic a
touch screen as used to collect touch and swipe gestures from participants. The
gestures are recorded over fixed segments of time, with position and force
measured for each gesture. Support Vector Machine, Logistic Regression, and
Gaussian mixture models were tested to learn individual touch patterns. Test
results showed true negative and true positive scores of over 95% accuracy for
all gesture types, with logistic regression models far outperforming the other
methods. A more expansive and varied data collection over longer periods of
time is needed to determine pragmatic usage of these results.
",Statistics,Computer Science
"Emergence of Structured Behaviors from Curiosity-Based Intrinsic Motivation   Infants are experts at playing, with an amazing ability to generate novel
structured behaviors in unstructured environments that lack clear extrinsic
reward signals. We seek to replicate some of these abilities with a neural
network that implements curiosity-driven intrinsic motivation. Using a simple
but ecologically naturalistic simulated environment in which the agent can move
and interact with objects it sees, the agent learns a world model predicting
the dynamic consequences of its actions. Simultaneously, the agent learns to
take actions that adversarially challenge the developing world model, pushing
the agent to explore novel and informative interactions with its environment.
We demonstrate that this policy leads to the self-supervised emergence of a
spectrum of complex behaviors, including ego motion prediction, object
attention, and object gathering. Moreover, the world model that the agent
learns supports improved performance on object dynamics prediction and
localization tasks. Our results are a proof-of-principle that computational
models of intrinsic motivation might account for key features of developmental
visuomotor learning in infants.
",Statistics,Computer Science; Statistics
"App Store 2.0: From Crowd Information to Actionable Feedback in Mobile Ecosystems   Given the increasing competition in mobile app ecosystems, improving the
experience of users has become a major goal for app vendors. This article
introduces a visionary app store, called APP STORE 2.0, which exploits
crowdsourced information about apps, devices and users to increase the overall
quality of the delivered mobile apps. We sketch a blueprint architecture of the
envisioned app stores and discuss the different kinds of actionable feedbacks
that app stores can generate using crowdsourced information.
",Computer Science,Computer Science
"Okapi: Causally Consistent Geo-Replication Made Faster, Cheaper and More Available   Okapi is a new causally consistent geo-replicated key- value store. Okapi
leverages two key design choices to achieve high performance. First, it relies
on hybrid logical/physical clocks to achieve low latency even in the presence
of clock skew. Second, Okapi achieves higher resource efficiency and better
availability, at the expense of a slight increase in update visibility latency.
To this end, Okapi implements a new stabilization protocol that uses a
combination of vector and scalar clocks and makes a remote update visible when
its delivery has been acknowledged by every data center. We evaluate Okapi with
different workloads on Amazon AWS, using three geographically distributed
regions and 96 nodes. We compare Okapi with two recent approaches to causal
consistency, Cure and GentleRain. We show that Okapi delivers up to two orders
of magnitude better performance than GentleRain and that Okapi achieves up to
3.5x lower latency and a 60% reduction of the meta-data overhead with respect
to Cure.
",Computer Science,Computer Science
"Network of vertically c-oriented prism shaped InN nanowalls grown on c-GaN/sapphire template by chemical vapor deposition technique   Networks of vertically c-oriented prism shaped InN nanowalls, are grown on
c-GaN/sapphire templates using a CVD technique, where pure indium and ammonia
are used as metal and nitrogen precursors. A systematic study of the growth,
structural and electronic properties of these samples shows a preferential
growth of the islands along [11-20] and [0001] directions leading to the
formation of such a network structure, where the vertically [0001] oriented
tapered walls are laterally align along one of the three [11-20] directions.
Inclined facets of these walls are identified as r-planes [(1-102)-planes] of
wurtzite InN. Onset of absorption for these samples is observed to be higher
than the band gap of InN suggesting a high background carrier concentration in
this material. Study of the valence band edge through XPS indicates the
formation of positive depletion regions below the r-plane side facets of the
walls. This is in contrast with the observation for c-plane InN epilayers,
where electron accumulation is often reported below the top surface.
",Physics,Physics
"On families of fibred knots with equal Seifert forms   For every genus $g\geq 2$, we construct an infinite family of strongly
quasipositive fibred knots having the same Seifert form as the torus knot
$T(2,2g+1)$. In particular, their signatures and four-genera are maximal and
their homological monodromies (hence their Alexander module structures) agree.
On the other hand, the geometric stretching factors are pairwise distinct and
the knots are pairwise not ribbon concordant.
",Mathematics,Mathematics
"Underapproximation of Reach-Avoid Sets for Discrete-Time Stochastic Systems via Lagrangian Methods   We examine Lagrangian techniques for computing underapproximations of
finite-time horizon, stochastic reach-avoid level-sets for discrete-time,
nonlinear systems. We use the concept of reachability of a target tube in the
control literature to define robust reach-avoid sets which are parameterized by
the target set, safe set, and the set in which the disturbance is drawn from.
We unify two existing Lagrangian approaches to compute these sets and establish
that there exists an optimal control policy of the robust reach-avoid sets
which is a Markov policy. Based on these results, we characterize the subset of
the disturbance space whose corresponding robust reach-avoid set for the given
target and safe set is a guaranteed underapproximation of the stochastic
reach-avoid level-set of interest. The proposed approach dramatically improves
the computational efficiency for obtaining an underapproximation of stochastic
reach-avoid level-sets when compared to the traditional approaches based on
gridding. Our method, while conservative, does not rely on a grid, implying
scalability as permitted by the known computational geometry constraints. We
demonstrate the method on two examples: a simple two-dimensional integrator,
and a space vehicle rendezvous-docking problem.
",Computer Science; Mathematics,Computer Science
"Mixed penalization in convolutive nonnegative matrix factorization for blind speech dereverberation   When a signal is recorded in an enclosed room, it typically gets affected by
reverberation. This degradation represents a problem when dealing with audio
signals, particularly in the field of speech signal processing, such as
automatic speech recognition. Although there are some approaches to deal with
this issue that are quite satisfactory under certain conditions, constructing a
method that works well in a general context still poses a significant
challenge. In this article, we propose a method based on convolutive
nonnegative matrix factorization that mixes two penalizers in order to impose
certain characteristics over the time-frequency components of the restored
signal and the reverberant components. An algorithm for implementing the method
is described and tested. Comparisons of the results against those obtained with
state of the art methods are presented, showing significant improvement.
",Computer Science,Computer Science
"Recursive Whitening Transformation for Speaker Recognition on Language Mismatched Condition   Recently in speaker recognition, performance degradation due to the channel
domain mismatched condition has been actively addressed. However, the
mismatches arising from language is yet to be sufficiently addressed. This
paper proposes an approach which employs recursive whitening transformation to
mitigate the language mismatched condition. The proposed method is based on the
multiple whitening transformation, which is intended to remove un-whitened
residual components in the dataset associated with i-vector length
normalization. The experiments were conducted on the Speaker Recognition
Evaluation 2016 trials of which the task is non-English speaker recognition
using development dataset consist of both a large scale out-of-domain (English)
dataset and an extremely low-quantity in-domain (non-English) dataset. For
performance comparison, we develop a state-of- the-art system using deep neural
network and bottleneck feature, which is based on a phonetically aware model.
From the experimental results, along with other prior studies, effectiveness of
the proposed method on language mismatched condition is validated.
",Computer Science,Computer Science
"Variational approach for learning Markov processes from time series data   Inference, prediction and control of complex dynamical systems from time
series is important in many areas, including financial markets, power grid
management, climate and weather modeling, or molecular dynamics. The analysis
of such highly nonlinear dynamical systems is facilitated by the fact that we
can often find a (generally nonlinear) transformation of the system coordinates
to features in which the dynamics can be excellently approximated by a linear
Markovian model. Moreover, the large number of system variables often change
collectively on large time- and length-scales, facilitating a low-dimensional
analysis in feature space. In this paper, we introduce a variational approach
for Markov processes (VAMP) that allows us to find optimal feature mappings and
optimal Markovian models of the dynamics from given time series data. The key
insight is that the best linear model can be obtained from the top singular
components of the Koopman operator. This leads to the definition of a family of
score functions called VAMP-r which can be calculated from data, and can be
employed to optimize a Markovian model. In addition, based on the relationship
between the variational scores and approximation errors of Koopman operators,
we propose a new VAMP-E score, which can be applied to cross-validation for
hyper-parameter optimization and model selection in VAMP. VAMP is valid for
both reversible and nonreversible processes and for stationary and
non-stationary processes or realizations.
",Statistics,Computer Science; Statistics
"JADE: Joint Autoencoders for Dis-Entanglement   The problem of feature disentanglement has been explored in the literature,
for the purpose of image and video processing and text analysis.
State-of-the-art methods for disentangling feature representations rely on the
presence of many labeled samples. In this work, we present a novel method for
disentangling factors of variation in data-scarce regimes. Specifically, we
explore the application of feature disentangling for the problem of supervised
classification in a setting where few labeled samples exist, and there are no
unlabeled samples for use in unsupervised training. Instead, a similar datasets
exists which shares at least one direction of variation with the
sample-constrained datasets. We train our model end-to-end using the framework
of variational autoencoders and are able to experimentally demonstrate that
using an auxiliary dataset with similar variation factors contribute positively
to classification performance, yielding competitive results with the
state-of-the-art in unsupervised learning.
",Computer Science; Statistics,Computer Science; Statistics
"Excitable behaviors   This chapter revisits the concept of excitability, a basic system property of
neurons. The focus is on excitable systems regarded as behaviors rather than
dynamical systems. By this we mean open systems modulated by specific
interconnection properties rather than closed systems classified by their
parameter ranges. Modeling, analysis, and synthesis questions can be formulated
in the classical language of circuit theory. The input-output characterization
of excitability is in terms of the local sensitivity of the current-voltage
relationship. It suggests the formulation of novel questions for non-linear
system theory, inspired by questions from experimental neurophysiology.
",Computer Science,Quantitative Biology
"LandmarkBoost: Efficient Visual Context Classifiers for Robust Localization   The growing popularity of autonomous systems creates a need for reliable and
efficient metric pose retrieval algorithms. Currently used approaches tend to
rely on nearest neighbor search of binary descriptors to perform the 2D-3D
matching and guarantee realtime capabilities on mobile platforms. These methods
struggle, however, with the growing size of the map, changes in viewpoint or
appearance, and visual aliasing present in the environment. The rigidly defined
descriptor patterns only capture a limited neighborhood of the keypoint and
completely ignore the overall visual context.
We propose LandmarkBoost - an approach that, in contrast to the conventional
2D-3D matching methods, casts the search problem as a landmark classification
task. We use a boosted classifier to classify landmark observations and
directly obtain correspondences as classifier scores. We also introduce a
formulation of visual context that is flexible, efficient to compute, and can
capture relationships in the entire image plane. The original binary
descriptors are augmented with contextual information and informative features
are selected by the boosting framework. Through detailed experiments, we
evaluate the retrieval quality and performance of LandmarkBoost, demonstrating
that it outperforms common state-of-the-art descriptor matching methods.
",Computer Science,Computer Science
"Improving Network Robustness against Adversarial Attacks with Compact Convolution   Though Convolutional Neural Networks (CNNs) have surpassed human-level
performance on tasks such as object classification and face verification, they
can easily be fooled by adversarial attacks. These attacks add a small
perturbation to the input image that causes the network to misclassify the
sample. In this paper, we focus on neutralizing adversarial attacks by compact
feature learning. In particular, we show that learning features in a closed and
bounded space improves the robustness of the network. We explore the effect of
L2-Softmax Loss, that enforces compactness in the learned features, thus
resulting in enhanced robustness to adversarial perturbations. Additionally, we
propose compact convolution, a novel method of convolution that when
incorporated in conventional CNNs improves their robustness. Compact
convolution ensures feature compactness at every layer such that they are
bounded and close to each other. Extensive experiments show that Compact
Convolutional Networks (CCNs) neutralize multiple types of attacks, and perform
better than existing methods in defending adversarial attacks, without
incurring any additional training overhead compared to CNNs.
",Computer Science; Statistics,Computer Science
"Effect algebras as presheaves on finite Boolean algebras   For an effect algebra $A$, we examine the category of all morphisms from
finite Boolean algebras into $A$. This category can be described as a category
of elements of a presheaf $R(A)$ on the category of finite Boolean algebras. We
prove that some properties (being an orthoalgebra, the Riesz decomposition
property, being a Boolean algebra) of an effect algebra $A$ can be
characterized by properties of the category of elements of the presheaf $R(A)$.
We prove that the tensor product of of effect algebras arises as a left Kan
extension of the free product of finite Boolean algebras along the inclusion
functor. As a consequence, the tensor product of effect algebras can be
expressed by means of the Day convolution of presheaves on finite Boolean
algebras.
",Mathematics,Mathematics
"A note on Weyl groups and crystallographic root lattices   We follow the dual approach to Coxeter systems and show for Weyl groups a
criterium which decides whether a set of reflections is generating the group
depending on the root and the coroot lattice. Further we study special
generating sets involving a parabolic subgroup and show that they are very
tame.
",Mathematics,Mathematics
"Multitarget search on complex networks: A logarithmic growth of global mean random cover time   We investigate multitarget search on complex networks and derive an exact
expression for the mean random cover time that quantifies the expected time a
walker needs to visit multiple targets. Based on this, we recover and extend
some interesting results of multitarget search on networks. Specifically, we
observe the logarithmic increase of the global mean random cover time with the
target number for a broad range of random search processes, including generic
random walks, biased random walks, and maximal entropy random walks. We show
that the logarithmic growth pattern is a universal feature of multi-target
search on networks by using the annealed network approach and the
Sherman-Morrison formula. Moreover, we find that for biased random walks, the
global mean random cover time can be minimized, and that the corresponding
optimal parameter also minimizes the global mean first passage time, pointing
towards its robustness. Our findings further confirm that the logarithmic
growth pattern is a universal law governing multitarget search in confined
media.
",Computer Science; Physics,Computer Science; Mathematics
"Convolutional Neural Networks for Page Segmentation of Historical Document Images   This paper presents a Convolutional Neural Network (CNN) based page
segmentation method for handwritten historical document images. We consider
page segmentation as a pixel labeling problem, i.e., each pixel is classified
as one of the predefined classes. Traditional methods in this area rely on
carefully hand-crafted features or large amounts of prior knowledge. In
contrast, we propose to learn features from raw image pixels using a CNN. While
many researchers focus on developing deep CNN architectures to solve different
problems, we train a simple CNN with only one convolution layer. We show that
the simple architecture achieves competitive results against other deep
architectures on different public datasets. Experiments also demonstrate the
effectiveness and superiority of the proposed method compared to previous
methods.
",Computer Science; Statistics,Computer Science
"Evidential Deep Learning to Quantify Classification Uncertainty   Deterministic neural nets have been shown to learn effective predictors on a
wide range of machine learning problems. However, as the standard approach is
to train the network to minimize a prediction loss, the resultant model remains
ignorant to its prediction confidence. Orthogonally to Bayesian neural nets
that indirectly infer prediction uncertainty through weight uncertainties, we
propose explicit modeling of the same using the theory of subjective logic. By
placing a Dirichlet distribution on the class probabilities, we treat
predictions of a neural net as subjective opinions and learn the function that
collects the evidence leading to these opinions by a deterministic neural net
from data. The resultant predictor for a multi-class classification problem is
another Dirichlet distribution whose parameters are set by the continuous
output of a neural net. We provide a preliminary analysis on how the
peculiarities of our new loss function drive improved uncertainty estimation.
We observe that our method achieves unprecedented success on detection of
out-of-distribution queries and endurance against adversarial perturbations.
",Statistics,Computer Science; Statistics
"An enthalpy-based multiple-relaxation-time lattice Boltzmann method for solid-liquid phase change heat transfer in metal foams   In this paper, an enthalpy-based multiple-relaxation-time (MRT) lattice
Boltzmann (LB) method is developed for solid-liquid phase change heat transfer
in metal foams under local thermal non-equilibrium (LTNE) condition. The
enthalpy-based MRT-LB method consists of three different MRT-LB models: one for
flow field based on the generalized non-Darcy model, and the other two for
phase change material (PCM) and metal foam temperature fields described by the
LTNE model. The moving solid-liquid phase interface is implicitly tracked
through the liquid fraction, which is simultaneously obtained when the energy
equations of PCM and metal foam are solved. The present method has several
distinctive features. First, as compared with previous studies, the present
method avoids the iteration procedure, thus it retains the inherent merits of
the standard LB method and is superior over the iteration method in terms of
accuracy and computational efficiency. Second, a volumetric LB scheme instead
of the bounce-back scheme is employed to realize the no-slip velocity condition
in the interface and solid phase regions, which is consistent with the actual
situation. Last but not least, the MRT collision model is employed, and with
additional degrees of freedom, it has the ability to reduce the numerical
diffusion across phase interface induced by solid-liquid phase change.
Numerical tests demonstrate that the present method can be served as an
accurate and efficient numerical tool for studying metal foam enhanced
solid-liquid phase change heat transfer in latent heat storage. Finally,
comparisons and discussions are made to offer useful information for practical
applications of the present method.
",Physics,Physics
"Cohomology of the flag variety under PBW degenerations   PBW degenerations are a particularly nice family of flat degenerations of
type A flag varieties. We show that the cohomology of any PBW degeneration of
the flag variety surjects onto the cohomology of the original flag variety, and
that this holds in an equivariant setting too. We also prove that the same is
true in the symplectic setting when considering Feigin's linear degeneration of
the symplectic flag variety.
",Mathematics,Mathematics
"A Bag-of-Paths Node Criticality Measure   This work compares several node (and network) criticality measures
quantifying to which extend each node is critical with respect to the
communication flow between nodes of the network, and introduces a new measure
based on the Bag-of-Paths (BoP) framework. Network disconnection simulation
experiments show that the new BoP measure outperforms all the other measures on
a sample of Erdos-Renyi and Albert-Barabasi graphs. Furthermore, a faster
(still O(n^3)), approximate, BoP criticality relying on the Sherman-Morrison
rank-one update of a matrix is introduced for tackling larger networks. This
approximate measure shows similar performances as the original, exact, one.
",Computer Science; Physics,Computer Science; Statistics
"DeepSketch2Face: A Deep Learning Based Sketching System for 3D Face and Caricature Modeling   Face modeling has been paid much attention in the field of visual computing.
There exist many scenarios, including cartoon characters, avatars for social
media, 3D face caricatures as well as face-related art and design, where
low-cost interactive face modeling is a popular approach especially among
amateur users. In this paper, we propose a deep learning based sketching system
for 3D face and caricature modeling. This system has a labor-efficient
sketching interface, that allows the user to draw freehand imprecise yet
expressive 2D lines representing the contours of facial features. A novel CNN
based deep regression network is designed for inferring 3D face models from 2D
sketches. Our network fuses both CNN and shape based features of the input
sketch, and has two independent branches of fully connected layers generating
independent subsets of coefficients for a bilinear face representation. Our
system also supports gesture based interactions for users to further manipulate
initial face models. Both user studies and numerical results indicate that our
sketching system can help users create face models quickly and effectively. A
significantly expanded face database with diverse identities, expressions and
levels of exaggeration is constructed to promote further research and
evaluation of face modeling techniques.
",Computer Science,Computer Science
"Colouring perfect graphs with bounded clique number   A graph is perfect if the chromatic number of every induced subgraph equals
the size of its largest clique, and an algorithm of Grötschel, Lovász, and
Schrijver from 1988 finds an optimal colouring of a perfect graph in polynomial
time. But this algorithm uses the ellipsoid method, and it is a well-known open
question to construct a ""combinatorial"" polynomial-time algorithm that yields
an optimal colouring of a perfect graph.
A skew partition in $G$ is a partition $(A,B)$ of $V(G)$ such that $G[A]$ is
not connected and $\bar{G}[B]$ is not connected, where $\bar{G}$ denotes the
complement graph ; and it is balanced if an additional parity condition of
paths in $G$ and $\bar{G}$ is satisfied.
In this paper we first give a polynomial-time algorithm that, with input a
perfect graph, outputs a balanced skew partition if there is one. Then we use
this to obtain a combinatorial algorithm that finds an optimal colouring of a
perfect graph with clique number $k$, in time that is polynomial for fixed $k$.
",Computer Science,Computer Science
"Measuring bot and human behavioral dynamics   Bots, social media accounts controlled by software rather than by humans,
have recently been under the spotlight for their association with various forms
of online manipulation. To date, much work has focused on social bot detection,
but little attention has been devoted to the characterization and measurement
of the behavior and activity of bots, as opposed to humans'. Over the course of
the years, bots have become more sophisticated, and capable to reflect some
short-term behavior, emulating that of human users. The goal of this paper is
to study the behavioral dynamics that bots exhibit over the course of one
activity session, and highlight if and how these differ from human activity
signatures. By using a large Twitter dataset associated with recent political
events, we first separate bots and humans, then isolate their activity
sessions. We compile a list of quantities to be measured, like the propensity
of users to engage in social interactions or to produce content. Our analysis
highlights the presence of short-term behavioral trends in humans, which can be
associated with a cognitive origin, that are absent in bots, intuitively due to
their automated activity. These findings are finally codified to create and
evaluate a machine learning algorithm to detect activity sessions produced by
bots and humans, to allow for more nuanced bot detection strategies.
",Computer Science,Computer Science
"A spin-gapped Mott insulator with the dimeric arrangement of twisted molecules Zn(tmdt)$_{2}$   $^{13}$C nuclear magnetic resonance measurements were performed for a
single-component molecular material Zn(tmdt)$_{2}$, in which tmdt's form an
arrangement similar to the so-called ${\kappa}$-type molecular packing in
quasi-two-dimensional Mott insulators and superconductors. Detailed analysis of
the powder spectra uncovered local spin susceptibility in the tmdt ${\pi}$
orbitals. The obtained shift and relaxation rate revealed the singlet-triplet
excitations of the ${\pi}$ spins, indicating that Zn(tmdt)$_{2}$ is a
spin-gapped Mott insulator with exceptionally large electron correlations
compared to conventional molecular Mott systems.
",Physics,Physics
"A temperate exo-Earth around a quiet M dwarf at 3.4 parsecs   The combination of high-contrast imaging and high-dispersion spectroscopy,
which has successfully been used to detect the atmosphere of a giant planet, is
one of the most promising potential probes of the atmosphere of Earth-size
worlds. The forthcoming generation of extremely large telescopes (ELTs) may
obtain sufficient contrast with this technique to detect O$_2$ in the
atmosphere of those worlds that orbit low-mass M dwarfs. This is strong
motivation to carry out a census of planets around cool stars for which
habitable zones can be resolved by ELTs, i.e. for M dwarfs within $\sim$5
parsecs. Our HARPS survey has been a major contributor to that sample of nearby
planets. Here we report on our radial velocity observations of Ross 128
(Proxima Virginis, GJ447, HIP 57548), an M4 dwarf just 3.4 parsec away from our
Sun. This source hosts an exo-Earth with a projected mass $m \sin i = 1.35
M_\oplus$ and an orbital period of 9.9 days. Ross 128 b receives $\sim$1.38
times as much flux as Earth from the Sun and its equilibrium ranges in
temperature between 269 K for an Earth-like albedo and 213 K for a Venus-like
albedo. Recent studies place it close to the inner edge of the conventional
habitable zone. An 80-day long light curve from K2 campaign C01 demonstrates
that Ross~128~b does not transit. Together with the All Sky Automated Survey
(ASAS) photometry and spectroscopic activity indices, the K2 photometry shows
that Ross 128 rotates slowly and has weak magnetic activity. In a habitability
context, this makes survival of its atmosphere against erosion more likely.
Ross 128 b is the second closest known exo-Earth, after Proxima Centauri b (1.3
parsec), and the closest temperate planet known around a quiet star. The 15 mas
planet-star angular separation at maximum elongation will be resolved by ELTs
($>$ 3$\lambda/D$) in the optical bands of O$_2$.
",Physics,Physics
"GlobeNet: Convolutional Neural Networks for Typhoon Eye Tracking from Remote Sensing Imagery   Advances in remote sensing technologies have made it possible to use
high-resolution visual data for weather observation and forecasting tasks. We
propose the use of multi-layer neural networks for understanding complex
atmospheric dynamics based on multichannel satellite images. The capability of
our model was evaluated by using a linear regression task for single typhoon
coordinates prediction. A specific combination of models and different
activation policies enabled us to obtain an interesting prediction result in
the northeastern hemisphere (ENH).
",Computer Science,Computer Science
"Enhancing Stratified Graph Sampling Algorithms based on Approximate Degree Distribution   Sampling technique has become one of the recent research focuses in the
graph-related fields. Most of the existing graph sampling algorithms tend to
sample the high degree or low degree nodes in the complex networks because of
the characteristic of scale-free. Scale-free means that degrees of different
nodes are subject to a power law distribution. So, there is a significant
difference in the degrees between the overall sampling nodes. In this paper, we
propose an idea of approximate degree distribution and devise a stratified
strategy using it in the complex networks. We also develop two graph sampling
algorithms combining the node selection method with the stratified strategy.
The experimental results show that our sampling algorithms preserve several
properties of different graphs and behave more accurately than other
algorithms. Further, we prove the proposed algorithms are superior to the
off-the-shelf algorithms in terms of the unbiasedness of the degrees and more
efficient than state-of-the-art FFS and ES-i algorithms.
",Computer Science,Computer Science; Statistics
"Description of the evolution of inhomogeneities on a Dark Matter halo with the Vlasov equation   We use a direct numerical integration of the Vlasov equation in spherical
symmetry with a background gravitational potential to determine the evolution
of a collection of particles in different models of a galactic halo. Such a
collection is assumed to represent a dark matter inhomogeneity which reaches a
stationary state determined by the virialization of the system. We describe
some features of the stationary states and, by using several halo models,
obtain distinctive signatures for the evolution of the inhomogeneities in each
of the models.
",Physics,Physics
"Adaptive Estimation for Nonlinear Systems using Reproducing Kernel Hilbert Spaces   This paper extends a conventional, general framework for online adaptive
estimation problems for systems governed by unknown nonlinear ordinary
differential equations. The central feature of the theory introduced in this
paper represents the unknown function as a member of a reproducing kernel
Hilbert space (RKHS) and defines a distributed parameter system (DPS) that
governs state estimates and estimates of the unknown function. This paper 1)
derives sufficient conditions for the existence and stability of the infinite
dimensional online estimation problem, 2) derives existence and stability of
finite dimensional approximations of the infinite dimensional approximations,
and 3) determines sufficient conditions for the convergence of finite
dimensional approximations to the infinite dimensional online estimates. A new
condition for persistency of excitation in a RKHS in terms of its evaluation
functionals is introduced in the paper that enables proof of convergence of the
finite dimensional approximations of the unknown function in the RKHS. This
paper studies two particular choices of the RKHS, those that are generated by
exponential functions and those that are generated by multiscale kernels
defined from a multiresolution analysis.
",Computer Science,Computer Science; Mathematics
"COREclust: a new package for a robust and scalable analysis of complex data   In this paper, we present a new R package COREclust dedicated to the
detection of representative variables in high dimensional spaces with a
potentially limited number of observations. Variable sets detection is based on
an original graph clustering strategy denoted CORE-clustering algorithm that
detects CORE-clusters, i.e. variable sets having a user defined size range and
in which each variable is very similar to at least another variable.
Representative variables are then robustely estimate as the CORE-cluster
centers. This strategy is entirely coded in C++ and wrapped by R using the Rcpp
package. A particular effort has been dedicated to keep its algorithmic cost
reasonable so that it can be used on large datasets. After motivating our work,
we will explain the CORE-clustering algorithm as well as a greedy extension of
this algorithm. We will then present how to use it and results obtained on
synthetic and real data.
",Statistics,Computer Science; Statistics
"Complete Cyclic Proof Systems for Inductive Entailments   In this paper we develop cyclic proof systems for the problem of inclusion
between the least sets of models of mutually recursive predicates, when the
ground constraints in the inductive definitions belong to the quantifier-free
fragments of (i) First Order Logic with the canonical Herbrand interpretation
and (ii) Separation Logic, respectively. Inspired by classical
automata-theoretic techniques of proving language inclusion between tree
automata, we give a small set of inference rules, that are proved to be sound
and complete, under certain semantic restrictions, involving the set of
constraints in the inductive system. Moreover, we investigate the decidability
and computational complexity of these restrictions for all the logical
fragments considered and provide a proof search semi-algorithm that becomes a
decision procedure for the entailment problem, for those systems that fulfill
the restrictions.
",Computer Science,Computer Science
"Nearly Instance Optimal Sample Complexity Bounds for Top-k Arm Selection   In the Best-$k$-Arm problem, we are given $n$ stochastic bandit arms, each
associated with an unknown reward distribution. We are required to identify the
$k$ arms with the largest means by taking as few samples as possible. In this
paper, we make progress towards a complete characterization of the
instance-wise sample complexity bounds for the Best-$k$-Arm problem. On the
lower bound side, we obtain a novel complexity term to measure the sample
complexity that every Best-$k$-Arm instance requires. This is derived by an
interesting and nontrivial reduction from the Best-$1$-Arm problem. We also
provide an elimination-based algorithm that matches the instance-wise lower
bound within doubly-logarithmic factors. The sample complexity of our algorithm
strictly dominates the state-of-the-art for Best-$k$-Arm (module constant
factors).
",Computer Science; Statistics,Computer Science; Statistics
"Towards a Deep Reinforcement Learning Approach for Tower Line Wars   There have been numerous breakthroughs with reinforcement learning in the
recent years, perhaps most notably on Deep Reinforcement Learning successfully
playing and winning relatively advanced computer games. There is undoubtedly an
anticipation that Deep Reinforcement Learning will play a major role when the
first AI masters the complicated game plays needed to beat a professional
Real-Time Strategy game player. For this to be possible, there needs to be a
game environment that targets and fosters AI research, and specifically Deep
Reinforcement Learning. Some game environments already exist, however, these
are either overly simplistic such as Atari 2600 or complex such as Starcraft II
from Blizzard Entertainment. We propose a game environment in between Atari
2600 and Starcraft II, particularly targeting Deep Reinforcement Learning
algorithm research. The environment is a variant of Tower Line Wars from
Warcraft III, Blizzard Entertainment. Further, as a proof of concept that the
environment can harbor Deep Reinforcement algorithms, we propose and apply a
Deep Q-Reinforcement architecture. The architecture simplifies the state space
so that it is applicable to Q-learning, and in turn improves performance
compared to current state-of-the-art methods. Our experiments show that the
proposed architecture can learn to play the environment well, and score 33%
better than standard Deep Q-learning which in turn proves the usefulness of the
game environment.
",Computer Science,Computer Science
"The unreasonable effectiveness of the forget gate   Given the success of the gated recurrent unit, a natural question is whether
all the gates of the long short-term memory (LSTM) network are necessary.
Previous research has shown that the forget gate is one of the most important
gates in the LSTM. Here we show that a forget-gate-only version of the LSTM
with chrono-initialized biases, not only provides computational savings but
outperforms the standard LSTM on multiple benchmark datasets and competes with
some of the best contemporary models. Our proposed network, the JANET, achieves
accuracies of 99% and 92.5% on the MNIST and pMNIST datasets, outperforming the
standard LSTM which yields accuracies of 98.5% and 91%.
",Statistics,Computer Science; Statistics
"One-step and Two-step Classification for Abusive Language Detection on Twitter   Automatic abusive language detection is a difficult but important task for
online social media. Our research explores a two-step approach of performing
classification on abusive language and then classifying into specific types and
compares it with one-step approach of doing one multi-class classification for
detecting sexist and racist languages. With a public English Twitter corpus of
20 thousand tweets in the type of sexism and racism, our approach shows a
promising performance of 0.827 F-measure by using HybridCNN in one-step and
0.824 F-measure by using logistic regression in two-steps.
",Computer Science,Computer Science
"VEGAS: A VST Early-type GAlaxy Survey. II. Photometric study of giant ellipticals and their stellar halos   Observations of diffuse starlight in the outskirts of galaxies are thought to
be a fundamental source of constraints on the cosmological context of galaxy
assembly in the $\Lambda$CDM model. Such observations are not trivial because
of the extreme faintness of such regions. In this work, we investigate the
photometric properties of six massive early type galaxies (ETGs) in the VEGAS
sample (NGC 1399, NGC 3923, NGC 4365, NGC 4472, NGC 5044, and NGC 5846) out to
extremely low surface brightness levels, with the goal of characterizing the
global structure of their light profiles for comparison to state-of-the-art
galaxy formation models. We carry out deep and detailed photometric mapping of
our ETG sample taking advantage of deep imaging with VST/OmegaCAM in the g and
i bands. By fitting the light profiles, and comparing the results to
simulations of elliptical galaxy assembly, we identify signatures of a
transition between ""relaxed"" and ""unrelaxed"" accreted components and can
constrain the balance between in situ and accreted stars. The very good
agreement of our results with predictions from theoretical simulations
demonstrates that the full VEGAS sample of $\sim 100$ ETGs will allow us to use
the distribution of diffuse light as a robust statistical probe of the
hierarchical assembly of massive galaxies.
",Physics,Physics
"Emission line galaxies behind the planetary nebula IC 5148: Potential for a serendipity survey with archival data   During the start of a survey program using FORS2 long slit spectroscopy on
planetary nebulae (PN) and their haloes, we serendipitously discovered six
background emission line galaxies (ELG) with redshifts of z = 0.2057, 0.3137,
0.37281, 0.4939, 0.7424 and 0.8668. Thus they clearly do not belong to a common
cluster structure. We derived the major physical properties of the targets.
Since the used long slit covers a sky area of only 570 arcsec^2, we discuss
further potential of serendipitous discoveries in archival data, beside the
deep systematic work of the ongoing and upcoming big surveys. We conclude that
archival data provide a decent potential for extending the overall data on ELGs
without any selection bias.
",Physics,Physics
"Community Aware Random Walk for Network Embedding   Social network analysis provides meaningful information about behavior of
network members that can be used for diverse applications such as
classification, link prediction. However, network analysis is computationally
expensive because of feature learning for different applications. In recent
years, many researches have focused on feature learning methods in social
networks. Network embedding represents the network in a lower dimensional
representation space with the same properties which presents a compressed
representation of the network. In this paper, we introduce a novel algorithm
named ""CARE"" for network embedding that can be used for different types of
networks including weighted, directed and complex. Current methods try to
preserve local neighborhood information of nodes, whereas the proposed method
utilizes local neighborhood and community information of network nodes to cover
both local and global structure of social networks. CARE builds customized
paths, which are consisted of local and global structure of network nodes, as a
basis for network embedding and uses the Skip-gram model to learn
representation vector of nodes. Subsequently, stochastic gradient descent is
applied to optimize our objective function and learn the final representation
of nodes. Our method can be scalable when new nodes are appended to network
without information loss. Parallelize generation of customized random walks is
also used for speeding up CARE. We evaluate the performance of CARE on multi
label classification and link prediction tasks. Experimental results on various
networks indicate that the proposed method outperforms others in both Micro and
Macro-f1 measures for different size of training data.
",Computer Science,Computer Science; Statistics
"New zirconium hydrides predicted by structure search method based on first principles calculations   The formation of precipitated zirconium (Zr) hydrides is closely related to
the hydrogen embrittlement problem for the cladding materials of pressured
water reactors (PWR). In this work, we systematically investigated the crystal
structures of zirconium hydride (ZrHx) with different hydrogen concentrations
(x = 0~2, atomic ratio) by combining the basin hopping algorithm with first
principles calculations. We conclude that the P3m1 {\zeta}-ZrH0.5 is
dynamically unstable, while a novel dynamically stable P3m1 ZrH0.5 structure
was discovered in the structure search. The stability of bistable P42/nnm
ZrH1.5 structures and I4/mmm ZrH2 structures are also revisited. We find that
the P42/nnm (c/a > 1) ZrH1.5 is dynamically unstable, while the I4/mmm (c/a =
1.57) ZrH2 is dynamically stable.The P42/nnm (c/a < 1) ZrH1.5 might be a key
intermediate phase for the transition of {\gamma}->{\delta}->{\epsilon} phases.
Additionally, by using the thermal dynamic simulations, we find that
{\delta}-ZrH1.5 is the most stable structure at high temperature while ZrH2 is
the most stable hydride at low temperature. Slow cooling process will promote
the formation of {\delta}-ZrH1.5, and fast cooling process will promote the
formation of {\gamma}-ZrH. These results may help to understand the phase
transitions of zirconium hydrides.
",Physics,Physics
"Towards Classification of Web ontologies using the Horizontal and Vertical Segmentation   The new era of the Web is known as the semantic Web or the Web of data. The
semantic Web depends on ontologies that are seen as one of its pillars. The
bigger these ontologies, the greater their exploitation. However, when these
ontologies become too big other problems may appear, such as the complexity to
charge big files in memory, the time it needs to download such files and
especially the time it needs to make reasoning on them. We discuss in this
paper approaches for segmenting such big Web ontologies as well as its
usefulness. The segmentation method extracts from an existing ontology a
segment that represents a layer or a generation in the existing ontology; i.e.
a horizontally extraction. The extracted segment should be itself an ontology.
",Computer Science,Computer Science
"An Automated Text Categorization Framework based on Hyperparameter Optimization   A great variety of text tasks such as topic or spam identification, user
profiling, and sentiment analysis can be posed as a supervised learning problem
and tackle using a text classifier. A text classifier consists of several
subprocesses, some of them are general enough to be applied to any supervised
learning problem, whereas others are specifically designed to tackle a
particular task, using complex and computational expensive processes such as
lemmatization, syntactic analysis, etc. Contrary to traditional approaches, we
propose a minimalistic and wide system able to tackle text classification tasks
independent of domain and language, namely microTC. It is composed by some easy
to implement text transformations, text representations, and a supervised
learning algorithm. These pieces produce a competitive classifier even in the
domain of informally written text. We provide a detailed description of microTC
along with an extensive experimental comparison with relevant state-of-the-art
methods. mircoTC was compared on 30 different datasets. Regarding accuracy,
microTC obtained the best performance in 20 datasets while achieves competitive
results in the remaining 10. The compared datasets include several problems
like topic and polarity classification, spam detection, user profiling and
authorship attribution. Furthermore, it is important to state that our approach
allows the usage of the technology even without knowledge of machine learning
and natural language processing.
",Computer Science; Statistics,Computer Science
"Do altmetrics correlate with the quality of papers? A large-scale empirical study based on F1000Prime data   In this study, we address the question whether (and to what extent,
respectively) altmetrics are related to the scientific quality of papers (as
measured by peer assessments). Only a few studies have previously investigated
the relationship between altmetrics and assessments by peers. In the first
step, we analyse the underlying dimensions of measurement for traditional
metrics (citation counts) and altmetrics - by using principal component
analysis (PCA) and factor analysis (FA). In the second step, we test the
relationship between the dimensions and quality of papers (as measured by the
post-publication peer-review system of F1000Prime assessments) - using
regression analysis. The results of the PCA and FA show that altmetrics operate
along different dimensions, whereas Mendeley counts are related to citation
counts, and tweets form a separate dimension. The results of the regression
analysis indicate that citation-based metrics and readership counts are
significantly more related to quality, than tweets. This result on the one hand
questions the use of Twitter counts for research evaluation purposes and on the
other hand indicates potential use of Mendeley reader counts.
",Computer Science,Computer Science; Statistics
"When Streams of Optofluidics Meet the Sea of Life   Luke P. Lee is a Tan Chin Tuan Centennial Professor at the National
University of Singapore. In this contribution he describes the power of
optofluidics as a research tool and reviews new insights within the areas of
single cell analysis, microphysiological analysis, and integrated systems.
",Quantitative Biology,Physics
"An Approximate Bayesian Long Short-Term Memory Algorithm for Outlier Detection   Long Short-Term Memory networks trained with gradient descent and
back-propagation have received great success in various applications. However,
point estimation of the weights of the networks is prone to over-fitting
problems and lacks important uncertainty information associated with the
estimation. However, exact Bayesian neural network methods are intractable and
non-applicable for real-world applications. In this study, we propose an
approximate estimation of the weights uncertainty using Ensemble Kalman Filter,
which is easily scalable to a large number of weights. Furthermore, we optimize
the covariance of the noise distribution in the ensemble update step using
maximum likelihood estimation. To assess the proposed algorithm, we apply it to
outlier detection in five real-world events retrieved from the Twitter
platform.
",Computer Science; Statistics,Computer Science; Statistics
"Compile-Time Symbolic Differentiation Using C++ Expression Templates   Template metaprogramming is a popular technique for implementing compile time
mechanisms for numerical computing. We demonstrate how expression templates can
be used for compile time symbolic differentiation of algebraic expressions in
C++ computer programs. Given a positive integer $N$ and an algebraic function
of multiple variables, the compiler generates executable code for the $N$th
partial derivatives of the function. Compile-time simplification of the
derivative expressions is achieved using recursive templates. A detailed
analysis indicates that current C++ compiler technology is already sufficient
for practical use of our results, and highlights a number of issues where
further improvements may be desirable.
",Computer Science,Computer Science
"Automatic White-Box Testing of First-Order Logic Ontologies   Formal ontologies are axiomatizations in a logic-based formalism. The
development of formal ontologies, and their important role in the Semantic Web
area, is generating considerable research on the use of automated reasoning
techniques and tools that help in ontology engineering. One of the main aims is
to refine and to improve axiomatizations for enabling automated reasoning tools
to efficiently infer reliable information. Defects in the axiomatization can
not only cause wrong inferences, but can also hinder the inference of expected
information, either by increasing the computational cost of, or even
preventing, the inference. In this paper, we introduce a novel, fully automatic
white-box testing framework for first-order logic ontologies. Our methodology
is based on the detection of inference-based redundancies in the given
axiomatization. The application of the proposed testing method is fully
automatic since a) the automated generation of tests is guided only by the
syntax of axioms and b) the evaluation of tests is performed by automated
theorem provers. Our proposal enables the detection of defects and serves to
certify the grade of suitability --for reasoning purposes-- of every axiom. We
formally define the set of tests that are generated from any axiom and prove
that every test is logically related to redundancies in the axiom from which
the test has been generated. We have implemented our method and used this
implementation to automatically detect several non-trivial defects that were
hidden in various first-order logic ontologies. Throughout the paper we provide
illustrative examples of these defects, explain how they were found, and how
each proof --given by an automated theorem-prover-- provides useful hints on
the nature of each defect. Additionally, by correcting all the detected
defects, we have obtained an improved version of one of the tested ontologies:
Adimen-SUMO.
",Computer Science,Computer Science
"Multilevel maximum likelihood estimation with application to covariance matrices   The asymptotic variance of the maximum likelihood estimate is proved to
decrease when the maximization is restricted to a subspace that contains the
true parameter value. Maximum likelihood estimation allows a systematic fitting
of covariance models to the sample, which is important in data assimilation.
The hierarchical maximum likelihood approach is applied to the spectral
diagonal covariance model with different parameterizations of eigenvalue decay,
and to the sparse inverse covariance model with specified parameter values on
different sets of nonzero entries. It is shown computationally that using
smaller sets of parameters can decrease the sampling noise in high dimension
substantially.
",Mathematics; Statistics,Mathematics; Statistics
"Sitatapatra: Blocking the Transfer of Adversarial Samples   Convolutional Neural Networks (CNNs) are widely used to solve classification
tasks in computer vision. However, they can be tricked into misclassifying
specially crafted `adversarial' samples -- and samples built to trick one model
often work alarmingly well against other models trained on the same task. In
this paper we introduce Sitatapatra, a system designed to block the transfer of
adversarial samples. It diversifies neural networks using a key, as in
cryptography, and provides a mechanism for detecting attacks. What's more, when
adversarial samples are detected they can typically be traced back to the
individual device that was used to develop them. The run-time overheads are
minimal permitting the use of Sitatapatra on constrained systems.
",Computer Science; Statistics,Computer Science; Statistics
"Wasserstein Soft Label Propagation on Hypergraphs: Algorithm and Generalization Error Bounds   Inspired by recent interests of developing machine learning and data mining
algorithms on hypergraphs, we investigate in this paper the semi-supervised
learning algorithm of propagating ""soft labels"" (e.g. probability
distributions, class membership scores) over hypergraphs, by means of optimal
transportation. Borrowing insights from Wasserstein propagation on graphs
[Solomon et al. 2014], we re-formulate the label propagation procedure as a
message-passing algorithm, which renders itself naturally to a generalization
applicable to hypergraphs through Wasserstein barycenters. Furthermore, in a
PAC learning framework, we provide generalization error bounds for propagating
one-dimensional distributions on graphs and hypergraphs using 2-Wasserstein
distance, by establishing the \textit{algorithmic stability} of the proposed
semi-supervised learning algorithm. These theoretical results also shed new
lights upon deeper understandings of the Wasserstein propagation on graphs.
",Statistics,Computer Science; Statistics
"LocalNysation: A bottom up approach to efficient localized kernel regression   We consider a localized approach in the well-established setting of
reproducing kernel learning under random design. The input space $X$ is
partitioned into local disjoint subsets $X_j$ ($j=1,...,m$) equipped with a
local reproducing kernel $K_j$. It is then straightforward to define local KRR
estimates. Our first main contribution is in showing that minimax optimal rates
of convergence are preserved if the number $m$ of partitions grows sufficiently
slowly with the sample size, under locally different degrees on smoothness
assumptions on the regression function. As a byproduct, we show that low
smoothness on exceptional sets of small probability does not contribute,
leading to a faster rate of convergence. Our second contribution lies in
showing that the partitioning approach for KRR can be efficiently combined with
local Nyström subsampling, improving computational cost twofold. If the
number of locally subsampled inputs grows sufficiently fast with the sample
size, minimax optimal rates of convergence are maintained.
",Mathematics; Statistics,Statistics
"Supersymmetric field theories and geometric Langlands: The other side of the coin   This note announces results on the relations between the approach of
Beilinson and Drinfeld to the geometric Langlands correspondence based on
conformal field theory, the approach of Kapustin and Witten based on $N=4$ SYM,
and the AGT-correspondence. The geometric Langlands correspondence is described
as the Nekrasov-Shatashvili limit of a generalisation of the AGT-correspondence
in the presence of surface operators. Following the approaches of Kapustin -
Witten and Nekrasov - Witten we interpret some aspects of the resulting picture
using an effective description in terms of two-dimensional sigma models having
Hitchin's moduli spaces as target-manifold.
",Mathematics,Mathematics
"A Penrose type inequaltiy for graphs over Reissner-Nordström-anti-deSitter manifold   In this paper, we use the inverse mean curvature flow to establish an optimal
Minkowski type inquality, weighted Alexandrov-Fenchel inequality for the mean
convex star shaped hypersurfaces in Reissner-Nordström-anti-deSitter manifold
and Penrose type inequality for asymptotically locally hyperbolic manifolds in
which can be realized as graphs over Reissner-Nordström-anti-deSitter
manifold.
",Mathematics,Mathematics
"Existence of Noise Induced Order, a Computer Aided Proof   We prove, by a computer aided proof, the existence of noise induced order in
the model of chaotic chemical reactions where it was first discovered
numerically by Matsumoto and Tsuda in 1983. We prove that in this random
dynamical system the increase in amplitude of the noise causes the Lyapunov
exponent to decrease from positive to negative, stabilizing the system. The
method used is based on a certified approximation of the stationary measure in
the $L^{1}$ norm. This is done by an efficient algorithm which is general
enough to be adapted to any piecewise differentiable dynamical system on the
interval with additive noise. We also prove that the stationary measure of the
system and its Lyapunov exponent have a Lipschitz stability under several kinds
of perturbation of the noise and of the system itself. The Lipschitz constants
of this stability result are also estimated explicitly.
",Mathematics,Physics
"A Survey of Runtime Monitoring Instrumentation Techniques   Runtime Monitoring is a lightweight and dynamic verification technique that
involves observing the internal operations of a software system and/or its
interactions with other external entities, with the aim of determining whether
the system satisfies or violates a correctness specification. Compilation
techniques employed in Runtime Monitoring tools allow monitors to be
automatically derived from high-level correctness specifications (aka.
properties). This allows the same property to be converted into different types
of monitors, which may apply different instrumentation techniques for checking
whether the property was satisfied or not. In this paper we compare and
contrast the various types of monitoring methodologies found in the current
literature, and classify them into a spectrum of monitoring instrumentation
techniques, ranging from completely asynchronous monitoring on the one end and
completely synchronous monitoring on the other.
",Computer Science,Computer Science
"Stochastic Feedback Control of Systems with Unknown Nonlinear Dynamics   This paper studies the stochastic optimal control problem for systems with
unknown dynamics. First, an open-loop deterministic trajectory optimization
problem is solved without knowing the explicit form of the dynamical system.
Next, a Linear Quadratic Gaussian (LQG) controller is designed for the nominal
trajectory-dependent linearized system, such that under a small noise
assumption, the actual states remain close to the optimal trajectory. The
trajectory-dependent linearized system is identified using input-output
experimental data consisting of the impulse responses of the nominal system. A
computational example is given to illustrate the performance of the proposed
approach.
",Computer Science,Computer Science
"Nonsequential double ionization of helium in IR+XUV two-color laser fields II: Collision-excitation ionization process   The collision-ionization mechanism of nonsequential double ionization (NSDI)
process in IR+XUV two-color laser fields [\PRA \textbf{93}, 043417 (2016)] has
been investigated by us recently. Here we extend this work to study the
collision-excitation-ionization (CEI) mechanism of NSDI processes in the
two-color laser fields with different laser conditions. It is found that the
CEI mechanism makes a dominant contribution to the NSDI as the XUV photon
energy is smaller than the ionization threshold of the He$^+$ ion, and the
momentum spectrum shows complex interference patterns and symmetrical
structures. By channel analysis, we find that, as the energy carried by the
recollision electron is not enough to excite the bound electron, the bound
electron will absorb XUV photons during their collision, as a result, both
forward and backward collisions make a comparable contributions to the NSDI
processes. However, it is found that, as the energy carried by the recollision
electron is large enough to excite the bound electron, the bound electron does
not absorb any XUV photon and it is excited only by sharing the energy carried
by the recollsion electron, hence the forward collision plays a dominant role
on the NSDI processes. Moreover, we find that the interference patterns of the
NSDI spectra can be reconstructed by the spectra of two above-threshold
ionization (ATI) processes, which may be used to analyze the structure of the
two separate ATI spectra by NSDI processes.
",Physics,Physics
"Multiplicities of Character Values of Binary Sidel'nikov-Lempel-Cohn-Eastman Sequences   Binary Sidel'nikov-Lempel-Cohn-Eastman sequences (or SLCE sequences) over F 2
have even period and almost perfect autocorrelation. However, the evaluation of
the linear complexity of these sequences is really difficult. In this paper, we
continue the study of [1]. We first express the multiple roots of character
polynomials of SLCE sequences into certain kinds of Jacobi sums. Then by making
use of Gauss sums and Jacobi sums in the ""semiprimitive"" case, we derive new
divisibility results for SLCE sequences.
",Computer Science; Mathematics,Mathematics
"A note on the bijectivity of antipode of a Hopf algebra and its applications   Certain sufficient homological and ring-theoretical conditions are given for
a Hopf algebra to have bijective antipode with applications to noetherian Hopf
algebras regarding their homological behaviors.
",Mathematics,Mathematics
"Shannon entropy: a study of confined hydrogenic-like atoms   The Shannon entropy in the atomic, molecular and chemical physics context is
presented by using as test cases the hydrogenic-like atoms $H_c$, ${He_c}^+$
and ${Li_c}^{2+}$ confined by an impenetrable spherical box. Novel expressions
for entropic uncertainty relation and Shannon entropies $S_r$ and $S_p$ are
proposed to ensure their physical dimensionless characteristic. The electronic
ground state energy and the quantities $S_r$, $S_p$ and $S_t$ are calculated
for the hydrogenic-like atoms to different confinement radii by using a
variational method. The global behavior of these quantities and different
conjectures are analyzed. The results are compared, when available, with those
previously published.
",Physics,Physics
"Inconsistency of Template Estimation with the Fr{é}chet mean in Quotient Space   We tackle the problem of template estimation when data have been randomly
transformed under an isometric group action in the presence of noise. In order
to estimate the template, one often minimizes the variance when the influence
of the transformations have been removed (computation of the Fr{é}chet mean
in quotient space). The consistency bias is defined as the distance (possibly
zero) between the orbit of the template and the orbit of one element which
minimizes the variance. In this article we establish an asymptotic behavior of
the consistency bias with respect to the noise level. This behavior is linear
with respect to the noise level. As a result the inconsistency is unavoidable
as soon as the noise is large enough. In practice, the template estimation with
a finite sample is often done with an algorithm called max-max. We show the
convergence of this algorithm to an empirical Karcher mean. Finally, our
numerical experiments show that the bias observed in practice cannot be
attributed to the small sample size or to a convergence problem but is indeed
due to the previously studied inconsistency.
",Mathematics; Statistics,Mathematics; Statistics
"An Optimal Control Problem for the Steady Nonhomogeneous Asymmetric Fluids   We study an optimal boundary control problem for the two-dimensional
stationary micropolar fluids system with variable density. We control the
system by considering boundary controls, for the velocity vector and angular
velocity of rotation of particles, on parts of the boundary of the flow domain.
On the remaining part of the boundary, we consider mixed boundary conditions
for the vector velocity (Dirichlet and Navier conditions) and Dirichlet
boundary conditions for the angular velocity. We analyze the existence of a
weak solution obtaining the fluid density as a scalar function of the stream
function. We prove the existence of an optimal solution and, by using the
Lagrange multipliers theorem, we state first-order optimality conditions. We
also derive, through a penalty method, some optimality conditions satisfied by
the optimal controls.
",Mathematics,Physics
"Human Eye Visual Hyperacuity: A New Paradigm for Sensing?   The human eye appears to be using a low number of sensors for image
capturing. Furthermore, regarding the physical dimensions of
cones-photoreceptors responsible for the sharp central vision-, we may realize
that these sensors are of a relatively small size and area. Nonetheless, the
eye is capable to obtain high resolution images due to visual hyperacuity and
presents an impressive sensitivity and dynamic range when set against
conventional digital cameras of similar characteristics. This article is based
on the hypothesis that the human eye may be benefiting from diffraction to
improve both image resolution and acquisition process. The developed method
intends to explain and simulate using MATLAB software the visual hyperacuity:
the introduction of a controlled diffraction pattern at an initial stage,
enables the use of a reduced number of sensors for capturing the image and
makes possible a subsequent processing to improve the final image resolution.
The results have been compared with the outcome of an equivalent system but in
absence of diffraction, achieving promising results. The main conclusion of
this work is that diffraction could be helpful for capturing images or signals
when a small number of sensors available, which is far from being a
resolution-limiting factor.
",Computer Science,Computer Science
"Grayscale Image Authentication using Neural Hashing   Many different approaches for neural network based hash functions have been
proposed. Statistical analysis must correlate security of them. This paper
proposes novel neural hashing approach for gray scale image authentication. The
suggested system is rapid, robust, useful and secure. Proposed hash function
generates hash values using neural network one-way property and non-linear
techniques. As a result security and performance analysis are performed and
satisfying results are achieved. These features are dominant reasons for
preferring against traditional ones.
",Computer Science,Computer Science
"A Deep Network Model for Paraphrase Detection in Short Text Messages   This paper is concerned with paraphrase detection. The ability to detect
similar sentences written in natural language is crucial for several
applications, such as text mining, text summarization, plagiarism detection,
authorship authentication and question answering. Given two sentences, the
objective is to detect whether they are semantically identical. An important
insight from this work is that existing paraphrase systems perform well when
applied on clean texts, but they do not necessarily deliver good performance
against noisy texts. Challenges with paraphrase detection on user generated
short texts, such as Twitter, include language irregularity and noise. To cope
with these challenges, we propose a novel deep neural network-based approach
that relies on coarse-grained sentence modeling using a convolutional neural
network and a long short-term memory model, combined with a specific
fine-grained word-level similarity matching model. Our experimental results
show that the proposed approach outperforms existing state-of-the-art
approaches on user-generated noisy social media data, such as Twitter texts,
and achieves highly competitive performance on a cleaner corpus.
",Computer Science,Computer Science
"Motion Planning Networks   Fast and efficient motion planning algorithms are crucial for many
state-of-the-art robotics applications such as self-driving cars. Existing
motion planning methods such as RRT*, A*, and D*, become ineffective as their
computational complexity increases exponentially with the dimensionality of the
motion planning problem. To address this issue, we present a neural
network-based novel planning algorithm which generates end-to-end
collision-free paths irrespective of the obstacles' geometry. The proposed
method, called MPNet (Motion Planning Network), comprises of a Contractive
Autoencoder which encodes the given workspaces directly from a point cloud
measurement, and a deep feedforward neural network which takes the workspace
encoding, start and goal configuration, and generates end-to-end feasible
motion trajectories for the robot to follow. We evaluate MPNet on multiple
planning problems such as planning of a point-mass robot, rigid-body, and 7 DOF
Baxter robot manipulators in various 2D and 3D environments. The results show
that MPNet is not only consistently computationally efficient in all 2D and 3D
environments but also show remarkable generalization to completely unseen
environments. The results also show that computation time of MPNet consistently
remains less than 1 second which is significantly lower than existing
state-of-the-art motion planning algorithms. Furthermore, through transfer
learning, the MPNet trained in one scenario (e.g., indoor living places) can
also quickly adapt to new scenarios (e.g., factory floors) with a little amount
of data.
",Computer Science; Statistics,Computer Science
"Learning General Latent-Variable Graphical Models with Predictive Belief Propagation and Hilbert Space Embeddings   In this paper, we propose a new algorithm for learning general
latent-variable probabilistic graphical models using the techniques of
predictive state representation, instrumental variable regression, and
reproducing-kernel Hilbert space embeddings of distributions. Under this new
learning framework, we first convert latent-variable graphical models into
corresponding latent-variable junction trees, and then reduce the hard
parameter learning problem into a pipeline of supervised learning problems,
whose results will then be used to perform predictive belief propagation over
the latent junction tree during the actual inference procedure. We then give
proofs of our algorithm's correctness, and demonstrate its good performance in
experiments on one synthetic dataset and two real-world tasks from
computational biology and computer vision - classifying DNA splice junctions
and recognizing human actions in videos.
",Computer Science; Statistics,Computer Science; Statistics
"Structure theorems for star-commuting power partial isometries   We give a new formulation and proof of a theorem of Halmos and Wallen on the
structure of power partial isometries on Hilbert space. We then use this
theorem to give a structure theorem for a finite set of partial isometries
which star-commute: each operator commutes with the others and with their
adjoints.
",Mathematics,Mathematics
"Nonparametric inference for continuous-time event counting and link-based dynamic network models   A flexible approach for modeling both dynamic event counting and dynamic
link-based networks based on counting processes is proposed, and estimation in
these models is studied. We consider nonparametric likelihood based estimation
of parameter functions via kernel smoothing. The asymptotic behavior of these
estimators is rigorously analyzed by allowing the number of nodes to tend to
infinity. The finite sample performance of the estimators is illustrated
through an empirical analysis of bike share data.
",Mathematics; Statistics,Mathematics; Statistics
"Phonon-assisted oscillatory exciton dynamics in monolayer MoSe2   In monolayer semiconductor transition metal dichalcogenides, the
exciton-phonon interaction is expected to strongly affect the photocarrier
dynamics. Here, we report on an unusual oscillatory enhancement of the neutral
exciton photoluminescence with the excitation laser frequency in monolayer
MoSe2. The frequency of oscillation matches that of the M-point longitudinal
acoustic phonon, LA(M). Oscillatory behavior is also observed in the
steady-state emission linewidth and in timeresolved photoluminescence
excitation data, which reveals variation with excitation energy in the exciton
lifetime. These results clearly expose the key role played by phonons in the
exciton formation and relaxation dynamics of two-dimensional van der Waals
semiconductors.
",Physics,Physics
"Scalable Cryogenic Read-out Circuit for a Superconducting Nanowire Single-Photon Detector System   The superconducting nanowire single photon detector (SNSPD) is a leading
technology for quantum information science applications using photons, and they
are finding increasing uses in photon-starved classical imaging applications.
Critical detector characteristics, such as timing resolution (jitter), reset
time and maximum count rate, are heavily influenced by the readout electronics
that sense and amplify the photon detection signal. We describe a readout
circuit for SNSPDs using commercial off-the-shelf amplifiers operating at
cryogenic temperatures. Our design demonstrates a 35 ps timing resolution and a
maximum count rate of over 2x10^7 counts per second while maintaining <3 mW
power consumption per channel, making it suitable for a multichannel readout.
",Physics,Computer Science
"Rigorous statistical analysis of HTTPS reachability   The use of secure connections using HTTPS as the default means, or even the
only means, to connect to web servers is increasing. It is being pushed from
both sides: from the bottom up by client distributions and plugins, and from
the top down by organisations such as Google. However, there are potential
technical hurdles that might lock some clients out of the modern web. This
paper seeks to measure and precisely quantify those hurdles in the wild. More
than three million measurements provide statistically significant evidence of
degradation. We show this through a variety of statistical techniques. Various
factors are shown to influence the problem, ranging from the client's browser,
to the locale from which they connect.
",Computer Science; Statistics,Computer Science
"Asymptotic Exponentiality of the First Exit Time of the Shiryaev-Roberts Diffusion with Constant Positive Drift   We consider the first exit time of a Shiryaev-Roberts diffusion with constant
positive drift from the interval $[0,A]$ where $A>0$. We show that the moment
generating function (Laplace transform) of a suitably standardized version of
the first exit time converges to that of the unit-mean exponential distribution
as $A\to+\infty$. The proof is explicit in that the moment generating function
of the first exit time is first expressed analytically and in a closed form,
and then the desired limit as $A\to+\infty$ is evaluated directly. The result
is of importance in the area of quickest change-point detection, and its
discrete-time counterpart has been previously established - although in a
different manner - by Pollak and Tartakovsky (2009).
",Mathematics; Statistics,Mathematics; Statistics
"Singular Riemannian flows and characteristic numbers   Let $M$ be an even-dimensional, oriented closed manifold. We show that the
restriction of a singular Riemannian flow on $M$ to a small tubular
neighborhood of each connected component of its singular stratum is
foliated-diffeomorphic to an isometric flow on the same neighborhood. We then
prove a formula that computes characteristic numbers of $M$ as the sum of
residues associated to the infinitesimal foliation at the components of the
singular stratum of the flow.
",Mathematics,Mathematics
"$\texttt{PyTranSpot}$ - A tool for multiband light curve modeling of planetary transits and stellar spots   Several studies have shown that stellar activity features, such as occulted
and non-occulted starspots, can affect the measurement of transit parameters
biasing studies of transit timing variations and transmission spectra. We
present $\texttt{PyTranSpot}$, which we designed to model multiband transit
light curves showing starspot anomalies, inferring both transit and spot
parameters. The code follows a pixellation approach to model the star with its
corresponding limb darkening, spots, and transiting planet on a two dimensional
Cartesian coordinate grid. We combine $\texttt{PyTranSpot}$ with an MCMC
framework to study and derive exoplanet transmission spectra, which provides
statistically robust values for the physical properties and uncertainties of a
transiting star-planet system. We validate $\texttt{PyTranSpot}$'s performance
by analyzing eleven synthetic light curves of four different star-planet
systems and 20 transit light curves of the well-studied WASP-41b system. We
also investigate the impact of starspots on transit parameters and derive
wavelength dependent transit depth values for WASP-41b covering a range of
6200-9200 $\AA$, indicating a flat transmission spectrum.
",Physics,Physics
"Periods and factors of weak model sets   There is a renewed interest in weak model sets due to their connection to
$\mathcal B$-free systems, which emerged from Sarnak's program on the Möbius
disjointness conjecture. Here we continue our recent investigation
[arXiv:1511.06137] of the extended hull ${\mathcal M}^{\scriptscriptstyle
G}_{\scriptscriptstyle W}$, a dynamical system naturally associated to a weak
model set in an abelian group $G$ with relatively compact window $W$. For
windows having a nowhere dense boundary (this includes compact windows), we
identify the maximal equicontinuous factor of ${\mathcal M}^{\scriptscriptstyle
G}_{\scriptscriptstyle W}$ and give a sufficient condition when ${\mathcal
M}^{\scriptscriptstyle G}_{\scriptscriptstyle W}$ is an almost 1:1 extension of
its maximal equicontinuous factor. If the window is measurable with positive
Haar measure and is almost compact, then the system ${\mathcal
M}^{\scriptscriptstyle G}_{\scriptscriptstyle W}$ equipped with its Mirsky
measure is isomorphic to its Kronecker factor. For general nontrivial ergodic
probability measures on ${\mathcal M}^{\scriptscriptstyle
G}_{\scriptscriptstyle W}$, we provide a kind of lower bound for the Kronecker
factor. All relevant factor systems are natural $G$-actions on quotient
subgroups of the torus underlying the weak model set. These are obtained by
factoring out suitable window periods. Our results are specialised to the usual
hull of the weak model set, and they are also interpreted for ${\mathcal
B}$-free systems.
",Mathematics,Mathematics
"Non-convex Conditional Gradient Sliding   We investigate a projection free method, namely conditional gradient sliding
on batched, stochastic and finite-sum non-convex problem. CGS is a smart
combination of Nesterov's accelerated gradient method and Frank-Wolfe (FW)
method, and outperforms FW in the convex setting by saving gradient
computations. However, the study of CGS in the non-convex setting is limited.
In this paper, we propose the non-convex conditional gradient sliding (NCGS)
which surpasses the non-convex Frank-Wolfe method in batched, stochastic and
finite-sum setting.
",Mathematics,Mathematics
"Comprehensive evaluation of statistical speech waveform synthesis   Statistical TTS systems that directly predict the speech waveform have
recently reported improvements in synthesis quality. This investigation
evaluates Amazon's statistical speech waveform synthesis (SSWS) system. An
in-depth evaluation of SSWS is conducted across a number of domains to better
understand the consistency in quality. The results of this evaluation are
validated by repeating the procedure on a separate group of testers. Finally,
an analysis of the nature of speech errors of SSWS compared to hybrid unit
selection synthesis is conducted to identify the strengths and weaknesses of
SSWS. Having a deeper insight into SSWS allows us to better define the focus of
future work to improve this new technology.
",Computer Science,Computer Science
"Is the kinetic equation for turbulent gas-particle flows ill-posed?   This paper is about well-posedness and realizability of the kinetic equation
for gas-particle flows and its relationship to the Generalized Langevin Model
(GLM) PDF equation. Previous analyses claim that this kinetic equation is
ill-posed, that in particular it has the properties of a backward heat equation
and as a consequence, its solutions will in the course of time exhibit
finite-time singularities. We show that the analysis leading to this conclusion
is fundamentally incorrect because it ignores the coupling between the phase
space variables in the kinetic equation and the time and particle inertia
dependence of the phase space diffusion tensor. This contributes an extra $+ve$
diffusion that always outweighs the contribution from the$-ve$ diffusion
associated with the dispersion along one of the principal axes of the phase
space diffusion tensor. This is confirmed by a numerical evaluation of analytic
solutions of these $+ve$ and $-ve$ contributions to the particle diffusion
coefficient along this principal axis. We also examine other erroneous claims
and assumptions made in previous studies that demonstrate the apparent
superiority of the GLM PDF approach over the kinetic approach. In so doing we
have drawn attention to the limitations of the GLM approach which these studies
have ignored or not properly considered, to give a more balanced appraisal of
the benefits of both PDF approaches.
",Physics,Physics
"Calibrating the Planck Cluster Mass Scale with Cluster Velocity Dispersions   We measure the Planck cluster mass bias using dynamical mass measurements
based on velocity dispersions of a subsample of 17 Planck-detected clusters.
The velocity dispersions were calculated using redshifts determined from
spectra obtained at Gemini observatory with the GMOS multi-object spectrograph.
We correct our estimates for effects due to finite aperture, Eddington bias and
correlated scatter between velocity dispersion and the Planck mass proxy. The
result for the mass bias parameter, $(1-b)$, depends on the value of the galaxy
velocity bias $b_v$ adopted from simulations: $(1-b)=(0.51\pm0.09) b_v^3$.
Using a velocity bias of $b_v=1.08$ from Munari et al., we obtain
$(1-b)=0.64\pm 0.11$, i.e, an error of 17% on the mass bias measurement with 17
clusters. This mass bias value is consistent with most previous weak lensing
determinations. It lies within $1\sigma$ of the value needed to reconcile the
Planck cluster counts with the Planck primary CMB constraints. We emphasize
that uncertainty in the velocity bias severely hampers precision measurements
of the mass bias using velocity dispersions. On the other hand, when we fix the
Planck mass bias using the constraints from Penna-Lima et al., based on weak
lensing measurements, we obtain a positive velocity bias $b_v \gtrsim 0.9$ at
$3\sigma$.
",Physics,Physics
"A Data-Driven MHD Model of the Global Solar Corona within Multi-Scale Fluid-Kinetic Simulation Suite (MS-FLUKSS)   We have developed a data-driven magnetohydrodynamic (MHD) model of the global
solar corona which uses characteristically-consistent boundary conditions (BCs)
at the inner boundary. Our global solar corona model can be driven by different
observational data including Solar Dynamics Observatory/Helioseismic and
Magnetic Imager (SDO/HMI) synoptic vector magnetograms together with the
horizontal velocity data in the photosphere obtained by the time-distance
helioseismology method, and the line-of-sight (LOS) magnetogram data obtained
by HMI, Solar and Heliospheric Observatory/Michelson Doppler Imager (SOHO/MDI),
National Solar Observatory/Global Oscillation Network Group (NSO/GONG) and
Wilcox Solar Observatory (WSO). We implemented our model in the Multi-Scale
Fluid-Kinetic Simulation Suite (MS-FLUKSS) - a suite of adaptive mesh
refinement (AMR) codes built upon the Chombo AMR framework developed at the
Lawrence Berkeley National Laboratory. We present an overview of our model,
characteristic BCs, and two results we obtained using our model: A benchmark
test of relaxation of a dipole field using characteristic BCs, and relaxation
of an initial PFSS field driven by HMI LOS magnetogram data, and horizontal
velocity data obtained by the time-distance helioseismology method using a set
of non-characteristic BCs.
",Physics,Physics
"Stellar Absorption Line Analysis of Local Star-Forming Galaxies: The Relation Between Stellar Mass, Metallicity, Dust Attenuation and Star Formation Rate   We analyze the optical continuum of star-forming galaxies in SDSS by fitting
stacked spectra with stellar population synthesis models to investigate the
relation between stellar mass, stellar metallicity, dust attenuation and star
formation rate. We fit models calculated with star formation and chemical
evolution histories that are derived empirically from multi-epoch observations
of the stellar mass---star formation rate and the stellar mass---gas-phase
metallicity relations, respectively. We also fit linear combinations of single
burst models with a range of metallicities and ages. Star formation and
chemical evolution histories are unconstrained for these models. The stellar
mass---stellar metallicity relations obtained from the two methods agree with
the relation measured from individual supergiant stars in nearby galaxies.
These relations are also consistent with the relation obtained from emission
line analysis of gas-phase metallicity after accounting for systematic offsets
in the gas-phase-metallicity. We measure dust attenuation of the stellar
continuum and show that its dependence on stellar mass and star formation rate
is consistent with previously reported results derived from nebular emission
lines. However, stellar continuum attenuation is smaller than nebular emission
line attenuation. The continuum-to-nebular attenuation ratio depends on stellar
mass and is smaller in more massive galaxies. Our consistent analysis of
stellar continuum and nebular emission lines paves the way for a comprehensive
investigation of stellar metallicities of star-forming and quiescent galaxies.
",Physics,Physics
"Conjoined constraints on modified gravity from the expansion history and cosmic growth   In this paper we present conjoined constraints on several cosmological models
from the expansion history $H(z)$ and cosmic growth $f\sigma_8(z)$. The models
we study include the CPL $w_0w_a$ parametrization, the Holographic Dark Energy
(HDE) model, the Time varying vacuum ($\Lambda_t$CDM) model, the Dvali,
Gabadadze and Porrati (DGP) and Finsler-Randers (FRDE) models, a power law
$f(T)$ model and finally the Hu-Sawicki $f(R)$ model. In all cases we perform a
simultaneous fit to the SnIa, CMB, BAO, $H(z)$ and growth data, while also
following the conjoined visualization of $H(z)$ and $f\sigma_8(z)$ as in Linder
(2017). Furthermore, we introduce the Figure of Merit (FoM) in the
$H(z)-f\sigma_8(z)$ parameter space as a way to constrain models that jointly
fit both probes well. We use both the latest $H(z)$ and $f\sigma_8(z)$ data,
but also LSST-like mocks with $1\%$ measurements and we find that the conjoined
method of constraining the expansion history and cosmic growth simultaneously
is able not only to place stringent constraints on these parameters but also to
provide an easy visual way to discriminate cosmological models. Finally, we
confirm the existence of a tension between the growth rate and Planck CMB data
and we find that the FoM in the conjoined parameter space of
$H(z)-f\sigma_8(z)$ can be used to discriminate between the $\Lambda$CDM model
and certain classes of modified gravity models, namely the DGP and $f(T)$.
",Physics,Physics
"Facial Keypoints Detection   Detect facial keypoints is a critical element in face recognition. However,
there is difficulty to catch keypoints on the face due to complex influences
from original images, and there is no guidance to suitable algorithms. In this
paper, we study different algorithms that can be applied to locate keyponits.
Specifically: our framework (1)prepare the data for further investigation
(2)Using PCA and LBP to process the data (3) Apply different algorithms to
analysis data, including linear regression models, tree based model, neural
network and convolutional neural network, etc. Finally we will give our
conclusion and further research topic. A comprehensive set of experiments on
dataset demonstrates the effectiveness of our framework.
",Computer Science; Statistics,Computer Science
"Beam-induced Back-streaming Electron Suppression Analysis for Accelerator Type Neutron Generators   A facility based on a next-generation, high-flux D-D neutron generator has
been commissioned and it is now operational at the University of California,
Berkeley. The current generator design produces near monoenergetic 2.45 MeV
neutrons at outputs of 10^8 n/s. Calculations provided show that future
conditioning at higher currents and voltages will allow for a production rate
over 10^10 n/s. A significant problem encountered was beam-induced electron
backstreaming, that needed to be resolved to achieve meaningful beam currents.
Two methods of suppressing secondary electrons resulting from the deuterium
beam striking the target were tested: the application of static electric and
magnetic fields. Computational simulations of both techniques were done using a
finite element analysis in COMSOL Multiphysics. Experimental tests verified
these simulation results. The most reliable suppression was achieved via the
implementation of an electrostatic shroud with a voltage offset of -800 V
relative to the target.
",Physics,Physics
"Factors in Recommending Contrarian Content on Social Media   Polarization is a troubling phenomenon that can lead to societal divisions
and hurt the democratic process. It is therefore important to develop methods
to reduce it.
We propose an algorithmic solution to the problem of reducing polarization.
The core idea is to expose users to content that challenges their point of
view, with the hope broadening their perspective, and thus reduce their
polarity. Our method takes into account several aspects of the problem, such as
the estimated polarity of the user, the probability of accepting the
recommendation, the polarity of the content, and popularity of the content
being recommended.
We evaluate our recommendations via a large-scale user study on Twitter users
that were actively involved in the discussion of the US elections results.
Results shows that, in most cases, the factors taken into account in the
recommendation affect the users as expected, and thus capture the essential
features of the problem.
",Computer Science,Computer Science
"Membrane Trafficking in the Yeast Saccharomyces cerevisiae Model   The yeast Saccharomyces cerevisiae is one of the best characterized
eukaryotic models. The secretory pathway was the first trafficking pathway
clearly understood mainly thanks to the work done in the laboratory of Randy
Schekman in the 1980s. They have isolated yeast sec mutants unable to secrete
an extracellular enzyme and these SEC genes were identified as encoding key
effectors of the secretory machinery. For this work, the 2013 Nobel Prize in
Physiology and Medicine has been awarded to Randy Schekman; the prize is shared
with James Rothman and Thomas S{ü}dhof. Here, we present the different
trafficking pathways of yeast S. cerevisiae. At the Golgi apparatus newly
synthesized proteins are sorted between those transported to the plasma
membrane (PM), or the external medium, via the exocytosis or secretory pathway
(SEC), and those targeted to the vacuole either through endosomes (vacuolar
protein sorting or VPS pathway) or directly (alkaline phosphatase or ALP
pathway). Plasma membrane proteins can be internalized by endocytosis (END) and
transported to endosomes where they are sorted between those targeted for
vacuolar degradation and those redirected to the Golgi (recycling or RCY
pathway). Studies in yeast S. cerevisiae allowed the identification of most of
the known effectors, protein complexes, and trafficking pathways in eukaryotic
cells, and most of them are conserved among eukaryotes.
",Quantitative Biology,Quantitative Biology
"Dynamical characteristics of electromagnetic field under conditions of total reflection   The dynamical characteristics of electromagnetic fields include energy,
momentum, angular momentum (spin) and helicity. We analyze their spatial
distributions near the planar interface between two transparent and
non-dispersive media, when the incident monochromatic plane wave with arbitrary
polarization is totally reflected, and an evanescent wave is formed in the
medium with lower optical density. Based on the recent arguments in favor of
the Minkowski definition of the electromagnetic momentum in a material medium
[Phys. Rev. A 83, 013823 (2011); 86, 055802 (2012); Phys. Rev. Lett. 119,
073901 (2017)], we derive the explicit expressions for the dynamical
characteristics in both media, with special attention to their behavior at the
interface. Especially, the ""extraordinary"" spin and momentum components
orthogonal to the plane of incidence are described, and the canonical (spin -
orbital) momentum decomposition is performed that contains no singular terms.
The field energy, helicity, the spin momentum and orbital momentum components
are everywhere regular but experience discontinuities at the interface; the
spin components parallel to the interface appear to be continuous, which
testifies for the consistency of the adopted Minkowski picture. The results
supply a meaningful example of the electromagnetic momentum decomposition, with
separation of spatial and polarization degrees of freedom, in inhomogeneous
media, and can be used in engineering the structured fields designed for
optical sorting, dispatching and micromanipulation.
",Physics,Physics
"From Distance Correlation to Multiscale Graph Correlation   Understanding and developing a correlation measure that can detect general
dependencies is not only imperative to statistics and machine learning, but
also crucial to general scientific discovery in the big data age. In this
paper, we establish a new framework that generalizes distance correlation --- a
correlation measure that was recently proposed and shown to be universally
consistent for dependence testing against all joint distributions of finite
moments --- to the Multiscale Graph Correlation (MGC). By utilizing the
characteristic functions and incorporating the nearest neighbor machinery, we
formalize the population version of local distance correlations, define the
optimal scale in a given dependency, and name the optimal local correlation as
MGC. The new theoretical framework motivates a theoretically sound Sample MGC
and allows a number of desirable properties to be proved, including the
universal consistency, convergence and almost unbiasedness of the sample
version. The advantages of MGC are illustrated via a comprehensive set of
simulations with linear, nonlinear, univariate, multivariate, and noisy
dependencies, where it loses almost no power in monotone dependencies while
achieving better performance in general dependencies, compared to distance
correlation and other popular methods.
",Statistics,Computer Science; Statistics
"Applying the Delta method in metric analytics: A practical guide with novel ideas   During the last decade, the information technology industry has adopted a
data-driven culture, relying on online metrics to measure and monitor business
performance. Under the setting of big data, the majority of such metrics
approximately follow normal distributions, opening up potential opportunities
to model them directly without extra model assumptions and solve big data
problems via closed-form formulas using distributed algorithms at a fraction of
the cost of simulation-based procedures like bootstrap. However, certain
attributes of the metrics, such as their corresponding data generating
processes and aggregation levels, pose numerous challenges for constructing
trustworthy estimation and inference procedures. Motivated by four real-life
examples in metric development and analytics for large-scale A/B testing, we
provide a practical guide to applying the Delta method, one of the most
important tools from the classic statistics literature, to address the
aforementioned challenges. We emphasize the central role of the Delta method in
metric analytics by highlighting both its classic and novel applications.
",Statistics,Computer Science; Statistics
"Adversarial Attacks on Neural Networks for Graph Data   Deep learning models for graphs have achieved strong performance for the task
of node classification. Despite their proliferation, currently there is no
study of their robustness to adversarial attacks. Yet, in domains where they
are likely to be used, e.g. the web, adversaries are common. Can deep learning
models for graphs be easily fooled? In this work, we introduce the first study
of adversarial attacks on attributed graphs, specifically focusing on models
exploiting ideas of graph convolutions. In addition to attacks at test time, we
tackle the more challenging class of poisoning/causative attacks, which focus
on the training phase of a machine learning model. We generate adversarial
perturbations targeting the node's features and the graph structure, thus,
taking the dependencies between instances in account. Moreover, we ensure that
the perturbations remain unnoticeable by preserving important data
characteristics. To cope with the underlying discrete domain we propose an
efficient algorithm Nettack exploiting incremental computations. Our
experimental study shows that accuracy of node classification significantly
drops even when performing only few perturbations. Even more, our attacks are
transferable: the learned attacks generalize to other state-of-the-art node
classification models and unsupervised approaches, and likewise are successful
even when only limited knowledge about the graph is given.
",Statistics,Computer Science; Statistics
"Invitation to Alexandrov geometry: CAT[0] spaces   The idea is to demonstrate the beauty and power of Alexandrov geometry by
reaching interesting applications with a minimum of preparation.
The topics include
1. Estimates on the number of collisions in billiards.
2. Construction of exotic aspherical manifolds.
3. The geometry of two-convex sets in Euclidean space.
",Mathematics,Mathematics
"Advantages of versatile neural-network decoding for topological codes   Finding optimal correction of errors in generic stabilizer codes is a
computationally hard problem, even for simple noise models. While this task can
be simplified for codes with some structure, such as topological stabilizer
codes, developing good and efficient decoders still remains a challenge. In our
work, we systematically study a very versatile class of decoders based on
feedforward neural networks. To demonstrate adaptability, we apply neural
decoders to the triangular color and toric codes under various noise models
with realistic features, such as spatially-correlated errors. We report that
neural decoders provide significant improvement over leading efficient decoders
in terms of the error-correction threshold. Using neural networks simplifies
the process of designing well-performing decoders, and does not require prior
knowledge of the underlying noise model.
",Statistics,Computer Science; Statistics
"Evaluating openEHR for storing computable representations of electronic health record phenotyping algorithms   Electronic Health Records (EHR) are data generated during routine clinical
care. EHR offer researchers unprecedented phenotypic breadth and depth and have
the potential to accelerate the pace of precision medicine at scale. A main EHR
use-case is creating phenotyping algorithms to define disease status, onset and
severity. Currently, no common machine-readable standard exists for defining
phenotyping algorithms which often are stored in human-readable formats. As a
result, the translation of algorithms to implementation code is challenging and
sharing across the scientific community is problematic. In this paper, we
evaluate openEHR, a formal EHR data specification, for computable
representations of EHR phenotyping algorithms.
",Computer Science,Computer Science
"A Non-Gaussian, Nonparametric Structure for Gene-Gene and Gene-Environment Interactions in Case-Control Studies Based on Hierarchies of Dirichlet Processes   It is becoming increasingly clear that complex interactions among genes and
environmental factors play crucial roles in triggering complex diseases. Thus,
understanding such interactions is vital, which is possible only through
statistical models that adequately account for such intricate, albeit unknown,
dependence structures. Bhattacharya & Bhattacharya (2016b) attempt such
modeling, relating finite mixtures composed of Dirichlet processes that
represent unknown number of genetic sub-populations through a hierarchical
matrix-normal structure that incorporates gene-gene interactions, and possible
mutations, induced by environmental variables. However, the product dependence
structure implied by their matrix-normal model seems to be too simple to be
appropriate for general complex, realistic situations. In this article, we
propose and develop a novel nonparametric Bayesian model for case-control
genotype data using hierarchies of Dirichlet processes that offers a more
realistic and nonparametric dependence structure between the genes, induced by
the environmental variables. In this regard, we propose a novel and highly
parallelisable MCMC algorithm that is rendered quite efficient by the
combination of modern parallel computing technology, effective Gibbs sampling
steps, retrospective sampling and Transformation based Markov Chain Monte Carlo
(TMCMC). We use appropriate Bayesian hypothesis testing procedures to detect
the roles of genes and environment in case-control studies. We apply our ideas
to 5 biologically realistic case-control genotype datasets simulated under
distinct set-ups, and obtain encouraging results in each case. We finally apply
our ideas to a real, myocardial infarction dataset, and obtain interesting
results on gene-gene and gene-environment interaction, while broadly agreeing
with the results reported in the literature.
",Statistics,Statistics
"Discrete Gradient Line Fields on Surfaces   A line field on a manifold is a smooth map which assigns a tangent line to
all but a finite number of points of the manifold. As such, it can be seen as a
generalization of vector fields. They model a number of geometric and physical
properties, e.g. the principal curvature directions dynamics on surfaces or the
stress flux in elasticity.
We propose a discretization of a Morse-Smale line field on surfaces,
extending Forman's construction for discrete vector fields. More general
critical elements and their indices are defined from local matchings, for which
Euler theorem and the characterization of homotopy type in terms of critical
cells still hold.
",Computer Science; Mathematics,Physics
"Minors of two-connected graphs of large path-width   Let $P$ be a graph with a vertex $v$ such that $P\backslash v$ is a forest,
and let $Q$ be an outerplanar graph. We prove that there exists a number
$p=p(P,Q)$ such that every 2-connected graph of path-width at least $p$ has a
minor isomorphic to $P$ or $Q$. This result answers a question of Seymour and
implies a conjecture of Marshall and Wood. The proof is based on a new property
of tree-decompositions.
",Computer Science,Mathematics
"On the character degrees of a Sylow $p$-subgroup of a finite Chevalley group $G(p^f)$ over a bad prime   Let $q$ be a power of a prime $p$ and let $U(q)$ be a Sylow $p$-subgroup of a
finite Chevalley group $G(q)$ defined over the field with $q$ elements. We
first give a parametrization of the set $\text{Irr}(U(q))$ of irreducible
characters of $U(q)$ when $G(q)$ is of type $\mathrm{G}_2$. This is uniform for
primes $p \ge 5$, while the bad primes $p=2$ and $p=3$ have to be considered
separately. We then use this result and the contribution of several authors to
show a general result, namely that if $G(q)$ is any finite Chevalley group with
$p$ a bad prime, then there exists a character $\chi \in \text{Irr}(U(q))$ such
that $\chi(1)=q^n/p$ for some $n \in \mathbb{Z}_{\ge_0}$. In particular, for
each $G(q)$ and every bad prime $p$, we construct a family of characters of
such degree as inflation followed by an induction of linear characters of an
abelian subquotient $V(q)$ of $U(q)$.
",Mathematics,Mathematics
"Disentangling and Assessing Uncertainties in Multiperiod Corporate Default Risk Predictions   Measuring the corporate default risk is broadly important in economics and
finance. Quantitative methods have been developed to predictively assess future
corporate default probabilities. However, as a more difficult yet crucial
problem, evaluating the uncertainties associated with the default predictions
remains little explored. In this paper, we attempt to fill this blank by
developing a procedure for quantifying the level of associated uncertainties
upon carefully disentangling multiple contributing sources. Our framework
effectively incorporates broad information from historical default data,
corporates' financial records, and macroeconomic conditions by a)
characterizing the default mechanism, and b) capturing the future dynamics of
various features contributing to the default mechanism. Our procedure overcomes
the major challenges in this large scale statistical inference problem and
makes it practically feasible by using parsimonious models, innovative methods,
and modern computational facilities. By predicting the marketwide total number
of defaults and assessing the associated uncertainties, our method can also be
applied for evaluating the aggregated market credit risk level. Upon analyzing
a US market data set, we demonstrate that the level of uncertainties associated
with default risk assessments is indeed substantial. More informatively, we
also find that the level of uncertainties associated with the default risk
predictions is correlated with the level of default risks, indicating potential
for new scopes in practical applications including improving the accuracy of
default risk assessments.
",Statistics; Quantitative Finance,Statistics
"Joint Routing, Scheduling and Power Control Providing Hard Deadline in Wireless Multihop Networks   We consider optimal/efficient power allocation policies in a single/multihop
wireless network in the presence of hard end-to-end deadline delay constraints
on the transmitted packets. Such constraints can be useful for real time voice
and video. Power is consumed in only transmission of the data. We consider the
case when the power used in transmission is a convex function of the data
transmitted. We develop a computationally efficient online algorithm, which
minimizes the average power for the single hop. We model this problem as
dynamic program (DP) and obtain the optimal solution. Next, we generalize it to
the multiuser, multihop scenario when there are multiple real time streams with
different hard deadline constraints.
",Computer Science,Computer Science
"Tunnelling in Dante's Inferno   We study quantum tunnelling in Dante's Inferno model of large field
inflation. Such a tunnelling process, which will terminate inflation, becomes
problematic if the tunnelling rate is rapid compared to the Hubble time scale
at the time of inflation. Consequently, we constrain the parameter space of
Dante's Inferno model by demanding a suppressed tunnelling rate during
inflation. The constraints are derived and explicit numerical bounds are
provided for representative examples. Our considerations are at the level of an
effective field theory; hence, the presented constraints have to hold
regardless of any UV completion.
",Physics,Physics
"Room-temperature spin transport in n-Ge probed by four-terminal nonlocal measurements   We demonsrtate electrical spin injection and detection in $n$-type Ge
($n$-Ge) at room temperature using four-terminal nonlocal spin-valve and
Hanle-effect measurements in lateral spin-valve (LSV) devices with
Heusler-alloy Schottky tunnel contacts. The spin diffusion length
($\lambda$$_{\rm Ge}$) of the Ge layer used ($n \sim$ 1 $\times$ 10$^{19}$
cm$^{-3}$) at 296 K is estimated to be $\sim$ 0.44 $\pm$ 0.02 $\mu$m.
Room-temperature spin signals can be observed reproducibly at the low bias
voltage range ($\le$ 0.7 V) for LSVs with relatively low resistance-area
product ($RA$) values ($\le$ 1 k$\Omega$$\mu$m$^{2}$). This means that the
Schottky tunnel contacts used here are more suitable than ferromagnet/MgO
tunnel contacts ($RA \ge$ 100 k$\Omega$$\mu$m$^{2}$) for developing Ge
spintronic applications.
",Physics,Physics
"Fast and Robust Shortest Paths on Manifolds Learned from Data   We propose a fast, simple and robust algorithm for computing shortest paths
and distances on Riemannian manifolds learned from data. This amounts to
solving a system of ordinary differential equations (ODEs) subject to boundary
conditions. Here standard solvers perform poorly because they require
well-behaved Jacobians of the ODE, and usually, manifolds learned from data
imply unstable and ill-conditioned Jacobians. Instead, we propose a fixed-point
iteration scheme for solving the ODE that avoids Jacobians. This enhances the
stability of the solver, while reduces the computational cost. In experiments
involving both Riemannian metric learning and deep generative models we
demonstrate significant improvements in speed and stability over both
general-purpose state-of-the-art solvers as well as over specialized solvers.
",Computer Science; Statistics,Computer Science; Statistics
"mGPfusion: Predicting protein stability changes with Gaussian process kernel learning and data fusion   Proteins are commonly used by biochemical industry for numerous processes.
Refining these proteins' properties via mutations causes stability effects as
well. Accurate computational method to predict how mutations affect protein
stability are necessary to facilitate efficient protein design. However,
accuracy of predictive models is ultimately constrained by the limited
availability of experimental data. We have developed mGPfusion, a novel
Gaussian process (GP) method for predicting protein's stability changes upon
single and multiple mutations. This method complements the limited experimental
data with large amounts of molecular simulation data. We introduce a Bayesian
data fusion model that re-calibrates the experimental and in silico data
sources and then learns a predictive GP model from the combined data. Our
protein-specific model requires experimental data only regarding the protein of
interest and performs well even with few experimental measurements. The
mGPfusion models proteins by contact maps and infers the stability effects
caused by mutations with a mixture of graph kernels. Our results show that
mGPfusion outperforms state-of-the-art methods in predicting protein stability
on a dataset of 15 different proteins and that incorporating molecular
simulation data improves the model learning and prediction accuracy.
",Statistics; Quantitative Biology,Statistics
"Spatial structure of shock formation   The formation of a singularity in a compressible gas, as described by the
Euler equation, is characterized by the steepening, and eventual overturning of
a wave. Using a self-similar description in two space dimensions, we show that
the spatial structure of this process, which starts at a point, is equivalent
to the formation of a caustic, i.e. to a cusp catastrophe. The lines along
which the profile has infinite slope correspond to the caustic lines, from
which we construct the position of the shock. By solving the similarity
equation, we obtain a complete local description of wave steepening and of the
spreading of the shock from a point.
",Physics; Mathematics,Physics
"A Survey on Content-Aware Video Analysis for Sports   Sports data analysis is becoming increasingly large-scale, diversified, and
shared, but difficulty persists in rapidly accessing the most crucial
information. Previous surveys have focused on the methodologies of sports video
analysis from the spatiotemporal viewpoint instead of a content-based
viewpoint, and few of these studies have considered semantics. This study
develops a deeper interpretation of content-aware sports video analysis by
examining the insight offered by research into the structure of content under
different scenarios. On the basis of this insight, we provide an overview of
the themes particularly relevant to the research on content-aware systems for
broadcast sports. Specifically, we focus on the video content analysis
techniques applied in sportscasts over the past decade from the perspectives of
fundamentals and general review, a content hierarchical model, and trends and
challenges. Content-aware analysis methods are discussed with respect to
object-, event-, and context-oriented groups. In each group, the gap between
sensation and content excitement must be bridged using proper strategies. In
this regard, a content-aware approach is required to determine user demands.
Finally, the paper summarizes the future trends and challenges for sports video
analysis. We believe that our findings can advance the field of research on
content-aware video analysis for broadcast sports.
",Computer Science,Computer Science
"Synergistic Team Composition   Effective teams are crucial for organisations, especially in environments
that require teams to be constantly created and dismantled, such as software
development, scientific experiments, crowd-sourcing, or the classroom. Key
factors influencing team performance are competences and personality of team
members. Hence, we present a computational model to compose proficient and
congenial teams based on individuals' personalities and their competences to
perform tasks of different nature. With this purpose, we extend Wilde's
post-Jungian method for team composition, which solely employs individuals'
personalities. The aim of this study is to create a model to partition agents
into teams that are balanced in competences, personality and gender. Finally,
we present some preliminary empirical results that we obtained when analysing
student performance. Results show the benefits of a more informed team
composition that exploits individuals' competences besides information about
their personalities.
",Computer Science,Computer Science
"On analyzing and evaluating privacy measures for social networks under active attack   Widespread usage of complex interconnected social networks such as Facebook,
Twitter and LinkedIn in modern internet era has also unfortunately opened the
door for privacy violation of users of such networks by malicious entities. In
this article we investigate, both theoretically and empirically, privacy
violation measures of large networks under active attacks that was recently
introduced in (Information Sciences, 328, 403-417, 2016). Our theoretical
result indicates that the network manager responsible for prevention of privacy
violation must be very careful in designing the network if its topology does
not contain a cycle. Our empirical results shed light on privacy violation
properties of eight real social networks as well as a large number of synthetic
networks generated by both the classical Erdos-Renyi model and the scale-free
random networks generated by the Barabasi-Albert preferential-attachment model.
",Computer Science,Computer Science
"The Theory is Predictive, but is it Complete? An Application to Human Perception of Randomness   When we test a theory using data, it is common to focus on correctness: do
the predictions of the theory match what we see in the data? But we also care
about completeness: how much of the predictable variation in the data is
captured by the theory? This question is difficult to answer, because in
general we do not know how much ""predictable variation"" there is in the
problem. In this paper, we consider approaches motivated by machine learning
algorithms as a means of constructing a benchmark for the best attainable level
of prediction.
We illustrate our methods on the task of predicting human-generated random
sequences. Relative to an atheoretical machine learning algorithm benchmark, we
find that existing behavioral models explain roughly 15 percent of the
predictable variation in this problem. This fraction is robust across several
variations on the problem. We also consider a version of this approach for
analyzing field data from domains in which human perception and generation of
randomness has been used as a conceptual framework; these include sequential
decision-making and repeated zero-sum games. In these domains, our framework
for testing the completeness of theories provides a way of assessing their
effectiveness over different contexts; we find that despite some differences,
the existing theories are fairly stable across our field domains in their
performance relative to the benchmark. Overall, our results indicate that (i)
there is a significant amount of structure in this problem that existing models
have yet to capture and (ii) there are rich domains in which machine learning
may provide a viable approach to testing completeness.
",Computer Science; Statistics,Computer Science; Statistics
"Generalized Fréchet Bounds for Cell Entries in Multidimensional Contingency Tables   We consider the lattice, $\mathcal{L}$, of all subsets of a multidimensional
contingency table and establish the properties of monotonicity and
supermodularity for the marginalization function, $n(\cdot)$, on $\mathcal{L}$.
We derive from the supermodularity of $n(\cdot)$ some generalized Fréchet
inequalities complementing and extending inequalities of Dobra and Fienberg.
Further, we construct new monotonic and supermodular functions from $n(\cdot)$,
and we remark on the connection between supermodularity and some correlation
inequalities for probability distributions on lattices. We also apply an
inequality of Ky Fan to derive a new approach to Fréchet inequalities for
multidimensional contingency tables.
",Mathematics; Statistics,Mathematics; Statistics
"An application of the Hylleraas-B-splines basis set: High accuracy calculations of the static dipole polarizabilities of helium   The Hylleraas-B-splines basis set is introduced in this paper, which can be
used to obtain the eigenvalues and eigenstates of helium-like system's
Hamiltonian. Comparing with traditional B-splines basis, the rate of
convergence of our results has been significantly improved. Through combine
this method and pseudo-states sum over scheme, we obtained the high precision
values of static dipole porlarizabilities of the $1{}^1S-5{}^1S$,
$2{}^3S-6{}^3S$ states of helium in length and velocity gauges respectively,
and the results get good agreements. The final extrapolate results of
porlarizabilities in different quantum states arrived eight significant digits
at least, which fully illustrates the advantage and convenience of this method
in the problems involving continuous states.
",Physics,Physics
"On the Evaluation of Silicon Photomultipliers for Use as Photosensors in Liquid Xenon Detectors   Silicon photomultipliers (SiPMs) are potential solid-state alternatives to
traditional photomultiplier tubes (PMTs) for single-photon detection. In this
paper, we report on evaluating SensL MicroFC-10035-SMT SiPMs for their
suitability as PMT replacements. The devices were successfully operated in a
liquid-xenon detector, which demonstrates that SiPMs can be used in noble
element time projection chambers as photosensors. The devices were also cooled
down to 170 K to observe dark count dependence on temperature. No dependencies
on the direction of an applied 3.2 kV/cm electric field were observed with
respect to dark-count rate, gain, or photon detection efficiency.
",Physics,Physics
"On the bottom of spectra under coverings   For a Riemannian covering $M_1\to M_0$ of complete Riemannian manifolds with
boundary (possibly empty) and respective fundamental groups
$\Gamma_1\subseteq\Gamma_0$, we show that the bottoms of the spectra of $M_0$
and $M_1$ coincide if the right action of $\Gamma_0$ on
$\Gamma_1\backslash\Gamma_0$ is amenable.
",Mathematics,Mathematics
"New Directions In Cellular Automata   We Propose A Novel Automaton Model which uses Arithmetic Operations as the
Evolving Rules, each cell has the states of the Natural Numbers k = (N), a
radius of r = 1/2 and operates on an arbitrary input size. The Automaton reads
an Arithmetic Expression as an input and outputs another Arithmetic Expression.
In Addition, we simulate a variety of One Dimensional Cellular Automata
Structures with different Dynamics including Elementary Cellular Automata.
",Computer Science; Physics,Computer Science
"A Coherent vorticity preserving eddy viscosity correction for Large-Eddy Simulation   This paper introduces a new approach to Large-Eddy Simulation (LES) where
subgrid-scale (SGS) dissipation is applied proportionally to the degree of
local spectral broadening, hence mitigated or deactivated in regions dominated
by large-scale and/or laminar vortical motion. The proposed Coherent vorticity
preserving (CvP) LES methodology is based on the evaluation of the ratio of the
test-filtered to resolved (or grid-filtered) enstrophy $\sigma$. Values of
$\sigma$ close to 1 indicate low sub-test-filter turbulent activity, justifying
local deactivation of the SGS dissipation. The intensity of the SGS dissipation
is progressively increased for $\sigma < 1$ which corresponds to a small-scale
spectral broadening. The SGS dissipation is then fully activated in developed
turbulence characterized by $\sigma \le \sigma_{eq}$, where the value
$\sigma_{eq}$ is derived assuming a Kolmogorov spectrum. The proposed approach
can be applied to any eddy-viscosity model, is algorithmically simple and
computationally inexpensive. LES of Taylor-Green vortex breakdown demonstrates
that the CvP methodology improves the performance of traditional, non-dynamic
dissipative SGS models, capturing the peak of total turbulent kinetic energy
dissipation during transition. Similar accuracy is obtained by adopting
Germano's dynamic procedure albeit at more than twice the computational
overhead. A CvP-LES of a pair of unstable periodic helical vortices is shown to
predict accurately the experimentally observed growth rate using coarse
resolutions. The ability of the CvP methodology to dynamically sort the
coherent, large-scale motion from the smaller, broadband scales during
transition is demonstrated via flow visualizations. LES of compressible channel
are carried out and show a good match with a reference DNS.
",Computer Science; Physics,Physics
"Magnetic properties of nanoparticles compacts with controlled broadening of the particle size distribution   Binary random compacts with different proportions of small (volume V) and
large (volume 2V) bare maghemite nanoparticles (NPs) are used to investigate
the effect of controllably broadening the particle size distribution on the
magnetic properties of magnetic NP assemblies with strong dipolar interaction.
A series of eight random mixtures of highly uniform 9.0 and 11.5 nm diameter
maghemite particles prepared by thermal decomposition are studied. In spite of
severely broadened size distributions in the mixed samples, well defined
superspin glass transition temperatures are observed across the series, their
values increasing linearly with the weight fraction of large particles.
",Physics,Physics
"Projection Theorems Using Effective Dimension   In this paper we use the theory of computing to study fractal dimensions of
projections in Euclidean spaces. A fundamental result in fractal geometry is
Marstrand's projection theorem, which shows that for every analytic set E, for
almost every line L, the Hausdorff dimension of the orthogonal projection of E
onto L is maximal. We use Kolmogorov complexity to give two new results on the
Hausdorff and packing dimensions of orthogonal projections onto lines. The
first shows that the conclusion of Marstrand's theorem holds whenever the
Hausdorff and packing dimensions agree on the set E, even if E is not analytic.
Our second result gives a lower bound on the packing dimension of projections
of arbitrary sets. Finally, we give a new proof of Marstrand's theorem using
the theory of computing.
",Computer Science; Mathematics,Mathematics
"General Bayesian Updating and the Loss-Likelihood Bootstrap   In this paper we revisit the weighted likelihood bootstrap, a method that
generates samples from an approximate Bayesian posterior of a parametric model.
We show that the same method can be derived, without approximation, under a
Bayesian nonparametric model with the parameter of interest defined as
minimising an expected negative log-likelihood under an unknown sampling
distribution. This interpretation enables us to extend the weighted likelihood
bootstrap to posterior sampling for parameters minimizing an expected loss. We
call this method the loss-likelihood bootstrap. We make a connection between
this and general Bayesian updating, which is a way of updating prior belief
distributions without needing to construct a global probability model, yet
requires the calibration of two forms of loss function. The loss-likelihood
bootstrap is used to calibrate the general Bayesian posterior by matching
asymptotic Fisher information. We demonstrate the methodology on a number of
examples.
",Statistics,Mathematics; Statistics
"Deep learning for universal linear embeddings of nonlinear dynamics   Identifying coordinate transformations that make strongly nonlinear dynamics
approximately linear is a central challenge in modern dynamical systems. These
transformations have the potential to enable prediction, estimation, and
control of nonlinear systems using standard linear theory. The Koopman operator
has emerged as a leading data-driven embedding, as eigenfunctions of this
operator provide intrinsic coordinates that globally linearize the dynamics.
However, identifying and representing these eigenfunctions has proven to be
mathematically and computationally challenging. This work leverages the power
of deep learning to discover representations of Koopman eigenfunctions from
trajectory data of dynamical systems. Our network is parsimonious and
interpretable by construction, embedding the dynamics on a low-dimensional
manifold that is of the intrinsic rank of the dynamics and parameterized by the
Koopman eigenfunctions. In particular, we identify nonlinear coordinates on
which the dynamics are globally linear using a modified auto-encoder. We also
generalize Koopman representations to include a ubiquitous class of systems
that exhibit continuous spectra, ranging from the simple pendulum to nonlinear
optics and broadband turbulence. Our framework parametrizes the continuous
frequency using an auxiliary network, enabling a compact and efficient
embedding at the intrinsic rank, while connecting our models to half a century
of asymptotics. In this way, we benefit from the power and generality of deep
learning, while retaining the physical interpretability of Koopman embeddings.
",Computer Science; Statistics,Computer Science; Statistics
"Random matrices and the New York City subway system   We analyze subway arrival times in the New York City subway system. We find
regimes where the gaps between trains exhibit both (unitarily invariant) random
matrix statistics and Poisson statistics. The departure from random matrix
statistics is captured by the value of the Coulomb potential along the subway
route. This departure becomes more pronounced as trains make more stops.
",Physics,Mathematics
"Effective perturbation theory for linear operators   We propose a new approach to the spectral theory of perturbed linear
operators , in the case of a simple isolated eigenvalue. We obtain two kind of
results: ""radius bounds"" which ensure perturbation theory applies for
perturbations up to an explicit size, and ""regularity bounds"" which control the
variations of eigendata to any order. Our method is based on the Implicit
Function Theorem and proceeds by establishing differential inequalities on two
natural quantities: the norm of the projection to the eigendirection, and the
norm of the reduced resolvent. We obtain completely explicit results without
any assumption on the underlying Banach space. In companion articles, on the
one hand we apply the regularity bounds to Markov chains, obtaining
non-asymptotic concentration and Berry-Ess{é}en inequalities with explicit
constants, and on the other hand we apply the radius bounds to transfer
operator of intermittent maps, obtaining explicit high-temperature regimes
where a spectral gap occurs.
",Mathematics,Mathematics
"The toric Frobenius morphism and a conjecture of Orlov   We combine the Bondal-Uehara method for producing exceptional collections on
toric varieties with a result of the first author and Favero to expand the set
of varieties satisfying Orlov's Conjecture on derived dimension.
",Mathematics,Mathematics
"Fast counting of medium-sized rooted subgraphs   We prove that counting copies of any graph $F$ in another graph $G$ can be
achieved using basic matrix operations on the adjacency matrix of $G$.
Moreover, the resulting algorithm is competitive for medium-sized $F$: our
algorithm recovers the best known complexity for rooted 6-clique counting and
improves on the best known for 9-cycle counting. Underpinning our proofs is the
new result that, for a general class of graph operators, matrix operations are
homomorphisms for operations on rooted graphs.
",Computer Science; Mathematics,Computer Science; Mathematics
"Dynamic analysis and control PID path of a model type gantry crane   This paper presents an alternate form for the dynamic modelling of a
mechanical system that simulates in real life a gantry crane type, using
Euler's classical mechanics and Lagrange formalism, which allows find the
equations of motion that our model describe. Moreover, it has a basic model
design system using the SolidWorks software, based on the material and
dimensions of the model provides some physical variables necessary for
modelling. In order to verify the theoretical results obtained, a contrast was
made between solutions obtained by simulation in SimMechanics-Matlab and
Euler-Lagrange equations system, has been solved through Matlab libraries for
solving equation's systems of the type and order obtained. The force is
determined, but not as exerted by the spring, as this will be the control
variable. The objective to bring the mass of the pendulum from one point to
another with a specified distance without the oscillation from it, so that, the
answer is overdamped. This article includes an analysis of PID control in which
the equations of motion of Euler-Lagrange are rewritten in the state space,
once there, they were implemented in Simulink to get the natural response of
the system to a step input in F and then draw the desired trajectories.
",Computer Science; Physics,Physics
"On Quaternionic Tori and their Moduli Spaces   Quaternionic tori are defined as quotients of the skew field $\mathbb{H}$ of
quaternions by rank-4 lattices. Using slice regular functions, these tori are
endowed with natural structures of quaternionic manifolds (in fact quaternionic
curves), and a fundamental region in a $12$-dimensional real subspace is then
constructed to classify them up to biregular diffeomorphisms. The points of the
moduli space correspond to suitable \emph{special} bases of rank-4 lattices,
which are studied with respect to the action of the group $GL(4, \mathbb{Z})$,
and up to biregular diffeomeorphisms. All tori with a non trivial group of
biregular automorphisms - and all possible groups of their biregular
automorphisms - are then identified, and recognized to correspond to five
different subsets of boundary points of the moduli space.
",Mathematics,Mathematics
"Dynamically reconfigurable metal-semiconductor Yagi-Uda nanoantenna   We propose a novel type of tunable Yagi-Uda nanoantenna composed of
metal-dielectric (Ag-Ge) core-shell nanoparticles. We show that, due to the
combination of two types of resonances in each nanoparticle, such hybrid
Yagi-Uda nanoantenna can operate in two different regimes. Besides the
conventional nonresonant operation regime at low frequencies, characterized by
highly directive emission in the forward direction, there is another one at
higher frequencies caused by hybrid magneto-electric response of the core-shell
nanoparticles. This regime is based on the excitation of the van Hove
singularity, and emission in this regime is accompanied by high values of
directivity and Purcell factor within the same narrow frequency range. Our
analysis reveals the possibility of flexible dynamical tuning of the hybrid
nanoantenna emission pattern via electron-hole plasma excitation by 100
femtosecond pump pulse with relatively low peak intensities $\sim$200
MW/cm$^2$.
",Physics,Physics
"A Note on Property Testing Sum of Squares and Multivariate Polynomial Interpolation   In this paper, we investigate property testing whether or not a degree d
multivariate poly- nomial is a sum of squares or is far from a sum of squares.
We show that if we require that the property tester always accepts YES
instances and uses random samples, $n^{\Omega(d)}$ samples are required, which
is not much fewer than it would take to completely determine the polynomial. To
prove this lower bound, we show that with high probability, multivariate
polynomial in- terpolation matches arbitrary values on random points and the
resulting polynomial has small norm. We then consider a particular polynomial
which is non-negative yet not a sum of squares and use pseudo-expectation
values to prove it is far from being a sum of squares.
",Computer Science,Mathematics; Statistics
"Origin of layer dependence in band structures of two-dimensional materials   We study the origin of layer dependence in band structures of two-dimensional
materials. We find that the layer dependence, at the density functional theory
(DFT) level, is a result of quantum confinement and the non-linearity of the
exchange-correlation functional. We use this to develop an efficient scheme for
performing DFT and GW calculations of multilayer systems. We show that the DFT
and quasiparticle band structures of a multilayer system can be derived from a
single calculation on a monolayer of the material. We test this scheme on
multilayers of MoS$_2$, graphene and phosphorene. This new scheme yields
results in excellent agreement with the standard methods at a fraction of the
computation cost. This helps overcome the challenge of performing fully
converged GW calculations on multilayers of 2D materials, particularly in the
case of transition metal dichalcogenides which involve very stringent
convergence parameters.
",Physics,Physics
"Opportunistic Downlink Interference Alignment for Multi-Cell MIMO Networks   In this paper, we propose an opportunistic downlink interference alignment
(ODIA) for interference-limited cellular downlink, which intelligently combines
user scheduling and downlink IA techniques. The proposed ODIA not only
efficiently reduces the effect of inter-cell interference from other-cell base
stations (BSs) but also eliminates intra-cell interference among spatial
streams in the same cell. We show that the minimum number of users required to
achieve a target degrees-of-freedom (DoF) can be fundamentally reduced, i.e.,
the fundamental user scaling law can be improved by using the ODIA, compared
with the existing downlink IA schemes. In addition, we adopt a limited feedback
strategy in the ODIA framework, and then analyze the number of feedback bits
required for the system with limited feedback to achieve the same user scaling
law of the ODIA as the system with perfect CSI. We also modify the original
ODIA in order to further improve sum-rate, which achieves the optimal multiuser
diversity gain, i.e., $\log\log N$, per spatial stream even in the presence of
downlink inter-cell interference, where $N$ denotes the number of users in a
cell. Simulation results show that the ODIA significantly outperforms existing
interference management techniques in terms of sum-rate in realistic cellular
environments. Note that the ODIA operates in a non-collaborative and decoupled
manner, i.e., it requires no information exchange among BSs and no iterative
beamformer optimization between BSs and users, thus leading to an easier
implementation.
",Computer Science; Mathematics,Computer Science
"Photometric Redshifts for Hyper Suprime-Cam Subaru Strategic Program Data Release 1   Photometric redshifts are a key component of many science objectives in the
Hyper Suprime-Cam Subaru Strategic Program (HSC-SSP). In this paper, we
describe and compare the codes used to compute photometric redshifts for
HSC-SSP, how we calibrate them, and the typical accuracy we achieve with the
HSC five-band photometry (grizy). We introduce a new point estimator based on
an improved loss function and demonstrate that it works better than other
commonly used estimators. We find that our photo-z's are most accurate at
0.2<~zphot<~1.5, where we can straddle the 4000A break. We achieve
sigma(d_zphot/(1+zphot))~0.05 and an outlier rate of about 15% for galaxies
down to i=25 within this redshift range. If we limit to a brighter sample of
i<24, we achieve sigma~0.04 and ~8% outliers. Our photo-z's should thus enable
many science cases for HSC-SSP. We also characterize the accuracy of our
redshift probability distribution function (PDF) and discover that some codes
over/under-estimate the redshift uncertainties, which have implications for
N(z) reconstruction. Our photo-z products for the entire area in the Public
Data Release 1 are publicly available, and both our catalog products (such as
point estimates) and full PDFs can be retrieved from the data release site,
this https URL.
",Physics,Physics
"A Spatio-Temporal Multivariate Shared Component Model with an Application in Iran Cancer Data   Among the proposals for joint disease mapping, the shared component model has
become more popular. Another recent advance to strengthen inference of disease
data has been the extension of purely spatial models to include time and
space-time interaction. Such analyses have additional benefits over purely
spatial models. However, only a few proposed spatio-temporal models could
address analysing multiple diseases jointly.
In the proposed model, each component is shared by different subsets of
diseases, spatial and temporal trends are considered for each component, and
the relative weight of these trends for each component for each relevant
disease can be estimated. We present an application of the proposed method on
incidence rates of seven prevalent cancers in Iran. The effect of the shared
components on the individual cancer types can be identified. Regional and
temporal variation in relative risks is shown. We present a model which
combines the benefits of shared-components with spatio-temporal techniques for
multivariate data. We show, how the model allows to analyse geographical and
temporal variation among diseases beyond previous approaches.
",Statistics,Statistics
"Lattice Boltzmann study of chemically-driven self-propelled droplets   We numerically study the behavior of self-propelled liquid droplets whose
motion is triggered by a Marangoni-like flow. This latter is generated by
variations of surfactant concentration which affect the droplet surface tension
promoting its motion. In the present paper a model for droplets with a third
amphiphilic component is adopted. The dynamics is described by Navier-Stokes
and convection-diffusion equations, solved by lattice Boltzmann method coupled
with finite-difference schemes. We focus on two cases. First the study of
self-propulsion of an isolated droplet is carried on and, then, the interaction
of two self-propelled droplets is investigated. In both cases, when the
surfactant migrates towards the interface, a quadrupolar vortex of the velocity
field forms inside the droplet and causes the motion. A weaker dipolar field
emerges instead when the surfactant is mainly diluted in the bulk. The dynamics
of two interacting droplets is more complex and strongly depends on their
reciprocal distance. If, in a head-on collision, droplets are close enough, the
velocity field initially attracts them until a motionless steady state is
achieved. If the droplets are vertically shifted, the hydrodynamic field leads
to an initial reciprocal attraction followed by a scattering along opposite
directions. This hydrodynamic interaction acts on a separation of some droplet
radii otherwise it becomes negligible and droplets motion is only driven by
Marangoni effect. Finally, if one of the droplets is passive, this latter is
generally advected by the fluid flow generated by the active one.
",Physics,Physics
"Wavelength Does Not Equal Pressure: Vertical Contribution Functions and their Implications for Mapping Hot Jupiters   Multi-band phase variations in principle allow us to infer the longitudinal
temperature distributions of planets as a function of height in their
atmospheres. For example, 3.6 micron emission originates from deeper layers of
the atmosphere than 4.5 micron due to greater water vapor absorption at the
longer wavelength. Since heat transport efficiency increases with pressure, we
expect thermal phase curves at 3.6 micron to exhibit smaller amplitudes and
greater phase offsets than at 4.5 micron; this trend is not observed. Of the
seven hot Jupiters with full-orbit phase curves at 3.6 and 4.5 micron, all have
greater phase amplitude at 3.6 micron than at 4.5 micron, while four of seven
exhibit a greater phase offset at 3.6 micron. We use a 3D
radiative-hydrodynamic model to calculate theoretical phase curves of HD
189733b, assuming thermo-chemical equilibrium. The model exhibits temperature,
pressure, and wavelength dependent opacity, primarily driven by carbon
chemistry: CO is energetically favored on the dayside, while CH4 is favored on
the cooler nightside. Infrared opacity therefore changes by orders of magnitude
between day and night, producing dramatic vertical shifts in the
wavelength-specific photospheres, which would complicate eclipse or phase
mapping with spectral data. The model predicts greater relative phase amplitude
and greater phase offset at 3.6 micron than at 4.5 micron, in agreement with
the data. Our model qualitatively explains the observed phase curves, but is in
tension with current thermo-chemical kinetics models that predict zonally
uniform atmospheric composition due to transport of CO from the hot regions of
the atmosphere.
",Physics,Physics
"Temporal Graph Offset Reconstruction: Towards Temporally Robust Graph Representation Learning   Graphs are a commonly used construct for representing relationships between
elements in complex high dimensional datasets. Many real-world phenomenon are
dynamic in nature, meaning that any graph used to represent them is inherently
temporal. However, many of the machine learning models designed to capture
knowledge about the structure of these graphs ignore this rich temporal
information when creating representations of the graph. This results in models
which do not perform well when used to make predictions about the future state
of the graph -- especially when the delta between time stamps is not small. In
this work, we explore a novel training procedure and an associated unsupervised
model which creates graph representations optimised to predict the future state
of the graph. We make use of graph convolutional neural networks to encode the
graph into a latent representation, which we then use to train our temporal
offset reconstruction method, inspired by auto-encoders, to predict a later
time point -- multiple time steps into the future. Using our method, we
demonstrate superior performance for the task of future link prediction
compared with none-temporal state-of-the-art baselines. We show our approach to
be capable of outperforming non-temporal baselines by 38% on a real world
dataset.
",Computer Science,Computer Science; Statistics
"Using Human Brain Activity to Guide Machine Learning   Machine learning is a field of computer science that builds algorithms that
learn. In many cases, machine learning algorithms are used to recreate a human
ability like adding a caption to a photo, driving a car, or playing a game.
While the human brain has long served as a source of inspiration for machine
learning, little effort has been made to directly use data collected from
working brains as a guide for machine learning algorithms. Here we demonstrate
a new paradigm of ""neurally-weighted"" machine learning, which takes fMRI
measurements of human brain activity from subjects viewing images, and infuses
these data into the training process of an object recognition learning
algorithm to make it more consistent with the human brain. After training,
these neurally-weighted classifiers are able to classify images without
requiring any additional neural data. We show that our neural-weighting
approach can lead to large performance gains when used with traditional machine
vision features, as well as to significant improvements with already
high-performing convolutional neural network features. The effectiveness of
this approach points to a path forward for a new class of hybrid machine
learning algorithms which take both inspiration and direct constraints from
neuronal data.
",Computer Science,Computer Science; Statistics
"DepthSynth: Real-Time Realistic Synthetic Data Generation from CAD Models for 2.5D Recognition   Recent progress in computer vision has been dominated by deep neural networks
trained over large amounts of labeled data. Collecting such datasets is however
a tedious, often impossible task; hence a surge in approaches relying solely on
synthetic data for their training. For depth images however, discrepancies with
real scans still noticeably affect the end performance. We thus propose an
end-to-end framework which simulates the whole mechanism of these devices,
generating realistic depth data from 3D models by comprehensively modeling
vital factors e.g. sensor noise, material reflectance, surface geometry. Not
only does our solution cover a wider range of sensors and achieve more
realistic results than previous methods, assessed through extended evaluation,
but we go further by measuring the impact on the training of neural networks
for various recognition tasks; demonstrating how our pipeline seamlessly
integrates such architectures and consistently enhances their performance.
",Computer Science,Computer Science; Statistics
"Topic supervised non-negative matrix factorization   Topic models have been extensively used to organize and interpret the
contents of large, unstructured corpora of text documents. Although topic
models often perform well on traditional training vs. test set evaluations, it
is often the case that the results of a topic model do not align with human
interpretation. This interpretability fallacy is largely due to the
unsupervised nature of topic models, which prohibits any user guidance on the
results of a model. In this paper, we introduce a semi-supervised method called
topic supervised non-negative matrix factorization (TS-NMF) that enables the
user to provide labeled example documents to promote the discovery of more
meaningful semantic structure of a corpus. In this way, the results of TS-NMF
better match the intuition and desired labeling of the user. The core of TS-NMF
relies on solving a non-convex optimization problem for which we derive an
iterative algorithm that is shown to be monotonic and convergent to a local
optimum. We demonstrate the practical utility of TS-NMF on the Reuters and
PubMed corpora, and find that TS-NMF is especially useful for conceptual or
broad topics, where topic key terms are not well understood. Although
identifying an optimal latent structure for the data is not a primary objective
of the proposed approach, we find that TS-NMF achieves higher weighted Jaccard
similarity scores than the contemporary methods, (unsupervised) NMF and latent
Dirichlet allocation, at supervision rates as low as 10% to 20%.
",Computer Science; Statistics,Computer Science
"Dynamic Advisor-Based Ensemble (dynABE): Case Study in Stock Trend Prediction of Critical Metal Companies   The demand for metals by modern technology has been shifting from common base
metals to a variety of minor metals, such as cobalt or indium. The industrial
importance and limited geological availability of some minor metals have led to
them being considered more ""critical,"" and there is a growing investment
interest in such critical metals and their producing companies. In this
research, we create a novel framework, Dynamic Advisor-Based Ensemble (dynABE),
for stock prediction and use critical metal companies as case study. dynABE
uses domain knowledge to diversify the feature set by dividing them into
different ""advisors."" creates high-level ensembles with complex base models for
each advisor, and combines the advisors together dynamically during validation
with a novel and effective online update strategy. We test dynABE on three
cobalt-related companies, and it achieves the best-case misclassification error
of 31.12% and excess return of 477% compared to the stock itself in a year and
a half. In addition to presenting an effective stock prediction model with
decent profitabilities, this research further analyzes dynABE to visualize how
it works in practice, which also yields discoveries of its interesting
behaviors when processing time-series data.
",Quantitative Finance,Computer Science
"Lower bounds for several online variants of bin packing   We consider several previously studied online variants of bin packing and
prove new and improved lower bounds on the asymptotic competitive ratios for
them. For that, we use a method of fully adaptive constructions. In particular,
we improve the lower bound for the asymptotic competitive ratio of online
square packing significantly, raising it from roughly 1.68 to above 1.75.
",Computer Science,Mathematics
"Monte Carlo study of the Coincidence Resolving Time of a liquid xenon PET scanner, using Cherenkov radiation   In this paper we use detailed Monte Carlo simulations to demonstrate that
liquid xenon (LXe) can be used to build a Cherenkov-based TOF-PET, with an
intrinsic coincidence resolving time (CRT) in the vicinity of 10 ps. This
extraordinary performance is due to three facts: a) the abundant emission of
Cherenkov photons by liquid xenon; b) the fact that LXe is transparent to
Cherenkov light; and c) the fact that the fastest photons in LXe have
wavelengths higher than 300 nm, therefore making it possible to separate the
detection of scintillation and Cherenkov light. The CRT in a Cherenkov LXe
TOF-PET detector is, therefore, dominated by the resolution (time jitter)
introduced by the photosensors and the electronics. However, we show that for
sufficiently fast photosensors (e.g, an overall 40 ps jitter, which can be
achieved by current micro-channel plate photomultipliers) the overall CRT
varies between 30 and 55 ps, depending of the detection efficiency. This is
still one order of magnitude better than commercial CRT devices and improves by
a factor 3 the best CRT obtained with small laboratory prototypes.
",Physics,Physics
"A study on text-score disagreement in online reviews   In this paper, we focus on online reviews and employ artificial intelligence
tools, taken from the cognitive computing field, to help understanding the
relationships between the textual part of the review and the assigned numerical
score. We move from the intuitions that 1) a set of textual reviews expressing
different sentiments may feature the same score (and vice-versa); and 2)
detecting and analyzing the mismatches between the review content and the
actual score may benefit both service providers and consumers, by highlighting
specific factors of satisfaction (and dissatisfaction) in texts.
To prove the intuitions, we adopt sentiment analysis techniques and we
concentrate on hotel reviews, to find polarity mismatches therein. In
particular, we first train a text classifier with a set of annotated hotel
reviews, taken from the Booking website. Then, we analyze a large dataset, with
around 160k hotel reviews collected from Tripadvisor, with the aim of detecting
a polarity mismatch, indicating if the textual content of the review is in
line, or not, with the associated score.
Using well established artificial intelligence techniques and analyzing in
depth the reviews featuring a mismatch between the text polarity and the score,
we find that -on a scale of five stars- those reviews ranked with middle scores
include a mixture of positive and negative aspects.
The approach proposed here, beside acting as a polarity detector, provides an
effective selection of reviews -on an initial very large dataset- that may
allow both consumers and providers to focus directly on the review subset
featuring a text/score disagreement, which conveniently convey to the user a
summary of positive and negative features of the review target.
",Computer Science,Computer Science
"Perfect spike detection via time reversal   Spiking neuronal networks are usually simulated with three main simulation
schemes: the classical time-driven and event-driven schemes, and the more
recent hybrid scheme. All three schemes evolve the state of a neuron through a
series of checkpoints: equally spaced in the first scheme and determined
neuron-wise by spike events in the latter two. The time-driven and the hybrid
scheme determine whether the membrane potential of a neuron crosses a threshold
at the end of of the time interval between consecutive checkpoints. Threshold
crossing can, however, occur within the interval even if this test is negative.
Spikes can therefore be missed. The present work derives, implements, and
benchmarks a method for perfect retrospective spike detection. This method can
be applied to neuron models with affine or linear subthreshold dynamics. The
idea behind the method is to propagate the threshold with a time-inverted
dynamics, testing whether the threshold crosses the neuron state to be evolved,
rather than vice versa. Algebraically this translates into a set of
inequalities necessary and sufficient for threshold crossing. This test is
slower than the imperfect one, but faster than an alternative perfect tests
based on bisection or root-finding methods. Comparison confirms earlier results
that the imperfect test rarely misses spikes (less than a fraction $1/10^8$ of
missed spikes) in biologically relevant settings. This study offers an
alternative geometric point of view on neuronal dynamics.
",Physics; Mathematics,Quantitative Biology
"Estimating the reproductive number, total outbreak size, and reporting rates for Zika epidemics in South and Central America   As South and Central American countries prepare for increased birth defects
from Zika virus outbreaks and plan for mitigation strategies to minimize
ongoing and future outbreaks, understanding important characteristics of Zika
outbreaks and how they vary across regions is a challenging and important
problem. We developed a mathematical model for the 2015 Zika virus outbreak
dynamics in Colombia, El Salvador, and Suriname. We fit the model to publicly
available data provided by the Pan American Health Organization, using
Approximate Bayesian Computation to estimate parameter distributions and
provide uncertainty quantification. An important model input is the at-risk
susceptible population, which can vary with a number of factors including
climate, elevation, population density, and socio-economic status. We informed
this initial condition using the highest historically reported dengue incidence
modified by the probable dengue reporting rates in the chosen countries. The
model indicated that a country-level analysis was not appropriate for Colombia.
We then estimated the basic reproduction number, or the expected number of new
human infections arising from a single infected human, to range between 4 and 6
for El Salvador and Suriname with a median of 4.3 and 5.3, respectively. We
estimated the reporting rate to be around 16% in El Salvador and 18% in
Suriname with estimated total outbreak sizes of 73,395 and 21,647 people,
respectively. The uncertainty in parameter estimates highlights a need for
research and data collection that will better constrain parameter ranges.
",Statistics,Statistics
"Generalized Moran sets Generated by Step-wise Adjustable Iterated Function Systems   In this article we provide a systematic way of creating generalized Moran
sets using an analogous iterated function system (IFS) procedure. We use a
step-wise adjustable IFS to introduce some variance (such as
non-self-similarity) in the fractal limit sets. The process retains the
computational simplicity of a standard IFS procedure. In our construction of
the generalized Moran sets, we also weaken the fourth Moran Structure Condition
that requires the same pattern of diameter ratios be used across a generation.
Moreover, we provide upper and lower bounds for the Hausdorff dimension of the
fractals created from this generalized process. Specific examples (Cantor-like
sets, Sierpinski-like Triangles, etc) with the calculations of their
corresponding dimensions are studied.
",Physics; Mathematics,Computer Science
"End-to-end Networks for Supervised Single-channel Speech Separation   The performance of single channel source separation algorithms has improved
greatly in recent times with the development and deployment of neural networks.
However, many such networks continue to operate on the magnitude spectrogram of
a mixture, and produce an estimate of source magnitude spectrograms, to perform
source separation. In this paper, we interpret these steps as additional neural
network layers and propose an end-to-end source separation network that allows
us to estimate the separated speech waveform by operating directly on the raw
waveform of the mixture. Furthermore, we also propose the use of masking based
end-to-end separation networks that jointly optimize the mask and the latent
representations of the mixture waveforms. These networks show a significant
improvement in separation performance compared to existing architectures in our
experiments. To train these end-to-end models, we investigate the use of
composite cost functions that are derived from objective evaluation metrics as
measured on waveforms. We present subjective listening test results that
demonstrate the improvement attained by using masking based end-to-end networks
and also reveal insights into the performance of these cost functions for
end-to-end source separation.
",Computer Science,Computer Science
"Faster Boosting with Smaller Memory   The two state-of-the-art implementations of boosted trees: XGBoost and
LightGBM, can process large training sets extremely fast. However, this
performance requires that memory size is sufficient to hold a 2-3 multiple of
the training set size. This paper presents an alternative approach to
implementing boosted trees. which achieves a significant speedup over XGBoost
and LightGBM, especially when memory size is small. This is achieved using a
combination of two techniques: early stopping and stratified sampling, which
are explained and analyzed in the paper. We describe our implementation and
present experimental results to support our claims.
",Computer Science; Statistics,Computer Science
"Probing Spatial Locality in Ionic Liquids with the Grand Canonical Adaptive Resolution Molecular Dynamics Technique   We employ the Grand Canonical Adaptive Resolution Molecular Dynamics
Technique (GC-AdResS) to test the spatial locality of the 1-ethyl 3-methyl
imidazolium chloride liquid. In GC-AdResS atomistic details are kept only in an
open sub-region of the system while the environment is treated at
coarse-grained level, thus if spatial quantities calculated in such a
sub-region agree with the equivalent quantities calculated in a full atomistic
simulation then the atomistic degrees of freedom outside the sub-region play a
negligible role. The size of the sub-region fixes the degree of spatial
locality of a certain quantity. We show that even for sub-regions whose radius
corresponds to the size of a few molecules, spatial properties are reasonably
{reproduced} thus suggesting a higher degree of spatial locality, a hypothesis
put forward also by other {researchers} and that seems to play an important
role for the characterization of fundamental properties of a large class of
ionic liquids.
",Physics,Physics
"Algebraic entropy of (integrable) lattice equations and their reductions   We study the growth of degrees in many autonomous and non-autonomous lattice
equations defined by quad rules with corner boundary values, some of which are
known to be integrable by other characterisations. Subject to an enabling
conjecture, we prove polynomial growth for a large class of equations which
includes the Adler-Bobenko-Suris equations and Viallet's $Q_V$ and its
non-autonomous generalization. Our technique is to determine the ambient degree
growth of the projective version of the lattice equations and to conjecture the
growth of their common factors at each lattice vertex, allowing the true degree
growth to be found. The resulting degrees satisfy a linear partial difference
equation which is universal, i.e. the same for all the integrable lattice
equations considered. When we take periodic reductions of these equations,
which includes staircase initial conditions, we obtain from this linear partial
difference equation an ordinary difference equation for degrees that implies
quadratic or linear degree growth. We also study growth of degree of several
non-integrable lattice equations. Exponential growth of degrees of these
equations, and their mapping reductions, is also proved subject to a
conjecture.
",Physics,Mathematics
"Weakly supervised training of deep convolutional neural networks for overhead pedestrian localization in depth fields   Overhead depth map measurements capture sufficient amount of information to
enable human experts to track pedestrians accurately. However, fully automating
this process using image analysis algorithms can be challenging. Even though
hand-crafted image analysis algorithms are successful in many common cases,
they fail frequently when there are complex interactions of multiple objects in
the image. Many of the assumptions underpinning the hand-crafted solutions do
not hold in these cases and the multitude of exceptions are hard to model
precisely. Deep Learning (DL) algorithms, on the other hand, do not require
hand crafted solutions and are the current state-of-the-art in object
localization in images. However, they require exceeding amount of annotations
to produce successful models. In the case of object localization these
annotations are difficult and time consuming to produce. In this work we
present an approach for developing pedestrian localization models using DL
algorithms with efficient weak supervision from an expert. We circumvent the
need for annotation of large corpus of data by annotating only small amount of
patches and relying on synthetic data augmentation as a vehicle for injecting
expert knowledge in the model training. This approach of weak supervision
through expert selection of representative patches, suitable transformations
and synthetic data augmentations enables us to successfully develop DL models
for pedestrian localization efficiently.
",Computer Science; Physics,Computer Science; Statistics
"Using Minimum Path Cover to Boost Dynamic Programming on DAGs: Co-Linear Chaining Extended   Aligning sequencing reads on graph representations of genomes is an important
ingredient of pan-genomics. Such approaches typically find a set of local
anchors that indicate plausible matches between substrings of a read to
subpaths of the graph. These anchor matches are then combined to form a
(semi-local) alignment of the complete read on a subpath. Co-linear chaining is
an algorithmically rigorous approach to combine the anchors. It is a well-known
approach for the case of two sequences as inputs. Here we extend the approach
so that one of the inputs can be a directed acyclic graph (DAGs), e.g. a
splicing graph in transcriptomics or a variant graph in pan-genomics.
This extension to DAGs turns out to have a tight connection to the minimum
path cover problem, asking for a minimum-cardinality set of paths that cover
all the nodes of a DAG. We study the case when the size $k$ of a minimum path
cover is small, which is often the case in practice. First, we propose an
algorithm for finding a minimum path cover of a DAG $(V,E)$ in $O(k|E|\log|V|)$
time, improving all known time-bounds when $k$ is small and the DAG is not too
dense. Second, we introduce a general technique for extending dynamic
programming (DP) algorithms from sequences to DAGs. This is enabled by our
minimum path cover algorithm, and works by mimicking the DP algorithm for
sequences on each path of the minimum path cover. This technique generally
produces algorithms that are slower than their counterparts on sequences only
by a factor $k$. Our technique can be applied, for example, to the classical
longest increasing subsequence and longest common subsequence problems,
extended to labeled DAGs. Finally, we apply this technique to the co-linear
chaining problem. We also implemented the new co-linear chaining approach.
Experiments on splicing graphs show that the new method is efficient also in
practice.
",Computer Science,Computer Science
"The Diverse Club: The Integrative Core of Complex Networks   A complex system can be represented and analyzed as a network, where nodes
represent the units of the network and edges represent connections between
those units. For example, a brain network represents neurons as nodes and axons
between neurons as edges. In many networks, some nodes have a
disproportionately high number of edges. These nodes also have many edges
between each other, and are referred to as the rich club. In many different
networks, the nodes of this club are assumed to support global network
integration. However, another set of nodes potentially exhibits a connectivity
structure that is more advantageous to global network integration. Here, in a
myriad of different biological and man-made networks, we discover the diverse
club--a set of nodes that have edges diversely distributed across the network.
The diverse club exhibits, to a greater extent than the rich club, properties
consistent with an integrative network function--these nodes are more highly
interconnected and their edges are more critical for efficient global
integration. Moreover, we present a generative evolutionary network model that
produces networks with a diverse club but not a rich club, thus demonstrating
that these two clubs potentially evolved via distinct selection pressures.
Given the variety of different networks that we analyzed--the c. elegans, the
macaque brain, the human brain, the United States power grid, and global air
traffic--the diverse club appears to be ubiquitous in complex networks. These
results warrant the distinction and analysis of two critical clubs of nodes in
all complex systems.
",Physics,Computer Science; Physics
"A Reassessment of Absolute Energies of the X-ray L Lines of Lanthanide Metals   We introduce a new technique for determining x-ray fluorescence line energies
and widths, and we present measurements made with this technique of 22 x-ray L
lines from lanthanide-series elements. The technique uses arrays of
transition-edge sensors, microcalorimeters with high energy-resolving power
that simultaneously observe both calibrated x-ray standards and the x-ray
emission lines under study. The uncertainty in absolute line energies is
generally less than 0.4 eV in the energy range of 4.5 keV to 7.5 keV. Of the
seventeen line energies of neodymium, samarium, and holmium, thirteen are found
to be consistent with the available x-ray reference data measured after 1990;
only two of the four lines for which reference data predate 1980, however, are
consistent with our results. Five lines of terbium are measured with
uncertainties that improve on those of existing data by factors of two or more.
These results eliminate a significant discrepancy between measured and
calculated x-ray line energies for the terbium Ll line (5.551 keV). The line
widths are also measured, with uncertainties of 0.6 eV or less on the
full-width at half-maximum in most cases. These measurements were made with an
array of approximately one hundred superconducting x- ray microcalorimeters,
each sensitive to an energy band from 1 keV to 8 keV. No energy-dispersive
spectrometer has previously been used for absolute-energy estimation at this
level of accuracy. Future spectrometers, with superior linearity and energy
resolution, will allow us to improve on these results and expand the
measurements to more elements and a wider range of line energies.
",Physics,Physics
"Critical Points of Neural Networks: Analytical Forms and Landscape Properties   Due to the success of deep learning to solving a variety of challenging
machine learning tasks, there is a rising interest in understanding loss
functions for training neural networks from a theoretical aspect. Particularly,
the properties of critical points and the landscape around them are of
importance to determine the convergence performance of optimization algorithms.
In this paper, we provide full (necessary and sufficient) characterization of
the analytical forms for the critical points (as well as global minimizers) of
the square loss functions for various neural networks. We show that the
analytical forms of the critical points characterize the values of the
corresponding loss functions as well as the necessary and sufficient conditions
to achieve global minimum. Furthermore, we exploit the analytical forms of the
critical points to characterize the landscape properties for the loss functions
of these neural networks. One particular conclusion is that: The loss function
of linear networks has no spurious local minimum, while the loss function of
one-hidden-layer nonlinear networks with ReLU activation function does have
local minimum that is not global minimum.
",Computer Science; Statistics,Computer Science; Statistics
"Calderón-type inequalities for affine frames   We prove sharp upper and lower bounds for generalized Calderón's sums
associated to frames on LCA groups generated by affine actions of cocompact
subgroup translations and general measurable families of automorphisms. The
proof makes use of techniques of analysis on metric spaces, and relies on a
counting estimate of lattice points inside metric balls. We will deduce as
special cases Calderón-type inequalities for families of expanding
automorphisms as well as for LCA-Gabor systems.
",Mathematics,Mathematics
"Deep Learning: A Critical Appraisal   Although deep learning has historical roots going back decades, neither the
term ""deep learning"" nor the approach was popular just over five years ago,
when the field was reignited by papers such as Krizhevsky, Sutskever and
Hinton's now classic (2012) deep network model of Imagenet. What has the field
discovered in the five subsequent years? Against a background of considerable
progress in areas such as speech recognition, image recognition, and game
playing, and considerable enthusiasm in the popular press, I present ten
concerns for deep learning, and suggest that deep learning must be supplemented
by other techniques if we are to reach artificial general intelligence.
",Statistics,Computer Science
"On winning strategies for Banach-Mazur games   We give topological and game theoretic definitions and theorems nec- essary
for defining a Banach-Mazur game, and apply these definitions to formalize the
game. We then state and prove two theorems which give necessary conditions for
existence of winning strategies for players in a Banach-Mazur game.
",Mathematics,Computer Science
"The role of the background in past and future X-ray missions   Background has played an important role in X-ray missions, limiting the
exploitation of science data in several and sometimes unexpected ways. In this
presentation I review past X-ray missions focusing on some important lessons we
can learn from them. I then go on discussing prospects for overcoming
background related limitations in future ones.
",Physics,Physics
"A Cloud-based Service for Real-Time Performance Evaluation of NoSQL Databases   We have created a cloud-based service that allows the end users to run tests
on multiple different databases to find which databases are most suitable for
their project. From our research, we could not find another application that
enables the user to test several databases to gauge the difference between
them. This application allows the user to choose which type of test to perform
and which databases to target. The application also displays the results of
different tests that were run by other users previously. There is also a map to
show the location where all the tests are run to give the user an estimate of
the location. Unlike the orthodox static tests and reports conducted to
evaluate NoSQL databases, we have created a web application to run and analyze
these tests in real time. This web application evaluates the performance of
several NoSQL databases. The databases covered are MongoDB, DynamoDB, CouchDB,
and Firebase. The web service is accessible from: nosqldb.nextproject.ca.
",Computer Science,Computer Science
"Efficient Kinematic Planning for Mobile Manipulators with Non-holonomic Constraints Using Optimal Control   This work addresses the problem of kinematic trajectory planning for mobile
manipulators with non-holonomic constraints, and holonomic operational-space
tracking constraints. We obtain whole-body trajectories and time-varying
kinematic feedback controllers by solving a Constrained Sequential Linear
Quadratic Optimal Control problem. The employed algorithm features high
efficiency through a continuous-time formulation that benefits from adaptive
step-size integrators and through linear complexity in the number of
integration steps. In a first application example, we solve kinematic
trajectory planning problems for a 26 DoF wheeled robot. In a second example,
we apply Constrained SLQ to a real-world mobile manipulator in a
receding-horizon optimal control fashion, where we obtain optimal controllers
and plans at rates up to 100 Hz.
",Computer Science,Computer Science
"The placement of the head that maximizes predictability. An information theoretic approach   The minimization of the length of syntactic dependencies is a
well-established principle of word order and the basis of a mathematical theory
of word order. Here we complete that theory from the perspective of information
theory, adding a competing word order principle: the maximization of
predictability of a target element. These two principles are in conflict: to
maximize the predictability of the head, the head should appear last, which
maximizes the costs with respect to dependency length minimization. The
implications of such a broad theoretical framework to understand the
optimality, diversity and evolution of the six possible orderings of subject,
object and verb are reviewed.
",Computer Science; Physics,Computer Science
"On the Importance of Correlations in Rational Choice: A Case for Non-Nashian Game Theory   The Nash equilibrium paradigm, and Rational Choice Theory in general, rely on
agents acting independently from each other. This note shows how this
assumption is crucial in the definition of Rational Choice Theory. It explains
how a consistent Alternate Rational Choice Theory, as suggested by Jean-Pierre
Dupuy, can be built on the exact opposite assumption, and how it provides a
viable account for alternate, actually observed behavior of rational agents
that is based on correlations between their decisions.
The end goal of this note is three-fold: (i) to motivate that the Perfect
Prediction Equilibrium, implementing Dupuy's notion of projected time and
previously called ""projected equilibrium"", is a reasonable approach in certain
real situations and a meaningful complement to the Nash paradigm, (ii) to
summarize common misconceptions about this equilibrium, and (iii) to give a
concise motivation for future research on non-Nashian game theory.
",Computer Science,Computer Science
"PorePy: An Open-Source Simulation Tool for Flow and Transport in Deformable Fractured Rocks   Fractures are ubiquitous in the subsurface and strongly affect flow and
deformation. The physical shape of the fractures, they are long and thin
objects, puts strong limitations on how the effect of this dynamics can be
incorporated into standard reservoir simulation tools. This paper reports the
development of an open-source software framework, termed PorePy, which is aimed
at simulation of flow and transport in three-dimensional fractured reservoirs,
as well as deformation of the reservoir due to shearing along fracture and
fault planes. Starting from a description of fractures as polygons embedded in
a 3D domain, PorePy provides semi-automatic gridding to construct a
discrete-fracture-matrix model, which forms the basis for subsequent
simulations. PorePy allows for flow and transport in all lower-dimensional
objects, including planes (2D) representing fractures, and lines (1D) and
points (0D), representing fracture intersections. Interaction between processes
in neighboring domains of different dimension is implemented as a sequence of
couplings of objects one dimension apart. This readily allows for handling of
complex fracture geometries compared to capabilities of existing software. In
addition to flow and transport, PorePy provides models for rock mechanics,
poro-elasticity and coupling with fracture deformation models. The software is
fully open, and can serve as a framework for transparency and reproducibility
of simulations. We describe the design principles of PorePy from a user
perspective, with focus on possibilities within gridding, covered physical
processes and available discretizations. The power of the framework is
illustrated with two sets of simulations; involving respectively coupled flow
and transport in a fractured porous medium, and low-pressure stimulation of a
geothermal reservoir.
",Computer Science; Physics,Physics
"Multifractal Analysis of Pulsar Timing Residuals: Assessment of Gravitational Wave Detection   We introduce a pipeline including multifractal detrended cross-correlation
analysis (MF-DXA) modified by either singular value decomposition or the
adaptive method to examine the statistical properties of the pulsar timing
residual ($PTR$) induced by a gravitational wave (GW) signal. We propose a new
algorithm, the so-called irregular-MF-DXA, to deal with irregular data
sampling. Inspired by the quadrupolar nature of the spatial cross-correlation
function of a gravitational wave background, a new cross-correlation function,
$\bar{\sigma}_{\times}$, derived from irregular-MF-DXA will be introduced. We
show that, this measure reveals the quadrupolar signature in the $PTRs$ induced
by stochastic GWB. We propose four strategies based on the $y$-intercept of
fluctuation functions, the generalized Hurst exponent, and the width of the
singularity spectrum to determine the dimensionless amplitude and power-law
exponent of the characteristic strain spectrum as
$\mathcal{H}_c(f)\sim\mathcal{A}_{yr}(f/f_{yr})^{\zeta}$ for stochastic GWB.
Using the value of Hurst exponent, one can clarify the type of GWs. We apply
our pipeline to explore 20 millisecond pulsars observed by Parkes Pulsar Timing
Array. The computed scaling exponents confirm that all data are classified into
a nonstationary class implying the universality feature. The value of the Hurst
exponent is in the range $H\in [0.56,0.87]$. The $q$-dependency of the
generalized Hurst exponent demonstrates that the observed $PTRs$ have
multifractal behavior, and the source of this multifractality is mainly
attributed to the correlation of data which is another universality of the
observed datasets. Multifractal analysis of available $PTRs$ datasets reveals
an upper bound on the dimensionless amplitude of the GWB, $\mathcal{A}_{yr}<
2.0\times 10^{-15}$.
",Physics,Physics
"Interesting Paths in the Mapper   The Mapper produces a compact summary of high dimensional data as a
simplicial complex. We study the problem of quantifying the interestingness of
subpopulations in a Mapper, which appear as long paths, flares, or loops.
First, we create a weighted directed graph G using the 1-skeleton of the
Mapper. We use the average values at the vertices of a target function to
direct edges (from low to high). The difference between the average values at
vertices (high-low) is set as the edge's weight. Covariation of the remaining h
functions (independent variables) is captured by a h-bit binary signature
assigned to the edge. An interesting path in G is a directed path whose edges
all have the same signature. We define the interestingness score of such a path
as a sum of its edge weights multiplied by a nonlinear function of their ranks
in the path.
Second, we study three optimization problems on this graph G. In the problem
Max-IP, we seek an interesting path in G with the maximum interestingness
score. We show that Max-IP is NP-complete. For the special case when G is a
directed acyclic graph (DAG), we show that Max-IP can be solved in polynomial
time - in O(mnd_i) where d_i is the maximum indegree of a vertex in G.
In the more general problem IP, the goal is to find a collection of
edge-disjoint interesting paths such that the overall sum of their
interestingness scores is maximized. We also study a variant of IP termed k-IP,
where the goal is to identify a collection of edge-disjoint interesting paths
each with k edges, and their total interestingness score is maximized. While
k-IP can be solved in polynomial time for k <= 2, we show k-IP is NP-complete
for k >= 3 even when G is a DAG. We develop polynomial time heuristics for IP
and k-IP on DAGs.
",Computer Science; Mathematics,Computer Science
"Random taste heterogeneity in discrete choice models: Flexible nonparametric finite mixture distributions   This study proposes a mixed logit model with multivariate nonparametric
finite mixture distributions. The support of the distribution is specified as a
high-dimensional grid over the coefficient space, with equal or unequal
intervals between successive points along the same dimension; the location of
each point on the grid and the probability mass at that point are model
parameters that need to be estimated. The framework does not require the
analyst to specify the shape of the distribution prior to model estimation, but
can approximate any multivariate probability distribution function to any
arbitrary degree of accuracy. The grid with unequal intervals, in particular,
offers greater flexibility than existing multivariate nonparametric
specifications, while requiring the estimation of a small number of additional
parameters. An expectation maximization algorithm is developed for the
estimation of these models. Multiple synthetic datasets and a case study on
travel mode choice behavior are used to demonstrate the value of the model
framework and estimation algorithm. Compared to extant models that incorporate
random taste heterogeneity through continuous mixture distributions, the
proposed model provides better out-of-sample predictive ability. Findings
reveal significant differences in willingness to pay measures between the
proposed model and extant specifications. The case study further demonstrates
the ability of the proposed model to endogenously recover patterns of attribute
non-attendance and choice set formation.
",Statistics,Computer Science; Statistics
"Particular type of gap in the spectrum of multiband superconductors   We show, that in contrast to the free electron model (standard BCS model), a
particular gap in the spectrum of multiband superconductors opens at some
distance from the Fermi energy, if conduction band is composed of hybridized
atomic orbitals of different symmetries. This gap has composite
superconducting-hybridization origin, because it exists only if both the
superconductivity and the hybridization between different kinds of orbitals are
present. So for many classes of superconductors with multiorbital structure
such spectrum changes should take place. These particular changes in the
spectrum at some distance from the Fermi level result in slow convergence of
the spectral weight of the optical conductivity even in quite conventional
superconductors with isotropic s-wave pairing mechanism.
",Physics,Physics
"Characterising exo-ringsystems around fast-rotating stars using the Rossiter-McLaughlin effect   Planetary rings produce a distinct shape distortion in transit lightcurves.
However, to accurately model such lightcurves the observations need to cover
the entire transit, especially ingress and egress, as well as an out-of-transit
baseline. Such observations can be challenging for long period planets, where
the transits may last for over a day. Planetary rings will also impact the
shape of absorption lines in the stellar spectrum, as the planet and rings
cover different parts of the rotating star (the Rossiter-McLaughlin effect).
These line-profile distortions depend on the size, structure, opacity,
obliquity and sky projected angle of the ring system. For slow rotating stars,
this mainly impacts the amplitude of the induced velocity shift, however, for
fast rotating stars the large velocity gradient across the star allows the line
distortion to be resolved, enabling direct determination of the ring
parameters. We demonstrate that by modeling these distortions we can recover
ring system parameters (sky-projected angle, obliquity and size) using only a
small part of the transit. Substructure in the rings, e.g. gaps, can be
recovered if the width of the features ($\delta W$) relative to the size of the
star is similar to the intrinsic velocity resolution (set by the width of the
local stellar profile, $\gamma$) relative to the stellar rotation velocity ($v$
sin$i$, i.e. $\delta W / R_* \gtrsim v$sin$i$/$\gamma$). This opens up a new
way to study the ring systems around planets with long orbital periods, where
observations of the full transit, covering the ingress and egress, are not
always feasible.
",Physics,Physics
"On the semigroup rank of a group   For an arbitrary group $G$, it is shown that either the semigroup rank $G{\rm
rk}S$ equals the group rank $G{\rm rk}G$, or $G{\rm rk}S = G{\rm rk}G+1$. This
is the starting point for the rest of the article, where the semigroup rank for
diverse kinds of groups is analysed. The semigroup rank of relatively free
groups, for any variety of groups, is computed. For a finitely generated
abelian group~$G$, it is proven that $G{\rm rk}S = G{\rm rk}G+1$ if and only if
$G$ is torsion-free. In general, this is not true. Partial results are obtained
in the nilpotent case. It is also proven that if $M$ is a connected closed
surface, then $(\pi_1(M)){\rm rk}S = (\pi_1(M)){\rm rk}G+1$ if and only if $M$
is orientable.
",Mathematics,Mathematics
"A General Sequential Delay-Doppler Estimation Scheme for Sub-Nyquist Pulse-Doppler Radar   Sequential estimation of the delay and Doppler parameters for sub-Nyquist
radars by analog-to-information conversion (AIC) systems has received wide
attention recently. However, the estimation methods reported are AIC-dependent
and have poor performance for off-grid targets. This paper develops a general
estimation scheme in the sense that it is applicable to all AICs regardless
whether the targets are on or off the grids. The proposed scheme estimates the
delay and Doppler parameters sequentially, in which the delay estimation is
formulated into a beamspace direction-of- arrival problem and the Doppler
estimation is translated into a line spectrum estimation problem. Then the
well-known spatial and temporal spectrum estimation techniques are used to
provide efficient and high-resolution estimates of the delay and Doppler
parameters. In addition, sufficient conditions on the AIC to guarantee the
successful estimation of off-grid targets are provided, while the existing
conditions are mostly related to the on-grid targets. Theoretical analyses and
numerical experiments show the effectiveness and the correctness of the
proposed scheme.
",Computer Science; Mathematics,Computer Science
"Accelerating Prototype-Based Drug Discovery using Conditional Diversity Networks   Designing a new drug is a lengthy and expensive process. As the space of
potential molecules is very large (10^23-10^60), a common technique during drug
discovery is to start from a molecule which already has some of the desired
properties. An interdisciplinary team of scientists generates hypothesis about
the required changes to the prototype. In this work, we develop an algorithmic
unsupervised-approach that automatically generates potential drug molecules
given a prototype drug. We show that the molecules generated by the system are
valid molecules and significantly different from the prototype drug. Out of the
compounds generated by the system, we identified 35 FDA-approved drugs. As an
example, our system generated Isoniazid - one of the main drugs for
Tuberculosis. The system is currently being deployed for use in collaboration
with pharmaceutical companies to further analyze the additional generated
molecules.
",Statistics,Computer Science
"A Sparse Completely Positive Relaxation of the Modularity Maximization for Community Detection   In this paper, we consider the community detection problem under either the
stochastic block model (SBM) assumption or the degree-correlated stochastic
block model (DCSBM) assumption. The modularity maximization formulation for the
community detection problem is NP-hard in general. In this paper, we propose a
sparse and low-rank completely positive relaxation for the modularity
maximization problem, we then develop an efficient row-by-row (RBR) type block
coordinate descent (BCD) algorithm to solve the relaxation and prove an
$\mathcal{O}(1/\sqrt{N})$ convergence rate to a stationary point where $N$ is
the number of iterations. A fast rounding scheme is constructed to retrieve the
community structure from the solution. Non-asymptotic high probability bounds
on the misclassification rate are established to justify our approach. We
further develop an asynchronous parallel RBR algorithm to speed up the
convergence. Extensive numerical experiments on both synthetic and real world
networks show that the proposed approach enjoys advantages in both clustering
accuracy and numerical efficiency. Our numerical results indicate that the
newly proposed method is a quite competitive alternative for community
detection on sparse networks with over 50 million nodes.
",Mathematics,Computer Science; Mathematics
"Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic and Visualize Its Change Over Time   Trending topic of newspapers is an indicator to understand the situation of a
country and also a way to evaluate the particular newspaper. This paper
represents a model describing few techniques to select trending topics from
Bangla Newspaper. Topics that are discussed more frequently than other in
Bangla newspaper will be marked and how a very famous topic loses its
importance with the change of time and another topic takes its place will be
demonstrated. Data from two popular Bangla Newspaper with date and time were
collected. Statistical analysis was performed after on these data after
preprocessing. Popular and most used keywords were extracted from the stream of
Bangla keyword with this analysis. This model can also cluster category wise
news trend or a list of news trend in daily or weekly basis with enough data. A
pattern can be found on their news trend too. Comparison among past news trend
of Bangla newspapers will give a visualization of the situation of Bangladesh.
This visualization will be helpful to predict future trending topics of Bangla
Newspaper.
",Computer Science,Computer Science
"Replay spoofing detection system for automatic speaker verification using multi-task learning of noise classes   In this paper, we propose a replay attack spoofing detection system for
automatic speaker verification using multitask learning of noise classes. We
define the noise that is caused by the replay attack as replay noise. We
explore the effectiveness of training a deep neural network simultaneously for
replay attack spoofing detection and replay noise classification. The
multi-task learning includes classifying the noise of playback devices,
recording environments, and recording devices as well as the spoofing
detection. Each of the three types of the noise classes also includes a genuine
class. The experiment results on the ASVspoof2017 datasets demonstrate that the
performance of our proposed system is improved by 30% relatively on the
evaluation set.
",Computer Science; Statistics,Computer Science
"PHAST: Protein-like heteropolymer analysis by statistical thermodynamics   PHAST is a software package written in standard Fortran, with MPI and CUDA
extensions, able to efficiently perform parallel multicanonical Monte Carlo
simulations of single or multiple heteropolymeric chains, as coarse-grained
models for proteins. The outcome data can be straightforwardly analyzed within
its microcanonical Statistical Thermodynamics module, which allows for
computing the entropy, caloric curve, specific heat and free energies. As a
case study, we investigate the aggregation of heteropolymers bioinspired on
$A\beta_{25-33}$ fragments and their cross-seeding with $IAPP_{20-29}$
isoforms. Excellent parallel scaling is observed, even under numerically
difficult first-order like phase transitions, which are properly described by
the built-in fully reconfigurable force fields. Still, the package is free and
open source, this shall motivate users to readily adapt it to specific
purposes.
",Physics,Physics
"Recent implementations, applications, and extensions of the Locally Optimal Block Preconditioned Conjugate Gradient method (LOBPCG)   Since introduction [A. Knyazev, Toward the optimal preconditioned
eigensolver: Locally optimal block preconditioned conjugate gradient method,
SISC (2001) DOI:10.1137/S1064827500366124] and efficient parallel
implementation [A. Knyazev et al., Block locally optimal preconditioned
eigenvalue xolvers (BLOPEX) in HYPRE and PETSc, SISC (2007)
DOI:10.1137/060661624], LOBPCG has been used is a wide range of applications in
mechanics, material sciences, and data sciences. We review its recent
implementations and applications, as well as extensions of the local optimality
idea beyond standard eigenvalue problems.
",Computer Science; Statistics,Mathematics
"Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data   One of the defining properties of deep learning is that models are chosen to
have many more parameters than available training data. In light of this
capacity for overfitting, it is remarkable that simple algorithms like SGD
reliably return solutions with low test error. One roadblock to explaining
these phenomena in terms of implicit regularization, structural properties of
the solution, and/or easiness of the data is that many learning bounds are
quantitatively vacuous when applied to networks learned by SGD in this ""deep
learning"" regime. Logically, in order to explain generalization, we need
nonvacuous bounds. We return to an idea by Langford and Caruana (2001), who
used PAC-Bayes bounds to compute nonvacuous numerical bounds on generalization
error for stochastic two-layer two-hidden-unit neural networks via a
sensitivity analysis. By optimizing the PAC-Bayes bound directly, we are able
to extend their approach and obtain nonvacuous generalization bounds for deep
stochastic neural network classifiers with millions of parameters trained on
only tens of thousands of examples. We connect our findings to recent and old
work on flat minima and MDL-based explanations of generalization.
",Computer Science,Computer Science; Statistics
"Transition rates and radiative lifetimes of Ca I   We tabulate spontaneous emission rates for all possible 811
electric-dipole-allowed transitions between the 75 lowest-energy states of Ca
I. These involve the $4sns$ ($n=4-8$), $4snp$ ($n=4-7$), $4snd$ ($n=3-6$),
$4snf$ ($n=4-6$), $4p^2$, and $3d4p$ electronic configurations. We compile the
transition rates by carrying out ab initio relativistic calculations using the
combined method of configuration interaction and many-body perturbation theory.
The results are compared to the available literature values. The tabulated
rates can be useful in various applications, such as optimizing laser cooling
in magneto-optical traps, estimating various systematic effects in optical
clocks and evaluating static or dynamic polarizabilities and long-range
atom-atom interaction coefficients and related atomic properties.
",Physics,Physics
"Convergence of Stochastic Approximation Monte Carlo and modified Wang-Landau algorithms: Tests for the Ising model   We investigate the behavior of the deviation of the estimator for the density
of states (DOS) with respect to the exact solution in the course of Wang-Landau
and Stochastic Approximation Monte Carlo (SAMC) simulations of the
two-dimensional Ising model. We find that the deviation saturates in the
Wang-Landau case. This can be cured by adjusting the refinement scheme. To this
end, the 1/t-modification of the Wang-Landau algorithm has been suggested. A
similar choice of refinement scheme is employed in the SAMC algorithm. The
convergence behavior of all three algorithms is examined. It turns out that the
convergence of the SAMC algorithm is very sensitive to the onset of the
refinement. Finally, the internal energy and specific heat of the Ising model
are calculated from the SAMC DOS and compared to exact values.
",Physics,Mathematics; Statistics
"Nonlinear dynamics of polar regions in paraelectric phase of (Ba1-x,Srx)TiO3 ceramics   The dynamic dielectric nonlinearity of barium strontium titanate
(Ba1-x,Srx)TiO3 ceramics is investigated in their paraelectric phase. With the
goal to contribute to the identification of the mechanisms that govern the
dielectric nonlinearity in this family, we analyze the amplitude and the phase
angles of the first and the third harmonics of polarization. Our study shows
that an interpretation of the field-dependent polarization in paraelectric
(Ba1-x,Srx)TiO3 ceramics in terms of the Rayleigh-type dynamics is inadequate
for our samples and that their nonlinear response rather resembles that
observed in canonical relaxor Pb(Mg1/3Nb2/3)O3.
",Physics,Physics
"Investigating early-type galaxy evolution with a multiwavelength approach. II. The UV structure of 11 galaxies with Swift-UVOT   GALEX detected a significant fraction of early-type galaxies showing Far-UV
bright structures. These features suggest the occurrence of recent star
formation episodes. We aim at understanding their evolutionary path[s] and the
mechanisms at the origin of their UV-bright structures. We investigate with a
multi-lambda approach 11 early-types selected because of their nearly passive
stage of evolution in the nuclear region. The paper, second of a series,
focuses on the comparison between UV features detected by Swift-UVOT, tracing
recent star formation, and the galaxy optical structure mapping older stellar
populations. We performed their UV surface photometry and used BVRI photometry
from other sources. Our integrated magnitudes have been analyzed and compared
with corresponding values in the literature. We characterize the overall galaxy
structure best fitting the UV and optical luminosity profiles using a single
Sersic law. NGC 1366, NGC 1426, NGC 3818, NGC 3962 and NGC 7192 show
featureless luminosity profiles. Excluding NGC 1366 which has a clear edge-on
disk , n~1-2, and NGC 3818, the remaining three have Sersic's indices n~3-4 in
optical and a lower index in the UV. Bright ring/arm-like structures are
revealed by UV images and luminosity profiles of NGC 1415, NGC 1533, NGC 1543,
NGC 2685, NGC 2974 and IC 2006. The ring/arm-like structures are different from
galaxy to galaxy. Sersic indices of UV profiles for those galaxies are in the
range n=1.5-3 both in S0s and in Es. In our sample optical Sersic indices are
usually larger than the UV ones. (M2-V) color profiles are bluer in
ring/arm-like structures with respect to the galaxy body. The lower values of
Sersic's indices in the UV bands with respect to optical ones, suggesting the
presence of a disk, point out that the role of the dissipation cannot be
neglected in recent evolutionary phases of these early-type galaxies.
",Physics,Physics
"The Phenotypes of Fluctuating Flow: Development of Distribution Networks in Biology and the Trade-off between Efficiency, Cost, and Resilience   Complex distribution networks are pervasive in biology. Examples include
nutrient transport in the slime mold $Physarum$ $polycephalum$ as well as
mammalian and plant venation. Adaptive rules are believed to guide development
of these networks and lead to a reticulate, hierarchically nested topology that
is both efficient and resilient against perturbations. However, as of yet no
mechanism is known that can generate such networks on all scales. We show how
hierarchically organized reticulation can be generated and maintained through
spatially collective load fluctuations on a particular length scale. We
demonstrate that the resulting network topologies represent a trade-off between
optimizing power dissipation, construction cost, and damage robustness and
identify the Pareto-efficient front that evolution is expected to favor and
select for. We show that the typical fluctuation length scale controls the
position of the networks on the Pareto front and thus on the spectrum of
venation phenotypes. We compare the Pareto archetypes predicted by our model
with examples of real leaf networks.
",Physics,Physics
"Non-linear motor control by local learning in spiking neural networks   Learning weights in a spiking neural network with hidden neurons, using
local, stable and online rules, to control non-linear body dynamics is an open
problem. Here, we employ a supervised scheme, Feedback-based Online Local
Learning Of Weights (FOLLOW), to train a network of heterogeneous spiking
neurons with hidden layers, to control a two-link arm so as to reproduce a
desired state trajectory. The network first learns an inverse model of the
non-linear dynamics, i.e. from state trajectory as input to the network, it
learns to infer the continuous-time command that produced the trajectory.
Connection weights are adjusted via a local plasticity rule that involves
pre-synaptic firing and post-synaptic feedback of the error in the inferred
command. We choose a network architecture, termed differential feedforward,
that gives the lowest test error from different feedforward and recurrent
architectures. The learned inverse model is then used to generate a
continuous-time motor command to control the arm, given a desired trajectory.
",Computer Science; Statistics,Computer Science; Statistics
"Emotion Intensities in Tweets   This paper examines the task of detecting intensity of emotion from text. We
create the first datasets of tweets annotated for anger, fear, joy, and sadness
intensities. We use a technique called best--worst scaling (BWS) that improves
annotation consistency and obtains reliable fine-grained scores. We show that
emotion-word hashtags often impact emotion intensity, usually conveying a more
intense emotion. Finally, we create a benchmark regression system and conduct
experiments to determine: which features are useful for detecting emotion
intensity, and, the extent to which two emotions are similar in terms of how
they manifest in language.
",Computer Science,Computer Science
"Sample, computation vs storage tradeoffs for classification using tensor subspace models   In this paper, we exhibit the tradeoffs between the (training) sample,
computation and storage complexity for the problem of supervised classification
using signal subspace estimation. Our main tool is the use of tensor subspaces,
i.e. subspaces with a Kronecker structure, for embedding the data into lower
dimensions. Among the subspaces with a Kronecker structure, we show that using
subspaces with a hierarchical structure for representing data leads to improved
tradeoffs. One of the main reasons for the improvement is that embedding data
into these hierarchical Kronecker structured subspaces prevents overfitting at
higher latent dimensions.
",Computer Science; Statistics,Computer Science; Statistics
"RoboJam: A Musical Mixture Density Network for Collaborative Touchscreen Interaction   RoboJam is a machine-learning system for generating music that assists users
of a touchscreen music app by performing responses to their short
improvisations. This system uses a recurrent artificial neural network to
generate sequences of touchscreen interactions and absolute timings, rather
than high-level musical notes. To accomplish this, RoboJam's network uses a
mixture density layer to predict appropriate touch interaction locations in
space and time. In this paper, we describe the design and implementation of
RoboJam's network and how it has been integrated into a touchscreen music app.
A preliminary evaluation analyses the system in terms of training, musical
generation and user interaction.
",Computer Science,Computer Science
"In silico optimization of critical currents in superconductors   For many technological applications of superconductors the performance of a
material is determined by the highest current it can carry losslessly - the
critical current. In turn, the critical current can be controlled by adding
non-superconducting defects in the superconductor matrix. Here we report on
systematic comparison of different local and global optimization strategies to
predict optimal structures of pinning centers leading to the highest possible
critical currents. We demonstrate performance of these methods for a
superconductor with randomly placed spherical, elliptical, and columnar
defects.
",Physics,Physics
"System of unbiased representatives for a collection of bicolorings   Let $\mathcal{B}$ denote a set of bicolorings of $[n]$, where each bicoloring
is a mapping of the points in $[n]$ to $\{-1,+1\}$.
For each $B \in \mathcal{B}$, let $Y_B=(B(1),\ldots,B(n))$.
For each $A \subseteq [n]$, let $X_A \in \{0,1\}^n$ denote the incidence
vector of $A$.
A non-empty set $A$ is said to be an `unbiased representative' for a
bicoloring $B \in \mathcal{B}$ if $\left\langle X_A,Y_B\right\rangle =0$.
Given a set $\mathcal{B}$ of bicolorings, we study the minimum cardinality of
a family $\mathcal{A}$ consisting of subsets of $[n]$ such that every
bicoloring in $\mathcal{B}$ has an unbiased representative in $\mathcal{A}$.
",Computer Science,Mathematics
"Tangent: Automatic Differentiation Using Source Code Transformation in Python   Automatic differentiation (AD) is an essential primitive for machine learning
programming systems. Tangent is a new library that performs AD using source
code transformation (SCT) in Python. It takes numeric functions written in a
syntactic subset of Python and NumPy as input, and generates new Python
functions which calculate a derivative. This approach to automatic
differentiation is different from existing packages popular in machine
learning, such as TensorFlow and Autograd. Advantages are that Tangent
generates gradient code in Python which is readable by the user, easy to
understand and debug, and has no runtime overhead. Tangent also introduces
abstractions for easily injecting logic into the generated gradient code,
further improving usability.
",Computer Science; Statistics,Computer Science
"Why Abeta42 Is Much More Toxic Than Abeta40   Amyloid precursor with 770 amino acids dimerizes and aggregates, as do its c
terminal 99 amino acids and amyloid 40,42 amino acids fragments. The titled
question has been discussed extensively, and here it is addressed further using
thermodynamic scaling theory to analyze mutational trends in structural factors
and kinetics. Special attention is given to Family Alzheimer's Disease
mutations outside amyloid 42. The scaling analysis is connected to extensive
docking simulations which included membranes, thereby confirming their results
and extending them to Amyloid precursor.
",Quantitative Biology,Physics
"ACVAE-VC: Non-parallel many-to-many voice conversion with auxiliary classifier variational autoencoder   This paper proposes a non-parallel many-to-many voice conversion (VC) method
using a variant of the conditional variational autoencoder (VAE) called an
auxiliary classifier VAE (ACVAE). The proposed method has three key features.
First, it adopts fully convolutional architectures to construct the encoder and
decoder networks so that the networks can learn conversion rules that capture
time dependencies in the acoustic feature sequences of source and target
speech. Second, it uses an information-theoretic regularization for the model
training to ensure that the information in the attribute class label will not
be lost in the conversion process. With regular CVAEs, the encoder and decoder
are free to ignore the attribute class label input. This can be problematic
since in such a situation, the attribute class label will have little effect on
controlling the voice characteristics of input speech at test time. Such
situations can be avoided by introducing an auxiliary classifier and training
the encoder and decoder so that the attribute classes of the decoder outputs
are correctly predicted by the classifier. Third, it avoids producing
buzzy-sounding speech at test time by simply transplanting the spectral details
of the input speech into its converted version. Subjective evaluation
experiments revealed that this simple method worked reasonably well in a
non-parallel many-to-many speaker identity conversion task.
",Computer Science; Statistics,Computer Science
"Predicting Demographics of High-Resolution Geographies with Geotagged Tweets   In this paper, we consider the problem of predicting demographics of
geographic units given geotagged Tweets that are composed within these units.
Traditional survey methods that offer demographics estimates are usually
limited in terms of geographic resolution, geographic boundaries, and time
intervals. Thus, it would be highly useful to develop computational methods
that can complement traditional survey methods by offering demographics
estimates at finer geographic resolutions, with flexible geographic boundaries
(i.e. not confined to administrative boundaries), and at different time
intervals. While prior work has focused on predicting demographics and health
statistics at relatively coarse geographic resolutions such as the county-level
or state-level, we introduce an approach to predict demographics at finer
geographic resolutions such as the blockgroup-level. For the task of predicting
gender and race/ethnicity counts at the blockgroup-level, an approach adapted
from prior work to our problem achieves an average correlation of 0.389
(gender) and 0.569 (race) on a held-out test dataset. Our approach outperforms
this prior approach with an average correlation of 0.671 (gender) and 0.692
(race).
",Computer Science; Statistics,Computer Science; Statistics
"Basin stability for chimera states   Chimera states, namely complex spatiotemporal patterns that consist of
coexisting domains of spatially coherent and incoherent dynamics, are
investigated in a network of coupled identical oscillators. These intriguing
spatiotemporal patterns were first reported in nonlocally coupled phase
oscillators, and it was shown that such mixed type behavior occurs only for
specific initial conditions in nonlocally and globally coupled networks. The
influence of initial conditions on chimera states has remained a fundamental
problem since their discovery. In this report, we investigate the robustness of
chimera states together with incoherent and coherent states in dependence on
the initial conditions. For this, we use the basin stability method which is
related to the volume of the basin of attraction, and we consider nonlocally
and globally coupled time-delayed Mackey-Glass oscillators as example.
Previously, it was shown that the existence of chimera states can be
characterized by mean phase velocity and a statistical measure, such as the
strength of incoherence, by using well prepared initial conditions. Here we
show further how the coexistence of different dynamical states can be
identified and quantified by means of the basin stability measure over a wide
range of the parameter space.
",Physics,Physics
"Sparse geometries handling in lattice-Boltzmann method implementation for graphic processors   We describe a high-performance implementation of the lattice-Boltzmann method
(LBM) for sparse geometries on graphic processors. In our implementation we
cover the whole geometry with a uniform mesh of small tiles and carry out
calculations for each tile independently with a proper data synchronization at
tile edges. For this method we provide both the theoretical analysis of
complexity and the results for real implementations for 2D and 3D geometries.
Based on the theoretical model, we show that tiles offer significantly smaller
bandwidth overhead than solutions based on indirect addressing. For
2-dimensional lattice arrangements a reduction of memory usage is also
possible, though at the cost of diminished performance. We reached the
performance of 682 MLUPS on GTX Titan (72\% of peak theoretical memory
bandwidth) for D3Q19 lattice arrangement and double precision data.
",Computer Science,Computer Science
"Improving Massive MIMO Belief Propagation Detector with Deep Neural Network   In this paper, deep neural network (DNN) is utilized to improve the belief
propagation (BP) detection for massive multiple-input multiple-output (MIMO)
systems. A neural network architecture suitable for detection task is firstly
introduced by unfolding BP algorithms. DNN MIMO detectors are then proposed
based on two modified BP detectors, damped BP and max-sum BP. The correction
factors in these algorithms are optimized through deep learning techniques,
aiming at improved detection performance. Numerical results are presented to
demonstrate the performance of the DNN detectors in comparison with various BP
modifications. The neural network is trained once and can be used for multiple
online detections. The results show that, compared to other state-of-the-art
detectors, the DNN detectors can achieve lower bit error rate (BER) with
improved robustness against various antenna configurations and channel
conditions at the same level of complexity.
",Statistics,Computer Science
"Thresholds for hanger slackening and cable shortening in the Melan equation for suspension bridges   The Melan equation for suspension bridges is derived by assuming small
displacements of the deck and inextensible hangers. We determine the thresholds
for the validity of the Melan equation when the hangers slacken, thereby
violating the inextensibility assumption. To this end, we preliminarily study
the possible shortening of the cables: it turns out that there is a striking
difference between even and odd vibrating modes since the former never shorten
the cables. These problems are studied both on beams and plates.
",Physics; Mathematics,Physics
"Poisson brackets with prescribed family of functions in involution   It is well known that functions in involution with respect to Poisson
brackets have a privileged role in the theory of completely integrable systems.
Finding functionally independent functions in involution with a given function
$h$ on a Poisson manifold is a fundamental problem of this theory and is very
useful for the explicit integration of the equations of motion defined by $h$.
In this paper, we present our results on the study of the inverse, so to speak,
problem. By developing a technique analogous to that presented in P. Damianou
and F. Petalidou, Poisson brackets with prescribed Casimirs, Canad. J. Math.,
2012, vol. 64, 991-1018, for the establishment of Poisson brackets with
prescribed Casimir invariants, we construct an algorithm which yields Poisson
brackets having a given family of functions in involution. Our approach allows
us to deal with bi-Hamiltonian structures constructively and therefore allows
us to also deal with the completely integrable systems that arise in such a
framework.
",Mathematics,Mathematics
"Independence in generic incidence structures   We study the theory $T_{m,n}$ of existentially closed incidence structures
omitting the complete incidence structure $K_{m,n}$, which can also be viewed
as existentially closed $K_{m,n}$-free bipartite graphs. In the case $m = n =
2$, this is the theory of existentially closed projective planes. We give an
$\forall\exists$-axiomatization of $T_{m,n}$, show that $T_{m,n}$ does not have
a countable saturated model when $m,n\geq 2$, and show that the existence of a
prime model for $T_{2,2}$ is equivalent to a longstanding open question about
finite projective planes. Finally, we analyze model theoretic notions of
complexity for $T_{m,n}$. We show that $T_{m,n}$ is NSOP$_1$, but not simple
when $m,n\geq 2$, and we show that $T_{m,n}$ has weak elimination of
imaginaries but not full elimination of imaginaries. These results rely on
combinatorial characterizations of various notions of independence, including
algebraic independence, Kim independence, and forking independence.
",Mathematics,Mathematics
"Using MRI Cell Tracking to Monitor Immune Cell Recruitment in Response to a Peptide-Based Cancer Vaccine   Purpose: MRI cell tracking can be used to monitor immune cells involved in
the immunotherapy response, providing insight into the mechanism of action,
temporal progression of tumour growth and individual potency of therapies. To
evaluate whether MRI could be used to track immune cell populations in response
to immunotherapy, CD8+ cytotoxic T cells (CTLs), CD4+CD25+FoxP3+ regulatory T
cells (Tregs) and myeloid derived suppressor cells (MDSCs) were labelled with
superparamagnetic iron oxide (SPIO) particles.
Methods: SPIO-labelled cells were injected into mice (one cell type/mouse)
implanted with an HPV-based cervical cancer model. Half of these mice were also
vaccinated with DepoVaxTM, a lipid-based vaccine platform that was developed to
enhance the potency of peptide-based vaccines.
Results: MRI visualization of CTLs, Tregs and MDSCs was apparent 24 hours
post-injection, with hypointensities due to iron labelled cells clearing
approximately 72 hours post-injection. Vaccination resulted in increased
recruitment of CTLs and decreased recruitment of MDSCs and Tregs to the tumour.
We also found that MDSC and Treg recruitment was positively correlated with
final tumour volume.
Conclusion: This type of analysis can be used to non-invasively study changes
in immune cell recruitment in individual mice over time, potentially allowing
improved application and combination of immunotherapies.
",Physics,Quantitative Biology
"Towards Detection of Exoplanetary Rings Via Transit Photometry: Methodology and a Possible Candidate   Detection of a planetary ring of exoplanets remains as one of the most
attractive but challenging goals in the field. We present a methodology of a
systematic search for exoplanetary rings via transit photometry of long-period
planets. The methodology relies on a precise integration scheme we develop to
compute a transit light curve of a ringed planet. We apply the methodology to
89 long-period planet candidates from the Kepler data so as to estimate, and/or
set upper limits on, the parameters of possible rings. While a majority of our
samples do not have a sufficiently good signal-to-noise ratio for meaningful
constraints on ring parameters, we find that six systems with a higher
signal-to-noise ratio are inconsistent with the presence of a ring larger than
1.5 times the planetary radius assuming a grazing orbit and a tilted ring.
Furthermore, we identify five preliminary candidate systems whose light curves
exhibit ring-like features. After removing four false positives due to the
contamination from nearby stars, we identify KIC 10403228 as a reasonable
candidate for a ringed planet. A systematic parameter fit of its light curve
with a ringed planet model indicates two possible solutions corresponding to a
Saturn-like planet with a tilted ring. There also remain other two possible
scenarios accounting for the data; a circumstellar disk and a hierarchical
triple. Due to large uncertain factors, we cannot choose one specific model
among the three.
",Physics,Physics
"Thermoregulation in mice, rats and humans: An insight into the evolution of human hairlessness   The thermoregulation system in animals removes body heat in hot temperatures
and retains body heat in cold temperatures. The better the animal removes heat,
the worse the animal retains heat and visa versa. It is the balance between
these two conflicting goals that determines the mammal's size, heart rate and
amount of hair. The rat's loss of tail hair and human's loss of its body hair
are responses to these conflicting thermoregulation needs as these animals
evolved to larger size over time.
",Quantitative Biology,Quantitative Biology
"Gamma-Ray Emission from Arp 220: Indications of an Active Galactic Nucleus   Extragalactic cosmic ray populations are important diagnostic tools for
tracking the distribution of energy in nuclei and for distinguishing between
activity powered by star formation versus active galactic nuclei (AGNs). Here,
we compare different diagnostics of the cosmic ray populations of the nuclei of
Arp 220 based on radio synchrotron observations and the recent gamma-ray
detection. We find the gamma-ray and radio emission to be incompatible; a joint
solution requires at minimum a factor of 4 - 8 times more energy coming from
supernovae and a factor of 40 - 70 more mass in molecular gas than is observed.
We conclude that this excess of gamma-ray flux in comparison to all other
diagnostics of star-forming activity indicates that there is an AGN present
that is providing the extra cosmic rays, likely in the western nucleus.
",Physics,Physics
"Results from the first cryogenic NaI detector for the COSINUS project   Recently there is a flourishing and notable interest in the crystalline
scintillator material sodium iodide (NaI) as target for direct dark matter
searches. This is mainly driven by the long-reigning contradicting situation in
the dark matter sector: the positive evidence for the detection of a dark
matter modulation signal claimed by the DAMA/LIBRA collaboration is (under
so-called standard assumptions) inconsistent with the null-results reported by
most of the other direct dark matter experiments. We present the results of a
first prototype detector using a new experimental approach in comparison to
\textit{conventional} single-channel NaI scintillation light detectors: a NaI
crystal operated as a scintillating calorimeter at milli-Kelvin temperatures
simultaneously providing a phonon (heat) plus scintillation light signal and
particle discrimination on an event-by-event basis. We evaluate energy
resolution, energy threshold and further performance parameters of this
prototype detector developed within the COSINUS R&D project.
",Physics,Physics
"Asymptotic Analysis of Plausible Tree Hash Modes for SHA-3   Discussions about the choice of a tree hash mode of operation for a
standardization have recently been undertaken. It appears that a single tree
mode cannot address adequately all possible uses and specifications of a
system. In this paper, we review the tree modes which have been proposed, we
discuss their problems and propose remedies. We make the reasonable assumption
that communicating systems have different specifications and that software
applications are of different types (securing stored content or live-streamed
content). Finally, we propose new modes of operation that address the resource
usage problem for the three most representative categories of devices and we
analyse their asymptotic behavior.
",Computer Science,Computer Science
"Exact lowest-Landau-level solutions for vortex precession in Bose-Einstein condensates   The Lowest Landau Level (LLL) equation emerges as an accurate approximation
for a class of dynamical regimes of Bose-Einstein Condensates (BEC) in
two-dimensional isotropic harmonic traps in the limit of weak interactions.
Building on recent developments in the field of spatially confined extended
Hamiltonian systems, we find a fully nonlinear solution of this equation
representing periodically modulated precession of a single vortex. Motions of
this type have been previously seen in numerical simulations and experiments at
moderately weak coupling. Our work provides the first controlled analytic
prediction for trajectories of a single vortex, suggests new targets for
experiments, and opens up the prospect of finding analytic multi-vortex
solutions.
",Physics; Mathematics,Physics
"Measure-geometric Laplacians for discrete distributions   In 2002 Freiberg and Zähle introduced and developed a harmonic calculus for
measure-geometric Laplacians associated to continuous distributions. We show
their theory can be extended to encompass distributions with finite support and
give a matrix representation for the resulting operators. In the case of a
uniform discrete distribution we make use of this matrix representation to
explicitly determine the eigenvalues and the eigenfunctions of the associated
Laplacian.
",Mathematics,Mathematics; Statistics
"Production of 82Se enriched Zinc Selenide (ZnSe) crystals for the study of neutrinoless double beta decay   High purity Zinc Selenide (ZnSe) crystals are produced starting from
elemental Zn and Se to be used for the search of the neutrinoless double beta
decay (0{\nu}DBD) of 82Se. In order to increase the number of emitting
nuclides, enriched 82Se is used. Dedicated production lines for the synthesis
and conditioning of the Zn82Se powder in order to make it suitable for crystal
growth were assembled compliant with radio-purity constraints specific to rare
event physics experiments. Besides routine check of impurities concentration,
high sensitivity measurements are made for radio-isotope concentrations in raw
materials, reactants, consumables, ancillaries and intermediary products used
for ZnSe crystals production. Indications are given on the crystals perfection
and how it is achieved. Since very expensive isotopically enriched material
(82Se) is used, a special attention is given for acquiring the maximum yield in
the mass balance of all production stages. Production and certification
protocols are presented and resulting ready-to-use Zn82Se crystals are
described.
",Physics,Physics
"Differential Forms, Linked Fields and the $u$-Invariant   We associate an Albert form to any pair of cyclic algebras of prime degree
$p$ over a field $F$ with $\operatorname{char}(F)=p$ which coincides with the
classical Albert form when $p=2$. We prove that if every Albert form is
isotropic then $H^4(F)=0$. As a result, we obtain that if $F$ is a linked field
with $\operatorname{char}(F)=2$ then its $u$-invariant is either $0,2,4$ or
$8$.
",Mathematics,Mathematics
"Infrared Flares from M Dwarfs: a Hinderance to Future Transiting Exoplanet Studies   Many current and future exoplanet missions are pushing to infrared (IR)
wavelengths where the flux contrast between the planet and star is more
favorable (Deming et al. 2009), and the impact of stellar magnetic activity is
decreased. Indeed, a recent analysis of starspots and faculae found these forms
of stellar activity do not substantially impact the transit signatures or
science potential for FGKM stars with JWST (Zellem et al. 2017). However, this
is not true in the case of flares, which I demonstrate can be a hinderance to
transit studies in this note.
",Physics,Physics
"Self-organization and the Maximum Empower Principle in the Framework of max-plus Algebra   Self-organization is a process where order of a whole system arises out of
local interactions between small components of a system.
Emergy, defined as the amount of (solar) energy used to make a product or a
service, is becoming an important ecological indicator. To explain observed
self-organization of systems by emergy the Maximum Empower Principle (MEP) was
proposed initially without a mathematical formulation.
Emergy analysis is based on four rules called emergy algebra. Most of emergy
computations in steady state are in fact approximate results, which rely on
linear algebra. In such a context, a mathematical formulation of the MEP has
been proposed by Giannantoni (2002).
In 2012 Le Corre and the second author of this paper have proposed a rigorous
mathematical framework for emergy analysis. They established that the exact
computation of emergy is based on the so-called max-plus algebra and seven
coherent axioms that replace the emergy algebra. In this paper the MEP in
steady state is formalized in the context of the max-plus algebra and graph
theory. The main concepts of the paper are (a) a particular graph called
'emergy graph', (b) the notion of compatible paths of the emergy graph, and (c)
sets of compatible paths, which are called 'emergy states'. The main results of
the paper are as follows:
(1) Emergy is mathematically expressed as a maximum over all possible emergy
states. (2) The maximum is always reached by an emergy state. (3) Only prevail
emergy states for which the maximum is reached.
",Computer Science; Physics,Mathematics
"Adaptive Inferential Method for Monotone Graph Invariants   We consider the problem of undirected graphical model inference. In many
applications, instead of perfectly recovering the unknown graph structure, a
more realistic goal is to infer some graph invariants (e.g., the maximum
degree, the number of connected subgraphs, the number of isolated nodes). In
this paper, we propose a new inferential framework for testing nested multiple
hypotheses and constructing confidence intervals of the unknown graph
invariants under undirected graphical models. Compared to perfect graph
recovery, our methods require significantly weaker conditions. This paper makes
two major contributions: (i) Methodologically, for testing nested multiple
hypotheses, we propose a skip-down algorithm on the whole family of monotone
graph invariants (The invariants which are non-decreasing under addition of
edges). We further show that the same skip-down algorithm also provides valid
confidence intervals for the targeted graph invariants. (ii) Theoretically, we
prove that the length of the obtained confidence intervals are optimal and
adaptive to the unknown signal strength. We also prove generic lower bounds for
the confidence interval length for various invariants. Numerical results on
both synthetic simulations and a brain imaging dataset are provided to
illustrate the usefulness of the proposed method.
",Mathematics; Statistics,Computer Science; Statistics
"Parameter Estimation in Mean Reversion Processes with Periodic Functional Tendency   This paper describes the procedure to estimate the parameters in mean
reversion processes with functional tendency defined by a periodic continuous
deterministic function, expressed as a series of truncated Fourier. Two phases
of estimation are defined, in the first phase through Gaussian techniques using
the Euler-Maruyama discretization, we obtain the maximum likelihood function,
that will allow us to find estimators of the external parameters and an
estimation of the expected value of the process. In the second phase, a
reestimate of the periodic functional tendency with it's parameters of phase
and amplitude is carried out, this will allow, improve the initial estimation.
Some experimental result using simulated data sets are graphically illustrated.
",Statistics,Mathematics; Statistics
"Statistical Analysis of Precipitation Events   In the present paper we demonstrate the results of a statistical analysis of
some characteristics of precipitation events and propose a kind of a
theoretical explanation of the proposed models in terms of mixed Poisson and
mixed exponential distributions based on the information-theoretical entropy
reasoning. The proposed models can be also treated as the result of following
the popular Bayesian approach.
",Statistics,Mathematics; Statistics
"Some Repeated-Root Constacyclic Codes over Galois Rings   Codes over Galois rings have been studied extensively during the last three
decades. Negacyclic codes over $GR(2^a,m)$ of length $2^s$ have been
characterized: the ring $\mathcal{R}_2(a,m,-1)= \frac{GR(2^a,m)[x]}{\langle
x^{2^s}+1\rangle}$ is a chain ring. Furthermore, these results have been
generalized to $\lambda$-constacyclic codes for any unit $\lambda$ of the form
$4z-1$, $z\in GR(2^a, m)$. In this paper, we study more general cases and
investigate all cases where $\mathcal{R}_p(a,m,\gamma)=
\frac{GR(p^a,m)[x]}{\langle x^{p^s}-\gamma \rangle}$ is a chain ring. In
particular, necessary and sufficient conditions for the ring
$\mathcal{R}_p(a,m,\gamma)$ to be a chain ring are obtained. In addition, by
using this structure we investigate all $\gamma$-constacyclic codes over
$GR(p^a,m)$ when $\mathcal{R}_p(a,m,\gamma)$ is a chain ring. Necessary and
sufficient conditions for the existence of self-orthogonal and self-dual
$\gamma$-constacyclic codes are also provided. Among others, for any prime $p$,
the structure of $\mathcal{R}_p(a,m,\gamma)=\frac{GR(p^a,m)[x]}{\langle
x^{p^s}-\gamma\rangle}$ is used to establish the Hamming and homogeneous
distances of $\gamma$-constacyclic codes.
",Computer Science; Mathematics,Mathematics
"Prediction of half-metallic properties in TlCrS2 and TlCrSe2 based on density functional theory   Half-metallic properties of TlCrS2, TlCrSe2 and hypothetical TlCrSSe have
been investigated by first-principles all-electron full-potential linearized
augmented plane wave plus local orbital (FP-LAPW+lo) method based on density
functional theory (DFT). The results of calculations show that TlCrS2 and
TlCrSSe are half-metals with energy gap (Eg ) ~0.12 ev for spin-down channel.
Strong hybridization of p-state of chalchogen and d-state of Cr leads to
bonding and antibonding states and subsequently to the appearance of a gap in
spin-down channel of TlCrS2 and TlCrSSe. In the case of TlCrSe2, there is a
partial hybridization and p-state is partially present in the DOS at Fermi
level making this compound nearly half- metallic. The present calculations
revealed that total magnetic moment keeps its integer value on a relatively
wide range of changes in volume (-10% 10%) for TlCrS2 and TlCrSSe, while total
magnetic moment of TlCrSe2 decreases with increasing volume approaching to
integer value 3{\mu}B.
",Physics,Physics
"The Informativeness of $k$-Means and Dimensionality Reduction for Learning Mixture Models   The learning of mixture models can be viewed as a clustering problem. Indeed,
given data samples independently generated from a mixture of distributions, we
often would like to find the correct target clustering of the samples according
to which component distribution they were generated from. For a clustering
problem, practitioners often choose to use the simple k-means algorithm.
k-means attempts to find an optimal clustering which minimizes the
sum-of-squared distance between each point and its cluster center. In this
paper, we provide sufficient conditions for the closeness of any optimal
clustering and the correct target clustering assuming that the data samples are
generated from a mixture of log-concave distributions. Moreover, we show that
under similar or even weaker conditions on the mixture model, any optimal
clustering for the samples with reduced dimensionality is also close to the
correct target clustering. These results provide intuition for the
informativeness of k-means (with and without dimensionality reduction) as an
algorithm for learning mixture models. We verify the correctness of our
theorems using numerical experiments and demonstrate using datasets with
reduced dimensionality significant speed ups for the time required to perform
clustering.
",Computer Science; Statistics,Computer Science; Statistics
"Minimal surfaces and Schwarz lemma   We prove a sharp Schwarz type inequality for the Weierstrass-Enneper
representation of the minimal surfaces.
",Mathematics,Mathematics
"Odd holes in bull-free graphs   The complexity of testing whether a graph contains an induced odd cycle of
length at least five is currently unknown. In this paper we show that this can
be done in polynomial time if the input graph has no induced subgraph
isomorphic to the bull (a triangle with two disjoint pendant edges).
",Computer Science,Mathematics
"Isomonodromy aspects of the tt* equations of Cecotti and Vafa III. Iwasawa factorization and asymptotics   This paper, the third in a series, completes our description of all (radial)
solutions on C* of the tt*-Toda equations, using a combination of methods from
p.d.e., isomonodromic deformations (Riemann-Hilbert method), and loop groups.
We place these global solutions into the broader context of solutions which are
smooth near 0. For such solutions, we compute explicitly the Stokes data and
connection matrix of the associated meromorphic system, in the resonant cases
as well as the non-resonant case. This allows us to give a complete picture of
the monodromy data of the global solutions.
",Mathematics,Mathematics
"Segmentation of Instances by Hashing   We propose a novel approach to address the Simultaneous Detection and
Segmentation problem. Using hierarchical structures we use an efficient and
accurate procedure that exploits the hierarchy feature information using
Locality Sensitive Hashing. We build on recent work that utilizes convolutional
neural networks to detect bounding boxes in an image and then use the top
similar hierarchical region that best fits each bounding box after hashing, we
call this approach CZ Segmentation. We then refine our final segmentation
results by automatic hierarchy pruning. CZ Segmentation introduces a train-free
alternative to Hypercolumns. We conduct extensive experiments on PASCAL VOC
2012 segmentation dataset, showing that CZ gives competitive state-of-the-art
object segmentations.
",Computer Science,Computer Science
"Learning to Acquire Information   We consider the problem of diagnosis where a set of simple observations are
used to infer a potentially complex hidden hypothesis. Finding the optimal
subset of observations is intractable in general, thus we focus on the problem
of active diagnosis, where the agent selects the next most-informative
observation based on the results of previous observations. We show that under
the assumption of uniform observation entropy, one can build an implication
model which directly predicts the outcome of the potential next observation
conditioned on the results of past observations, and selects the observation
with the maximum entropy. This approach enjoys reduced computation complexity
by bypassing the complicated hypothesis space, and can be trained on
observation data alone, learning how to query without knowledge of the hidden
hypothesis.
",Computer Science; Statistics,Computer Science; Statistics
"Adaptive Quantization for Deep Neural Network   In recent years Deep Neural Networks (DNNs) have been rapidly developed in
various applications, together with increasingly complex architectures. The
performance gain of these DNNs generally comes with high computational costs
and large memory consumption, which may not be affordable for mobile platforms.
Deep model quantization can be used for reducing the computation and memory
costs of DNNs, and deploying complex DNNs on mobile equipment. In this work, we
propose an optimization framework for deep model quantization. First, we
propose a measurement to estimate the effect of parameter quantization errors
in individual layers on the overall model prediction accuracy. Then, we propose
an optimization process based on this measurement for finding optimal
quantization bit-width for each layer. This is the first work that
theoretically analyse the relationship between parameter quantization errors of
individual layers and model accuracy. Our new quantization algorithm
outperforms previous quantization optimization methods, and achieves 20-40%
higher compression rate compared to equal bit-width quantization at the same
model prediction accuracy.
",Computer Science; Statistics,Computer Science
"On the isoperimetric quotient over scalar-flat conformal classes   Let $(M,g)$ be a smooth compact Riemannian manifold of dimension $n$ with
smooth boundary $\partial M$. Suppose that $(M,g)$ admits a scalar-flat
conformal metric. We prove that the supremum of the isoperimetric quotient over
the scalar-flat conformal class is strictly larger than the best constant of
the isoperimetric inequality in the Euclidean space, and consequently is
achieved, if either (i) $n\ge 12$ and $\partial M$ has a nonumbilic point; or
(ii) $n\ge 10$, $\partial M$ is umbilic and the Weyl tensor does not vanish at
some boundary point.
",Mathematics,Mathematics
"25 Tweets to Know You: A New Model to Predict Personality with Social Media   Predicting personality is essential for social applications supporting
human-centered activities, yet prior modeling methods with users written text
require too much input data to be realistically used in the context of social
media. In this work, we aim to drastically reduce the data requirement for
personality modeling and develop a model that is applicable to most users on
Twitter. Our model integrates Word Embedding features with Gaussian Processes
regression. Based on the evaluation of over 1.3K users on Twitter, we find that
our model achieves comparable or better accuracy than state of the art
techniques with 8 times fewer data.
",Computer Science,Computer Science
"Wright-Fisher diffusions for evolutionary games with death-birth updating   We investigate spatial evolutionary games with death-birth updating in large
finite populations. Within growing spatial structures subject to appropriate
conditions, the density processes of a fixed type are proven to converge to the
Wright-Fisher diffusions with drift. In addition, convergence in the
Wasserstein distance of the laws of their occupation measures holds. The proofs
of these results develop along an equivalence between the laws of the
evolutionary games and certain voter models and rely on the analogous results
of voter models on large finite sets by convergences of the Radon-Nikodym
derivative processes. As another application of this equivalence of laws, we
show that in a general, large population of size $N$, for which the stationary
probabilities of the corresponding voting kernel are comparable to uniform
probabilities, a first-derivative test among the major methods for these
evolutionary games is applicable at least up to weak selection strengths in the
usual biological sense (that is, selection strengths of the order $\mathcal
O(1/N)$).
",Mathematics,Physics; Mathematics
"Prediction of Sea Surface Temperature using Long Short-Term Memory   This letter adopts long short-term memory(LSTM) to predict sea surface
temperature(SST), which is the first attempt, to our knowledge, to use
recurrent neural network to solve the problem of SST prediction, and to make
one week and one month daily prediction. We formulate the SST prediction
problem as a time series regression problem. LSTM is a special kind of
recurrent neural network, which introduces gate mechanism into vanilla RNN to
prevent the vanished or exploding gradient problem. It has strong ability to
model the temporal relationship of time series data and can handle the
long-term dependency problem well. The proposed network architecture is
composed of two kinds of layers: LSTM layer and full-connected dense layer.
LSTM layer is utilized to model the time series relationship. Full-connected
layer is utilized to map the output of LSTM layer to a final prediction. We
explore the optimal setting of this architecture by experiments and report the
accuracy of coastal seas of China to confirm the effectiveness of the proposed
method. In addition, we also show its online updated characteristics.
",Computer Science,Computer Science; Statistics
"Open problems in mathematical physics   We present a list of open questions in mathematical physics. After a
historical introduction, a number of problems in a variety of different fields
are discussed, with the intention of giving an overall impression of the
current status of mathematical physics, particularly in the topical fields of
classical general relativity, cosmology and the quantum realm. This list is
motivated by the recent article proposing 42 fundamental questions (in physics)
which must be answered on the road to full enlightenment. But paraphrasing a
famous quote by the British football manager Bill Shankly, in response to the
question of whether mathematics can answer the Ultimate Question of Life, the
Universe, and Everything, mathematics is, of course, much more important than
that.
",Physics; Mathematics,Physics
"Univalent Foundations and the UniMath Library   We give a concise presentation of the Univalent Foundations of mathematics
outlining the main ideas, followed by a discussion of the UniMath library of
formalized mathematics implementing the ideas of the Univalent Foundations
(section 1), and the challenges one faces in attempting to design a large-scale
library of formalized mathematics (section 2). This leads us to a general
discussion about the links between architecture and mathematics where a meeting
of minds is revealed between architects and mathematicians (section 3). On the
way our odyssey from the foundations to the ""horizon"" of mathematics will lead
us to meet the mathematicians David Hilbert and Nicolas Bourbaki as well as the
architect Christopher Alexander.
",Computer Science; Mathematics,Mathematics
"Radar, without tears   A brief introduction to radar: principles, Doppler effect, antennas,
waveforms, power budget - and future radars. [13 pages]
",Physics,Physics
"Second-Order Optimization for Non-Convex Machine Learning: An Empirical Study   While first-order optimization methods such as stochastic gradient descent
(SGD) are popular in machine learning (ML), they come with well-known
deficiencies, including relatively-slow convergence, sensitivity to the
settings of hyper-parameters such as learning rate, stagnation at high training
errors, and difficulty in escaping flat regions and saddle points. These issues
are particularly acute in highly non-convex settings such as those arising in
neural networks. Motivated by this, there has been recent interest in
second-order methods that aim to alleviate these shortcomings by capturing
curvature information. In this paper, we report detailed empirical evaluations
of a class of Newton-type methods, namely sub-sampled variants of trust region
(TR) and adaptive regularization with cubics (ARC) algorithms, for non-convex
ML problems. In doing so, we demonstrate that these methods not only can be
computationally competitive with hand-tuned SGD with momentum, obtaining
comparable or better generalization performance, but also they are highly
robust to hyper-parameter settings. Further, in contrast to SGD with momentum,
we show that the manner in which these Newton-type methods employ curvature
information allows them to seamlessly escape flat regions and saddle points.
",Computer Science; Statistics,Computer Science; Statistics
"Proactive Edge Computing in Latency-Constrained Fog Networks   In this paper, the fundamental problem of distribution and proactive caching
of computing tasks in fog networks is studied under latency and reliability
constraints. In the proposed scenario, computing can be executed either locally
at the user device or offloaded to an edge cloudlet. Moreover, cloudlets
exploit both their computing and storage capabilities by proactively caching
popular task computation results to minimize computing latency. To this end, a
clustering method to group spatially proximate user devices with mutual task
popularity interests and their serving cloudlets is proposed. Then, cloudlets
can proactively cache the popular tasks' computations of their cluster members
to minimize computing latency. Additionally, the problem of distributing tasks
to cloudlets is formulated as a matching game in which a cost function of
computing delay is minimized under latency and reliability constraints.
Simulation results show that the proposed scheme guarantees reliable
computations with bounded latency and achieves up to 91% decrease in computing
latency as compared to baseline schemes.
",Computer Science,Computer Science
